{"prompt": "  We study the radiation from charged particles crossing a cholesteric plate in\nthe shortwave approximation when the wavelength of photons is much smaller than\nthe pitch of the cholesteric helix whereas the escaping angle of the photon and\nthe anisotropy of the permittivity tensor can be arbitrary. The radiation of\nphotons is treated in the framework of quantum electrodynamics with classical\ncurrents. The radiation of the plane-wave photons and the photons with definite\nprojection of the angular momentum (the twisted photons) produced by charged\nparticles crossing the cholesteric plate and moving rectilinearly and uniformly\nis considered. The explicit expressions for the average number of radiated\nphotons and their spectra with respect to the energy and the projection of the\nangular momentum are obtained in this case. It turns out that in the paraxial\napproximation the projection of the orbital angular momentum, $l$, of radiated\ntwisted photons is related to the harmonic number $n\\in \\mathbb{Z}$ as\n$l=2n+1$, i.e., the given system is a pure source of twisted photons as\nexpected. It is shown that in the paraxial shortwave regime the main part of\nradiated photons is linearly polarized with $l=\\pm1$ at the harmonics\n$n=\\{-1,0\\}$. The applicability conditions of the approach developed are\ndiscussed. As the examples, we consider the production of $6.3$ eV twisted\nphotons from uranium nuclei and the production of X-ray twisted photons from\n$120$ MeV electrons.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report a comprehensive study of the centrosymmetric Re$_3$B and\nnoncentrosymmetric Re$_7$B$_3$ superconductors. At a macroscopic level, their\nbulk superconductivity (SC), with $T_c$ = 5.1 K (Re$_3$B) and 3.3 K\n(Re$_7$B$_3$), was characterized via electrical-resistivity, magnetization, and\nheat-capacity measurements, while their microscopic superconducting properties\nwere investigated by means of muon-spin rotation/relaxation ($\\mu$SR). In both\nRe$_3$B and Re$_7$B$_3$ the low-$T$ zero-field electronic specific heat and the\nsuperfluid density (determined via tranverse-field $\\mu$SR) suggest a nodeless\nSC. Both compounds exhibit some features of multigap SC, as evidenced by\ntemperature-dependent upper critical fields $H_\\mathrm{c2}(T)$, as well as by\nelectronic band-structure calculations. The absence of spontaneous magnetic\nfields below the onset of SC, as determined from zero-field $\\mu$SR\nmeasurements, indicates a preserved time-reversal symmetry in the\nsuperconducting state of both Re$_3$B and Re$_7$B$_3$. Our results suggest that\na lack of inversion symmetry and the accompanying antisymmetric spin-orbit\ncoupling effects are not essential for the occurrence of multigap SC in these\nrhenium-boron compounds.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Subsampling is a general statistical method developed in the 1990s aimed at\nestimating the sampling distribution of a statistic $\\hat \\theta _n$ in order\nto conduct nonparametric inference such as the construction of confidence\nintervals and hypothesis tests. Subsampling has seen a resurgence in the Big\nData era where the standard, full-resample size bootstrap can be infeasible to\ncompute. Nevertheless, even choosing a single random subsample of size $b$ can\nbe computationally challenging with both $b$ and the sample size $n$ being very\nlarge. In the paper at hand, we show how a set of appropriately chosen,\nnon-random subsamples can be used to conduct effective -- and computationally\nfeasible -- distribution estimation via subsampling. Further, we show how the\nsame set of subsamples can be used to yield a procedure for subsampling\naggregation -- also known as subagging -- that is scalable with big data.\nInterestingly, the scalable subagging estimator can be tuned to have the same\n(or better) rate of convergence as compared to $\\hat \\theta _n$. The paper is\nconcluded by showing how to conduct inference, e.g., confidence intervals,\nbased on the scalable subagging estimator instead of the original $\\hat \\theta\n_n$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the switching dynamics in a $\\mathcal{PT}$-symmetric fiber\ncoupler composed of a saturable nonlinear material as the core. In such a\nsaturable nonlinear medium, bistable solitons may evolve due to the balance\nbetween dispersion and saturable nonlinearity, which we extend in the context\nof the $\\mathcal{PT}$-symmetric coupler. Our investigations of power-controlled\nand phase-sensitive switching show richer soliton switching dynamics than the\ncurrently existing conventional counterparts, which may lead to ultrafast and\nefficient all-optical switching dynamics at very low power owing to the\ncombined effects of $\\mathcal{PT}$ symmetry and saturable nonlinearity. In\naddition to the input power, the relative phase of the input solitons and\nsaturable coefficient are additional controlling parameters that efficiently\ntailor the switching dynamics. Also, we provide a suitable range of system and\npulse parameters that would be helpful for the practical realization of the\ncoupler to use in all-optical switching devices and photonic circuits. Finally,\nwe develop a variational approach to analytically investigate the switching\ndynamics in such $\\mathcal{PT}$-symmetric couplers that excellently predicts\nthe numerical findings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Backprop algorithm for learning in neural networks utilizes two\nmechanisms: first, stochastic gradient descent and second, initialization with\nsmall random weights, where the latter is essential to the effectiveness of the\nformer. We show that in continual learning setups, Backprop performs well\ninitially, but over time its performance degrades. Stochastic gradient descent\nalone is insufficient to learn continually; the initial randomness enables only\ninitial learning but not continual learning. To the best of our knowledge, ours\nis the first result showing this degradation in Backprop's ability to learn. To\naddress this degradation in Backprop's plasticity, we propose an algorithm that\ncontinually injects random features alongside gradient descent using a new\ngenerate-and-test process. We call this the \\textit{Continual Backprop}\nalgorithm. We show that, unlike Backprop, Continual Backprop is able to\ncontinually adapt in both supervised and reinforcement learning (RL) problems.\nContinual Backprop has the same computational complexity as Backprop and can be\nseen as a natural extension of Backprop for continual learning.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the problem of set discovery where given a few example tuples of a\ndesired set, we want to find the set in a collection of sets. A challenge is\nthat the example tuples may not uniquely identify a set, and a large number of\ncandidate sets may be returned. Our focus is on interactive exploration to set\ndiscovery where additional example tuples from the candidate sets are shown and\nthe user either accepts or rejects them as members of the target set. The goal\nis to find the target set with the least number of user interactions. The\nproblem can be cast as an optimization problem where we want to find a decision\ntree that can guide the search to the target set with the least number of\nquestions to be answered by the user. We propose a general algorithm that is\ncapable of reaching an optimal solution and two variations of it that strike a\nbalance between the quality of a solution and the running time. We also propose\na novel pruning strategy that safely reduces the search space without\nintroducing false negatives. We evaluate the efficiency and the effectiveness\nof our algorithms through an extensive experimental study using both real and\nsynthetic datasets and comparing them to previous approaches in the literature.\nWe show that our pruning strategy reduces the running time of the search\nalgorithms by 2-5 orders of magnitude.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Antibiotic resistance is a topical problem for both humans and animals and\nhas been the subject of special monitoring for two decades. Several recent\nstudies, including ours, have shown that this phenomenon is accentuated by the\ntransfer of certain genetic elements and cross resistance acquisition, the\nlatter or both resulting from the misuse of these antimicrobial drugs. A more\ncareful use of antimicrobials, the search for new antibacterial compounds\n(including probiotics and phages) are the most recommended alternatives to\novercome this situation. However, tests for modulations of antimicrobial\nactivity can also play a major role. The main goal of synergy studies is to\nassess whether substances with antibacterial properties can improve the\neffectiveness of existing antimicrobials or give them a second life against\nresistant germs. Moreover, recent studies have demonstrated the ability of\nsilver nanoparticles and extracts of certain plants to boost the effectiveness\nof certain antibiotics. such as ampicillin, benzylpenicillin, cefazolin,\nciprofloxacin, nitrofurantoin, and kanamycin. Yet, from these studies we found\nthat there was a serious problem with the interpretation of the results when\nusing the disk method with determination of the increase in fold area. Indeed,\nthe use of this method firstly requires the determination of the diameter of\ninhibition of the antibiotic alone; follow by the determination of the\ncombination of antibiotic + modulating substance (MS) or extract.....\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Setting an effective reserve price for strategic bidders in repeated auctions\nis a central question in online advertising. In this paper, we investigate how\nto set an anonymous reserve price in repeated auctions based on historical bids\nin a way that balances revenue and incentives to misreport. We propose two\nsimple and computationally efficient methods to set reserve prices based on the\nnotion of a clearing price and make them robust to bidder misreports. The first\napproach adds random noise to the reserve price, drawing on techniques from\ndifferential privacy. The second method applies a smoothing technique by adding\nnoise to the training bids used to compute the reserve price. We provide\ntheoretical guarantees on the trade-offs between the revenue performance and\nbid-shading incentives of these two mechanisms. Finally, we empirically\nevaluate our mechanisms on synthetic data to validate our theoretical findings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The non-trivial topology in the layered $\\text{FeTe}_{0.55}\\text{Se}_{0.45}$\n(FTS) superconductor has been suggested by both theory and experiment to be\nstrongly dependent on the Te concentration. Motivated by this together with the\nTe fluctuations expected from alloy disorder, we develop a simple layered model\nfor a strong topological insulator that allows us to describe a scenario where\ntopologically trivial domains permeate the sample. We refer to such a phase as\ntopological domain disordered and study the local density (LDOS) of the\ntopological surface states that can be measured using scanning tunneling\nspectroscopy (STS) in this phase. We find that topologically trivial domains on\nthe surface, where one would expect the topological surface state to be absent,\nappear as regions of suppressed LDOS surrounded by domain walls with enhanced\nLDOS. Furthermore, we show that studying the energy dependence of the STS\nshould allow us to distinguish the topologically trivial parts of the surface\nfrom other forms of disorder. Finally, we discuss implications of such local\ndisappearance of the topological surface states for the observation of Majorana\nmodes in vortices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The simulation of fermionic relativistic physics, e.g., Dirac and Weyl\nphysics, has led to the discovery of many unprecedented phenomena in photonics,\nof which the optical-frequency realization is, however, still challenging.\nHere, surprisingly, we discover that the woodpile photonic crystals commonly\nused for optical frequency applications host exotic fermion-like relativistic\ndegeneracies: a Dirac nodal line and a fourfold quadratic point, as protected\nby the nonsymmorphic crystalline symmetry. Deforming the woodpile photonic\ncrystal leads to the emergence of type-II Dirac points from the fourfold\nquadratic point. Such type-II Dirac points can be detected by its anomalous\nrefraction property which is manifested as a giant birefringence in a slab\nsetup. Our findings provide a promising route towards 3D optical Dirac physics\nin all-dielectric photonic crystals.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-source-extractors are functions that extract uniform randomness from\nmultiple (weak) sources of randomness. Quantum multi-source-extractors were\nconsidered by Kasher and Kempe (for the quantum-independent-adversary and the\nquantum-bounded-storage-adversary), Chung, Li and Wu (for the\ngeneral-entangled-adversary) and Arnon-Friedman, Portmann and Scholz (for the\nquantum-Markov-adversary). One of the main objectives of this work is to unify\nall the existing quantum multi-source adversary models. We propose two new\nmodels of adversaries: 1) the quantum-measurement-adversary (qm-adv), which\ngenerates side-information using entanglement and on post-measurement and 2)\nthe quantum-communication-adversary (qc-adv), which generates side-information\nusing entanglement and communication between multiple sources. We show that, 1.\nqm-adv is the strongest adversary among all the known adversaries, in the sense\nthat the side-information of all other adversaries can be generated by qm-adv.\n2. The (generalized) inner-product function (in fact a general class of\ntwo-wise independent functions) continues to work as a good extractor against\nqm-adv with matching parameters as that of Chor and Goldreich. 3. A\nnon-malleable-extractor proposed by Li (against classical-adversaries)\ncontinues to be secure against quantum side-information. This result implies a\nnon-malleable-extractor result of Aggarwal, Chung, Lin and Vidick with uniform\nseed. We strengthen their result via a completely different proof to make the\nnon-malleable-extractor of Li secure against quantum side-information even when\nthe seed is not uniform. 4. A modification (working with weak sources instead\nof uniform sources) of the Dodis and Wichs protocol for privacy-amplification\nis secure against active quantum adversaries. This strengthens on a recent\nresult due to Aggarwal, Chung, Lin and Vidick which uses uniform sources.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce a novel thick-target concept tailored to the extraction of\nrefractory 4d and 5d transition metal radionuclides of molybdenum, technetium,\nruthenium and tungsten for radioactive ion beam production. Despite the more\nthan 60-year old history of thick-target ISOL mass-separation facilities like\nISOLDE, the extraction of the most refractory elements as radioactive ion beam\nhas so far not been successful. In ordinary thick ISOL targets, their\nradioisotopes produced in the target are stopped within the condensed target\nmaterial and have to diffuse through a solid material. Here, we present a\nconcept which overcomes limitations associated with this method. We exploit the\nrecoil momentum of nuclear reaction products for their release from the solid\ntarget material. They are thermalized in a carbon monoxide-containing\natmosphere, in which volatile carbonyl complexes form readily at ambient\ntemperature and pressure. This compound serves as volatile carrier for\ntransport to the ion source. Excess carbon monoxide is removed by cryogenic gas\nseparation to enable low pressures in the source region, in which the species\nare ionized and hence made available for radioactive ion beam formation. The\nsetup is operated in batch mode, with the aim to extract isotopes having\nhalf-lives of at least several seconds. We report parameter studies of the key\nprocesses of the method, which validate this concept and which define the\nparameters for the setup. This would allow for the first time the extraction of\nradioactive molybdenum, tungsten and several other transition metals at\nthick-target ISOL facilities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum simulation, the simulation of quantum processes on quantum computers,\nsuggests a path forward for the efficient simulation of problems in\ncondensed-matter physics, quantum chemistry, and materials science. While the\nmajority of quantum simulation algorithms are deterministic, a recent surge of\nideas has shown that randomization can greatly benefit algorithmic performance.\nIn this work, we introduce a scheme for quantum simulation that unites the\nadvantages of randomized compiling on the one hand and higher-order\nmulti-product formulas, as they are used for example in\nlinear-combination-of-unitaries (LCU) algorithms or quantum error mitigation,\non the other hand. In doing so, we propose a framework of randomized sampling\nthat is expected to be useful for programmable quantum simulators and present\ntwo new multi-product formula algorithms tailored to it. Our framework reduces\nthe circuit depth by circumventing the need for oblivious amplitude\namplification required by the implementation of multi-product formulas using\nstandard LCU methods, rendering it especially useful for early quantum\ncomputers used to estimate the dynamics of quantum systems instead of\nperforming full-fledged quantum phase estimation. Our algorithms achieve a\nsimulation error that shrinks exponentially with the circuit depth. To\ncorroborate their functioning, we prove rigorous performance bounds as well as\nthe concentration of the randomized sampling procedure. We demonstrate the\nfunctioning of the approach for several physically meaningful examples of\nHamiltonians, including fermionic systems and the Sachdev-Ye-Kitaev model, for\nwhich the method provides a favorable scaling in the effort.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To obtain the highest confidence on the correction of numerical simulation\nprograms implementing the finite element method, one has to formalize the\nmathematical notions and results that allow to establish the soundness of the\nmethod. Sobolev spaces are the mathematical framework in which most weak\nformulations of partial derivative equations are stated, and where solutions\nare sought. These functional spaces are built on integration and measure\ntheory. Hence, this chapter in functional analysis is a mandatory theoretical\ncornerstone for the definition of the finite element method. The purpose of\nthis document is to provide the formal proof community with very detailed\npen-and-paper proofs of the main results from integration and measure theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Large-scale strongly nonlinear and nonconvex mixed-integer nonlinear\nprogramming (MINLP) models frequently appear in optimisation-based process\nsynthesis, integration, intensification, and process control. However, they are\nusually difficult to solve by existing algorithms within an acceptable time. In\nthis work, we propose two robust homotopy continuation enhanced branch and\nbound (HCBB) algorithms (denoted as HCBB-FP and HCBB-RB) where the homotopy\ncontinuation method is employed to gradually approach the optimum of the NLP\nsubproblem at a node from the solution at its parent node. A variable step\nlength is adapted to effectively balance feasibility and computational\nefficiency. The computational results from solving four existing process\nsynthesis problems demonstrate that the proposed HCBB algorithms can find the\nsame optimal solution from different initial points, while the existing MINLP\nalgorithms fail or find much worse solutions. In addition, HCBB-RB is superior\nto HCBB-FP due to the much lower computational effort required for the same\nlocally optimal solution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This is the first paper in a series aimed at modeling the black hole (BH)\nmass function, from the stellar to the intermediate to the (super)massive\nregime. In the present work we focus on stellar BHs and provide an ab-initio\ncomputation of their mass function across cosmic times. Specifically, we\nexploit the state-of-the-art stellar and binary evolutionary code\n\\texttt{SEVN}, and couple its outputs with redshift-dependent galaxy statistics\nand empirical scaling relations involving galaxy metallicity, star-formation\nrate and stellar mass. The resulting relic mass function ${\\rm d}N/{\\rm d}V{\\rm\nd}\\log m_\\bullet$ as a function of the BH mass $m_\\bullet$ features a rather\nflat shape up to $m_\\bullet\\approx 50\\, M_\\odot$ and then a log-normal decline\nfor larger masses, while its overall normalization at a given mass increases\nwith decreasing redshift. We highlight the contribution to the local mass\nfunction from isolated stars evolving into BHs and from binary stellar systems\nending up in single or binary BHs. We also include the distortion on the mass\nfunction induced by binary BH mergers, finding that it has a minor effect at\nthe high-mass end. We estimate a local stellar BH relic mass density of\n$\\rho_\\bullet\\approx 5\\times 10^7\\, M_\\odot$ Mpc$^{-3}$, which exceeds by more\nthan two orders of magnitude that in supermassive BHs; this translates into an\nenergy density parameter $\\Omega_\\bullet\\approx 4\\times 10^{-4}$, implying that\nthe total mass in stellar BHs amounts to $\\lesssim 1\\%$ of the local baryonic\nmatter. We show how our mass function for merging BH binaries compares with the\nrecent estimates from gravitational wave observations by LIGO/Virgo, and\ndiscuss the possible implications for dynamical formation of BH binaries in\ndense environments like star clusters. [abridged]\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For every naturla numbers $m,n $ and every field $K$, let $M(m \\times n, K)$\nbe the vector space of the $(m \\times n)$-matrices over $K$ and let $S(n,K)$ be\nthe vector space of the symmetric $(n \\times n)$-matrices over $K$. We say that\nan affine subspace $S$ of $M(m \\times n, K)$ or of $S(n,K)$ has constant rank\n$r$ if every matrix of $S$ has rank $r$. Define $${\\cal A}^K(m \\times n; r)= \\{\nS \\;| \\; S \\; \\mbox{\\rm affine subsapce of $M(m \\times n, K)$ of constant rank\n} r\\}$$ $${\\cal A}_{sym}^K(n;r)= \\{ S \\;| \\; S \\; \\mbox{\\rm affine subsapce of\n$S(n,K)$ of constant rank } r\\}$$ $$a^K(m \\times n;r) = \\max \\{\\dim S \\mid S\n\\in {\\cal A}^K(m \\times n; r ) \\}.$$ $$a_{sym}^K(n;r) = \\max \\{\\dim S \\mid S\n\\in {\\cal A}_{sym}^K(n,r) \\}.$$\n  In this paper we prove the following two formulas for $r \\leq m \\leq n$:\n$a_{sym}^{\\R}(n,r) = r(n-r)+ \\left[\\frac{r^2}{4} \\right],$ $a^{\\R}(m \\times\nn,r) = r(n-r)+ \\frac{r(r-1)}{2} .$\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present results on the nature of extreme ejective feedback episodes and\nthe physical conditions of a population of massive ($\\rm M_* \\sim 10^{11}\nM_{\\odot}$), compact starburst galaxies at z = 0.4-0.7. We use data from\nKeck/NIRSPEC, SDSS, Gemini/GMOS, MMT, and Magellan/MagE to measure rest-frame\noptical and near-IR spectra of 14 starburst galaxies with extremely high star\nformation rate surface densities (mean $\\rm \\Sigma_{SFR} \\sim 3000 \\,M_{\\odot}\nyr^{-1} kpc^{-2}$) and powerful galactic outflows (maximum speeds v$_{98} \\sim$\n1000-3000 km s$^{-1}$). Our unique data set includes an ensemble of both\nemission [OII]$\\lambda\\lambda$3726,3729, H$\\beta$,\n[OIII]$\\lambda\\lambda$4959,5007, H$\\alpha$, [NII]$\\lambda\\lambda$6548,6583, and\n[SII]$\\lambda\\lambda$6716,6731) and absorption MgII$\\lambda\\lambda$2796,2803,\nand FeII$\\lambda$2586) lines that allow us to investigate the kinematics of the\ncool gas phase (T$\\sim$10$^4$ K) in the outflows. Employing a suite of line\nratio diagnostic diagrams, we find that the central starbursts are\ncharacterized by high electron densities (median n$_e \\sim$ 530 cm$^{-3}$), and\nhigh metallicity (solar or super-solar). We show that the outflows are most\nlikely driven by stellar feedback emerging from the extreme central starburst,\nrather than by an AGN. We also present multiple intriguing observational\nsignatures suggesting that these galaxies may have substantial Lyman continuum\n(LyC) photon leakage, including weak [SII] nebular emission lines. Our results\nimply that these galaxies may be captured in a short-lived phase of extreme\nstar formation and feedback where much of their gas is violently blown out by\npowerful outflows that open up channels for LyC photons to escape.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A chain of connections in compressed baryonic matter, up-to-date glaringly\nmissing in nuclear effective field theory, between intrinsic or emergent\nsymmetries of QCD, mesons-gluons dualities, vector meson dominance and\nChern-Simons fields has recently been revealed, presaging a possible new\nparadigm in nuclear theory. It indicates a ubiquitous role, thus far\nunexplored, of hidden symmetries -- flavor-local and scale -- permeating from\ndilute baryonic systems to normal nuclear matter and then to compact-star\nmatter. Here I give a brief account of the possibly \"indispensable\" relevance\nof the $\\eta^\\prime$ singular ring, a.k.a. fractional quantum Hall (FQH)\ndroplet, to the properties of the lowest-lying vector mesons $\\omega$ and\n$\\rho$, relevant to dilepton production processes, argued to be Seiberg-dual to\nthe gluons near the chiral restoration.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum machine learning (QML) is a rapidly growing area of research at the\nintersection of classical machine learning and quantum information theory. One\narea of considerable interest is the use of QML to learn information contained\nwithin quantum states themselves. In this work, we propose a novel approach in\nwhich the extraction of information from quantum states is undertaken in a\nclassical representational-space, obtained through the training of a hybrid\nquantum autoencoder (HQA). Hence, given a set of pure states, this variational\nQML algorithm learns to identify, and classically represent, their essential\ndistinguishing characteristics, subsequently giving rise to a new paradigm for\nclustering and semi-supervised classification. The analysis and employment of\nthe HQA model are presented in the context of amplitude encoded states - which\nin principle can be extended to arbitrary states for the analysis of structure\nin non-trivial quantum data sets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Never is the difference between thermal equilibrium and turbulence so\ndramatic, as when a quadratic invariant makes the equilibrium statistics\nexactly Gaussian with independently fluctuating modes. That happens in two very\ndifferent yet deeply connected classes of systems: incompressible hydrodynamics\nand resonantly interacting waves. This work presents the first case of a\ndetailed information-theoretic analysis of turbulence in such strongly\ninteracting systems. The analysis elucidates the fundamental roles of space and\ntime in setting the cascade direction and the changes of the statistics along\nit. We introduce a beautifully simple yet rich family of discrete models with\nneighboring triplet interactions and show that it has families of quadratic\nconservation laws defined by the Fibonacci numbers. Depending on the single\nmodel parameter, three types of turbulence were found: single direct cascade,\ndouble cascade, and the first ever case of a single inverse cascade. We\ndescribe quantitatively how deviation from thermal equilibrium all the way to\nturbulent cascades makes statistics increasingly non-Gaussian and find the\nself-similar form of the one-mode probability distribution. We reveal where the\ninformation (entropy deficit) is encoded and disentangle the communication\nchannels between modes, as quantified by the mutual information in pairs and\nthe interaction information inside triplets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Many researchers are trying to replace the aggregation server in federated\nlearning with a blockchain system to achieve better privacy, robustness and\nscalability. In this case, clients will upload their updated models to the\nblockchain ledger, and use a smart contract on the blockchain system to perform\nmodel averaging. However, running machine learning applications on the\nblockchain is almost impossible because a blockchain system, which usually\ntakes over half minute to generate a block, is extremely slow and unable to\nsupport machine learning applications.\n  This paper proposes a completely new public blockchain architecture called\nDFL, which is specially optimized for distributed federated machine learning.\nThis architecture inherits most traditional blockchain merits and achieves\nextremely high performance with low resource consumption by waiving global\nconsensus. To characterize the performance and robustness of our architecture,\nwe implement the architecture as a prototype and test it on a physical\nfour-node network. To test more nodes and more complex situations, we build a\nsimulator to simulate the network. The LeNet results indicate our system can\nreach over 90% accuracy for non-I.I.D. datasets even while facing model\npoisoning attacks, with the blockchain consuming less than 5% of hardware\nresources.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Tropical geometry with the max-plus algebra has been applied to statistical\nlearning models over tree spaces because geometry with the tropical metric over\ntree spaces has some nice properties such as convexity in terms of the tropical\nmetric. One of the challenges in applications of tropical geometry to tree\nspaces is the difficulty interpreting outcomes of statistical models with the\ntropical metric. This paper focuses on combinatorics of tree topologies along a\ntropical line segment, an intrinsic geodesic with the tropical metric, between\ntwo phylogenetic trees over the tree space and we show some properties of a\ntropical line segment between two trees. Specifically we show that a\nprobability of a tropical line segment of two randomly chosen trees going\nthrough the origin (the star tree) is zero if the number of leave is greater\nthan four, and we also show that if two given trees differ only one nearest\nneighbor interchange (NNI) move, then the tree topology of a tree in the\ntropical line segment between them is the same tree topology of one of these\ngiven two trees with possible zero branch lengths.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The high-precision measurement of spatial clustering of emission line\ngalaxies (ELGs) is a primary objective for upcoming cosmological spectroscopic\nsurveys. The source of strong emission of ELGs is nebular emission from\nsurrounding ionized gas irradiated by massive short-lived stars in star-forming\ngalaxies. As a result, ELGs are more likely to reside in newly-formed halos and\nthis leads to a nonlinear relation between ELG number density and matter\ndensity fields. In order to estimate the covariance matrix of cosmological\nobservables, it is essential to produce many independent realisations to\nsimulate ELG distributions for large survey volumes. To this end, we present a\nnovel and fast scheme to populate ELGs in dark-matter only $N$-body simulations\nbased on local density field. This method enables fast production of mock ELG\ncatalogues suitable for verifying analysis methods and quantifying\nobservational systematics in upcoming spectroscopic surveys and can populate\nELGs in moderately high-density regions even though the halo structure cannot\nbe resolved due to low resolution. The power spectrum of simulated ELGs is\nconsistent with results of hydrodynamical simulations up to fairly small scales\n($\\lesssim 1 h \\, \\mathrm{Mpc}^{-1}$), and the simulated ELGs are more likely\nto be found in filamentary structures, which is consistent with results of\nsemi-analytic and hydrodynamical simulations. Furthermore, we address the\nredshift-space power spectrum of simulated ELGs. The measured multipole moments\nof simulated ELGs clearly exhibit a weaker Finger-of-God effect than those of\nmatter due to infalling motions towards halo centre, rather than random virial\nmotions inside halos.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove that in the non-orientable setting, the minimal stretch factor of a\npseudo-Anosov homeomorphism of a surface of genus $g$ with a fixed number of\npunctures is asymptotically on the order of $\\frac{1}{g}$. Our result adapts\nthe work of Yazdi to non-orientable surfaces. We include the details of\nThurston's theory of fibered faces for non-orientable 3-manifolds.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Grant-free multiple-access (GFMA) is a valuable research topic, since it can\nsupport multiuser transmission with low latency. This paper constructs novel\nuniquely-decodable multi-amplitude sequence (UDAS) sets for GFMA systems, which\ncan provide high spectrum efficiency (SE) with low-complexity active user\ndetection (AUD) algorithm. First of all, we propose an UDAS-based\nmulti-dimensional bit interleaving coded modulation (MD-BICM) transmitter; then\nintroduce the definition of UDAS and construct two kinds of UDAS sets based on\ncyclic and quasi-cyclic matrix modes. Besides, we present a statistic of UDAS\nfeature based AUD algorithm (SoF-AUD), and a joint multiuser detection and\nimproved message passing algorithm for the proposed system. Finally, the active\nuser error rate (AUER) and Shannon limits of the proposed system are deduced in\ndetails. Simulation results show that our proposed system can simultaneously\nsupport four users without additional redundancy, and the AUER can reach an\nextremely low value $10^{-5}$ when $E_b/N_0$ is $0$ dB and the length of\ntransmit block is larger than a given value, i.e., 784, verifying the validity\nand flexibility of the proposed UDAS sets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We continue our study of the correspondence between BPS structures and\ntopological recursion in the uncoupled case, this time from the viewpoint of\nquantum curves. For spectral curves of hypergeometric type, we show the\nBorel-resummed Voros symbols of the corresponding quantum curves solve\nBridgeland's \"BPS Riemann-Hilbert problem\". In particular, they satisfy the\nrequired jump property in agreement with the generalized definition of BPS\nindices $\\Omega$ in our previous work. Furthermore, we observe the Voros\ncoefficients define a closed one-form on the parameter space, and show that\n(log of) Bridgeland's $\\tau$-function encoding the solution is none other than\nthe corresponding potential, up to a constant. When the quantization parameter\nis set to a special value, this agrees with the Borel sum of the topological\nrecursion partition function $Z_{\\rm TR}$, up to a simple factor.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the set $\\mathcal{S}$ of odd positive integers $n$ with the property\n${2n}/{\\sigma(n)} - 1 = 1/x$, for positive integer $x$, i.e., the set that\nrelates to odd perfect and odd \"spoof perfect\" numbers. As a consequence, we\nfind that if $D=pq$ denotes a spoof odd perfect number other than Descartes'\nexample, with pseudo-prime factor $p$, then $q>10^{12}$. Furthermore, we find\nirregularities in the ending digits of integers $n\\in\\mathcal{S}$ and study\naspects of its density, leading us to conjecture that the amount of numbers in\n$\\mathcal{S}$ below $k$ is $\\sim10 \\log(k)$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider positively supported Borel measures for which all moments exist.\nOn the set of compactly supported measures in this class a partial order is\ndefined via eventual dominance of the moment sequences. Special classes are\nidentified on which the order is total, but it is shown that already for the\nset of distributions with compactly supported smooth densities the order is not\ntotal. In particular we construct a pair of measures with smooth density for\nwhich infinitely many moments agree and another one for which the moments\nalternate infinitely often. This disproves some recently published claims to\nthe contrary. Some consequences for games with distributional payoffs are\ndiscussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  An eigenfunction of the Laplacian on a metric (quantum) graph has an excess\nnumber of zeros due to the graph's non-trivial topology. This number, called\nthe nodal surplus, is an integer between 0 and the graph's first Betti number\n$\\beta$. We study the distribution of the nodal surplus values in the countably\ninfinite set of the graph's eigenfunctions. We conjecture that this\ndistribution converges to Gaussian for any sequence of graphs of growing\n$\\beta$. We prove this conjecture for several special graph sequences and test\nit numerically for a variety of well-known graph families. Accurate computation\nof the distribution is made possible by a formula expressing the nodal surplus\ndistribution as an integral over a high-dimensional torus.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the two-dimensional multiphase Muskat problem describing the motion\nof three immiscible fluids with equal viscosities in a vertical homogeneous\nporous medium identified with $\\mathbb{R}^2$ under the effect of gravity. We\nfirst formulate the governing equations as a strongly coupled evolution problem\nfor the functions that parameterize the sharp interfaces between the fluids.\nAfterwards we prove that the problem is of parabolic type and establish its\nwell-posedness together with two parabolic smoothing properties. For solutions\nthat are not global we exclude, in a certain regime, that the interfaces come\ninto contact along a curve segment.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The last decade has witnessed a rapid growth in understanding of the pivotal\nroles of mechanical stresses and physical forces in cell biology. As a result\nan integrated view of cell biology is evolving, where genetic and molecular\nfeatures are scrutinized hand in hand with physical and mechanical\ncharacteristics of cells. Physics of liquid crystals has emerged as a\nburgeoning new frontier in cell biology over the past few years, fueled by an\nincreasing identification of orientational order and topological defects in\ncell biology, spanning scales from subcellular filaments to individual cells\nand multicellular tissues. Here, we provide an account of most recent findings\nand developments together with future promises and challenges in this rapidly\nevolving interdisciplinary research direction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Access control is a fundamental component of the design of distributed\nledgers, influencing many aspects of their design, such as fairness,\nefficiency, traditional notions of network security, and adversarial attacks\nsuch as Denial-of-Service (DoS) attacks. In this work, we consider the security\nof a recently proposed access control protocol for Directed Acyclic Graph-based\ndistributed ledgers. We present a number of attack scenarios and potential\nvulnerabilities of the protocol and introduce a number of additional features\nwhich enhance its resilience. Specifically, a blacklisting algorithm, which is\nbased on a reputation-weighted threshold, is introduced to handle both spamming\nand multi-rate malicious attackers. The introduction of a solidification\nrequest component is also introduced to ensure the fairness and consistency of\nnetwork in the presence of attacks. Finally, a timestamp component is also\nintroduced to maintain the consistency of the network in the presence of\nmulti-rate attackers. Simulations to illustrate the efficacy and robustness of\nthe revised protocol are also described.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The pressure-induced insulator to metal transition (IMT) of layered magnetic\nnickel phosphorous tri-sulfide NiPS3 was studied in-situ under quasi-uniaxial\nconditions by means of electrical resistance (R) and X-ray diffraction (XRD)\nmeasurements. This sluggish transition is shown to occur at 35 GPa. Transport\nmeasurements show no evidence of superconductivity to the lowest measured\ntemperature (~ 2 K). The structure results presented here differ from earlier\nin-situ work that subjected the sample to a different pressure state,\nsuggesting that in NiPS3 the phase stability fields are highly dependent on\nstrain. It is suggested that careful control of the strain is essential when\nstudying the electronic and magnetic properties of layered van der Waals\nsolids.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The classical Khintchine-Groshev theorem is a generalization of Khintchine's\ntheorem on simultaneous Diophantine approximation, from approximation of points\nin $\\mathbb R^m$ to approximation of systems of linear forms in $\\mathbb\nR^{nm}$. In this paper, we present an inhomogeneous version of the\nKhintchine-Groshev theorem which does not carry a monotonicity assumption when\n$nm>2$. Our results bring the inhomogeneous theory almost in line with the\nhomogeneous theory, where it is known by a result of Beresnevich and Velani\n(2010) that monotonicity is not required when $nm>1$. That result resolved a\nconjecture of Beresnevich, Bernik, Dodson, and Velani (2009), and our work\nresolves almost every case of the natural inhomogeneous generalization of that\nconjecture. Regarding the two cases where $nm=2$, we are able to remove\nmonotonicity by assuming extra divergence of a measure sum, akin to a linear\nforms version of the Duffin-Schaeffer conjecture. When $nm=1$ it is known by\nwork of Duffin and Schaeffer (1941) that the monotonicity assumption cannot be\ndropped.\n  The key new result is an independence inheritance phenomenon; the underlying\nidea is that the sets involved in the $((n+k)\\times m)$-dimensional\nKhintchine-Groshev theorem ($k\\geq 0$) are always $k$-levels more\nprobabilistically independent than the sets involved the $(n\\times\nm)$-dimensional theorem. Hence, it is shown that Khintchine's theorem itself\nunderpins the Khintchine-Groshev theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Common sense has always been of interest in Artificial Intelligence, but has\nrarely taken center stage. Despite its mention in one of John McCarthy's\nearliest papers and years of work by dedicated researchers, arguably no AI\nsystem with a serious amount of general common sense has ever emerged. Why is\nthat? What's missing? Examples of AI systems' failures of common sense abound,\nand they point to AI's frequent focus on expertise as the cause. Those\nattempting to break the resulting brittleness barrier, even in the context of\nmodern deep learning, have tended to invest their energy in large numbers of\nsmall bits of commonsense knowledge. While important, all the commonsense\nknowledge fragments in the world don't add up to a system that actually\ndemonstrates common sense in a human-like way. We advocate examining common\nsense from a broader perspective than in the past. Common sense should be\nconsidered in the context of a full cognitive system with history, goals,\ndesires, and drives, not just in isolated circumscribed examples. A fresh look\nis needed: common sense is worthy of its own dedicated scientific exploration.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper presents a new fast power series solution method to solve the\nHierarchal Method of Moment(MoM) matrix for a large complex,perfectly electric\nconducting (PEC) 3D structures. The proposed power series solution converges in\njust two iterations which is faster than the conventional fast solver-based\niterative solution. The method is purely algebraic in nature and, as such\napplicable to existing conventional methods. The method uses regular fast\nsolver Hierarchal Matrix (H-Matrix) and can also be applied to Multilevel Fast\nMultipole Method Algorithm(MLFMA). In the proposed method, we use the scaling\nof the symmetric near-field matrix to develop a diagonally dominant overall\nmatrix to enable a power series solution. Left and right block scaling\ncoefficients are required for scaling near-field blocks to diagonal blocks\nusing Schur's complement method. However,only the right-hand scaling\ncoefficients are computed for symmetric near-field matrix leading to saving of\ncomputation time and memory. Due to symmetric property, the left side-block\nscaling coefficients are just the transpose of the right-scaling blocks. Next,\nthe near-field blocks are replaced by scaled near-field diagonal blocks. Now\nthe scaled near-field blocks in combination with far-field and scaling\ncoefficients are subjected to power series solution terminating after only two\nterms. As all the operations are performed on the near-field blocks, the\ncomplexity of scaling coefficient computation is retained as O(N). The power\nseries solution only involves the matrix-vector product of the far-field,\nscaling coefficients blocks, and inverse of scaled near-field blocks. Hence,\nthe solution cost remains O(NlogN). Several numerical results are presented to\nvalidate the efficiency and robustness of the proposed numerical method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Against common sense, auxetic materials expand or contract perpendicularly\nwhen stretched or compressed, respectively, by uniaxial strain, being\ncharacterized by a negative Poisson's ratio $\\nu$. The amount of deformation in\nresponse to the applied force can be at most equal to the imposed one, so that\n$\\nu=-1$ is the lowest bound for the mechanical stability of solids, a\ncondition here defined as \"hyper-auxeticity\". In this work, we numerically show\nthat ultra-low-crosslinked polymer networks under tension display hyper-auxetic\nbehavior at a finite crosslinker concentration. At this point, the nearby\nmechanical instability triggers the onset of a critical-like transition between\ntwo states of different densities. This phenomenon displays similar features as\nwell as important differences with respect to gas-liquid phase separation.\nSince our model is able to faithfully describe real-world hydrogels, the\npresent results can be readily tested in laboratory experiments, paving the way\nto explore this unconventional phase behavior.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In deep reinforcement learning (RL), data augmentation is widely considered\nas a tool to induce a set of useful priors about semantic consistency and\nimprove sample efficiency and generalization performance. However, even when\nthe prior is useful for generalization, distilling it to RL agent often\ninterferes with RL training and degenerates sample efficiency. Meanwhile, the\nagent is forgetful of the prior due to the non-stationary nature of RL. These\nobservations suggest two extreme schedules of distillation: (i) over the entire\ntraining; or (ii) only at the end. Hence, we devise a stand-alone network\ndistillation method to inject the consistency prior at any time (even after\nRL), and a simple yet efficient framework to automatically schedule the\ndistillation. Specifically, the proposed framework first focuses on mastering\ntrain environments regardless of generalization by adaptively deciding which\n{\\it or no} augmentation to be used for the training. After this, we add the\ndistillation to extract the remaining benefits for generalization from all the\naugmentations, which requires no additional new samples. In our experiments, we\ndemonstrate the utility of the proposed framework, in particular, that\nconsiders postponing the augmentation to the end of RL training.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a thorough numerical study on the MRI using the smoothed particle\nmagnetohydrodynamics method (SPMHD) with the geometric density average force\nexpression (GDSPH). We perform shearing box simulations with different initial\nsetups and a wide range of resolution and dissipation parameters. We show, for\nthe first time, that MRI with sustained turbulence can be simulated\nsuccessfully with SPH, with results consistent with prior work with grid-based\ncodes. In particular, for the stratified boxes, our simulations reproduce the\ncharacteristic butterfly diagram of the MRI dynamo with saturated turbulence\nfor at least 100 orbits. On the contrary, traditional SPH simulations suffer\nfrom runaway growth and develop unphysically large azimuthal fields, similar to\nthe results from a recent study with mesh-less methods. We investigated the\ndependency of MRI turbulence on the numerical Prandtl number in SPH, focusing\non the unstratified, zero net-flux case. We found that turbulence can only be\nsustained with a Prandtl number larger than $\\sim$2.5, similar to the critical\nvalues of physical Prandtl number found in grid-code simulations. However,\nunlike grid-based codes, the numerical Prandtl number in SPH increases with\nresolution, and for a fixed Prandtl number, the resulting magnetic energy and\nstresses are independent of resolution. Mean-field analyses were performed on\nall simulations, and the resulting transport coefficients indicate no\n$\\alpha$-effect in the unstratified cases, but an active $\\alpha\\Omega$ dynamo\nand a diamagnetic pumping effect in the stratified medium, which are generally\nin agreement with previous studies. There is no clear indication of a\nshear-current dynamo in our simulation, which is likely to be responsible for a\nweaker mean-field growth in the tall, unstratified, zero net-flux simulation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  (See the complete abstract within the thesis in both English and German\nversions)\n  In this thesis, the process conditions of the epitaxial graphene growth\nthrough a socalled polymer-assisted sublimation growth method are minutely\ninvestigated. Atomic force microscopy (AFM) is used to show that the previously\nneglected flow-rate of the argon process gas has a significant influence on the\nmorphology of the SiC substrate and atop carbon layers. The results can be well\nexplained using a simple model for the thermodynamic conditions at the layer\nadjacent to the surface. The resulting control option of step-bunching on the\nsub-nanometer scales is used to produce the ultra-flat, monolayer graphene\nlayers without the bilayer inclusions that exhibit the vanishing of the\nresistance anisotropy. The comparison of four-point and scanning tunneling\npotentiometry measurements shows that the remaining small anisotropy represents\nthe ultimate limit, which is given solely by the remaining resistances at the\nSiC terrace steps. ... The precise control of step-bunching using the Ar flow\nalso enables the preparation of periodic non-identical SiC surfaces under the\ngraphene layer. Based on the work function measurements by Kelvin-Probe force\nmicroscopy and X-ray photoemission electron microscopy, it is shown for the\nfirst time that there is a doping variation in graphene, induced by a proximity\neffect of the different near-surface SiC stacks. The comparison of the AFM and\nlow-energy electron microscopy measurements have enabled the exact assignment\nof the SiC stacks, and the examinations have led to an improved understanding\nof the surface restructuring in the framework of a step-flow mode. ...\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we present a rigorous end-to-end control strategy for\nautonomous vehicles aimed at minimizing lap times in a time attack racing\nevent. We also introduce AutoRACE Simulator developed as a part of this\nresearch project, which was employed to simulate accurate vehicular and\nenvironmental dynamics along with realistic audio-visual effects. We adopted a\nhybrid imitation-reinforcement learning architecture and crafted a novel reward\nfunction to train a deep neural network policy to drive (using imitation\nlearning) and race (using reinforcement learning) a car autonomously in less\nthan 20 hours. Deployment results were reported as a direct comparison of 10\nautonomous laps against 100 manual laps by 10 different human players. The\nautonomous agent not only exhibited superior performance by gaining 0.96\nseconds over the best manual lap, but it also dominated the human players by\n1.46 seconds with regard to the mean lap time. This dominance could be\njustified in terms of better trajectory optimization and lower reaction time of\nthe autonomous agent.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In contrast to offline working fashions, two research paradigms are devised\nfor online learning: (1) Online Meta Learning (OML) learns good priors over\nmodel parameters (or learning to learn) in a sequential setting where tasks are\nrevealed one after another. Although it provides a sub-linear regret bound,\nsuch techniques completely ignore the importance of learning with fairness\nwhich is a significant hallmark of human intelligence. (2) Online\nFairness-Aware Learning. This setting captures many classification problems for\nwhich fairness is a concern. But it aims to attain zero-shot generalization\nwithout any task-specific adaptation. This therefore limits the capability of a\nmodel to adapt onto newly arrived data. To overcome such issues and bridge the\ngap, in this paper for the first time we proposed a novel online meta-learning\nalgorithm, namely FFML, which is under the setting of unfairness prevention.\nThe key part of FFML is to learn good priors of an online fair classification\nmodel's primal and dual parameters that are associated with the model's\naccuracy and fairness, respectively. The problem is formulated in the form of a\nbi-level convex-concave optimization. Theoretic analysis provides sub-linear\nupper bounds for loss regret and for violation of cumulative fairness\nconstraints. Our experiments demonstrate the versatility of FFML by applying it\nto classification on three real-world datasets and show substantial\nimprovements over the best prior work on the tradeoff between fairness and\nclassification accuracy\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In our paper, we give a necessary and sufficient conditions for the kernels\nof the linear maps of finite Abelian group algebras to be Mathieu-Zhao spaces\nof $K[G]$ if $G$ is a finite Abelian group and $K$ is a split field for $G$.\nHence we classify all Mathieu-Zhao spaces of the finite Abelian group algebras\nif $K$ is a split field for $G$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work we demonstrate spectral-temporal correlation measurements of the\nHong-Ou-Mandel (HOM) interference effect with the use of a spectrometer based\non a photon-counting camera. This setup allows us to take, within seconds,\nspectral temporal correlation measurements on entangled photon sources with\nsub-nanometer spectral resolution and nanosecond timing resolution. Through\npost processing, we can observe the HOM behaviour for any number of spectral\nfilters of any shape and width at any wavelength over the observable spectral\nrange. Our setup also offers great versatility in that it is capable of\noperating at a wide spectral range from the visible to the near infrared and\ndoes not require a pulsed pump laser for timing purposes. This work offers the\nability to gain large amounts of spectral and temporal information from a HOM\ninterferometer quickly and efficiently and will be a very useful tool for many\nquantum technology applications and fundamental quantum optics research.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-access edge computing (MEC) and network virtualization technologies are\nimportant enablers for fifth-generation (5G) networks to deliver diverse\napplications and services. Services are often provided as fully connected\nvirtual network functions (VNF)s, through service function chaining (SFC).\nHowever, the problem of allocating SFC resources at the network edge still\nfaces many challenges related to the way VNFs are placed, chained and\nscheduled. In this paper, to solve these problems, we propose a game\ntheory-based approach with the objective to reduce service latency in the\ncontext of SFC at the network edge. The problem of allocating SFC resources can\nbe divided into two subproblems. 1) The VNF placement and routing subproblem,\nand 2) the VNF scheduling subproblem. For the former subproblem, we formulate\nit as a mean field game (MFG) in which VNFs are modeled as entities contending\nover edge resources with the goal of reducing the resource consumption of MEC\nnodes and reducing latency for users. We propose a on a reinforcement\nlearning-based technique, where the Ishikawa-Mann learning algorithm (IMLA) is\nused. For the later subproblem we formulate it as a matching game between the\nVFNs and an edge resources in order to find the execution order of the VNFs\nwhile reducing the latency. To efficiently solve it, we propose a modified\nversion of the many-to-one deferred acceptance algorithm (DAA), called the\nenhanced multi-step deferred acceptance algorithm (eMSDA). To illustrate the\nperformance of the proposed approaches, we perform extensive simulations. The\nobtained results show that the proposed approaches outperform the benchmarks\nother state-of-the-art methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, the sharp maximal theorem is generalized to mixed-norm ball\nBanach function spaces, which is defined as Definition 2.7. As an application,\nwe give a characterization of BMO via the boundedness of commutators of\nfractional integral operators on mixed-norm Lebesgue spaces. Moreover, the\ncharacterization of homogeneous Lipschitz space is also given by the\nboundedness of commutators of fractional integral operators on mixed-norm\nLebesgue spaces. Finally, two applications of Corollary 6.4 are given.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The set of synchronizing words of a given $n$-state automaton forms a regular\nlanguage recognizable by an automaton with $2^n - n$ states. The size of a\nrecognizing automaton for the set of synchronizing words is linked to\ncomputational problems related to synchronization and to the length of\nsynchronizing words. Hence, it is natural to investigate synchronizing automata\nextremal with this property, i.e., such that the minimal deterministic\nautomaton for the set of synchronizing words has $2^n - n$ states. The\nsync-maximal permutation groups have been introduced in [{\\sc S. Hoffmann},\nCompletely Reachable Automata, Primitive Groups and the State Complexity of the\nSet of Synchronizing Words, LATA 2021] by stipulating that an associated\nautomaton to the group and a non-permutation has this extremal property. The\ndefinition is in analogy with the synchronizing groups and analog to a\ncharacterization of primitivity obtained in the mentioned work. The precise\nrelation to other classes of groups was mentioned as an open problem. Here, we\nsolve this open problem by showing that the sync-maximal groups are precisely\nthe primitive groups. Our result gives a new characterization of the primitive\ngroups. Lastly, we explore an alternative and stronger definition than\nsync-maximality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report on the evolution of the largest physics course at the Technical\nUniversity Berlin during the COVID-19 pandemic, which boosted the development\nof online learning content, electronic assessments, and, not at least, the\nreconceptualization of our teaching methods. By shifting to distance education,\nwe created features that can be easily implemented and combined with any\nphysics-type course at a university level. Thereby, we incidentally made\nintroductory physics sustainably accessible to students having difficulties\nvisiting the university. Besides our experiences gained on blended learning\nmethods, we provide a guide to online exercises, microlearning videos, and\ne-assessments, including exams realized in a virtual setting via the\nuniversity's Moodle. Since the course is attended annually by more than 800\nstudents enrolled in at least twelve bachelor degree engineering and all STEM\n(science, technology, engineering, and mathematics) fields, our reformations\ngreatly impact the student's learning style. The teaching concepts presented\nhere are supported by results from the Force Concept Inventory, student\nfeedback, and exam results. The Force Concept Inventory shows that our online\nteaching did not negatively affect the student's learning process compared with\na traditional face-to-face lecture. Student's feedback shows that the new\nformat and material being available online are received very well. Finally, the\nexam results show that virtual exams conducted in a remote setting can be\ndesigned to minimize cheating possibilities. In addition, the test was able to\nyield a similar distribution of points as in the previous traditional ones.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-Label Image Classification (MLIC) aims to predict a set of labels that\npresent in an image. The key to deal with such problem is to mine the\nassociations between image contents and labels, and further obtain the correct\nassignments between images and their labels. In this paper, we treat each image\nas a bag of instances, and reformulate the task of MLIC as an instance-label\nmatching selection problem. To model such problem, we propose a novel deep\nlearning framework named Graph Matching based Multi-Label Image Classification\n(GM-MLIC), where Graph Matching (GM) scheme is introduced owing to its\nexcellent capability of excavating the instance and label relationship.\nSpecifically, we first construct an instance spatial graph and a label semantic\ngraph respectively, and then incorporate them into a constructed assignment\ngraph by connecting each instance to all labels. Subsequently, the graph\nnetwork block is adopted to aggregate and update all nodes and edges state on\nthe assignment graph to form structured representations for each instance and\nlabel. Our network finally derives a prediction score for each instance-label\ncorrespondence and optimizes such correspondence with a weighted cross-entropy\nloss. Extensive experiments conducted on various image datasets demonstrate the\nsuperiority of our proposed method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A model capturing the dynamics between virus and tumour cells in the context\nof oncolytic virotherapy is presented and analysed. The ability of the virus to\nbe internalised by uninfected cells is described by an infectivity parameter,\nwhich is inferred from available experimental data. The parameter is also able\nto describe the effects of changes in the tumour environment that affect viral\nuptake from tumour cells. Results show that when a virus is inoculated inside a\ngrowing tumour, strategies for enhancing infectivity do not lead to a complete\neradication of the tumour. Within typical times of experiments and treatments,\nwe observe the onset of oscillations, which always prevent a full destruction\nof the tumour mass. These findings are in good agreement with available\nlaboratory results. Further analysis shows why a fully successful therapy\ncannot exist for the proposed model and that care must be taken when designing\nand engineering viral vectors with enhanced features. In particular,\nbifurcation analysis reveals that creating longer lasting virus particles or\nusing strategies for reducing infected cell lifespan can cause unexpected and\nunwanted surges in the overall tumour load over time. Our findings suggest that\nvirotherapy alone seems unlikely to be effective in clinical settings unless\nadjuvant strategies are included.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a lightweight CPU network based on the MKLDNN acceleration\nstrategy, named PP-LCNet, which improves the performance of lightweight models\non multiple tasks. This paper lists technologies which can improve network\naccuracy while the latency is almost constant. With these improvements, the\naccuracy of PP-LCNet can greatly surpass the previous network structure with\nthe same inference time for classification. As shown in Figure 1, it\noutperforms the most state-of-the-art models. And for downstream tasks of\ncomputer vision, it also performs very well, such as object detection, semantic\nsegmentation, etc. All our experiments are implemented based on PaddlePaddle.\nCode and pretrained models are available at PaddleClas.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Automatic meter reading technology is not yet widespread. Gas, electricity,\nor water accumulation meters reading is mostly done manually on-site either by\nan operator or by the homeowner. In some countries, the operator takes a\npicture as reading proof to confirm the reading by checking offline with\nanother operator and/or using it as evidence in case of conflicts or\ncomplaints. The whole process is time-consuming, expensive, and prone to\nerrors. Automation can optimize and facilitate such labor-intensive and human\nerror-prone processes. With the recent advances in the fields of artificial\nintelligence and computer vision, automatic meter reading systems are becoming\nmore viable than ever. Motivated by the recent advances in the field of\nartificial intelligence and inspired by open-source open-access initiatives in\nthe research community, we introduce a novel large benchmark dataset of\nreal-life gas meter images, named the NRC-GAMMA dataset. The data were\ncollected from an Itron 400A diaphragm gas meter on January 20, 2020, between\n00:05 am and 11:59 pm. We employed a systematic approach to label the images,\nvalidate the labellings, and assure the quality of the annotations. The dataset\ncontains 28,883 images of the entire gas meter along with 57,766 cropped images\nof the left and the right dial displays. We hope the NRC-GAMMA dataset helps\nthe research community to design and implement accurate, innovative,\nintelligent, and reproducible automatic gas meter reading solutions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Background: The evolution of symptoms over time is at the heart of\nunderstanding and treating mental disorders. However, a principled,\nquantitative framework explaining symptom dynamics remains elusive. Here, we\npropose a Network Control Theory of Psychopathology allowing us to formally\nderive a theoretical control energy which we hypothesize quantifies resistance\nto future symptom improvement in Major Depressive Disorder (MDD). We test this\nhypothesis and investigate the relation to genetic and environmental risk as\nwell as resilience.\n  Methods: We modelled longitudinal symptom-network dynamics derived from\nN=2,059 Beck Depression Inventory measurements acquired over a median of 134\ndays in a sample of N=109 patients suffering from MDD. We quantified the\ntheoretical energy required for each patient and time-point to reach a\nsymptom-free state given individual symptom-network topology (E 0 ) and 1)\ntested if E 0 predicts future symptom improvement and 2) whether this\nrelationship is moderated by Polygenic Risk Scores (PRS) of mental disorders,\nchildhood maltreatment experience, and self-reported resilience.\n  Outcomes: We show that E 0 indeed predicts symptom reduction at the next\nmeasurement and reveal that this coupling between E 0 and future symptom change\nincreases with higher genetic risk and childhood maltreatment while it\ndecreases with resilience.\n  Interpretation: Our study provides a mechanistic framework capable of\npredicting future symptom improvement based on individual symptom-network\ntopology and clarifies the role of genetic and environmental risk as well as\nresilience. Our control-theoretic framework makes testable, quantitative\npredictions for individual therapeutic response and provides a starting-point\nfor the theory-driven design of personalized interventions.\n  Funding: German Research Foundation and Interdisciplinary Centre for Clinical\nResearch, M\\\"unster\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Relatively dominated representations give a common generalization of\ngeometrically finiteness in rank one on the one hand, and the Anosov condition\nwhich serves as a higher-rank analogue of convex cocompactness on the other.\nThis note proves three results about these representations.\n  Firstly, we remove the technical assumption of quadratic gaps involved in the\noriginal definition. Secondly, we give a characterization using eigenvalue\ngaps, providing a relative analogue of a result of Kassel-Potrie for Anosov\nrepresentations. Thirdly, we formulate characterizations in terms of singular\nvalue or eigenvalue gaps combined with limit maps, in the spirit of\nGu\\'eritaud-Guichard-Kassel-Wienhard for Anosov representations, and use them\nto show that inclusion representations of certain groups playing weak ping-pong\nand positive representations in the sense of Fock-Goncharov are relatively\ndominated.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We highlight the fact that in undergraduate calculus, the number pi is\ndefined via the length of the circle, the length of the circle is defined as a\ncertain value of an inverse trigonometric function, and this value is defined\nvia pi, thus forming a circular definition. We present a way in which this\nerror can be rectified. We explain that this error is instructive and can be\nused as an enlightening topic for discussing different approaches to\nmathematics with undergraduate students.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recommendation is the task of ranking items (e.g. movies or products)\naccording to individual user needs. Current systems rely on collaborative\nfiltering and content-based techniques, which both require structured training\ndata. We propose a framework for recommendation with off-the-shelf pretrained\nlanguage models (LM) that only used unstructured text corpora as training data.\nIf a user $u$ liked \\textit{Matrix} and \\textit{Inception}, we construct a\ntextual prompt, e.g. \\textit{\"Movies like Matrix, Inception, ${<}m{>}$\"} to\nestimate the affinity between $u$ and $m$ with LM likelihood. We motivate our\nidea with a corpus analysis, evaluate several prompt structures, and we compare\nLM-based recommendation with standard matrix factorization trained on different\ndata regimes. The code for our experiments is publicly available\n(https://colab.research.google.com/drive/1f1mlZ-FGaLGdo5rPzxf3vemKllbh2esT?usp=sharing).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Rate-induced tipping (R-tipping) occurs when time-variation of input\nparameters of a dynamical system interacts with system timescales to give\ngenuine nonautonomous instabilities. Such instabilities appear as the input\nvaries at some critical rates and cannot, in general, be understood in terms of\nautonomous bifurcations in the frozen system with a fixed-in-time input.\n  This paper develops an accessible mathematical framework for R-tipping in\nmultidimensional nonautonomous dynamical systems with an autonomous future\nlimit. We focus on R-tipping via loss of tracking of base attractors that are\nequilibria in the frozen system, due to crossing what we call regular\nthresholds. These thresholds are associated with regular edge states: compact\nhyperbolic invariant sets with one unstable direction and orientable stable\nmanifold, that lie on a basin boundary in the frozen system. We define\nR-tipping and critical rates for the nonautonomous system in terms of special\nsolutions that limit to a compact invariant set of the future limit system that\nis not an attractor. We focus on the case when the limit set is a regular edge\nstate, which we call the regular R-tipping edge state that anchors the\nassociated regular R-tipping threshold at infinity. We introduce the concept of\nedge tails to rigorously classify R-tipping into reversible, irreversible, and\ndegenerate cases.\n  The main idea is to compactify the problem and use regular edge states of the\nfuture limit system to analyse R-tipping in the nonautonomous system. This\nallows us to give sufficient conditions for the occurrence of R-tipping in\nterms of easily testable properties of the frozen system and input variation,\nand necessary and sufficient conditions for the occurrence of reversible and\nirreversible R-tipping in terms of computationally verifiable (heteroclinic)\nconnections to regular R-tipping edge states in the compactified system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Advanced persistent threat (APT) is widely acknowledged to be the most\nsophisticated and potent class of security threat. APT refers to knowledgeable\nhuman attackers that are organized, highly sophisticated and motivated to\nachieve their objectives against a targeted organization(s) over a prolonged\nperiod. Strategically-motivated APTs or S-APTs are distinct in that they draw\ntheir objectives from the broader strategic agenda of third parties such as\ncriminal syndicates, nation-states, and rival corporations. In this paper we\nreview the use of the term - Advanced Persistent Threat - and present a formal\ndefinition. We then draw on military science, the science of organized\nconflict, for a theoretical basis to develop a rigorous and holistic model of\nthe stages of an APT operation which we subsequently use to explain how S-APTs\nexecute their strategically motivated operations using tactics, techniques and\nprocedures. Finally, we present a general disinformation model, derived from\nsituation awareness theory, and explain how disinformation can be used to\nattack the situation awareness and decision making of not only S-APT operators,\nbut also the entities that back them.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum computing can become scalable through error correction, but logical\nerror rates only decrease with system size when physical errors are\nsufficiently uncorrelated. During computation, unused high energy levels of the\nqubits can become excited, creating leakage states that are long-lived and\nmobile. Particularly for superconducting transmon qubits, this leakage opens a\npath to errors that are correlated in space and time. Here, we report a reset\nprotocol that returns a qubit to the ground state from all relevant higher\nlevel states. We test its performance with the bit-flip stabilizer code, a\nsimplified version of the surface code for quantum error correction. We\ninvestigate the accumulation and dynamics of leakage during error correction.\nUsing this protocol, we find lower rates of logical errors and an improved\nscaling and stability of error suppression with increasing qubit number. This\ndemonstration provides a key step on the path towards scalable quantum\ncomputing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Generative adversarial networks (GAN) are a widely used class of deep\ngenerative models, but their minimax training dynamics are not understood very\nwell. In this work, we show that GANs with a 2-layer infinite-width generator\nand a 2-layer finite-width discriminator trained with stochastic gradient\nascent-descent have no spurious stationary points. We then show that when the\nwidth of the generator is finite but wide, there are no spurious stationary\npoints within a ball whose radius becomes arbitrarily large (to cover the\nentire parameter space) as the width goes to infinity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $F_{a_1,\\dots,a_k}$ be a graph consisting of $k$ cycles of odd length\n$2a_1+1,\\dots, 2a_k+1$, respectively which intersect in exactly a common\nvertex, where $k\\geq1$ and $a_1\\ge a_2\\ge \\cdots\\ge a_k\\ge 1$. In this paper,\nwe present a sharp upper bound for the signless Laplacian spectral radius of\nall $F_{a_1,\\dots,a_k}$-free graphs and characterize all extremal graphs which\nattain the bound. The stability methods and structure of graphs associated with\nthe eigenvalue are adapted for the proof.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The rapid development in visual crowd analysis shows a trend to count people\nby positioning or even detecting, rather than simply summing a density map. It\nalso enlightens us back to the essence of the field, detection to count, which\ncan give more abundant crowd information and has more practical applications.\nHowever, some recent work on crowd localization and detection has two\nlimitations: 1) The typical detection methods can not handle the dense crowds\nand a large variation in scale; 2) The density map heuristic methods suffer\nfrom performance deficiency in position and box prediction, especially in high\ndensity or large-size crowds. In this paper, we devise a tailored baseline for\ndense crowds location, detection, and counting from a new perspective, named as\nLDC-Net for convenience, which has the following features: 1) A strong but\nminimalist paradigm to detect objects by only predicting a location map and a\nsize map, which endows an ability to detect in a scene with any capacity ($0\n\\sim 10,000+$ persons); 2) Excellent cross-scale ability in facing a large\nvariation, such as the head ranging in $0 \\sim 100,000+$ pixels; 3) Achieve\nsuperior performance in location and box prediction tasks, as well as a\ncompetitive counting performance compared with the density-based methods.\nFinally, the source code and pre-trained models will be released.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider quantum stochastic processes and discuss a level 2.5 large\ndeviation formalism providing an explicit and complete characterisation of\nfluctuations of time-averaged quantities, in the large-time limit. We analyse\ntwo classes of quantum stochastic dynamics, within this framework. The first\nclass consists of the quantum jump trajectories related to photon detection;\nthe second is quantum state diffusion related to homodyne detection. For both\nprocesses, we present the level 2.5 functional starting from the corresponding\nquantum stochastic Schr\\\"odinger equation and we discuss connections of these\nfunctionals to optimal control theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Medical image acquisition is often intervented by unwanted noise that\ncorrupts the information content. This paper introduces an unsupervised medical\nimage denoising technique that learns noise characteristics from the available\nimages and constructs denoised images. It comprises of two blocks of data\nprocessing, viz., patch-based dictionaries that indirectly learn the noise and\nresidual learning (RL) that directly learns the noise. The model is generalized\nto account for both 2D and 3D images considering different medical imaging\ninstruments. The images are considered one-by-one from the stack of MRI/CT\nimages as well as the entire stack is considered, and decomposed into\noverlapping image/volume patches. These patches are given to the patch-based\ndictionary learning to learn noise characteristics via sparse representation\nwhile given to the RL part to directly learn the noise properties. K-singular\nvalue decomposition (K-SVD) algorithm for sparse representation is used for\ntraining patch-based dictionaries. On the other hand, residue in the patches is\ntrained using the proposed deep residue network. Iterating on these two parts,\nan optimum noise characterization for each image/volume patch is captured and\nin turn it is subtracted from the available respective image/volume patch. The\nobtained denoised image/volume patches are finally assembled to a denoised\nimage or 3D stack. We provide an analysis of the proposed approach with other\napproaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer\nand the comparative results show that the proposed algorithm preserves the\ncritical information in the images as well as improves the visual quality of\nthe images.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We strengthen some results in \\'etale (and real \\'etale) motivic stable\nhomotopy theory, by eliminating finiteness hypotheses, additional localizations\nand/or extending to spectra from HZ-modules.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The optical properties of monolayer transition metal dichalcogenides are\ndominated by tightly-bound excitons. They form at distinct valleys in\nreciprocal space, and can interact via the valley-exchange coupling, modifying\ntheir dispersion considerably. Here, we predict that angle-resolved\nphotoluminescence can be used to probe the changes of the excitonic dispersion.\nThe exchange-coupling leads to a unique angle dependence of the emission\nintensity for both circularly and linearly-polarised light. We show that these\nemission characteristics can be strongly tuned by an external magnetic field\ndue to the valley-specific Zeeman-shift. We propose that angle-dependent\nphotoluminescence measurements involving both circular and linear optical\npolarisation as well as magnetic fields should act as strong verification of\nthe role of valley-exchange coupling on excitonic dispersionand its signatures\nin optical spectra\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Information management has enter a completely new era, quantum era. However,\nthere exists a lack of sufficient theory to extract truly useful quantum\ninformation and transfer it to a form which is intuitive and straightforward\nfor decision making. Therefore, based on the quantum model of mass function, a\nfortified dual check system is proposed to ensure the judgment generated\nretains enough high accuracy. Moreover, considering the situations in real\nlife, everything takes place in an observable time interval, then the concept\nof time interval is introduced into the frame of the check system. The proposed\nmodel is very helpful in disposing uncertain quantum information in this paper.\nAnd some applications are provided to verify the rationality and correctness of\nthe proposed method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Long-lived particles have become a new frontier in the exploration of physics\nbeyond the Standard Model. In this paper, we present the implementation of four\ntypes of long-lived particle searches, viz. displaced leptons, disappearing\ntrack, displaced vertex (together with muons or with missing energy), and heavy\ncharged tracks. These four categories cover the signatures of a large range of\nphysics models. We illustrate their potential for exclusion and discuss their\nmutual overlaps in mass-lifetime space for two simple phenomenological models\ninvolving either a U(1)-charged or a coloured scalar.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Vision-based semantic segmentation of waterbodies and nearby related objects\nprovides important information for managing water resources and handling\nflooding emergency. However, the lack of large-scale labeled training and\ntesting datasets for water-related categories prevents researchers from\nstudying water-related issues in the computer vision field. To tackle this\nproblem, we present ATLANTIS, a new benchmark for semantic segmentation of\nwaterbodies and related objects. ATLANTIS consists of 5,195 images of\nwaterbodies, as well as high quality pixel-level manual annotations of 56\nclasses of objects, including 17 classes of man-made objects, 18 classes of\nnatural objects and 21 general classes. We analyze ATLANTIS in detail and\nevaluate several state-of-the-art semantic segmentation networks on our\nbenchmark. In addition, a novel deep neural network, AQUANet, is developed for\nwaterbody semantic segmentation by processing the aquatic and non-aquatic\nregions in two different paths. AQUANet also incorporates low-level feature\nmodulation and cross-path modulation for enhancing feature representation.\nExperimental results show that the proposed AQUANet outperforms other\nstate-of-the-art semantic segmentation networks on ATLANTIS. We claim that\nATLANTIS is the largest waterbody image dataset for semantic segmentation\nproviding a wide range of water and water-related classes and it will benefit\nresearchers of both computer vision and water resources engineering.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Making predictions and quantifying their uncertainty when the input data is\nsequential is a fundamental learning challenge, recently attracting increasing\nattention. We develop SigGPDE, a new scalable sparse variational inference\nframework for Gaussian Processes (GPs) on sequential data. Our contribution is\ntwofold. First, we construct inducing variables underpinning the sparse\napproximation so that the resulting evidence lower bound (ELBO) does not\nrequire any matrix inversion. Second, we show that the gradients of the GP\nsignature kernel are solutions of a hyperbolic partial differential equation\n(PDE). This theoretical insight allows us to build an efficient\nback-propagation algorithm to optimize the ELBO. We showcase the significant\ncomputational gains of SigGPDE compared to existing methods, while achieving\nstate-of-the-art performance for classification tasks on large datasets of up\nto 1 million multivariate time series.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present CrossSum, a large-scale cross-lingual abstractive summarization\ndataset comprising 1.7 million article-summary samples in 1500+ language pairs.\nWe create CrossSum by aligning identical articles written in different\nlanguages via cross-lingual retrieval from a multilingual summarization\ndataset. We propose a multi-stage data sampling algorithm to effectively train\na cross-lingual summarization model capable of summarizing an article in any\ntarget language. We also propose LaSE, a new metric for automatically\nevaluating model-generated summaries and showing a strong correlation with\nROUGE. Performance on ROUGE and LaSE indicate that pretrained models fine-tuned\non CrossSum consistently outperform baseline models, even when the source and\ntarget language pairs are linguistically distant. To the best of our knowledge,\nCrossSum is the largest cross-lingual summarization dataset and the first-ever\nthat does not rely solely on English as the pivot language. We are releasing\nthe dataset, alignment and training scripts, and the models to spur future\nresearch on cross-lingual abstractive summarization. The resources can be found\nat https://github.com/csebuetnlp/CrossSum.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We construct a weak categorification of the quantum toroidal algebra action\non the Grothendieck group of moduli space of stable (or framed) sheaves over an\nalgebraic surface, which is constructed by Schiffmann-Vasserot and Negu\\c{t}.\nThe new ingredient is two intersection-theoretic descriptions of the quadruple\nmoduli space of stable sheaves.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Accurate and efficient valuation of property is of utmost importance in a\nvariety of settings, such as when securing mortgage finance to purchase a\nproperty, or where residential property taxes are set as a percentage of a\nproperty's resale value. Internationally, resale based property taxes are most\ncommon due to ease of implementation and the difficulty of establishing site\nvalues. In an Irish context, property valuations are currently based on\ncomparison to recently sold neighbouring properties, however, this approach is\nlimited by low property turnover. National property taxes based on property\nvalue, as opposed to site value, also act as a disincentive to improvement\nworks due to the ensuing increased tax burden. In this article we develop a\nspatial hedonic regression model to separate the spatial and non-spatial\ncontributions of property features to resale value. We mitigate the issue of\nlow property turnover through geographic correlation, borrowing information\nacross multiple property types and finishes. We investigate the impact of\naddress mislabelling on predictive performance, where vendors erroneously\nsupply a more affluent postcode, and evaluate the contribution of improvement\nworks to increased values. Our flexible geo-spatial model outperforms all\ncompetitors across a number of different evaluation metrics, including the\naccuracy of both price prediction and associated uncertainty intervals. While\nour models are applied in an Irish context, the ability to accurately value\nproperties in markets with low property turnover and to quantify the value\ncontributions of specific property features has widespread application. The\nability to separate spatial and non-spatial contributions to a property's value\nalso provides an avenue to site-value based property taxes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article, we derive first-order necessary optimality conditions for a\nconstrained optimal control problem formulated in the Wasserstein space of\nprobability measures. To this end, we introduce a new notion of localised\nmetric subdifferential for compactly supported probability measures, and\ninvestigate the intrinsic linearised Cauchy problems associated to non-local\ncontinuity equations. In particular, we show that when the velocity\nperturbations belong to the tangent cone to the convexification of the set of\nadmissible velocities, the solutions of these linearised problems are tangent\nto the solution set of the corresponding continuity inclusion. We then make use\nof these novel concepts to provide a synthetic and geometric proof of the\ncelebrated Pontryagin Maximum Principle for an optimal control problem with\ninequality final-point constraints. In addition, we propose sufficient\nconditions ensuring the normality of the maximum principle.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Automatically translating images to texts involves image scene understanding\nand language modeling. In this paper, we propose a novel model, termed\nRefineCap, that refines the output vocabulary of the language decoder using\ndecoder-guided visual semantics, and implicitly learns the mapping between\nvisual tag words and images. The proposed Visual-Concept Refinement method can\nallow the generator to attend to semantic details in the image, thereby\ngenerating more semantically descriptive captions. Our model achieves superior\nperformance on the MS-COCO dataset in comparison with previous visual-concept\nbased models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In bidisperse particle mixtures varying in size or density alone, large\nparticles rise (driven by percolation) and heavy particles sink (driven by\nbuoyancy). When the two particle species differ from each other in both size\nand density, the two segregation mechanisms either enhance (large/light and\nsmall/heavy) or oppose (large/heavy and small/light) each other. In the latter\ncase, an equilibrium condition exists in which the two segregation mechanisms\nbalance and the particles no longer segregate. This leads to a methodology to\ndesign non-segregating particle mixtures by specifying particle size ratio,\ndensity ratio, and mixture concentration to achieve the equilibrium condition.\nUsing DEM simulations of quasi-2D bounded heap flow, we show that segregation\nis significantly reduced for particle mixtures near the equilibrium condition.\nIn addition, the rise-sink transition for a range of particle size and density\nratios matches the combined size and density segregation model predictions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the pair production of the long-lived mediator particles from the\ndecay of the SM Higgs boson and their subsequent decay into standard model\nparticles. We compute the projected sensitivity, both model-independently and\nwith a minimal model, of using the muon spectrometer of the CMS detector at the\nHL-LHC experiment for ggF, VBF, and Vh production modes of the Higgs boson and\nvarious decay modes of the mediator particle, along with dedicated detectors\nfor LLP searches like CODEX-b and MATHUSLA. Subsequently, we study the\nimprovement with the FCC-hh detector at the 100\\,TeV collider experiment for\nsuch long-lived mediators, again focusing on the muon spectrometer. We propose\ndedicated LLP detector designs for the 100\\,TeV collider experiment, DELIGHT\n(\\textbf{De}tector for \\textbf{l}ong-l\\textbf{i}ved particles at hi\\textbf{gh}\nenergy of 100\\,\\textbf{T}eV), and study their sensitivities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recently a breakthrough has been achieved in laser-spectroscopic studies of\nshort-lived radioactive compounds with the first measurements of the radium\nmonofluoride molecule (RaF) UV/vis spectra. We report results from high\naccuracy \\emph{ab initio} calculations of the RaF electronic structure for\nground and low-lying excited electronic states. Two different methods agree\nexcellently with experimental excitation energies from the electronic ground\nstate to the $^2\\Pi_{1/2}$ and $^2\\Pi_{3/2}$ states, but lead consistently and\nunambiguously to deviations from experimental-based adiabatic transition energy\nestimates for the $^2\\Sigma_{1/2}$ excited electronic state and show that more\nmeasurements are needed to clarify spectroscopic assignment of the $^2\\Delta$\nstates.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the logarithmic Schr{\\\"o}dinger equations with damping, also\ncalled Schr{\\\"o}dinger-Langevin equation. On a periodic domain, this equation\npossesses plane wave solutions that are explicit. We prove that these solutions\nare asymptotically stable in Sobolev regularity. In the case without damping,\nwe prove that for almost all value of the nonlinear parameter, these solutions\nare stable in high Sobolev regularity for arbitrary long times when the\nsolution is close to a plane wave. We also show and discuss numerical\nexperiments illustrating our results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Terahertz (THz) Time domain spectroscopy (THz-TDS) is a broadband\nspectroscopic technique spreading its uses in multiple fields: in science from\nmaterial science to biology, in industry where it measures the thickness of a\npaint layer during the painting operation. Using such practical commercial\napparatus with broad spectrum for gas spectroscopy could be a major asset for\nair quality monitoring and tracking of atmospheric composition. However, gas\nspectroscopy needs high resolution and the usual approach in THz-TDS, where the\nrecorded time trace is Fourier transform, suffers from resolution limitation\ndue to the size of the delay line in the system. In this letter, we introduce\nthe concept of constraint reconstruction for super-resolution spectroscopy\nbased on the modeling of the spectroscopic lines in a sparse spectrum. Light\nmolecule gas typically shows sparse and narrow lines on a broad spectrum and we\npropose an algorithm reconstructing these lines with a resolution improvement\nof 10 the ultimate resolution reachable by the apparatus. We envision the\nproposed technique to lead to broadband, selective, rapid and cheap gas\nmonitoring applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we introduce statistical testing under distributional shifts.\nWe are interested in the hypothesis $P^* \\in H_0$ for a target distribution\n$P^*$, but observe data from a different distribution $Q^*$. We assume that\n$P^*$ is related to $Q^*$ through a known shift $\\tau$ and formally introduce\nhypothesis testing in this setting. We propose a general testing procedure that\nfirst resamples from the observed data to construct an auxiliary data set and\nthen applies an existing test in the target domain. We prove that if the size\nof the resample is at most $o(\\sqrt{n})$ and the resampling weights are\nwell-behaved, this procedure inherits the pointwise asymptotic level and power\nfrom the target test. If the map $\\tau$ is estimated from data, we can maintain\nthe above guarantees under mild conditions if the estimation works sufficiently\nwell. We further extend our results to finite sample level, uniform asymptotic\nlevel and a different resampling scheme. Testing under distributional shifts\nallows us to tackle a diverse set of problems. We argue that it may prove\nuseful in reinforcement learning and covariate shift, we show how it reduces\nconditional to unconditional independence testing and we provide example\napplications in causal inference.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We provide a new interpretation of the Mazur-Tate Conjecture and then use it\nto obtain the first (unconditional) theoretical evidence in support of the\nconjecture for elliptic curves of strictly positive rank.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that spectral walls are common phenomena in the dynamics of kinks in\n(1+1) dimensions. They occur in models based on two or more scalar fields with\na nonempty Bogomol'nyi-Prasam-Sommerfield (BPS) sector, hosting two zero modes,\nwhere they are one of the main factors governing the soliton dynamics. We also\nshow that spectral walls appear as singularities of the dynamical vibrational\nmoduli space.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent large-scale spectropolarimetric surveys have established that a small\nbut significant percentage of massive stars host stable, surface dipolar\nmagnetic fields with strengths on the order of kG. These fields channel the\ndense, radiatively driven stellar wind into circumstellar magnetospheres, whose\ndensity and velocity structure can be probed using ultraviolet (UV)\nspectroscopy of wind-sensitive resonance lines. Coupled with appropriate\nmagnetosphere models, UV spectroscopy provides a valuable way to investigate\nthe wind-field interaction, and can yield quantitative estimates of the wind\nparameters of magnetic massive stars. We report a systematic investigation of\nthe formation of UV resonance lines in slowly rotating magnetic massive stars\nwith dynamical magnetospheres. We pair the Analytic Dynamical Magnetosphere\n(ADM) formalism with a simplified radiative transfer technique to produce\nsynthetic UV line profiles. Using a grid of models, we examine the effect of\nmagnetosphere size, the line strength parameter, and the cooling parameter on\nthe structure and modulation of the line profile. We find that magnetic massive\nstars uniquely exhibit redshifted absorption at most viewing angles and\nmagnetosphere sizes, and that significant changes to the shape and variation of\nthe line profile with varying line strengths can be explained by examining the\nindividual wind components described in the ADM formalism. Finally, we show\nthat the cooling parameter has a negligible effect on the line profiles.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The ground state of the energy super-critical Gross--Pitaevskii equation with\na harmonic potential converges in the energy space to the singular solution in\nthe limit of large amplitudes. The ground state can be represented by a\nsolution curve which has either oscillatory or monotone behavior, depending on\nthe dimension of the system and the power of the focusing nonlinearity. We\naddress here the monotone case for the cubic nonlinearity in the spatial\ndimensions $d \\geq 13$. By using the shooting method for the radial\nSchr\\\"{o}dinger operators, we prove that the Morse index of the ground state is\nfinite and is independent of the (large) amplitude. It is equal to the Morse\nindex of the limiting singular solution, which can be computed from numerical\napproximations. The numerical results suggest that the Morse index of the\nground state is one and that it is stable in the time evolution of the cubic\nGross--Pitaevskii equation in dimensions $d \\geq 13$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We establish a new lower bound for Mathieu's series and present a new\nderivation of its expansions in terms of Riemann Zeta functions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the active flow around isolated defects and the self-propulsion\nvelocity of $+1/2$ defects in an active nematic film with both viscous\ndissipation (with viscosity $\\eta$) and frictional damping $\\Gamma$ with a\nsubstrate. The interplay between these two dissipation mechanisms is controlled\nby the hydrodynamic dissipation length $\\ell_d=\\sqrt{\\eta/\\Gamma}$ that screens\nthe flows. For an isolated defect, in the absence of screening from other\ndefects, the size of the vortical flows around the defect is controlled by the\nsystem size $R$. In the presence of friction that leads to a finite value of\n$\\ell_d$, the vorticity field decays to zero on the lengthscales larger than\n$\\ell_d$. We show that the self-propulsion velocity of $+1/2$ defects grows\nwith $R$ in small systems where $R<\\ell_d$, while in the infinite system limit\nor when $R\\gg \\ell_d$, it approaches a constant value determined by $\\ell_d$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present timing and spectral results of 2018 outburst of Cepheus X-4,\nobserved twice by AstroSat at luminosity of $2.04 \\times 10^{37}$ erg s$^{-1}$\nand $1.02 \\times 10^{37}$ erg s$^{-1}$ respectively. The light curves showed\nstrong pulsation and co-related X-ray intensity variation in SXT (0.5--8.0 keV)\nand LAXPC (3--60 keV) energy bands. Spin-period and spin-down rate of the\npulsar were determined from two observations as $65.35080\\pm0.00014$ s ,\n$(-2.10\\pm0.8)\\times10^{-12}$ Hz s$^{-1}$ at an epoch MJD 58301.61850 and\n$65.35290\\pm0.00017$ s, $(-1.6\\pm0.8)\\times10^{-12}$ Hz s$^{-1}$ for an epoch\nMJD 58307.40211. Pulse-shape studies with AstroSat showed energy and intensity\ndependent variations. The pulsar showed an overall continuous spin-down, over\n30 years at an average-rate of $(-2.455\\pm0.004)\\times10^{-14}$ Hz s$^{-1}$,\nattributed to propeller-effect in the subsonic-regime of the pulsar, in\naddition to variations during its outburst activities. Spectra between 0.7--55\nkeV energy band were well fitted by two continuum models, an absorbed\ncompTT-model and an absorbed power-law with a Fermi-Dirac cutoff (FD-cutoff)\nmodel with a black-body. These were combined with an iron-emission line and a\ncyclotron absorption line. The prominent cyclotron resonance scattering\nfeatures with a peak absorption energy of $30.48^{+0.33}_{-0.34}$ keV and\n$30.68^{+0.45}_{-0.44}$ keV for FD-cutoff-model and $30.46^{+0.32}_{-0.28}$ keV\nand $30.30^{+0.36}_{-0.34}$ keV for compTT-model were detected during two\nAstroSat observations. These when compared with earlier results, showed long\nterm stability of its average value of $30.23 \\pm 0.22$ keV. The pulsar showed\npulse-phase as well as luminosity dependent variations in cyclotron-line energy\nand width and in plasma optical-depth of its spectral continuum.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Iridates have been providing a fertile ground for studying emergent phases of\nmatter that arise from delicate interplay of various fundamental interactions\nwith approximate energy scale. Among these highly focused quantum materials,\nperovskite Sr2IrO4 belonging to the Ruddlesden-Popper series stands out and has\nbeen intensively addressed in the last decade, since it hosts a novel Jeff =\n1/2 state which is a profound manifestation of strong spin-orbit coupling.\nMoreover, the Jeff = 1/2 state represents a rare example of iridates that has\nbeen better understood both theoretically and experimentally. In this progress\nreport, we take Sr2IrO4 as an example to overview the recent advances of the\nJeff = 1/2 state in two aspects: materials fundamentals and functionality\npotentials. In the fundamentals part, we first illustrate basic issues for the\nlayered canted antiferromagnetic order of the Jeff = 1/2 magnetic moments in\nSr2IrO4, and then review the progress of the antiferromagnetic order modulation\nthrough diverse routes. Subsequently, for the functionality potentials,\nfascinating properties such as atomic-scale giant magnetoresistance,\nanisotropic magnetoresistance, and nonvolatile memory, will be addressed. This\nreport will be concluded with our prospected remarks and outlooks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Chinese word segmentation (CWS) is the basic of Chinese natural language\nprocessing (NLP). The quality of word segmentation will directly affect the\nrest of NLP tasks. Recently, with the artificial intelligence tide rising\nagain, Long Short-Term Memory (LSTM) neural network, as one of easily modeling\nin sequence, has been widely utilized in various kinds of NLP tasks, and\nfunctions well. Attention mechanism is an ingenious method to solve the memory\ncompression problem on LSTM. Furthermore, inspired by the powerful abilities of\nbidirectional LSTM models for modeling sequence and CRF model for decoding, we\npropose a Bidirectional LSTM-CRF Attention-based Model in this paper.\nExperiments on PKU and MSRA benchmark datasets show that our model performs\nbetter than the baseline methods modeling by other neural networks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $R$ be a commutative ring. An $R$-module $M$ is called a semi-regular\n$w$-flat module if Tor$_1^R(R/I,M)$ is GV-torsion for any finitely generated\nsemi-regular ideal $I$. In this article, we showed that the class of\nsemi-regular $w$-flat modules is a covering class. Moreover, we introduce the\nsemi-regular $w$-flat dimensions of $R$-modules and the $sr$-$w$-weak global\ndimensions of the commutative ring $R$. Utilizing these notions, we give some\nhomological characterizations of WQ-rings and $Q_0$-PvMRs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Many clustering algorithms are guided by certain cost functions such as the\nwidely-used $k$-means cost. These algorithms divide data points into clusters\nwith often complicated boundaries, creating difficulties in explaining the\nclustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and\nRashtchian (ICML 2020) introduced explainable clustering, where the cluster\nboundaries are axis-parallel hyperplanes and the clustering is obtained by\napplying a decision tree to the data. The central question here is: how much\ndoes the explainability constraint increase the value of the cost function?\n  Given $d$-dimensional data points, we show an efficient algorithm that finds\nan explainable clustering whose $k$-means cost is at most $k^{1 -\n2/d}\\,\\mathrm{poly}(d\\log k)$ times the minimum cost achievable by a clustering\nwithout the explainability constraint, assuming $k,d\\ge 2$. Taking the minimum\nof this bound and the $k\\,\\mathrm{polylog} (k)$ bound in independent work by\nMakarychev-Shan (ICML 2021), Gamlath-Jia-Polak-Svensson (2021), or\nEsfandiari-Mirrokni-Narayanan (2021), we get an improved bound of $k^{1 -\n2/d}\\,\\mathrm{polylog}(k)$, which we show is optimal for every choice of\n$k,d\\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in particular,\nwe show an $O(\\log k\\log\\log k)$ bound, improving near-exponentially over the\nprevious best bound of $O(k\\log k)$ by Laber and Murtinho (ICML 2021).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  MOLLER is a future experiment designed to measure parity violation in Moller\nscattering to extremely high precision. MOLLER will measure the right-left\nscattering differential cross-section parity-violating asymmetry APV , in the\nelastic scattering of polarized electrons off an unpolarized LH2 target to\nextreme ppb precision. To make this measurement, the polarized electron source,\ngenerated with a circularly polarized laser beam, must have the ability to\nswitch quickly between right and left helicity polarization states. The\npolarized source must also maintain minimal right-left helicity correlated beam\nasymmetries, including energy changes, position changes, intensity changes, or\nspot-size changes. These requirements can be met with appropriate choice and\ndesign of the Pockels cell used to generate the circularly polarized light.\nRubidium Titanyl Phosphate (RTP) has been used in recent years for ultra-fast\nPockels cell switches due to its lack of piezo-electric resonances at\nfrequencies up to several hundred MHz. However, crystal non-uniformity in this\nmaterial leads to poorer extinction ratios than in commonly used KD*P Pockels\ncells when used in hald-wave configuration. It leads to voltage dependent beam\nsteering when used in quarter-wave configuration. Here we present an innovative\nRTP Pockels cell design which uses electric field gradients to counteract\ncrystal non-uniformities and control beam steering down to the nm-level. We\ndemonstrate this RTP Pockels cell design is capable of producing precisely\ncontrolled polarized electron beam at Jefferson Laboratory, a national\naccelerator facility, for current experiments, including the recent PREX II\nmeasurement, as well as the future MOLLER experiment.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Browder (1960) proved that for every continuous function $F : X \\times Y \\to\nY$, where $X$ is the unit interval and $Y$ is a nonempty, convex, and compact\nsubset of $\\dR^n$, the set of fixed points of $F$, defined by $C_F := \\{ (x,y)\n\\in X \\times Y \\colon F(x,y)=y\\}$ has a connected component whose projection to\nthe first coordinate is $X$. We extend this result to the case where $X$ is a\nconnected and compact Hausdorff space.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Images captured under extremely low light conditions are noise-limited, which\ncan cause existing robotic vision algorithms to fail. In this paper we develop\nan image processing technique for aiding 3D reconstruction from images acquired\nin low light conditions. Our technique, based on burst photography, uses direct\nmethods for image registration within bursts of short exposure time images to\nimprove the robustness and accuracy of feature-based structure-from-motion\n(SfM). We demonstrate improved SfM performance in challenging light-constrained\nscenes, including quantitative evaluations that show improved feature\nperformance and camera pose estimates. Additionally, we show that our method\nconverges more frequently to correct reconstructions than the state-of-the-art.\nOur method is a significant step towards allowing robots to operate in low\nlight conditions, with potential applications to robots operating in\nenvironments such as underground mines and night time operation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the strong magnetic field limit for a nonlinear Iwatsuka-type model,\ni.e. a nonlinear Schr\\\"odinger equation in two spatial dimensions with a\nmagnetic vector potential that only depends on the $x$-coordinate. Using a\nhigh-frequency averaging technique, we show that this equation can be\neffectively described by a nonlocal nonlinear model, which is no longer\ndispersive. We also prove that, in this asymptotic regime, inhomogeneous\nnonlinearities are confined along the $y$-axis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Python type inference is challenging in practice. Due to its dynamic\nproperties and extensive dependencies on third-party libraries without type\nannotations, the performance of traditional static analysis techniques is\nlimited. Although semantics in source code can help manifest intended usage for\nvariables (thus help infer types), they are usually ignored by existing tools.\nIn this paper, we propose PYInfer, an end-to-end learning-based type inference\ntool that automatically generates type annotations for Python variables. The\nkey insight is that contextual code semantics is critical in inferring the type\nfor a variable. For each use of a variable, we collect a few tokens within its\ncontextual scope, and design a neural network to predict its type. One\nchallenge is that it is difficult to collect a high-quality human-labeled\ntraining dataset for this purpose. To address this issue, we apply an existing\nstatic analyzer to generate the ground truth for variables in source code.\n  Our main contribution is a novel approach to statically infer variable types\neffectively and efficiently. Formulating the type inference as a classification\nproblem, we can handle user-defined types and predict type probabilities for\neach variable. Our model achieves 91.2% accuracy on classifying 11 basic types\nin Python and 81.2% accuracy on classifying 500 most common types. Our results\nsubstantially outperform the state-of-the-art type annotators. Moreover,\nPYInfer achieves 5.2X more code coverage and is 187X faster than a\nstate-of-the-art learning-based tool. With similar time consumption, our model\nannotates 5X more variables than a state-of-the-art static analysis tool. Our\nmodel also outperforms a learning-based function-level annotator on annotating\ntypes for variables and function arguments. All our tools and datasets are\npublicly available to facilitate future research in this direction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Determining whether a dynamical system is integrable is generally a difficult\ntask which is currently done on a case by case basis requiring large human\ninput. Here we propose and test an automated method to search for the existence\nof relevant structures, the Lax pair and Lax connection respectively. By\nformulating this search as an optimization problem, we are able to identify\nappropriate structures via machine learning techniques. We test our method on\nstandard systems of classical integrability and find that we can single out\nsome integrable deformations of a system. Due to the ambiguity in defining a\nLax pair our algorithm identifies novel Lax pairs which can be easily verified\nanalytically.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A group with a geometric action on some hyperbolic space is necessarily word\nhyperbolic, but on the other hand every countable group acts (metrically)\nproperly by isometries on a locally finite hyperbolic graph. In this paper we\nconsider what happens when a group acts isometrically on a restricted class of\nhyperbolic spaces, for instance quasitrees. We obtain strong conclusions on the\ngroup structure if the action has a locally finite orbit, especially if the\ngroup is finitely generated.\n  We also look at group actions on finite products of quasitrees, where our\nactions may be by automorphisms or by isometries, including the Leary -\nMinasyan group.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the event of a black hole (BH) forming stellar collapse, the neutrino\nsignal should terminate abruptly at the moment of BH formation, after a phase\nof steady accretion. Since neutrinos are expected to reach Earth hours before\nthe electromagnetic signal, the combined detection of the neutrino burst\nthrough multiple neutrino telescopes could allow to promptly determine the\nangular location of a nearby stellar collapse in the sky with high precision.\nIn this paper, we contrast the triangulation pointing procedure that relies on\nthe rise time of the neutrino curve, often considered in the literature, to the\none that takes advantage of the cutoff of the neutrino curve at the moment of\nBH formation. By forecasting the neutrino signal expected in the IceCube\nNeutrino Observatory, Hyper-Kamiokande and DUNE, we devise a strategy to\noptimize the identification of the rise and cutoff time of the neutrino curve.\nWe show that the triangulation method developed by employing the end tail of\nthe neutrino curve allows to achieve at least one order of magnitude\nimprovement in the pointing precision for a galactic burst, while being\ninsensitive to the neutrino mixing scenario. The triangulation pointing method\nbased on the cutoff of the neutrino curve will also guarantee a better\nperformance for BH forming collapses occurring beyond our own Galaxy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this manuscript we show that the metric mean dimension of a free semigroup\naction satisfies three variational principles: (a) the first one is based on a\ndefinition of Shapira's entropy, introduced in \\cite{SH} for a singles dynamics\nand extended for a semigroup action in this note; (b) the second one treats\nabout a definition of Katok's entropy for a free semigroup action introduced in\n\\cite{CRV-IV}; (c) lastly we consider the local entropy function for a free\nsemigroup action and show that the metric mean dimension satisfies a\nvariational principle in terms of such function. Our results are inspired in\nthe ones obtained by \\cite{LT2019}, \\cite{VV}, \\cite{GS1} and \\cite{RX}.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider universal quantization with side information for Gaussian\nobservations, where the side information is a noisy version of the sender's\nobservation with noise variance unknown to the sender. In this paper, we\npropose a universally rate optimal and practical quantization scheme for all\nvalues of unknown noise variance. Our scheme uses Polar lattices from prior\nwork, and proceeds based on a structural decomposition of the underlying\nauxiliaries so that even when recovery fails in a round, the parties agree on a\ncommon \"reference point\" that is closer than the previous one. We also present\nthe finite blocklength analysis showing an sub-exponential convergence for\ndistortion and exponential convergence for rate. The overall complexity of our\nscheme is $O(N^2\\log^2 N)$ for any target distortion and fixed rate larger than\nthe rate-distortion bound.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Fields exhibit a variety of topological properties, like different\ntopological charges, when field space in the continuum is composed by more than\none topological sector. Lattice treatments usually encounter difficulties\ndescribing those properties. In this work, we show that by augmenting the usual\nlattice fields to include extra variables describing local topological\ninformation (more precisely, regarding homotopy), the topology of the space of\nfields in the continuum is faithfully reproduced in the lattice. We apply this\nextended lattice formulation to some simple models with non-trivial topological\ncharges, and we study their properties both analytically and via Monte Carlo\nsimulations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work we show that, given a linear map from a general operator space\ninto the dual of a C$^*$-algebra, its completely bounded norm is upper bounded\nby a universal constant times its $(1,cb)$-summing norm. This problem is\nmotivated by the study of quantum XOR games in the field of quantum information\ntheory. In particular, our results imply that for such games entangled\nstrategies cannot be arbitrarily better than those strategies using one-way\nclassical communication.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Automated commonsense reasoning is essential for building human-like AI\nsystems featuring, for example, explainable AI. Event Calculus (EC) is a family\nof formalisms that model commonsense reasoning with a sound, logical basis.\nPrevious attempts to mechanize reasoning using EC faced difficulties in the\ntreatment of the continuous change in dense domains (e.g., time and other\nphysical quantities), constraints among variables, default negation, and the\nuniform application of different inference methods, among others. We propose\nthe use of s(CASP), a query-driven, top-down execution model for Predicate\nAnswer Set Programming with Constraints, to model and reason using EC. We show\nhow EC scenarios can be naturally and directly encoded in s(CASP) and how it\nenables deductive and abductive reasoning tasks in domains featuring\nconstraints involving both dense time and dense fluents.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report the discovery of an extended very-high-energy (VHE) gamma-ray\nsource around the location of the middle-aged (207.8 kyr) pulsar PSR J0622+3749\nwith the Large High Altitude Air Shower Observatory (LHAASO). The source is\ndetected with a significance of $8.2\\sigma$ for $E>25$~TeV assuming a Gaussian\ntemplate. The best-fit location is (R.A.,\nDec.)$=(95^{\\circ}\\!.47\\pm0^{\\circ}\\!.11,\\,37^{\\circ}\\!.92 \\pm0^{\\circ}\\!.09)$,\nand the extension is $0^{\\circ}\\!.40\\pm0^{\\circ}\\!.07$. The energy spectrum can\nbe described by a power-law spectrum with an index of ${-2.92 \\pm 0.17_{\\rm\nstat} \\pm 0.02_{\\rm sys} }$. No clear extended multi-wavelength counterpart of\nthe LHAASO source has been found from the radio to sub-TeV bands. The LHAASO\nobservations are consistent with the scenario that VHE electrons escaped from\nthe pulsar, diffused in the interstellar medium, and scattered the interstellar\nradiation field. If interpreted as the pulsar halo scenario, the diffusion\ncoefficient, inferred for electrons with median energies of $\\sim160$~TeV, is\nconsistent with those obtained from the extended halos around Geminga and\nMonogem and much smaller than that derived from cosmic ray secondaries. The\nLHAASO discovery of this source thus likely enriches the class of so-called\npulsar halos and confirms that high-energy particles generally diffuse very\nslowly in the disturbed medium around pulsars.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Peer-review is a necessary and essential quality control step for scientific\npublications but lacks proper incentives. Indeed, the process, which is very\ncostly in terms of time and intellectual investment, not only is not\nremunerated by the journals but is also not openly recognized by the academic\ncommunity as a relevant scientific output for a researcher. Therefore,\nscientific dissemination is affected in timeliness, quality, and fairness.\nHere, to solve this issue, we propose a blockchain-based incentive system that\nrewards scientists for peer-reviewing other scientists' work and that builds up\ntrust and reputation. We designed a privacy-oriented protocol of smart\ncontracts called Ants-Review that allows authors to issue a bounty for open\nanonymous peer-reviews on Ethereum. If requirements are met, peer-reviews will\nbe accepted and paid by the approver proportionally to their assessed quality.\nTo promote ethical behavior and inclusiveness the system implements a gamified\nmechanism that allows the whole community to evaluate the peer-reviews and vote\nfor the best ones.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Data augmentation refers to a wide range of techniques for improving model\ngeneralization by augmenting training examples. Oftentimes such methods require\ndomain knowledge about the dataset at hand, spawning a plethora of recent\nliterature surrounding automated techniques for data augmentation. In this work\nwe apply one such method, bilevel optimization, to tackle the problem of graph\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\nmakes it the most effective augmenter for this classifier on the leaderboard.\nThis framework combines a GIN layer augmentation generator with a bias\ntransformation and outperforms the same classifier augmented using the\nstate-of-the-art FLAG augmentation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the sequential composite binary hypothesis testing problem in\nwhich one of the hypotheses is governed by a single distribution while the\nother is governed by a family of distributions whose parameters belong to a\nknown set $\\Gamma$. We would like to design a test to decide which hypothesis\nis in effect. Under the constraints that the probabilities that the length of\nthe test, a stopping time, exceeds $n$ are bounded by a certain threshold\n$\\epsilon$, we obtain certain fundamental limits on the asymptotic behavior of\nthe sequential test as $n$ tends to infinity. Assuming that $\\Gamma$ is a\nconvex and compact set, we obtain the set of all first-order error exponents\nfor the problem. We also prove a strong converse. Additionally, we obtain the\nset of second-order error exponents under the assumption that $\\mathcal{X}$ is\na finite alphabet. In the proof of second-order asymptotics, a main technical\ncontribution is the derivation of a central limit-type result for a maximum of\nan uncountable set of log-likelihood ratios under suitable conditions. This\nresult may be of independent interest. We also show that some important\nstatistical models satisfy the conditions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The verification of multimedia content over social media is one of the\nchallenging and crucial issues in the current scenario and gaining prominence\nin an age where user-generated content and online social web platforms are the\nleading sources in shaping and propagating news stories. As these sources allow\nusers to share their opinions without restriction, opportunistic users often\npost misleading/ unreliable content on social media such as Twitter, Facebook,\netc. At present, to lure users towards the news story, the text is often\nattached with some multimedia content (images/videos/audios). Verifying these\ncontents to maintain the credibility and reliability of social media\ninformation is of paramount importance. Motivated by this, we proposed a\ngeneralized system that supports the automatic classification of images into\ncredible or misleading. In this paper, we investigated machine learning-based\nas well as deep learning-based approaches utilized to verify misleading\nmultimedia content, where the available image traces are used to identify the\ncredibility of the content. The experiment is performed on the real-world\ndataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates\nthe efficiency of our proposed approach and features using both Machine and\nDeep Learning Model (Bi-directional LSTM). The experiment result reveals that\nthe Microsoft bings image search engine is quite effective in retrieving titles\nand performs better than our study's Google image search engine. It also shows\nthat gathering clues from attached multimedia content (image) is more effective\nthan detecting only posted content-based features.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Hairy black holes in the gravitational decoupling setup are studied from the\nperspective of conformal anomalies. Fluctuations of decoupled sources can be\ncomputed by measuring the way the trace anomaly-to-holographic Weyl anomaly\nratio differs from unit. Therefore the gravitational decoupling parameter\ngoverning three hairy black hole metrics is then bounded to a range wherein one\ncan reliably emulate AdS/CFT with gravitational decoupled solutions, in the\ntensor vacuum regime.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Employing very simple electro-mechanical principles known from classical\nphysics, the Kibble balance establishes a very precise and absolute link\nbetween quantum electrical standards and macroscopic mass or force\nmeasurements. The success of the Kibble balance, in both determining\nfundamental constants ($h$, $N_A$, $e$) and realizing a quasi-quantum mass in\nthe 2019 newly revised International System of Units, relies on the perfection\nof Maxwell's equations and the symmetry they describe between Lorentz's force\nand Faraday's induction, a principle and a symmetry stunningly demonstrated in\nthe weighing and velocity modes of Kibble balances to within $1\\times10^{-8}$,\nwith nothing but imperfect wires and magnets. However, recent advances in the\nunderstanding of the current effect in Kibble balances reveal a troubling\nparadox. A diamagnetic effect, a force that does not cancel between mass-on and\nmass-off measurement, is challenging balance maker's assumptions of symmetry at\nlevels that are almost two orders of magnitude larger than the reported\nuncertainties. The diamagnetic effect, if it exists, shows up in weighing mode\nwithout a readily apparent reciprocal effect in the velocity mode, begging\nquestions about systematic errors at the very foundation of the new measurement\nsystem. The hypothetical force is caused by the coil current changing the\nmagnetic field, producing an unaccounted force that is systematically modulated\nwith the weighing current. Here we show that this diamagnetic force exists, but\nthe additional force does not change the equivalence between weighing and\nvelocity measurements. We reveal the unexpected way that symmetry is preserved\nand show that for typical materials and geometries the total relative effect on\nthe measurement is $\\approx 1\\times10^{-9}$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Millions of patients suffer from rare diseases around the world. However, the\nsamples of rare diseases are much smaller than those of common diseases. In\naddition, due to the sensitivity of medical data, hospitals are usually\nreluctant to share patient information for data fusion citing privacy concerns.\nThese challenges make it difficult for traditional AI models to extract rare\ndisease features for the purpose of disease prediction. In this paper, we\novercome this limitation by proposing a novel approach for rare disease\nprediction based on federated meta-learning. To improve the prediction accuracy\nof rare diseases, we design an attention-based meta-learning (ATML) approach\nwhich dynamically adjusts the attention to different tasks according to the\nmeasured training effect of base learners. Additionally, a dynamic-weight based\nfusion strategy is proposed to further improve the accuracy of federated\nlearning, which dynamically selects clients based on the accuracy of each local\nmodel. Experiments show that with as few as five shots, our approach\nout-performs the original federated meta-learning algorithm in accuracy and\nspeed. Compared with each hospital's local model, the proposed model's average\nprediction accuracy increased by 13.28%.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the amplification of turbulence through gravitational\ncontraction of the primordial gas in minihalos. We perform numerical\nsimulations to follow the cloud collapse, assuming polytropic equations of\nstate for different initial turbulent Mach numbers and resolutions. We find\nthat the turbulent velocity is amplified solely by gravitational contraction,\nand eventually becomes comparable to the sound speed, even for small initial\nturbulent Mach numbers (${\\cal M}_0 \\gtrsim 0.05$). We derive an analytic\nformula for the amplification of turbulent velocity in a collapsing cloud, and\nfind that our numerical results are consistent with the formula. These results\nsuggest that the turbulence can play an important role in collapsing clouds for\ngeneral cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modern vehicles become increasingly digitalized with advanced information\ntechnology-based solutions like advanced driving assistance systems and\nvehicle-to-x communications. These systems are complex and interconnected.\nRising complexity and increasing outside exposure has created a steadily rising\ndemand for more cyber-secure systems. Thus, also standardization bodies and\nregulators issued standards and regulations to prescribe more secure\ndevelopment processes. This security, however, also has to be validated and\nverified. In order to keep pace with the need for more thorough, quicker and\ncomparable testing, today's generally manual testing processes have to be\nstructured and optimized. Based on existing and emerging standards for\ncybersecurity engineering, this paper therefore outlines a structured testing\nprocess for verifying and validating automotive cybersecurity, for which there\nis no standardized method so far. Despite presenting a commonly structured\nframework, the process is flexible in order to allow implementers to utilize\ntheir own, accustomed toolsets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Liquid Crystal Elastomers (LCEs) are an exciting category of material that\nhas tremendous application potential across a variety of fields, owing to their\nunique properties that enable both sensing and actuation. To some, LCEs are\nsimply another type of Shape Memory Polymer, while to others they are an\ninteresting on-going scientific experiment. In this visionary article, we bring\nan interdisciplinary discussion around creative and impactful ways that LCEs\ncan be applied in the Built Environment to support kinematic and kinetic\nbuildings and situational awareness. We focus particularly on the autonomy made\npossible by using LCEs, potentially removing needs for motors, wiring and\ntubing, and even enabling fully independent operation in response to natural\nenvironment variations, requiring no power sources. To illustrate the\npotential, we propose a number of concrete application scenarios where LCEs\ncould offer innovative solutions to problems of great societal importance, such\nas autonomous active ventilation, heliotropic solar panel systems which can\nalso remove snow or sand autonomously, and invisible coatings with strain\nmapping functionality, alerting residents in case of dangerous (static or\ndynamic) loads on roofs or windows, as well as assisting building safety\ninspection teams after earthquakes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A biomedical relation statement is commonly expressed in multiple sentences\nand consists of many concepts, including gene, disease, chemical, and mutation.\nTo automatically extract information from biomedical literature, existing\nbiomedical text-mining approaches typically formulate the problem as a\ncross-sentence n-ary relation-extraction task that detects relations among n\nentities across multiple sentences, and use either a graph neural network (GNN)\nwith long short-term memory (LSTM) or an attention mechanism. Recently,\nTransformer has been shown to outperform LSTM on many natural language\nprocessing (NLP) tasks. In this work, we propose a novel architecture that\ncombines Bidirectional Encoder Representations from Transformers with Graph\nTransformer (BERT-GT), through integrating a neighbor-attention mechanism into\nthe BERT architecture. Unlike the original Transformer architecture, which\nutilizes the whole sentence(s) to calculate the attention of the current token,\nthe neighbor-attention mechanism in our method calculates its attention\nutilizing only its neighbor tokens. Thus, each token can pay attention to its\nneighbor information with little noise. We show that this is critically\nimportant when the text is very long, as in cross-sentence or abstract-level\nrelation-extraction tasks. Our benchmarking results show improvements of 5.44%\nand 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and\nchemical-protein relation datasets, suggesting BERT-GT is a robust approach\nthat is applicable to other biomedical relation extraction tasks or datasets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the confinement/deconfinement phase transition of the radion field\nin a warped model with a polynomial bulk potential. The backreaction of the\nradion on the metric is taken into account by using the superpotential\nformalism, while the radion effective potential is obtained from a novel\nformulation which can incorporate the backreaction. The phase transition leads\nto a stochastic gravitational wave background that depends on the energy scale\nof the first Kaluza-Klein resonance, $m_{\\textrm{KK}}$. This work completes\nprevious studies in the following aspects: i) we detail the evaluation of the\nradion spectrum; ii) we report on the mismatches between the thick wall\napproximation and the numerical bounce solution; iii) we include a suppression\nfactor in the spectrum of sound waves accounting for their finite lifetime;\nand, iv) we update the bound on $m_{\\textrm{KK}}$ in view of the O3 LIGO and\nVirgo data. We find that the forthcoming gravitational wave interferometers can\nprobe scenarios where $m_{\\textrm{KK}} \\lesssim 10^9$ TeV, while the O3-run\nbounds rule out warped models with $10^4 \\textrm{TeV} \\lesssim m_{\\textrm{KK}}\n\\lesssim 10^7$ TeV exhibiting an extremely strong confinement/deconfinement\nphase transition.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  When a chaotic, ergodic Hamiltonian system with $N$ degrees of freedom is\nsubject to sufficiently rapid periodic driving, its energy evolves diffusively.\nWe derive a Fokker-Planck equation that governs the evolution of the system's\nprobability distribution in energy space, and we provide explicit expressions\nfor the energy drift and diffusion rates. Our analysis suggests that the system\ngenerically relaxes to a long-lived \"prethermal\" state characterized by minimal\nenergy absorption, eventually followed by more rapid heating. When $N\\gg 1$,\nthe system ultimately absorbs energy indefinitely from the drive, or at least\nuntil an infinite temperature state is reached.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Janus monolayers have long been captivated as a popular notion for breaking\nin-plane and out-of-plane structural symmetry. Originated from chemistry and\nmaterials science, the concept of Janus functions have been recently extended\nto ultrathin metasurfaces by arranging meta-atoms asymmetrically with respect\nto the propagation or polarization direction of the incident light. However,\nsuch metasurfaces are intrinsically static and the information they carry can\nbe straightforwardly decrypted by scanning the incident light directions and\npolarization states once the devices are fabricated. In this Letter, we present\na dynamic Janus metasurface scheme in the visible spectral region. In each\nsuper unit cell, three plasmonic pixels are categorized into two sets. One set\ncontains a magnesium nanorod and a gold nanorod that are orthogonally oriented\nwith respect to each other, working as counter pixels. The other set only\ncontains a magnesium nanorod. The effective pixels on the Janus metasurface can\nbe reversibly regulated by hydrogenation/dehydrogenation of the magnesium\nnanorods. Such dynamic controllability at visible frequencies allows for flat\noptical elements with novel functionalities including beam steering, bifocal\nlensing, holographic encryption, and dual optical function switching.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Large language models are increasingly capable of generating fluent-appearing\ntext with relatively little task-specific supervision. But can these models\naccurately explain classification decisions? We consider the task of generating\nfree-text explanations using human-written examples in a few-shot manner. We\nfind that (1) authoring higher quality prompts results in higher quality\ngenerations; and (2) surprisingly, in a head-to-head comparison, crowdworkers\noften prefer explanations generated by GPT-3 to crowdsourced explanations in\nexisting datasets. Our human studies also show, however, that while models\noften produce factual, grammatical, and sufficient explanations, they have room\nto improve along axes such as providing novel information and supporting the\nlabel. We create a pipeline that combines GPT-3 with a supervised filter that\nincorporates binary acceptability judgments from humans in the loop. Despite\nthe intrinsic subjectivity of acceptability judgments, we demonstrate that\nacceptability is partially correlated with various fine-grained attributes of\nexplanations. Our approach is able to consistently filter GPT-3-generated\nexplanations deemed acceptable by humans.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Interpretability has attracted increasing attention in earth observation\nproblems. We apply interactive visualization and representation analysis to\nguide interpretation of glacier segmentation models. We visualize the\nactivations from a U-Net to understand and evaluate the model performance. We\nbuild an online interface using the Shiny R package to provide comprehensive\nerror analysis of the predictions. Users can interact with the panels and\ndiscover model failure modes. Further, we discuss how visualization can provide\nsanity checks during data preprocessing and model training.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Stochastic modelling of complex systems plays an essential, yet often\ncomputationally intensive role across the quantitative sciences. Recent\nadvances in quantum information processing have elucidated the potential for\nquantum simulators to exhibit memory advantages for such tasks. Heretofore, the\nfocus has been on lossless memory compression, wherein the advantage is\ntypically in terms of lessening the amount of information tracked by the model,\nwhile -- arguably more practical -- reductions in memory dimension are not\nalways possible. Here we address the case of lossy compression for quantum\nstochastic modelling of continuous-time processes, introducing a method for\ncoarse-graining in quantum state space that drastically reduces the requisite\nmemory dimension for modelling temporal dynamics whilst retaining near-exact\nstatistics. In contrast to classical coarse-graining, this compression is not\nbased on sacrificing temporal resolution, and brings memory-efficient,\nhigh-fidelity stochastic modelling within reach of present quantum\ntechnologies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We demonstrate that the conformal loop ensemble (CLE) has a rich integrable\nstructure by establishing exact formulas for two CLE observables. The first\ndescribes the joint moments of the conformal radii of loops surrounding three\npoints for CLE on the sphere. Up to normalization, our formula agrees with the\nimaginary DOZZ formula due to Zamolodchikov (2005) and Kostov-Petkova (2007),\nwhich is the three-point structure constant of certain conformal field theories\nthat generalize the minimal models. This verifies the CLE interpretation of the\nimaginary DOZZ formula by Ikhlef, Jacobsen and Saleur (2015). Our second result\nis for the moments of the electrical thickness of CLE loops first considered by\nKenyon and Wilson (2004). Our proofs rely on the conformal welding of random\nsurfaces and two sources of integrability concerning CLE and Liouville quantum\ngravity (LQG). First, LQG surfaces decorated with CLE inherit a rich integrable\nstructure from random planar maps decorated with the O(n) loop model. Second,\nas the field theory describing LQG, Liouville conformal field theory is\nintegrable. In particular, the DOZZ formula and the FZZ formula for its\nstructure constants are crucial inputs to our results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  I partially identify the marginal treatment effect (MTE) function when the\ntreatment variable is misclassified. To do so, I explore three sets of\nrestrictions on the relationship between the instrument, the misclassified\ntreatment and the correctly measured treatment, allowing for dependence between\nthe instrument and the misclassification decision. If the signs of the\nderivatives of the correctly measured propensity score and the mismeasured one\nare the same, I identify the sign of the MTE function at every point in the\ninstrument's support. If those derivatives are close to each other, I bound the\nMTE function. Finally, by imposing a functional restriction between those two\npropensity scores, I derive sharp bounds around the MTE function and any\nweighted average of the MTE function. To illustrate the usefulness of my\npartial identification method, I analyze the impact of alternative sentences --\ne.g., fines or community services -- on recidivism using random assignment of\njudges within Brazilian court districts. In this context, misclassification is\nan issue when the researcher measures the treatment based solely on trial\njudge's rulings, ignoring that the Appeals Court may reverse sentences. I show\nthat, when I use the trial judge's rulings as my misclassified treatment\nvariable, the misclassification bias may be as large as 10% of the MTE\nfunction, which can be estimated using the final ruling in each case as my\ncorrectly measured treatment variable. Moreover, I show that the proposed\nbounds contain the MTE function in this empirical example.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Future wireless networks are expected to connect large-scale low-powered\ncommunication devices using the available spectrum resources. Backscatter\ncommunications (BC) is an emerging technology towards battery-free transmission\nin future wireless networks by leveraging ambient radio frequency (RF) waves\nthat enable communications among wireless devices. Non-orthogonal multiple\naccess (NOMA) has recently drawn significant attention due to its high spectral\nefficiency. The combination of these two technologies can play an important\nrole in the development of future networks. This paper proposes a new\noptimization approach to enhance the spectral efficiency of nonorthogonal\nmultiple access (NOMA)-BC network. Our framework simultaneously optimizes the\npower allocation of base station and reflection coefficient (RC) of the\nbackscatter device in each cell under the assumption of imperfect signal\ndecoding. The problem of spectral efficiency maximization is coupled on power\nand RC which is challenging to solve. To make this problem tractable, we first\ndecouple it into two subproblems and then apply the decomposition method and\nKarush-Kuhn-Tucker conditions to obtain the efficient solution. Numerical\nresults show the performance of the proposed NOMA-BC network over the pure NOMA\nnetwork without BC.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper examines long-term temporal and spatial fluctuations in the solar\nrotation (more than four solar cycles) by investigating radio emission escapes\nfrom various layers of the solar atmosphere during the years 1967-2010. The\nflux modulation approach can also be used to investigate variations in solar\nrotation, which is a contentious topic in solar physics. The current study\nmakes use of a time series of radio flux data at different frequencies\n(245-15400 MHz) obtained at Sagamore Hill Solar Radio Observatory in\nMassachusetts, USA, and other observatories from 1967 to 2010. The periodicity\npresent in the temporal variation of time series is estimated through Lomb\nScargle Periodogram (LSP). The rotation period estimated for five radio\nemissions (606, 1415, \\& 2695 MHz; from corona, and 4995 \\& 8800 MHz; from\ntransition region) through statistical approach shows continuous temporal and\nspatial variation throughout the years. The smoothed rotation period shows the\npresence of $\\sim$ 22-yrs periodic and $\\sim$ 11-yrs components in it. The\n22-year component could be linked to the reversal of the solar magnetic field\n(Hale's) cycle, while the 11-yrs component is most likely related to the\nsunspot (Schwabe's) cycle. Besides these two components, random components are\nalso prominently present in the analyzed data. The cross-correlation between\nthe sunspot number and the rotation period obtained shows a strong correlation\nwith 11-yrs Schwabe's and 22-yr Hale cycle. The corona rotates faster or slower\nthan transition region in different epoch. The swap of faster rotation speed\nbetween corona and transition region also follows the 22-yrs cycle.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper formulates and solves the optimal stopping problem for a loan made\nto one's self from a tax-advantaged retirement account such as a 401(k),\n403(b), or 457(b) plan. If the plan participant has access to an external asset\nwith a higher expected rate of return than the investment funds and indices\nthat are available within the retirement account, then he must decide how long\nto wait before exercising the loan option. On the one hand, taking the loan\nquickly will result in many years of exponential capital growth at the higher\n(external) rate; on the other hand, if we wait to accumulate more funds in the\n457(b), then we can make a larger deposit into the external asset (albeit for a\nshorter period of time). I derive a variety of cutoff rules for optimal loan\ncontrol; in general, the investor must wait until he accumulates a certain\namount of money (measured in contribution-years) that depends on the disparate\nyields, the loan parameters, and the date certain at which he will liquidate\nthe retirement account. Letting the horizon tend to infinity, the optimal\n(horizon-free) policy gains in elegance, simplicity, and practical robustness\nto different life outcomes. When asset prices and returns are stochastic, the\n(continuous time) cutoff rule turns into a \"wait region,\" whereby the mean of\nterminal wealth is rising and the variance of terminal wealth is falling. After\nhis sojourn through the wait region is over, the participant finds himself on\nthe mean-variance frontier, at which point his subsequent behavior is a matter\nof personal risk preference.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Discrete algebraic Riccati equations and their fixed points are well\nunderstood and arise in a variety of applications, however, the time-varying\nequations have not yet been fully explored in the literature. In this article\nwe provide a self-contained study of discrete time Riccati matrix difference\nequations. In particular, we provide a novel Riccati semigroup duality formula\nand a new Floquet-type representation for these equations. Due to the\naperiodicity of the underlying flow of the solution matrix, conventional\nFloquet theory does not apply in this setting and thus further analysis is\nrequired. We illustrate the impact of these formulae with an explicit\ndescription of the solution of time-varying Riccati difference equations and\nits fundamental-type solution in terms of the fixed point of the equation and\nan invertible linear matrix map, as well as uniform upper and lower bounds on\nthe Riccati maps. These are the first results of this type for time varying\nRiccati matrix difference equations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Signals can be interpreted as composed of a rapidly varying component\nmodulated by a slower varying envelope. Identifying this envelope is an\nessential operation in signal processing, with applications in areas ranging\nfrom seismology to medicine. Conventional envelope detection approaches based\non classic methods tend to lack generality, however, and need to be tailored to\neach specific application in order to yield reasonable results. Taking\ninspiration from geometric concepts, most notably the theory of alpha-shapes,\nwe introduce a general-purpose library to efficiently extract the envelope of\narbitrary signals.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The optimization problems with simple bounds are an important class of\nproblems. To facilitate the computation of such problems, an unconstrained-like\ndynamic method, motivated by the Lyapunov control principle, is proposed. This\nmethod employs the anti-windup limited integrator to address the bounds of\nparameters upon the dynamics for unconstrained problem, and then solves the\ntransformed Initial-value Problems (IVPs) with mature Ordinary Differential\nEquation (ODE) integration methods. It is proved that when the gain matrix is\ndiagonal, the result is equivalent to that of the general dynamic method which\ninvolves an intermediate Quadratic Programming (QP) sub-problem. Thus, the\nglobal convergence to the optimal solution is guaranteed without the\nrequirement of the strict complementarity condition. Since the estimation of\nthe right active constraints is avoided and no programming sub-problem is\ninvolved in the computation process, it shows higher efficiency than the\ngeneral dynamic method and other common iterative methods through the numerical\nexamples. In particular, the implementation is simple, and the proposed method\nis easy-to-use.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that counts of squarefree integers up to $X$ in short intervals of\nsize $H$ tend to a Gaussian distribution as long as $H\\rightarrow\\infty$ and $H\n= X^{o(1)}$. This answers a question posed by R.R. Hall in 1989. More generally\nwe prove a variant of Donsker's theorem, showing that these counts scale to a\nfractional Brownian motion with Hurst parameter $1/4$. In fact we are able to\nprove these results hold in general for collections of $B$-free integers as\nlong as the sieving set $B$ satisfies a very mild regularity property, for\nHurst parameter varying with the set $B$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  EUSO-TA is a ground-based experiment, placed at Black Rock Mesa of the\nTelescope Array site as a part of the JEM-EUSO (Joint Experiment Missions for\nthe Extreme Universe Space Observatory) program. The UV fluorescence imaging\ntelescope with a field of view of about 10.6 deg x 10.6 deg consisting of 2304\npixels (36 Multi-Anode Photomultipliers, 64 pixels each) works with\n2.5-microsecond time resolution. An experimental setup with two Fresnel lenses\nallows for measurements of Ultra High Energy Cosmic Rays in parallel with the\nTA experiment as well as the other sources like flashes of lightning,\nartificial signals from UV calibration lasers, meteors and stars. Stars\nincrease counts on pixels while crossing the field of view as the point-like\nsources. In this work, we discuss the method for calibration of EUSO\nfluorescence detectors based on signals from stars registered by the EUSO-TA\nexperiment during several campaigns. As the star position is known, the\nanalysis of signals gives an opportunity to determine the pointing accuracy of\nthe detector. This can be applied to space-borne or balloon-borne EUSO\nmissions. We describe in details the method of the analysis which provides\ninformation about detector parameters like the shape of the point spread\nfunction and is the way to perform absolute calibration of EUSO cameras.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Kerala, the south-west coastal state of India, was ravaged by a series of\nfloods during the South-West Monsoon of 2018. The season was marked by severely\nanomalous rainfall trends, with upto 150 mm of departures from the mean daily\nprecipitation in the northern districts of the State. Although, there were many\nstudies about the hydrogeological factors which aggravated the floods in\nKerala, no attempt was made to delve into the physics which actually resulted\nin anomalous precipitation during the year. This study intends to document the\ndynamical phenomenon which caused the Kerala Floods of 2018. The westward\npropagating convectively-coupled Mixed Rossby-Gravity (MRG) waves were excited\nby the synoptic disturbances of the tropical Pacific at the pressure level of\n700 hPa, during the Indian Monsoon of 2018. They travelled across the Indian\nOcean in two significant modes -- a predominant slow moving wave of 20-40 days\nperiod (as Madden-Julian Oscillation) and a secondary faster wave of 5-8 days\nperiod. They are characterised by vertical phase propagation to the upper\ntroposphere, a precursor to deep convection and intense precipitation. Further,\nthe propagation of these waves through a medium enhances its relative vorticity\nand the gyres or circulations thus formed are symmetrical about the equator.\nConsequently, the meanders in the wind field and widening of the Intertropical\nConvergence Zone were observed. The MRG waves, especially the slow mode induced\ndivergence in the wind field, which fueled convection in tropics and brought\nvery heavy rainfall to the State of Kerala in 2018.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The quantification of modern slavery has received increased attention\nrecently as organizations have come together to produce global estimates, where\nmultiple systems estimation (MSE) is often used to this end. Echoing a\nlong-standing controversy, disagreements have re-surfaced regarding the\nunderlying MSE assumptions, the robustness of MSE methodology, and the accuracy\nof MSE estimates in this application. Our goal is to help address and move past\nthese controversies. To do so, we review MSE, its assumptions, and commonly\nused models for modern slavery applications. We introduce all of the publicly\navailable modern slavery datasets in the literature, providing a reproducible\nanalysis and highlighting current issues. Specifically, we utilize an internal\nconsistency approach that constructs subsets of data for which ground truth is\navailable, allowing us to evaluate the accuracy of MSE estimators. Next, we\npropose a characterization of the large sample bias of estimators as a function\nof misspecified assumptions. Then, we propose an alternative to traditional\n(e.g., bootstrap-based) assessments of reliability, which allows us to\nvisualize trajectories of MSE estimates to illustrate the robustness of\nestimates. Finally, our complementary analyses are used to provide guidance\nregarding the application and reliability of MSE methodology.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In nonequilibrium systems, the relative fluctuation of a current has a\nuniversal trade-off relation with the entropy production, called the\nthermodynamic uncertainty relation (TUR). For systems with broken time reversal\nsymmetry, its violation has been reported in specific models or in the linear\nresponse regime. Here, we derive a modified version of the TUR analytically in\nthe overdamped limit for general Langevin dynamics with a magnetic Lorentz\nforce causing time reversal broken. Remarkably, this modified version is simply\ngiven by the conventional TUR scaled by the ratio of the reduced effective\ntemperature of the overdamped motion to the reservoir temperature, permitting a\nviolation of the conventional TUR. Without the Lorentz force, this ratio\nbecomes unity and the conventional TUR is restored. We verify our results both\nanalytically and numerically in a specific solvable system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Selectivity is an important phenomenon in chemical reaction dynamics. This\ncan be quantified by the branching ratio of the trajectories that visit one or\nthe other wells to the total number of trajectories in a system with a\npotential with two sequential index-1 saddles and two wells (top well and\nbottom well). In our case, the branching ratio is 1:1 because of the symmetry\nof our potential energy surface. The mechanisms of transport and the behavior\nof the trajectories in this kind of systems have been studied recently. In this\npaper we study the time evolution after the selectivity as energy varies using\nperiodic orbit dividing surfaces. We investigate what happens after the first\nvisit of a trajectory to the region of the top or the bottom well for different\nvalues of energy. We answer the natural question, what is the destiny of these\ntrajectories?\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We give a new description to obtain the shear viscosity in QCD at finite\ntemperature. Firstly, we obtain the correlation function of the renormalized\nenergy-momentum tensor using the gradient flow method. Secondly, we estimate\nthe spectral function from the smeared correlation functions using the sparse\nmodeling method. The combination of these two methods looks promising to\ndetermine the shear viscosity precisely.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  $\\beta$-decay half-lives of neutron-rich nuclei around $N=82$ are key data to\nunderstand the $r$-process nucleosynthesis. We performed large-scale\nshell-model calculations in this region using a newly constructed shell-model\nHamiltonian, and successfully described the low-lying spectra and half-lives of\nneutron-rich $N=82$ and $N=81$ isotones with $Z=42-49$ in a unified way. We\nfound that their Gamow-Teller strength distributions have a peak in the\nlow-excitation energies, which significantly contributes to the half-lives.\nThis peak, dominated by $\\nu 0g_{7/2} \\to \\pi 0g_{9/2}$ transitions, is\nenhanced on the proton deficient side because the Pauli-blocking effect caused\nby occupying the valence proton $0g_{9/2}$ orbit is weakened.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The paper focuses on improving the recent plug-and-play patch rescaling\nmodule (PRM) based approaches for crowd counting. In order to make full use of\nthe PRM potential and obtain more reliable and accurate results for challenging\nimages with crowd-variation, large perspective, extreme occlusions, and\ncluttered background regions, we propose a new PRM based multi-resolution and\nmulti-task crowd counting network by exploiting the PRM module with more\neffectiveness and potency. The proposed model consists of three deep-layered\nbranches with each branch generating feature maps of different resolutions.\nThese branches perform a feature-level fusion across each other to build the\nvital collective knowledge to be used for the final crowd estimate.\nAdditionally, early-stage feature maps undergo visual attention to strengthen\nthe later-stage channels understanding of the foreground regions. The\nintegration of these deep branches with the PRM module and the early-attended\nblocks proves to be more effective than the original PRM based schemes through\nextensive numerical and visual evaluations on four benchmark datasets. The\nproposed approach yields a significant improvement by a margin of 12.6% in\nterms of the RMSE evaluation criterion. It also outperforms state-of-the-art\nmethods in cross-dataset evaluations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The spin Hall effect of light, a spin-dependent transverse splitting of light\nat an optical interface, is intrinsically an incident-polarization-sensitive\nphenomenon. Recently, an approach to eliminate the polarization dependence by\nequalizing the reflection coefficients of two linear polarizations has been\nproposed, but is only valid when the beam waist is sufficiently larger than the\nwavelength. Here, we demonstrate that an interface, at which the reflection\ncoefficients of the two linear polarizations are the same and so are their\nderivatives with respect to the incident angle, supports the\npolarization-independent spin Hall shift, even when the beam waist is\ncomparable to the wavelength. In addition, an isotropic-anisotropic interface\nthat exhibits the polarization-independent spin Hall shift over the entire\nrange of incident angles is presented. Monte-Carlo simulations prove that spin\nHall shifts are degenerate under any polarization and reaches a half of beam\nwaist under unpolarized incidence. We suggest an application of the\nbeam-waist-scale spin Hall effect of light as a tunable beam-splitting device\nthat is responsive to the incident polarization. The spin Hall shift that is\nindependent of the incident polarization at any incident angle will facilitate\na wide range of applications including practical spin-dependent devices and\nactive beam splitters.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article we propose a $\\alpha$-hypergeometric model with uncertain\nvolatility (UV) where we derive a worst-case scenario for option pricing. The\napproach is based on the connexion between a certain class of nonlinear partial\ndifferential equations of HJB-type (G-HJB equations), that govern the nonlinear\nexpectation of the UV model and that provide an alternative to the difficult\nmodel calibration problem of UV models, and second-order backward stochastic\ndifferential equations (2BSDEs). Using asymptotic analysis for the G-HJB\nequation and the equivalent 2BSDE representation, we derive a limit model that\nprovides an accurate description of the worst-case price scenario in cases when\nthe bounds of the UV model are slowly varying. The analytical results are\ntested by numerical simulations using a deep learning based approximation of\nthe underlying 2BSDE.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper proposes a novel integrated dynamic method based on Behavior Trees\nfor planning and allocating tasks in mixed human robot teams, suitable for\nmanufacturing environments. The Behavior Tree formulation allows encoding a\nsingle job as a compound of different tasks with temporal and logic\nconstraints. In this way, instead of the well-studied offline centralized\noptimization problem, the role allocation problem is solved with multiple\nsimplified online optimization sub-problem, without complex and cross-schedule\ntask dependencies. These sub-problems are defined as Mixed-Integer Linear\nPrograms, that, according to the worker-actions related costs and the workers'\navailability, allocate the yet-to-execute tasks among the available workers. To\ncharacterize the behavior of the developed method, we opted to perform\ndifferent simulation experiments in which the results of the action-worker\nallocation and computational complexity are evaluated. The obtained results,\ndue to the nature of the algorithm and to the possibility of simulating the\nagents' behavior, should describe well also how the algorithm performs in real\nexperiments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we propose a new continuously learning generative model,\ncalled the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs\nlearns a sequence of tasks from several databases and its architecture consists\nof three components: two identical generators, namely the Teacher and\nAssistant, and one Discriminator. In order to allow for the LT-GANs to learn\nnew concepts without forgetting, we introduce a new lifelong training approach,\nnamely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the\nTeacher and Assistant to alternately teach each other, while learning a new\ndatabase. This training approach favours transferring knowledge from a more\nknowledgeable player to another player which knows less information about a\npreviously given task.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the task of grasping a target object based on a natural language\ncommand query. Previous work primarily focused on localizing the object given\nthe query, which requires a separate grasp detection module to grasp it. The\ncascaded application of two pipelines incurs errors in overlapping multi-object\ncases due to ambiguity in the individual outputs. This work proposes a model\nnamed Command Grasping Network(CGNet) to directly output command satisficing\ngrasps from RGB image and textual command inputs. A dataset with ground truth\n(image, command, grasps) tuple is generated based on the VMRD dataset to train\nthe proposed network. Experimental results on the generated test set show that\nCGNet outperforms a cascaded object-retrieval and grasp detection baseline by a\nlarge margin. Three physical experiments demonstrate the functionality and\nperformance of CGNet.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  JavaScript implementations are tested for conformance to the ECMAScript\nstandard using a large hand-written test suite. Not only in this a tedious\napproach, it also relies solely on the natural language specification for\ndifferentiating behaviors, while hidden implementation details can also affect\nbehavior and introduce divergences. We propose to generate conformance tests\nthrough dynamic symbolic execution of polyfills, drop-in replacements for newer\nJavaScript language features that are not yet widely supported. We then run\nthese generated tests against multiple implementations of JavaScript, using a\nmajority vote to identify the correct behavior. To facilitate test generation\nfor polyfill code, we introduce a model for structured symbolic inputs that is\nsuited to the dynamic nature of JavaScript. In our evaluation, we found 17\ndivergences in the widely used core-js polyfill and were able to increase\nbranch coverage in interpreter code by up to 15%. Because polyfills are\ntypically written even before standardization, our approach will allow to\nmaintain and extend standardization test suites with reduced effort.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  There is a long-time quest for understanding physical mechanisms of weak\nmagnetic field interaction with biological matter. Two factors impeded the\ndevelopment of such mechanisms: first, a high (room) temperature of a cellular\nenvironment, where a weak, static magnetic field induces a (classically) zero\nequilibrium response. Second, the friction in the cellular environment is\nlarge, preventing a weak field to alter non-equilibrium processes such as a\nfree diffusion of charges. Here we study a class of non-equilibrium steady\nstates of a cellular ion in a confining potential, where the response to a\n(weak, homogeneous, static) magnetic field survives strong friction and thermal\nfluctuations. The magnetic field induces a rotational motion of the ion that\nproceeds with the cyclotron frequency. Such non-equilibrium states are\ngenerated by a white noise acting on the ion additionally to the non-local\n(memory-containing) friction and noise generated by an equilibrium thermal\nbath. The intensity of this white noise can be weak, i.e. much smaller than the\nthermal noise intensity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Symmetry transformations have proved useful in determining the algebraic\nstructure and internal dynamical properties of physical systems. In the quantum\nRabi model, invariance under parity symmetry transformation has been used to\nobtain exact solutions of the eigenvalue equation and very good approximations\nof the internal dynamics of the interacting atom-light system. In this article,\ntwo symmetry operators, characterized as \"duality\" symmetry operators, have\nbeen introduced which transform the quantum Rabi Hamiltonian into duality\nconjugates. The parity and duality symmetry operators constitute an\nalgebraically closed set of symmetry transformation operators of the quantum\nRabi model. The closed $SU(2)$ Lie algebra provides the standard eigenvalues\nand eigenstates of the parity symmetry operator. It is established that\nJaynes-Cummings and anti-Jaynes-Cummings operators are duality symmetry\nconjugates. Symmetric or antisymmetric linear combinations of the Rabi\nHamiltonian and a corresponding duality conjugate yield the familiar\nspin-dependent force driven bosonic , coupling-only or quantized light mode\nquadrature-driven fermionic Hamiltonian. It is established that the effective\nbosonic, fermionic and coupling-only Hamiltonians are exact, not approximate\nforms of the quantum Rabi Hamiltonian as they have generally been interpreted.\nThe effective bosonic form generates the dynamics of the light mode driven by\nthe atomic spin-dependent force, while the fermionic form generates the\ndynamics of the atomic spin driven by the quantized light mode\nquadrature-dependent force, thus providing a complete picture of the quantum\nRabi dynamics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the typical behavior of the size of the ratio set $A/A$ for a random\nsubset $A\\subset \\{1,\\dots , n\\}$. For example, we prove that $|A/A|\\sim\n\\frac{2\\text{Li}_2(3/4)}{\\pi^2}n^2 $ for almost all subsets $A \\subset\\{1,\\dots\n,n\\}$. We also prove that the proportion of visible lattice points in the\nlattice $A_1\\times\\cdots \\times A_d$, where $A_i$ is taken at random in $[1,n]$\nwith $\\mathbb P(m\\in A_i)=\\alpha_i$ for any $m\\in [1,n]$, is asymptotic to a\nconstant $\\mu(\\alpha_1,\\dots,\\alpha_d)$ that involves the polylogarithm of\norder $d$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A new algorithm to perform coherent mode decomposition of the undulator\nradiation is proposed. It is based in separating the horizontal and vertical\ndirections, reducing the problem by working with one-dimension wavefronts. The\nvalidity conditions of this approximation are discussed. Simulations require\nlow computer resources, and run interactively in a laptop. We study the\nfocusing with lenses of the radiation emitted by an undulator in a\nfourth-generation storage ring (EBS-ESRF). Results are compared against\nmultiple optics packages implementing a variety of methods for dealing with\npartial coherence: full 2D coherent mode decomposition, Monte-Carlo combination\nof wavefronts from electrons entering the undulator with different initial\nconditions, and hybrid ray-tracing correcting geometrical optics with wave\noptics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  There is a general agreement that it is important to consider the practical\nrelevance of an effect in addition to its statistical significance, yet a\nformal definition of practical relevance is still pending and shall be provided\nwithin this paper. It appears that an underlying decision problem,\ncharacterized by actions and a loss function, is required to define the notion\nof practical relevance, rendering it a decision theoretic concept. In the\ncontext of hypothesis-based analyses, the notion of practical relevance relates\nto specifying the hypotheses reasonably, such that the null hypothesis does not\ncontain only a single parameter null value, but also all parameter values that\nare equivalent to the null value on a practical level. In that regard, the\ndefinition of practical relevance is also extended into the context of\nhypotheses. The formal elaborations on the notion of practical relevance within\nthis paper indicate that, typically, a specific decision problem is implicitly\nassumed when dealing with the practical relevance of an effect or some results.\nAs a consequence, involving decision theoretic considerations into a\nstatistical analysis suggests itself by the mere nature of the notion of\npractical relevance.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a theoretical model to elucidate intermolecular electrostatic\ninteractions between a virus and a substrate. Our model treats the virus as a\nhomogeneous particle having surface charge and the polymer fiber of the\nrespirator as a charged plane. Electric potentials surrounding the virus and\nfiber are influenced by the surface charge distribution of the virus. We use\nPoisson-Boltzmann equations to calculate electric potentials. Then, Derjaguin's\napproximation and a linear superposition of the potential function are extended\nto determine the electrostatic force. In this work, we apply this model for\ncoronavirus or SARS-CoV-2 case and numerical results quantitatively agree with\nprior simulation. We find that the influence of fiber's potential on the\nsurface charge of the virus is important and is considered in interaction\ncalculations to obtain better accuracy. The electrostatic interaction\nsignificantly decays with increasing separation distance, and this curve\nbecomes steeper when adding more salt. Although the interaction force increases\nwith heating, one can observe the repulsive-attractive transition when the\nenvironment is acidic.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider a diffusion in $\\mathbb{R}^n$ whose coordinates each behave as\none-dimensional Brownian motions, that behave independently when apart, but\nhave a sticky interaction when they meet. The diffusion in $\\mathbb{R}^n$ can\nbe viewed as the $n$-point motion of a stochastic flow of kernels. We derive\nthe Kolmogorov backwards equation and show that for a specific choice of\ninteraction it can be solved exactly with the Bethe ansatz. We then use our\nformulae to study the behaviour of the flow of kernels for the exactly solvable\nchoice of interaction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Digital tools play an important role in fighting the current global COVID-19\npandemic. We conducted a representative online study in Germany on a sample of\n599 participants to evaluate the user perception of vaccination certificates.\nWe investigated five different variants of vaccination certificates, based on\ndeployed and planned designs in a between-group design, including paper-based\nand app-based variants. Our main results show that the willingness to use and\nadopt vaccination certificates is generally high. Overall, paper-based\nvaccination certificates were favored over app-based solutions. The willingness\nto use digital apps decreased significantly by a higher disposition to privacy,\nand increased by higher worries about the pandemic and acceptance of the\ncoronavirus vaccination. Vaccination certificates resemble an interesting use\ncase for studying privacy perceptions for health related data. We hope that our\nwork will be able to educate the currently ongoing design of vaccination\ncertificates, will give us deeper insights into privacy of health-related data\nand apps, and prepare us for future potential applications of vaccination\ncertificates and health apps in general.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Persistent current is a small but perpetual electric current that flows in\nmetallic rings in the absence of any applied source. We compute the persistent\ncurrents of one-dimensional disordered metallic rings of interacting electrons\nin the presence of impurity on lattices up to 8-sites at half-filling and also\naway from half-filling using the Lanczos algorithm. For the case of\nhalf-filling, we observe that both interaction and disorder suppress the\namplitude of the persistent currents by localizing the electrons. However, in\nthe presence of disorder and away from half-filling, the Coulomb interaction is\nobserved to enhance the persistent current. Furthermore, in the half-filled\ncase, there is a transition from metal to insulator as U is increased\nsignificantly. In addition, shifting away from half-filling, the system is\nobserved to remain in the metallic state irrespective of the value of the\nCoulomb repulsion (U). The observations are quite in agreement with the results\nfrom other techniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We classify the maximal algebraic subgroups of Bir(CxPP^1), when C is a\nsmooth projective curve of positive genus.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The brain is in a state of perpetual reverberant neural activity, even in the\nabsence of specific tasks or stimuli. Shedding light on the origin and\nfunctional significance of such a dynamical state is essential to understanding\nhow the brain transmits, processes, and stores information. An inspiring,\nalbeit controversial, conjecture proposes that some statistical characteristics\nof empirically observed neuronal activity can be understood by assuming that\nbrain networks operate in a dynamical regime near the edge of a phase\ntransition. Moreover, the resulting critical behavior, with its concomitant\nscale invariance, is assumed to carry crucial functional advantages. Here, we\npresent a data-driven analysis based on simultaneous high-throughput recordings\nof the activity of thousands of individual neurons in various regions of the\nmouse brain. To analyze these data, we synergistically combine cutting-edge\nmethods for the study of brain activity (such as a phenomenological\nrenormalization group approach and techniques that infer the general dynamical\nstate of a neural population), while designing complementary tools. This\nstrategy allows us to uncover strong signatures of scale invariance that is\n\"quasi-universal\" across brain regions and reveal that all these areas operate,\nto a greater or lesser extent, near the edge of instability. Furthermore, this\nframework allows us to distinguish between quasi-universal background activity\nand non-universal input-related activity. Taken together, this study provides\nstrong evidence that brain networks actually operate in a critical regime\nwhich, among other functional advantages, provides them with a scale-invariant\nsubstrate of activity covariances that can sustain optimal input\nrepresentations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show the existence of self-dual (topological) solitons in a gauged version\nof the baby Skyrme model in which the Born-Infeld term governs the gauge field\ndynamics. The successful implementation of the Bogomol'nyi-Prasad-Sommerfield\nformalism provides a lower bound for the energy and the respective self-dual\nequations whose solutions are the solitons saturating such a limit. The energy\nlower bound (Bogomol'nyi bound) is proportional to the topological charge of\nthe Skyrme field and therefore quantized. In contrast, the total magnetic flux\nis a nonquantized quantity. Furthermore, the model supports three types of\nself-dual solitons profiles: the first describes compacton solitons, the second\nfollows a Gaussian decay law, and the third portrays a power-law decay.\nFinally, we perform numerical solutions of the self-dual equations and depicted\nthe soliton profiles for different values of the parameters controlling the\nnonlinearity of the model.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  While learning-based control techniques often outperform classical controller\ndesigns, safety requirements limit the acceptance of such methods in many\napplications. Recent developments address this issue through so-called\npredictive safety filters, which assess if a proposed learning-based control\ninput can lead to constraint violations and modifies it if necessary to ensure\nsafety for all future time steps. The theoretical guarantees of such predictive\nsafety filters rely on the model assumptions and minor deviations can lead to\nfailure of the filter putting the system at risk. This paper introduces an\nauxiliary soft-constrained predictive control problem that is always feasible\nat each time step and asymptotically stabilizes the feasible set of the\noriginal safety filter, thereby providing a recovery mechanism in\nsafety-critical situations. This is achieved by a simple constraint tightening\nin combination with a terminal control barrier function. By extending\ndiscrete-time control barrier function theory, we establish that the proposed\nauxiliary problem provides a `predictive' control barrier function. The\nresulting algorithm is demonstrated using numerical examples.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Ultra-fast transmission electron microscopy (UTEM) combines sub-picosecond\ntime-resolution with the versatility of TEM spectroscopies. It allows one to\nstudy the dynamics of materials properties combining complementary techniques.\nHowever, until now, time-resolved cathodoluminescence, which is expected to\ngive access to the optical properties dynamics, was still unavailable in a\nUTEM. In this paper, we report time-resolved cathodoluminescence measurements\nin an ultrafast transmission electron microscope. We measured lifetime maps,\nwith a 12 nm spatial resolution and sub-nanoseconds resolution, of\nnano-diamonds with a high density of NV center. This study paves the way to new\napplications of UTEM and to correlative studies of optically active\nnanostructures.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sudoku is a famous logic puzzle where the player has to fill a number between\n1 and 9 into each empty cell of a $9 \\times 9$ grid such that every number\nappears exactly once in each row, each column, and each $3 \\times 3$ block. In\n2020, Sasaki et al. developed a physical card-based protocol of zero-knowledge\nproof (ZKP) for Sudoku, which enables a prover to convince a verifier that\nhe/she knows a solution of the puzzle without revealing it. Their protocol uses\n90 cards, but requires nine identical copies of some cards, which cannot be\nfound in a standard deck of playing cards (consisting of 52 different cards and\ntwo jokers). Hence, nine identical standard decks are required to perform that\nprotocol, making the protocol not very practical. In this paper, we propose a\nnew ZKP protocol for Sudoku that can be performed using only two standard decks\nof playing cards, regardless of whether the two decks are identical or\ndifferent. In general, we also develop the first ZKP protocol for a generalized\n$n \\times n$ Sudoku that can be performed using a deck of all different cards.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $S$ be a Shimura variety and let $h$ be a Weil height function on $S$. We\nconjecture that the heights of special points in $S$ are discriminant\nnegligible. Assuming this conjecture to be true, we prove that the sizes of the\nGalois orbits of special points grow as a fixed power of their discriminant (an\ninvariant we will define in the text). In the case of Shimura varieties of\nabelian type, the height bound holds by the recently proved averaged Colmez\nformula, and our theorem gives a new proof of Tsimerman's Galois lower bound in\nthis case. The main novelty is that our approach avoids the use of\nMasser-W\\\"ustholz isogeny estimates, replacing them by a point-counting\nargument, and establishes lower bounds for Galois orbits conditional on height\nbounds for \\emph{arbitrary} Shimura varieties. In particular, following the\nPila-Zannier strategy (and Gao's work in the mixed case) this implies that the\nAndre-Oort conjecture for an arbitrary (mixed) Shimura variety follows from the\ncorresponding conjecture on heights of special points.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  One of the most fascinating aspects of quantum fields in curved spacetime is\nthe Unruh effect. The direct experimental detection of Unruh temperature has\nremained an elusive challenge up to now. Gradient optical waveguides\nmanipulating the dispersion of photons are assumed to realize the great\nacceleration of effective particles, leading to a high effective Unruh\ntemperature. However, experimentally achieving this optical waveguide has not\nyet been reported. In this work, we exploit a tapered fiber to simulate the\naccelerated motion of effective particles and obtain an effective Unruh\ntemperature. When light propagating in a tapered fiber is affected by the\nexternal high refractive index medium, a leaky phenomenon akin to\nbremsstrahlung will be observed, and the pattern of leaky radiation is\ndependent on the acceleration of photons. During the experiments, different\naccelerations corresponding to different Unruh temperatures are achieved by\ncontrolling the shape of the tapered waveguide.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the status of fair sampling on Noisy Intermediate Scale Quantum\n(NISQ) devices, in particular the IBM Q family of backends. Using the recently\nintroduced Grover Mixer-QAOA algorithm for discrete optimization, we generate\nfair sampling circuits to solve six problems of varying difficulty, each with\nseveral optimal solutions, which we then run on twenty backends across the IBM\nQ system. For a given circuit evaluated on a specific set of qubits, we\nevaluate: how frequently the qubits return an optimal solution to the problem,\nthe fairness with which the qubits sample from all optimal solutions, and the\nreported hardware error rate of the qubits. To quantify fairness, we define a\nnovel metric based on Pearson's $\\chi^2$ test. We find that fairness is\nrelatively high for circuits with small and large error rates, but drops for\ncircuits with medium error rates. This indicates that structured errors\ndominate in this regime, while unstructured errors, which are random and thus\ninherently fair, dominate in noisier qubits and longer circuits. Our results\nshow that fairness can be a powerful tool for understanding the intricate web\nof errors affecting current NISQ hardware.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The charmoniumlike state $Y(4260)$ is described as predominantly a $D_1\n\\bar{D}$ molecule in a coupled-channel quark model\n[Phys.\\,Rev.\\,D\\,\\textbf{96},\\,114022\\,(2017)]. The heavy quark spin symmetry\n(HQSS) thus implies the possible emergence of its heavy quark spin partners\nwith molecular configuration as $D_1 \\bar{D}^*$ and $D_2^* \\bar{D}^*$ below\nthese charmed mesons' thresholds. We analyze the probabilities of various\nintermediate charmed meson loops for $J^{PC}=1^{--}$ exotic state $Y(4360)$ and\nfind that the channel $D_1 \\bar{D}^*$ couples more strongly around its mass\nregime, and the coupling behavior remains the same even if the mass of\n$Y(4360)$ is pushed closer to $D_1 \\bar{D}^*$ threshold. This enlightens that\nthe most favorable molecular scenario for the $Y(4360)$ could be $D_1\n\\bar{D}^*$, and hence it can be interpreted as HQSS partner of the $Y(4260)$.\nWe also find the strong coupling behavior of $D_2^* \\bar{D}^*$ channel with the\n$\\psi(4415)$, which makes it a good candidate for a dominant $D_2^* \\bar{D}^*$\nmolecule. We discuss the important decay patterns of these resonances to\ndisentangle their long- and short-distance structures.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Plasticity in hexagonal close-packed zirconium is controlled by screw\ndislocations which easily glide in the prismatic planes where they are\ndissociated. At high enough temperatures, these dislocations can deviate out of\nthe prism planes to also glide in the first order pyramidal and basal planes.\nTo get a better understanding of these secondary slip systems, we have\nperformed molecular dynamics (MD) simulations of a screw dislocation gliding in\na basal plane. The gliding dislocation remains dissociated in the prism plane\nwhere it performs a random motion and occasionally cross-slips out of its habit\nplane by the nucleation and propagation of a kink-pair. Deviation planes are\nalways pyramidal, with an equal probability to cross-slip in the two pyramidal\nplanes on both sides of the basal plane, thus leading to basal slip on average.\nBasal slip appears therefore as a combination of prismatic and pyramidal slip\nin the high stress regime explored in MD simulations. This is confirmed by\nnudged elastic band (NEB) calculations. But NEB calculations also reveal a\nchange of glide mechanism for a decreasing applied stress. At low stress, kinks\ndo not lie anymore in the pyramidal planes. They are now spread in the basal\nplanes, thus fully compatible with a motion of the screw dislocation confined\nto the basal plane as seen in experiments. Basal slip, which is in competition\nwith pyramidal slip, appears therefore favoured at low stress in pure\nzirconium.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Spectroscopic signatures associated with symmetric Lorentzian and asymmetric\nFano line shapes are ubiquitous. Distinct features of Fano resonances in\ncontrast with conventional symmetric resonances have found several applications\nin photonics such as optical switching, sensing, lasing, and nonlinear and\nslow-light devices. Therefore, it is important to have control over the\ngeneration of these resonances. In this work, we show through ab initio\nsimulations of coupled light-matter systems that Fano interference phenomena\ncan be realized in a multimode photonic environment by strong coupling to the\nelectromagnetic continuum. Specifically, we show that by effectively enhancing\nthe light-matter coupling strength to the photon continuum in an experimentally\nfeasible way, we can achieve a transition from Lorentzian to Fano lines shapes\nfor both electronic and polaritonic excitations. An important outcome of\nswitching between these spectral signatures is the possibility to control the\nPurcell enhancement of spontaneous emission alongside electromagnetically\ninduced transparency which is a special case of Fano resonances. Switching from\nFano back to a Lorentzian profile can be achieved by physically reducing the\ncoupling strength to the continuum of modes. Our results hold potential for\nrealizing tunable Fano resonances of molecules and materials interacting with\nthe electromagnetic continuum within multimode photonic environments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  General reduction of the elliptic hypergeometric equation to the level of\ncomplex hypergeometric functions is described. The derived equation is\ngeneralized to the Hamiltonian eigenvalue problem for new rational integrable\n$N$-body systems emerging from particular degenerations of the elliptic\nRuijsenaars and van Diejen models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Probabilistic circuits (PCs) are a family of generative models which allows\nfor the computation of exact likelihoods and marginals of its probability\ndistributions. PCs are both expressive and tractable, and serve as popular\nchoices for discrete density estimation tasks. However, large PCs are\nsusceptible to overfitting, and only a few regularization strategies (e.g.,\ndropout, weight-decay) have been explored. We propose HyperSPNs: a new paradigm\nof generating the mixture weights of large PCs using a small-scale neural\nnetwork. Our framework can be viewed as a soft weight-sharing strategy, which\ncombines the greater expressiveness of large models with the better\ngeneralization and memory-footprint properties of small models. We show the\nmerits of our regularization strategy on two state-of-the-art PC families\nintroduced in recent literature -- RAT-SPNs and EiNETs -- and demonstrate\ngeneralization improvements in both models on a suite of density estimation\nbenchmarks in both discrete and continuous domains.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For a linear code $C$ of length $n$ with dimension $k$ and minimum distance\n$d$, it is desirable that the quantity $kd/n$ is large. Given an arbitrary\nfield $\\mathbb{F}$, we introduce a novel, but elementary, construction that\nproduces a recursively defined sequence of $\\mathbb{F}$-linear codes $C_1,C_2,\nC_3, \\dots$ with parameters $[n_i, k_i, d_i]$ such that $k_id_i/n_i$ grows\nquickly in the sense that $k_id_i/n_i>\\sqrt{k_i}-1>2i-1$. Another example of\nquick growth comes from a certain subsequence of Reed-Muller codes. Here the\nfield is $\\mathbb{F}=\\mathbb{F}_2$ and $k_i d_i/n_i$ is asymptotic to\n$3n_i^{c}/\\sqrt{\\pi\\log_2(n_i)}$ where $c=\\log_2(3/2)\\approx 0.585$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A tapered floating point encoding is proposed which uses the redundant signed\nradix 2 system and is based on the canonical recoding. By making use of ternary\ntechnology, the encoding has a dynamic range exceeding that of the\nrecently-proposed Posit number system and the IEEE 754-1985 Standard for\nFloating Point Arithmetic (IEEE-754-1985), and precision equal to or better\nthan that of the IEEE-754-1985 system and the recently proposed Posit system\nwhen equal input sizes are compared. In addition, the encoding is capable of\nsupporting several proposed extensions, including extensions to integers,\nboolean values, complex numbers, higher number systems, low-dimensional\nvectors, and system artifacts such as machine instructions. A detailed analytic\ncomparison is provided between the proposed encoding, the IEEE-754-1985 system,\nand the recently proposed Posit number system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry\nwith the popular Affine-Invariant (AI) geometry for Riemannian optimization on\nthe symmetric positive definite (SPD) matrix manifold. Our study begins with an\nobservation that the BW metric has a linear dependence on SPD matrices in\ncontrast to the quadratic dependence of the AI metric. We build on this to show\nthat the BW metric is a more suitable and robust choice for several Riemannian\noptimization problems over ill-conditioned SPD matrices. We show that the BW\ngeometry has a non-negative curvature, which further improves convergence rates\nof algorithms over the non-positively curved AI geometry. Finally, we verify\nthat several popular cost functions, which are known to be geodesic convex\nunder the AI geometry, are also geodesic convex under the BW geometry.\nExtensive experiments on various applications support our findings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Leading methods in the domain of action recognition try to distill\ninformation from both the spatial and temporal dimensions of an input video.\nMethods that reach State of the Art (SotA) accuracy, usually make use of 3D\nconvolution layers as a way to abstract the temporal information from video\nframes. The use of such convolutions requires sampling short clips from the\ninput video, where each clip is a collection of closely sampled frames. Since\neach short clip covers a small fraction of an input video, multiple clips are\nsampled at inference in order to cover the whole temporal length of the video.\nThis leads to increased computational load and is impractical for real-world\napplications. We address the computational bottleneck by significantly reducing\nthe number of frames required for inference. Our approach relies on a temporal\ntransformer that applies global attention over video frames, and thus better\nexploits the salient information in each frame. Therefore our approach is very\ninput efficient, and can achieve SotA results (on Kinetics dataset) with a\nfraction of the data (frames per video), computation and latency. Specifically\non Kinetics-400, we reach $80.5$ top-1 accuracy with $\\times 30$ less frames\nper video, and $\\times 40$ faster inference than the current leading method.\nCode is available at: https://github.com/Alibaba-MIIL/STAM\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Additive manufacturing, called 3D printing, starts to play an unprecedented\nrole in developing many applications in industrial or personalized products.\nThe conductive composite structures require additional treatment to achieve an\nelectroactive surface useful for electrochemical devices. In this paper, the\nsurfaces of carbon black/poly(lactic acid) CB-PLA printouts were activated by\nelectrolysis or enzymatic digestion with proteinase K, or a simultaneous\ncombination of both. Proposed modification protocols allowed for tailoring\nelectrochemically active surface area and electron transfer kinetics determined\nby electrochemical techniques (CV, EIS) with [Fe(CN)6]4-/3- redox probe. The\nX-ray photon spectroscopy and SEM imaging were applied to determine the\ndelivered surface chemistry. The CB-PLA hydrolysis in alkaline conditions and\nunder anodic polarization greatly impacts the charge transfer kinetics. The\nenzymatic hydrolysis of PLA with proteinase K has led to highly efficient\nresults yet requiring an unsatisfactory prolonged activation duration of 72 h,\nefficiently reduced by the electrolysis carried out in the presence of the\nenzyme. Our studies hint that the activation protocol originates from surface\nelectropolymerization rather than synergistic interaction between electrolysis\nand enzymatic hydrolysis. The detailed mechanism of CB-PLA hydrolysis supported\nby electrolysis has been elaborated since it pawed a new route towards a\ntime-efficient and environmentally-friendly activation procedure.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Assist-as-needed (AAN) control aims at promoting therapeutic outcomes in\nrobot-assisted rehabilitation by encouraging patients' active participation.\nImpedance control is used by most AAN controllers to create a compliant force\nfield around a target motion to ensure tracking accuracy while allowing\nmoderate kinematic errors. However, since the parameters governing the shape of\nthe force field are often tuned manually or adapted online based on simplistic\nassumptions about subjects' learning abilities, the effectiveness of\nconventional AAN controllers may be limited. In this work, we propose a novel\nadaptive AAN controller that is capable of autonomously reshaping the force\nfield in a phase-dependent manner according to each individual's motor\nabilities and task requirements. The proposed controller consists of a modified\nPolicy Improvement with Path Integral algorithm, a model-free, sampling-based\nreinforcement learning method that learns a subject-specific impedance\nlandscape in real-time, and a hierarchical policy parameter evaluation\nstructure that embeds the AAN paradigm by specifying performance-driven\nlearning goals. The adaptability of the proposed control strategy to subjects'\nmotor responses and its ability to promote short-term motor adaptations are\nexperimentally validated through treadmill training sessions with able-bodied\nsubjects who learned altered gait patterns with the assistance of a powered\nankle-foot orthosis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  While counterfactual examples are useful for analysis and training of NLP\nmodels, current generation methods either rely on manual labor to create very\nfew counterfactuals, or only instantiate limited types of perturbations such as\nparaphrases or word substitutions. We present Polyjuice, a general-purpose\ncounterfactual generator that allows for control over perturbation types and\nlocations, trained by finetuning GPT-2 on multiple datasets of paired\nsentences. We show that Polyjuice produces diverse sets of realistic\ncounterfactuals, which in turn are useful in various distinct applications:\nimproving training and evaluation on three different tasks (with around 70%\nless annotation effort than manual generation), augmenting state-of-the-art\nexplanation techniques, and supporting systematic counterfactual error analysis\nby revealing behaviors easily missed by human experts.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In many life science experiments or medical studies, subjects are repeatedly\nobserved and measurements are collected in factorial designs with multivariate\ndata. The analysis of such multivariate data is typically based on multivariate\nanalysis of variance (MANOVA) or mixed models, requiring complete data, and\ncertain assumption on the underlying parametric distribution such as continuity\nor a specific covariance structure, e.g., compound symmetry. However, these\nmethods are usually not applicable when discrete data or even ordered\ncategorical data are present. In such cases, nonparametric rank-based methods\nthat do not require stringent distributional assumptions are the preferred\nchoice. However, in the multivariate case, most rank-based approaches have only\nbeen developed for complete observations. It is the aim of this work is to\ndevelop asymptotic correct procedures that are capable of handling missing\nvalues, allowing for singular covariance matrices and are applicable for\nordinal or ordered categorical data. This is achieved by applying a wild\nbootstrap procedure in combination with quadratic form-type test statistics.\nBeyond proving their asymptotic correctness, extensive simulation studies\nvalidate their applicability for small samples. Finally, two real data examples\nare analyzed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present analysis of the spatial density structure for the outer disk from\n8$-$14 \\,kpc with the LAMOST DR5 13534 OB-type stars and observe similar\nflaring on north and south sides of the disk implying that the flaring\nstructure is symmetrical about the Galactic plane, for which the scale height\nat different Galactocentric distance is from 0.14 to 0.5 \\,kpc. By using the\naverage slope to characterize the flaring strength we find that the thickness\nof the OB stellar disk is similar but flaring is slightly stronger compared to\nthe thin disk as traced by red giant branch stars, possibly implying that\nsecular evolution is not the main contributor to the flaring but perturbation\nscenarios such as interactions with passing dwarf galaxies should be more\npossible. When comparing the scale height of OB stellar disk of the north and\nsouth sides with the gas disk, the former one is slightly thicker than the\nlater one by $\\approx$ 33 and 9 \\,pc, meaning that one could tentatively use\nyoung OB-type stars to trace the gas properties. Meanwhile, we unravel that the\nradial scale length of the young OB stellar disk is 1.17 $\\pm$ 0.05 \\,kpc,\nwhich is shorter than that of the gas disk, confirming that the gas disk is\nmore extended than stellar disk. What is more, by considering the mid-plane\ndisplacements ($Z_{0}$) in our density model we find that almost all of $Z_{0}$\nare within 100 \\,pc with the increasing trend as Galactocentric distance\nincreases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This article studies point-vortex models for the Euler and surface\nquasi-geostrophic equations. In the case of an inviscid fluid with planar\nmotion, the point-vortex model gives account of dynamics where the vorticity\nprofile is sharply concentrated around some points and approximated by Dirac\nmasses. This article contains three main results with several links between\neach other. In the first part, we provide two uniform bounds on the\ntrajectories for Euler and quasi-geostrophic vortices related to the\nnon-neutral cluster hypothesis. In a second part we focus on the Euler\npoint-vortex model and under the non-neutral cluster hypothesis we prove a\nconvergence result. The third part is devoted to the generalization of a\nclassical result by Marchioro and Pulvirenti concerning the improbability of\ncollapses and the extension of this result to the quasi-geostrophic case.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Longevity and safety of lithium-ion batteries are facilitated by efficient\nmonitoring and adjustment of the battery operating conditions. Hence, it is\ncrucial to implement fast and accurate algorithms for State of Health (SoH)\nmonitoring on the Battery Management System. The task is challenging due to the\ncomplexity and multitude of the factors contributing to the battery\ndegradation, especially because the different degradation processes occur at\nvarious timescales and their interactions play an important role. Data-driven\nmethods bypass this issue by approximating the complex processes with\nstatistical or machine learning models. This paper proposes a data-driven\napproach which is understudied in the context of battery degradation, despite\nits simplicity and ease of computation: the Multivariable Fractional Polynomial\n(MFP) regression. Models are trained from historical data of one exhausted cell\nand used to predict the SoH of other cells. The data are characterised by\nvarying loads simulating dynamic operating conditions. Two hypothetical\nscenarios are considered: one assumes that a recent capacity measurement is\nknown, the other is based only on the nominal capacity. It was shown that the\ndegradation behaviour of the batteries under examination is influenced by their\nhistorical data, as supported by the low prediction errors achieved (root mean\nsquared errors from 1.2% to 7.22% when considering data up to the battery End\nof Life). Moreover, we offer a multi-factor perspective where the degree of\nimpact of each different factor is analysed. Finally, we compare with a Long\nShort-Term Memory Neural Network and other works from the literature on the\nsame dataset. We conclude that the MFP regression is effective and competitive\nwith contemporary works, and provides several additional advantages e.g. in\nterms of interpretability, generalisability, and implementability.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Aspect sentiment triplet extraction (ASTE), which aims to identify aspects\nfrom review sentences along with their corresponding opinion expressions and\nsentiments, is an emerging task in fine-grained opinion mining. Since ASTE\nconsists of multiple subtasks, including opinion entity extraction, relation\ndetection, and sentiment classification, it is critical and challenging to\nappropriately capture and utilize the associations among them. In this paper,\nwe transform ASTE task into a multi-turn machine reading comprehension (MTMRC)\ntask and propose a bidirectional MRC (BMRC) framework to address this\nchallenge. Specifically, we devise three types of queries, including\nnon-restrictive extraction queries, restrictive extraction queries and\nsentiment classification queries, to build the associations among different\nsubtasks. Furthermore, considering that an aspect sentiment triplet can derive\nfrom either an aspect or an opinion expression, we design a bidirectional MRC\nstructure. One direction sequentially recognizes aspects, opinion expressions,\nand sentiments to obtain triplets, while the other direction identifies opinion\nexpressions first, then aspects, and at last sentiments. By making the two\ndirections complement each other, our framework can identify triplets more\ncomprehensively. To verify the effectiveness of our approach, we conduct\nextensive experiments on four benchmark datasets. The experimental results\ndemonstrate that BMRC achieves state-of-the-art performances.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Advances in biosignal signal processing and machine learning, in particular\nDeep Neural Networks (DNNs), have paved the way for the development of\ninnovative Human-Machine Interfaces for decoding the human intent and\ncontrolling artificial limbs. DNN models have shown promising results with\nrespect to other algorithms for decoding muscle electrical activity, especially\nfor recognition of hand gestures. Such data-driven models, however, have been\nchallenged by their need for a large number of trainable parameters and their\nstructural complexity. Here we propose the novel Temporal Convolutions-based\nHand Gesture Recognition architecture (TC-HGR) to reduce this computational\nburden. With this approach, we classified 17 hand gestures via surface\nElectromyogram (sEMG) signals by the adoption of attention mechanisms and\ntemporal convolutions. The proposed method led to 81.65% and 80.72%\nclassification accuracy for window sizes of 300ms and 200ms, respectively. The\nnumber of parameters to train the proposed TC-HGR architecture is 11.9 times\nless than that of its state-of-the-art counterpart.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Perovskite photovoltaics (PV) have achieved rapid development in the past\ndecade in terms of power conversion efficiency of small-area lab-scale devices;\nhowever, successful commercialization still requires further development of\nlow-cost, scalable, and high-throughput manufacturing techniques. One of the\ncritical challenges of developing a new fabrication technique is the\nhigh-dimensional parameter space for optimization, but machine learning (ML)\ncan readily be used to accelerate perovskite PV scaling. Herein, we present an\nML-guided framework of sequential learning for manufacturing process\noptimization. We apply our methodology to the Rapid Spray Plasma Processing\n(RSPP) technique for perovskite thin films in ambient conditions. With a\nlimited experimental budget of screening 100 process conditions, we\ndemonstrated an efficiency improvement to 18.5% as the best-in-our-lab device\nfabricated by RSPP, and we also experimentally found 10 unique process\nconditions to produce the top-performing devices of more than 17% efficiency,\nwhich is 5 times higher rate of success than the control experiments with\npseudo-random Latin hypercube sampling. Our model is enabled by three\ninnovations: (a) flexible knowledge transfer between experimental processes by\nincorporating data from prior experimental data as a probabilistic constraint;\n(b) incorporation of both subjective human observations and ML insights when\nselecting next experiments; (c) adaptive strategy of locating the region of\ninterest using Bayesian optimization first, and then conducting local\nexploration for high-efficiency devices. Furthermore, in virtual benchmarking,\nour framework achieves faster improvements with limited experimental budgets\nthan traditional design-of-experiments methods (e.g., one-variable-at-a-time\nsampling).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Object Skeletonization is the process of extracting skeletal, line-like\nrepresentations of shapes. It provides a very useful tool for geometric shape\nunderstanding and minimal shape representation. It also has a wide variety of\napplications, most notably in anatomical research and activity detection.\nSeveral mathematical algorithmic approaches have been developed to solve this\nproblem, and some of them have been proven quite robust. However, a lesser\namount of attention has been invested into deep learning solutions for it. In\nthis paper, we use a 2-stage variant of the famous U-Net architecture to split\nthe problem space into two sub-problems: shape minimization and corrective\nskeleton thinning. Our model produces results that are visually much better\nthan the baseline SkelNetOn model. We propose a new metric, M-CCORR, based on\nnormalized correlation coefficients as an alternative to F1 for this challenge\nas it solves the problem of class imbalance, managing to recognize skeleton\nsimilarity without suffering from F1's over-sensitivity to pixel-shifts.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Detection and recognition of scene texts of arbitrary shapes remain a grand\nchallenge due to the super-rich text shape variation in text line orientations,\nlengths, curvatures, etc. This paper presents a mask-guided multi-task network\nthat detects and rectifies scene texts of arbitrary shapes reliably. Three\ntypes of keypoints are detected which specify the centre line and so the shape\nof text instances accurately. In addition, four types of keypoint links are\ndetected of which the horizontal links associate the detected keypoints of each\ntext instance and the vertical links predict a pair of landmark points (for\neach keypoint) along the upper and lower text boundary, respectively. Scene\ntexts can be located and rectified by linking up the associated landmark points\n(giving localization polygon boxes) and transforming the polygon boxes via thin\nplate spline, respectively. Extensive experiments over several public datasets\nshow that the use of text keypoints is tolerant to the variation in text\norientations, lengths, and curvatures, and it achieves superior scene text\ndetection and rectification performance as compared with state-of-the-art\nmethods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper three heuristic algorithms using the Divide-and-Conquer\nparadigm are developed and assessed for three integer optimizations problems:\nMultidimensional Knapsack Problem (d-KP), Bin Packing Problem (BPP) and\nTravelling Salesman Problem (TSP). For each case, the algorithm is introduced,\ntogether with the design of numerical experiments, in order to empirically\nestablish its performance from both points of view: its computational time and\nits numerical accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider two kinds of superpositions of squeezed states of light. In the\ncase of superpositions of first kind, the squeezing and all higher order\nsqueezing vanishes. However, in the case of the second kind, it is possible to\nachieve a maximum amount of squeezing by adjusting the parameters in the\nsuperposition. The emergence and vanishing of squeezing for the superposition\nstates are explained on the basis of expectation values of the energy density.\nWe show that expectation values of energy density of quantum states which show\nno squeezing will be always positive and that of squeezed states will be\nnegative for some values of spacetime-dependent phase.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given any $d$-dimensional Lipschitz Riemannian manifold $(M,g)$ with heat\nkernel $\\mathsf{p}$, we establish uniform upper bounds on $\\mathsf{p}$ which\ncan always be decoupled in space and time. More precisely, we prove the\nexistence of a constant $C>0$ and a bounded Lipschitz function $R\\colon M \\to\n(0,\\infty)$ such that for every $x\\in M$ and every $t>0$, \\begin{align*}\n\\sup_{y\\in M} \\mathsf{p}(t,x,y) \\leq C\\min\\{t, R^2(x)\\}^{-d/2}. \\end{align*}\nThis allows us to identify suitable weighted Lebesgue spaces w.r.t. the given\nvolume measure as subsets of the Kato class induced by $(M,g)$. In the case\n$\\partial M \\neq \\emptyset$, we also provide an analogous inclusion for\nLebesgue spaces w.r.t. the surface measure on $\\partial M$.\n  We use these insights to give sufficient conditions for a possibly\nnoncomplete Lipschitz Riemannian manifold to be tamed, i.e. to admit a\nmeasure-valued lower bound on the Ricci curvature, formulated in a synthetic\nsense.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Gamma-ray bursts (GRBs), as a possible probe to extend the Hubble diagram to\nhigh redshifts, have attracted much attention recently. In this paper, we\nselect two samples of GRBs that have a plateau phase in X-ray afterglow. One is\nshort GRBs with plateau phases dominated by magnetic dipole (MD) radiations.\nThe other is long GRBs with gravitational-wave (GW) dominated plateau phases.\nThese GRBs can be well standardized using the correlation between the plateau\nluminosity $L_0$ and the end time of plateau $t_b$. The so-called circularity\nproblem is mitigated by using the observational Hubble parameter data and\nGaussian process method. The calibrated \\ltb ~correlations are also used to\nconstrain $\\Lambda$CDM and $w(z)$ = $w_{0}$ models. Combining the MD-LGRBs\nsample from Wang et al. (2021) and the MD-SGRBs sample, we find $\\Omega_{m} =\n0.33_{-0.09}^{+0.06}$ and $\\Omega_{\\Lambda}$ = $1.06_{-0.34}^{+0.15}$ excluding\nsystematic uncertainties in the nonflat $\\Lambda$CDM model. Adding type Ia\nsupernovae from Pantheon sample, the best-fitting results are $w_{0}$ =\n$-1.11_{-0.15}^{+0.11}$ and $\\Omega_{m}$ = $0.34_{-0.04}^{+0.05}$ in the\n$w=w_0$ model. These results are in agreement with the $\\Lambda$CDM model. Our\nresult supports that selection of GRBs from the same physical mechanism is\ncrucial for cosmological purposes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The amount of scholarly data has been increasing dramatically over the last\nyears. For newcomers to a particular science domain (e.g., IR, physics, NLP) it\nis often difficult to spot larger trends and to position the latest research in\nthe context of prior scientific achievements and breakthroughs. Similarly,\nresearchers in the history of science are interested in tools that allow them\nto analyze and visualize changes in particular scientific domains. Temporal\nsummarization and related methods should be then useful for making sense of\nlarge volumes of scientific discourse data aggregated over time. We demonstrate\na novel approach to analyze the collections of research papers published over\nlonger time periods to provide a high-level overview of important semantic\nchanges that occurred over the progress of time. Our approach is based on\ncomparing word semantic representations over time and aims to support users in\na better understanding of large domain-focused archives of scholarly\npublications. As an example dataset we use the ACL Anthology Reference Corpus\nthat spans from 1979 to 2015 and contains 22,878 scholarly articles.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Stellar activity due to different processes (magnetic activity, photospheric\nflows) affects the measurement of radial velocities (RV). Radial velocities\nhave been widely used to detect exoplanets, although the stellar signal\nsignificantly impacts the detection and characterisation performance,\nespecially for low mass planets. On the other hand, RV time series are also\nvery rich in information on stellar processes. In this lecture, I review the\ncontext of RV observations, describe how radial velocities are measured, and\nthe properties of typical observations. I present the challenges represented by\nstellar activity for exoplanet studies, and describe the processes at play.\nFinally, I review the approaches which have been developed, including\nobservations and simulations, as well as solar and stellar comparisons.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A result due to Williams, Stampfli and Fillmore shows that an essential\nisometry $T$ on a Hilbert space $\\mathcal{H}$ is a compact perturbation of an\nisometry if and only if ind$(T)\\le 0$. A recent result of S. Chavan yields an\nanalogous characterization of essential spherical isometries\n$T=(T_1,\\dots,T_n)\\in\\mathcal{B}(\\mathcal{H})^n$ with\ndim($\\bigcap_{i=1}^n\\ker(T_i))\\le$ dim$(\\bigcap_{i=1}^n\\ker(T_i^*))$. In the\npresent note we show that in dimension $n>1$ the result of Chavan holds without\nany condition on the dimensions of the joint kernels of $T$ and $T^*$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Inference of the marginal likelihood of sample allele configurations using\nbackward algorithms yields identical results with the Kingman coalescent, the\nMoran model, and the diffusion model (up to a scaling of time). For inference\nof probabilities of ancestral population allele frequencies at any given point\nin the past - either of discrete ancestral allele configurations as in the\ncoalescent, or of ancestral allele proportions as in the backward diffusion -\nbackward approaches need to be combined with corresponding forward ones. This\nis done in so-called forward-backward algorithms. In this article, we utilize\northogonal polynomials in forward-backward algorithms. They enable efficient\ncalculation of past allele configurations of an extant sample and probabilities\nof ancestral population allele frequencies in equilibrium and in\nnon-equilibrium. We show that the genealogy of a sample is fully described by\nthe backward polynomial expansion of the marginal likelihood of its allele\nconfiguration.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that thermodynamic scaling can be derived by combining the Murnaghan\nequation of state (EOS) with the generalized entropy theory (GET) of glass\nformation. In our theory, thermodynamic scaling arises in the non-Arrhenius\nrelaxation regime as a scaling property of the fluid configurational entropy\ndensity $s_c$, normalized by its value $s_c^*$ at the onset temperature $T_A$\nof glass formation, $s_c / s_c^*$, so that a constant value of $TV^{\\gamma}$\ncorresponds to a \\textit{reduced isoentropic} fluid condition. Molecular\ndynamics simulations on a coarse-grained polymer melt are utilized to confirm\nthat the predicted thermodynamic scaling of $\\tau_{\\alpha}$ by the GET holds\nboth above and below $T_A$ and to test whether the extent $L$ of stringlike\ncollective motion, normalized its value $L_A$ at $T_A$, also obeys\nthermodynamic scaling, as required for consistency with thermodynamic scaling.\nWhile the predicted thermodynamic scaling of both $\\tau_{\\alpha}$ and $L/ L_A$\nis confirmed by simulation, we find that the isothermal compressibility\n$\\kappa_T$ and the long wavelength limit $S(0)$ of the static structure factor\ndo not exhibit thermodynamic scaling, an observation that would appear to\neliminate some proposed models of glass formation emphasizing fluid `structure'\nover configurational entropy. It is found, however, that by defining a low\ntemperature hyperuniform reference state, we may define a compressibility\nrelative to this condition, $\\delta \\kappa_T$, a transformed dimensionless\nvariable that exhibits thermodynamic scaling and which can be directly related\nto $s_c / s_c^*$. Further, the Murnaghan EOS allows us to interpret $\\gamma$ as\na measure of intrinsic anharmonicity of intermolecular interactions that may be\ndirectly determined from the pressure derivative of the material bulk modulus.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Particle transport in complex environments such as the interior of living\ncells is often (transiently) non-Fickian or anomalous, that is, it deviates\nfrom the laws of Brownian motion. Such anomalies may be the result of\nsmall-scale spatio-temporal heterogeneities in, or viscoelastic properties of,\nthe medium, molecular crowding, etc. Often the observed dynamics displays\nmulti-state characteristics, i.e. distinct modes of transport dynamically\ninterconverting between each other in a stochastic manner. Reliably\ndistinguishing between single- and multi-state dynamics is challenging and\nrequires a combination of distinct approaches. To complement the existing\nmethods relying on the analysis of the particle's mean squared displacement,\nposition- or displacement-autocorrelation function, and propagators, we here\nfocus on \"scattering fingerprints\" of multi-state dynamics. We develop a\ntheoretical framework for two-state scattering signatures -- the intermediate\nscattering function and dynamic structure factor -- and apply it to the\nanalysis of simple model systems as well as particle-tracking experiments in\nliving cells. We consider inert tracer-particle motion as well as systems with\nan internal structure and dynamics. Our results may generally be relevant for\nthe interpretation of state-of-the-art differential dynamic microscopy\nexperiments on complex particulate systems, as well as inelastic or\nquasielastic neutron (incl. spin-echo) and X-ray scattering scattering probing\nstructural and dynamical properties of macromolecules, when the underlying\ndynamics displays two-state transport.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the 1958 paper \"Shall we count the living or the dead?\", Mindel C. Sheps\nproposed a principled solution to the familiar problem of asymmetry of the\nrelative risk. We provide causal models to clarify the scope and limitations of\nSheps' line of reasoning, and show that her preferred variant of the relative\nrisk will be stable between patient groups under certain biologically\ninterpretable conditions. Such stability is useful when findings from an\nintervention study must be generalized to support clinical decisions in\npatients whose risk profile differs from the participants in the study. We show\nthat Sheps' approach is consistent with a substantial body of psychological and\nphilosophical research on how human reasoners carry causal information from one\ncontext to another, and that it can be implemented in practice using van der\nLaan et al's Switch Relative Risk, or equivalently, using Baker and Jackson's\nGeneralized Relative Risk Reduction (GRRR).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-Target Multi-Camera (MTMC) vehicle tracking is an essential task of\nvisual traffic monitoring, one of the main research fields of Intelligent\nTransportation Systems. Several offline approaches have been proposed to\naddress this task; however, they are not compatible with real-world\napplications due to their high latency and post-processing requirements. In\nthis paper, we present a new low-latency online approach for MTMC tracking in\nscenarios with partially overlapping fields of view (FOVs), such as road\nintersections. Firstly, the proposed approach detects vehicles at each camera.\nThen, the detections are merged between cameras by applying cross-camera\nclustering based on appearance and location. Lastly, the clusters containing\ndifferent detections of the same vehicle are temporally associated to compute\nthe tracks on a frame-by-frame basis. The experiments show promising\nlow-latency results while addressing real-world challenges such as the a priori\nunknown and time-varying number of targets and the continuous state estimation\nof them without performing any post-processing of the trajectories.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a detailed comparison of the rheological behaviour of sheared\nsediment beds in a pressure-driven, straight channel configuration based on\ndata that was generated by means of fully coupled, grain-resolved direct\nnumerical simulations and experimental measurements reviously published by\nAussillous {\\it et al.} (J. Fluid Mech., vol. 736, 2013, pp. 594-615). The\nhighly-resolved simulation data allows to compute the stress balance of the\nsuspension in the streamwise and vertical directions and the stress exchange\nbetween the fluid and particle phase, which is information needed to infer the\nrheology, but has so far been unreachable in experiments. Applying this\nknowledge to the experimental and numerical data, we obtain the\nstatistically-stationary, depth-resolved profiles of the relevant rheological\nquantities. The scaling behavior of rheological quantities such as the shear\nand normal viscosities and the effective friction coefficient are examined and\ncompared to data coming from rheometry experiments and from widely-used\nrheological correlations. We show that rheological properties that have\npreviously been inferred for annular Couette-type shear flows with neutrally\nbuoyant particles still hold for our setup of sediment transport in a\nPoiseuille flow and in the dense regime we found good agreement with empirical\nrelationships derived therefrom. Subdividing the total stress into parts from\nparticle contact and hydrodynamics suggests a critical particle volume fraction\nof 0.3 to separate the dense from the dilute regime. In the dilute regime,\ni.e., the sediment transport layer, long-range hydrodynamic interactions are\nscreened by the porous media and the effective viscosity obeys the Einstein\nrelation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The increasing need for reducing greenhouse gas emissions and the drive for\ngreen cities have promoted the use of electric vehicles due to their\nenvironmental benefits. In fact, countries have set their own targets and are\noffering incentives for people to purchase EVs as opposed to traditional\ngasoline-powered cars. Manufacturers have been hastily deploying charging\nstations to meet the charging requirements of the EVs on the road. This rapid\ndeployment has contributed to the EV ecosystem's lack of proper security\nmeasures, raising multiple questions related to the power grid security and\nvulnerability. In this paper, we offer a complete examination of the EV\necosystem from the vulnerability to the attacks and finally the solutions. We\nstart by examining the existing vulnerabilities in the EV ecosystem that can be\nexploited to control the EV charging and launch attacks against the power grid.\nWe then discuss the non-linear nature of the EV charging load and simulate\nmultiple attacks that can be launched against the power grid using these EVs.\nEV loads have high reactive power demand which can have a larger impact on the\ngrid compared to residential loads. We perform simulations on two power grids\nand demonstrate that while the grid can recover after a 48 MW attack utilizing\ntraditional residential loads, a smaller 30 MW EV load attack can completely\ndestabilize the system. Finally, we suggest several patches for the existing\nvulnerabilities and discuss two methods aimed at detecting EV attacks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A prevalence of production of twisted (vortex) particles in noncentral\nheavy-ion collisions is shown. In such collisions, photons emitted due to the\nrotation of charges are highly twisted. Charged particles are produced in\nnonspreading multiwave states and have significant orbital angular momenta. It\ncan be expected that an emission of any twisted particles manifesting\nthemselves in specific effects is rather ubiquitous.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper reports a CPU-level real-time stereo matching method for surgical\nimages (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed\nmethod is built on the fast ''dense inverse searching'' algorithm, which\nestimates the disparity of the stereo images. The overlapping image patches\n(arbitrary squared image segment) from the images at different scales are\naligned based on the photometric consistency presumption. We propose a Bayesian\nframework to evaluate the probability of the optimized patch disparity at\ndifferent scales. Moreover, we introduce a spatial Gaussian mixed probability\ndistribution to address the pixel-wise probability within the patch. In-vivo\nand synthetic experiments show that our method can handle ambiguities resulted\nfrom the textureless surfaces and the photometric inconsistency caused by the\nLambertian reflectance. Our Bayesian method correctly balances the probability\nof the patch for stereo images at different scales. Experiments indicate that\nthe estimated depth has higher accuracy and fewer outliers than the baseline\nmethods in the surgical scenario.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Efforts toward stabilization of the Si$_{20}$ fullerene through different\nschemes have failed despite several theoretical predictions. However, recently\nTillmann {\\it et. al.} succeeded to stabilize the Si$_{20}$ fullerene through\nexohedral decoration with eight Cl substituents and twelve SiCl$_3$ groups on\nthe surface and enclosing Cl$^-$ ion. A deeper understanding on what factors\nlead to stabilization will open the path for stabilizing other systems of\ninterest. Here, we employ the minima hopping method within density functional\ntheory to understand the potential energy surface. The study shows that the\nexo-endo halide decoration of the cage alters the glassy nature of the\npotential energy surface of pure cage to structure seeker. Further analysis of\ndifferent properties of the global minima, reveal that the extra electron\ninstead of residing on the central encapsulated atom in the cage, it is\ndistributed on the cage and increases the encapsulation energy; thereby\nstabilizing the system. We also provide estimates of the stability for\ndifferent kind of exo-endo halide decorations and their feasible realization in\nexperiments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study quantum information masking of arbitrary dimensional states. Given a\nset of fixed reducing pure states, we study the linear combinations of them,\nsuch that they all have the same marginal states with the given ones. We define\nthe so called Hadamard set of quantum states whose Gram-Schmidt matrix can be\ndiagonalized by Hadamard unitary matrices. We show that any Hadamard set can be\ndeterministically masked by a unitary operation. We analyze the states which\ncan be masked together with the given Hadamard set using the result about the\nlinear combinations of fixed reducing states. Detailed examples are given to\nillustrate our results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Meta-learning aims to learn a model that can handle multiple tasks generated\nfrom an unknown but shared distribution. However, typical meta-learning\nalgorithms have assumed the tasks to be similar such that a single meta-learner\nis sufficient to aggregate the variations in all aspects. In addition, there\nhas been less consideration on uncertainty when limited information is given as\ncontext. In this paper, we devise a novel meta-learning framework, called\nMeta-learning Amidst Heterogeneity and Ambiguity (MAHA), that outperforms\nprevious works in terms of prediction based on its ability on task\nidentification. By extensively conducting several experiments in regression and\nclassification, we demonstrate the validity of our model, which turns out to be\nrobust to both task heterogeneity and ambiguity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In-memory deep learning computes neural network models where they are stored,\nthus avoiding long distance communication between memory and computation units,\nresulting in considerable savings in energy and time. In-memory deep learning\nhas already demonstrated orders of magnitude higher performance density and\nenergy efficiency. The use of emerging memory technology promises to increase\nthe gains in density, energy, and performance even further. However, emerging\nmemory technology is intrinsically unstable, resulting in random fluctuations\nof data reads. This can translate to non-negligible accuracy loss, potentially\nnullifying the gains. In this paper, we propose three optimization techniques\nthat can mathematically overcome the instability problem of emerging memory\ntechnology. They can improve the accuracy of the in-memory deep learning model\nwhile maximizing its energy efficiency. Experiments show that our solution can\nfully recover most models' state-of-the-art accuracy, and achieves at least an\norder of magnitude higher energy efficiency than the state-of-the-art.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present PSEUDo, an adaptive feature learning technique for exploring\nvisual patterns in multi-track sequential data. Our approach is designed with\nthe primary focus to overcome the uneconomic retraining requirements and\ninflexible representation learning in current deep learning-based systems.\nMulti-track time series data are generated on an unprecedented scale due to\nincreased sensors and data storage. These datasets hold valuable patterns, like\nin neuromarketing, where researchers try to link patterns in multivariate\nsequential data from physiological sensors to the purchase behavior of products\nand services. But a lack of ground truth and high variance make automatic\npattern detection unreliable. Our advancements are based on a novel query-aware\nlocality-sensitive hashing technique to create a feature-based representation\nof multivariate time series windows. Most importantly, our algorithm features\nsub-linear training and inference time. We can even accomplish both the\nmodeling and comparison of 10,000 different 64-track time series, each with 100\ntime steps (a typical EEG dataset) under 0.8 seconds. This performance gain\nallows for a rapid relevance feedback-driven adaption of the underlying pattern\nsimilarity model and enables the user to modify the speed-vs-accuracy trade-off\ngradually. We demonstrate superiority of PSEUDo in terms of efficiency,\naccuracy, and steerability through a quantitative performance comparison and a\nqualitative visual quality comparison to the state-of-the-art algorithms in the\nfield. Moreover, we showcase the usability of PSEUDo through a case study\ndemonstrating our visual pattern retrieval concepts in a large meteorological\ndataset. We find that our adaptive models can accurately capture the user's\nnotion of similarity and allow for an understandable exploratory visual pattern\nretrieval in large multivariate time series datasets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We remove the semisimple condition in the mod-$p$ local-global compatibility\nresult of arXiv:2106.10674. Namely, assuming flatness of\n$\\pi_{\\mathfrak{m}}^\\vee$ and\n$\\overline{\\sigma}_{\\mathfrak{m}}|_{\\mathrm{Gal}_{F^+_{\\mathfrak{p}}}}$ being\nmultiplicity free, we prove that\n$H^{n-1}_{\\text{\\'et}}(\\mathbb{P}_{\\mathbb{C}_p}^{n-1},\n\\mathcal{F}_{\\pi_{\\mathfrak{m}}[\\mathfrak{m}]})$ determines\n$\\overline{\\sigma}_{\\mathfrak{m}}|_{\\mathrm{Gal}_{F^+_{\\mathfrak{p}}}}$\nuniquely. We give remarks on our assumptions at the end of this note.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The task of image captioning aims to generate captions directly from images\nvia the automatically learned cross-modal generator. To build a well-performing\ngenerator, existing approaches usually need a large number of described images,\nwhich requires a huge effects on manual labeling. However, in real-world\napplications, a more general scenario is that we only have limited amount of\ndescribed images and a large number of undescribed images. Therefore, a\nresulting challenge is how to effectively combine the undescribed images into\nthe learning of cross-modal generator. To solve this problem, we propose a\nnovel image captioning method by exploiting the Cross-modal Prediction and\nRelation Consistency (CPRC), which aims to utilize the raw image input to\nconstrain the generated sentence in the commonly semantic space. In detail,\nconsidering that the heterogeneous gap between modalities always leads to the\nsupervision difficulty of using the global embedding directly, CPRC turns to\ntransform both the raw image and corresponding generated sentence into the\nshared semantic space, and measure the generated sentence from two aspects: 1)\nPrediction consistency. CPRC utilizes the prediction of raw image as soft label\nto distill useful supervision for the generated sentence, rather than employing\nthe traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel\nrelation consistency between augmented images and corresponding generated\nsentences to retain the important relational knowledge. In result, CPRC\nsupervises the generated sentence from both the informativeness and\nrepresentativeness perspectives, and can reasonably use the undescribed images\nto learn a more effective generator under the semi-supervised scenario.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum algorithms designed for noisy intermediate-scale quantum devices\nusually require repeatedly perform a large number of quantum measurements in\nestimating observable expectation values of a many-qubit quantum state.\nExploiting the ideas of importance sampling, observable compatibility, and\nclassical shadows of quantum states, different advanced quantum measurement\nschemes have been proposed to greatly reduce the large measurement cost. Yet,\nthe underline cost reduction mechanisms seem distinct to each other, and how to\nsystematically find the optimal scheme remains a critical theoretical\nchallenge. Here, we address this challenge by firstly proposing a unified\nframework of quantum measurements, incorporating the advanced measurement\nmethods as special cases. Our framework further allows us to introduce a\ngeneral scheme -- overlapped grouping measurement, which simultaneously\nexploits the advantages of the existing methods. We show that an optimal\nmeasurement scheme corresponds to partitioning the observables into overlapped\ngroups with each group consisting of compatible ones. We provide explicit\ngrouping strategies and numerically verify its performance for different\nmolecular Hamiltonians. Our numerical results show great improvements to the\noverall existing measurement schemes. Our work paves the way for efficient\nquantum measurement with near-term quantum devices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Suppose that Alice plans to buy a physical good from Bob over a programmable\nBlockchain. Alice does not trust Bob, so she is not willing to pay before the\ngood is delivered off-chain. Similarly, Bob does not trust Alice, so he is not\nwilling to deliver the good before getting paid on-chain. Moreover, they are\nnot inclined to use the services of a trusted third-party. Traditionally, such\nscenarios are handled by game-theoretic escrow smart contracts, such as\nBitHalo. In this work, we first show that the common method for this problem\nsuffers from a major flaw which can be exploited by Bob in order to extort\nAlice. We also show that, unlike the case of auctions, this flaw cannot be\naddressed by a commitment-scheme-based approach. We then provide a much more\ngeneral result: assuming that the two sides are rational actors and the smart\ncontract language is Turing-complete, there is no escrow smart contract that\ncan facilitate this exchange without either relying on third parties or\nenabling at least one side to extort the other.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Networks of optical oscillators simulating coupled Ising spins have been\nrecently proposed as a heuristic platform to solve hard optimization problems.\nThese networks, called coherent Ising machines (CIMs), exploit the fact that\nthe collective nonlinear dynamics of coupled oscillators can drive the system\nclose to the global minimum of the classical Ising Hamiltonian, encoded in the\ncoupling matrix of the network. To date, realizations of large-scale CIMs have\nbeen demonstrated using hybrid optical-electronic setups, where optical\noscillators simulating different spins are subject to electronic feedback\nmechanisms emulating their mutual interaction. While the optical evolution\nensures an ultrafast computation, the electronic coupling represents a\nbottleneck that causes the computational time to severely depend on the system\nsize. Here, we propose an all-optical scalable CIM with fully-programmable\ncoupling. Our setup consists of an optical parametric amplifier with a spatial\nlight modulator (SLM) within the parametric cavity. The spin variables are\nencoded in the binary phases of the optical wavefront of the signal beam at\ndifferent spatial points, defined by the pixels of the SLM. We first discuss\nhow different coupling topologies can be achieved by different configurations\nof the SLM, and then benchmark our setup with a numerical simulation that\nmimics the dynamics of the proposed machine. In our proposal, both the spin\ndynamics and the coupling are fully performed in parallel, paving the way\ntowards the realization of size-independent ultrafast optical hardware for\nlarge-scale computation purposes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Nowadays, most classification networks use one-hot encoding to represent\ncategorical data because of its simplicity. However, one-hot encoding may\naffect the generalization ability as it neglects inter-class correlations. We\nobserve that, even when a neural network trained with one-hot labels produces\nincorrect predictions, it still pays attention to the target image region and\nreveals which classes confuse the network. Inspired by this observation, we\npropose a confusion-focusing mechanism to address the class-confusion issue.\nOur confusion-focusing mechanism is implemented by a two-branch network\narchitecture. Its baseline branch generates confusing classes, and its FocusNet\nbranch, whose architecture is flexible, discriminates correct labels from these\nconfusing classes. We also introduce a novel focus-picking loss function to\nimprove classification accuracy by encouraging FocusNet to focus on the most\nconfusing classes. The experimental results validate that our FocusNet is\neffective for image classification on common datasets, and that our\nfocus-picking loss function can also benefit the current neural networks in\nimproving their classification accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Next-generation, high-brilliance x-ray photon sources call for new x-ray\noptics. Here we demonstrate the feasibility of using monolithic diamond\nchannel-cut crystals as high-heat-load, beam-multiplexing, narrow-band,\nmechanically-stable x-ray monochromators with high-power x-ray beams at\ncutting-edge, high-repetition-rate x-ray free-electron laser (XFEL) facilities.\nThe diamond channel-cut crystals fabricated and characterized in these studies\nare designed as two-bounce Bragg reflection monochromators directing 14.4-keV\nor 12.4-keV x-rays within a 15-meV-bandwidth to $^{57}$Fe or $^{45}$Sc nuclear\nresonant scattering experiments, respectively. The crystal design allows\nout-of-band x-rays within a $\\simeq 1$-eV XFEL bandwidth to be transmitted with\nminimal losses to alternative simultaneous experiments. Only $\\lesssim 2$\\% of\nthe incident $\\simeq 100$-W x-ray beam is absorbed in a 50-$\\mu$m-thick first\ndiamond crystal reflector, ensuring that the monochromator crystal is highly\nstable. Other x-ray optics applications of diamond channel-cut crystals are\nanticipated.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop an effective field theory (EFT) framework for superfluid ${}^4$He\nto model the interactions among quasiparticles, helium atoms and probe\nparticles. Our effective field theory approach brings together symmetry\narguments and power-counting and matches to classical fluid dynamics. We then\npresent the decay and scattering rates for the relevant processes involving\nquasiparticles and helium atoms. The presented EFT framework and results can be\nused to understand the dynamics of thermalization in the superfluid, and can be\nfurther applied to sub-GeV dark matter direct detection with superfluid\n${}^4$He.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this study, we introduce a novel open-source chemistry model for OpenFOAM\nto speed-up the reactive computational fluid dynamics (CFD) simulations using\nfinite-rate chemistry. First, a dynamic load balancing model called DLBFoam is\nintroduced to balance the chemistry load during runtime in parallel\nsimulations. In addition, the solution of the cell-based chemistry problem is\nimproved by utilizing an analytical Jacobian using an open-source library\ncalled pyJac and an efficient linear algebra library LAPACK. Combination of the\naforementioned efforts yields a speed-up factor 200 for a high-fidelity\nlarge-eddy simulation spray combustion case compared to the standard OpenFOAM\nimplementation. It is worth noting that the present implementation does not\ncompromise the solution accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The line potential energy in the cutset is used as the criterion for\nmonitoring the generator instability, but the criterion has the following two\nlimitations due to narrowly defined conditions. The assumption of an ideal\nconstant power load is difficult to satisfy, and the condition of the critical\ncutset is too narrow. The limitations are addressed by analyzing the\nrelationship between the power of the load elements and the power of the cutset\nbased on the two-machine equivalent. Modifications of the theoretical\nderivation steps of this criterion are presented to extend the applications of\nthe criterion and provide theoretical support for its online use. Two simple\nstrategies are proposed to ensure early detection of the generator instability.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The first full angular analysis and an updated measurement of the decay-rate\n$CP$ asymmetry of the $D^0 \\to \\pi^+\\pi^-\\mu^+\\mu^-$ and $D^0 \\to\nK^+K^-\\mu^+\\mu^-$ decays are reported. The analysis uses proton-proton\ncollision data collected with the LHCb detector at centre-of-mass energies of\n7, 8 and 13 TeV. The data set corresponds to an integrated luminosity of 9\nfb$^{-1}$. The full set of $CP$-averaged angular observables and their $CP$\nasymmetries are measured as a function of the dimuon invariant mass. The\nresults are consistent with expectations from the standard model and with $CP$\nsymmetry.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce a novel architecture for neural disparity refinement aimed at\nfacilitating deployment of 3D computer vision on cheap and widespread consumer\ndevices, such as mobile phones. Our approach relies on a continuous formulation\nthat enables to estimate a refined disparity map at any arbitrary output\nresolution. Thereby, it can handle effectively the unbalanced camera setup\ntypical of nowadays mobile phones, which feature both high and low resolution\nRGB sensors within the same device. Moreover, our neural network can process\nseamlessly the output of a variety of stereo methods and, by refining the\ndisparity maps computed by a traditional matching algorithm like SGM, it can\nachieve unpaired zero-shot generalization performance compared to\nstate-of-the-art end-to-end stereo models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we propose and evaluate the performance of a 3D-embedded\nneuromorphic computation block based on indium gallium zinc oxide\n($\\alpha$-IGZO) based nanosheet transistor and bi-layer resistive memory\ndevices. We have fabricated bi-layer resistive random-access memory (RRAM)\ndevices with Ta$_2$O$_5$ and Al$_2$O$_3$ layers. The device has been\ncharacterized and modeled. The compact models of RRAM and $\\alpha$-IGZO based\nembedded nanosheet structures have been used to evaluate the system-level\nperformance of 8 vertically stacked $\\alpha$-IGZO based nanosheet layers with\nRRAM for neuromorphic applications. The model considers the design space with\nuniform bit line (BL), select line (SL), and word line (WL) resistance.\nFinally, we have simulated the weighted sum operation with our proposed 8-layer\nstacked nanosheet-based embedded memory and evaluated the performance for\nVGG-16 convolutional neural network (CNN) for Fashion-MNIST and CIFAR-10 data\nrecognition, which yielded 92% and 75% accuracy respectively with drop out\nlayers amid device variation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This work investigates the fundamental mechanism(s) that drive galaxy\nevolution in the Local Universe. By comparing two proxies of star-formation\nsensitive to different timescales, such as EW(H$\\alpha$) and colours like\n$g-r$, one may distinguish between smooth secular evolution (ageing) and sudden\nchanges (quenching) on the recent star formation history of galaxies. Building\nupon the results obtained from a former study based on 80.000 SDSS single-fibre\nmeasurements, we now focus on spatially-resolved (on kpc scales) galaxies,\ncomparing with a sample of 637 nearby objects observed by the CALIFA survey. In\ngeneral, galaxies cannot be characterised in terms of a single `evolutionary\nstage'. Individual regions within galaxies arrange along a relatively narrow\nageing sequence, with some intrinsic scatter possibly due to their different\nevolutionary paths. These sequences, though, differ from one galaxy to another,\nalthough they are broadly consistent with the overall distribution found for\nthe (central) SDSS spectra. We find evidence of recent quenching episodes\n(relatively blue colours and strong H$\\alpha$ absorption) in a small fraction\nof galaxies (most notably, low-mass ellipticals), on global scales and\nindividual regions (particularly at high metallicity). However, we argue that\nmost of the systems, over their entire extent, are compatible with a secular\ninside-out scenario, where the evolutionary stage correlates with both global\n(mass, morphology, and environment) as well as local (surface brightness and\nmetallicity) properties.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a graph $G = (V,E)$, a threshold function $t~ :~ V \\rightarrow\n\\mathbb{N}$ and an integer $k$, we study the Harmless Set problem, where the\ngoal is to find a subset of vertices $S \\subseteq V$ of size at least $k$ such\nthat every vertex $v\\in V$ has less than $t(v)$ neighbors in $S$. We enhance\nour understanding of the problem from the viewpoint of parameterized\ncomplexity. Our focus lies on parameters that measure the structural properties\nof the input instance. We show that the problem is W[1]-hard parameterized by a\nwide range of fairly restrictive structural parameters such as the feedback\nvertex set number, pathwidth, treedepth, and even the size of a minimum vertex\ndeletion set into graphs of pathwidth and treedepth at most three. On dense\ngraphs, we show that the problem is W[1]-hard parameterized by cluster vertex\ndeletion number. We also show that the Harmless Set problem with majority\nthresholds is W[1]-hard when parameterized by the treewidth of the input graph.\nWe prove that the Harmless Set problem can be solved in polynomial time on\ngraph with bounded cliquewidth. On the positive side, we obtain fixed-parameter\nalgorithms for the problem with respect to neighbourhood diversity, twin cover\nand vertex integrity of the input graph. We show that the problem parameterized\nby the solution size is fixed parameter tractable on planar graphs. We thereby\nresolve two open questions stated in C. Bazgan and M. Chopin (2014) concerning\nthe complexity of {\\sc Harmless Set} parameterized by the treewidth of the\ninput graph and on planar graphs with respect to the solution size.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The goal of a well-controlled study is to remove unwanted variation when\nestimating the causal effect of the intervention of interest. Experiments\nconducted in the basic sciences frequently achieve this goal using experimental\ncontrols, such as \"negative\" and \"positive\" controls, which are measurements\ndesigned to detect systematic sources of unwanted variation. Here, we introduce\nclear, mathematically precise definitions of experimental controls using\npotential outcomes. Our definitions provide a unifying statistical framework\nfor fundamental concepts of experimental design from the biological and other\nbasic sciences. These controls are defined in terms of whether assumptions are\nbeing made about a specific treatment level, outcome, or contrast between\noutcomes. We discuss experimental controls as tools for researchers to wield in\ndesigning experiments and detecting potential design flaws, including using\ncontrols to diagnose unintended factors that influence the outcome of interest,\nassess measurement error, and identify important subpopulations. We believe\nthat experimental controls are powerful tools for reproducible research that\nare possibly underutilized by statisticians, epidemiologists, and social\nscience researchers.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Topological insulators have emerged as an important material class for\nefficient spin-charge interconversion. Most topological insulators considered\nto date are binary or ternary compounds, with the exception of $\\alpha$-Sn.\nHere we report a comprehensive characterization of the growth, magnetotransport\nproperties, and current-induced spin-orbit torques of $\\alpha$-Sn and\n$\\beta$-Sn-based ferromagnetic heterostructures. We show that $\\alpha$-Sn grown\nwith a Bi surfactant on CdTe(001) promotes large spin-orbit torques in a\nferromagnetic FeCo layer at room temperature, comparable to Pt, whereas\n$\\alpha$-Sn grown without Bi surfactant and the non-topological phase,\n$\\beta$-Sn, induce lower torques. The dampinglike and fieldlike spin-orbit\ntorque efficiency in $\\alpha$-Sn with Bi are 0.12 and 0.18, respectively.\nFurther, we show that $\\alpha$-Sn grown with and without Bi presents a spin\nHall-like magnetoresistance comparable to that found in heavy metal/ferromagnet\nbilayers. Our work demonstrates direct and efficient charge-to-spin conversion\nin $\\alpha$-Sn ferromagnetic heterostructures, showing that $\\alpha$-Sn is a\npromising material for current-induced magnetization control in spintronic\ndevices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Minimally invasive image-guided surgery heavily relies on vision. Deep\nlearning models for surgical video analysis could therefore support visual\ntasks such as assessing the critical view of safety (CVS) in laparoscopic\ncholecystectomy (LC), potentially contributing to surgical safety and\nefficiency. However, the performance, reliability and reproducibility of such\nmodels are deeply dependent on the quality of data and annotations used in\ntheir development. Here, we present a protocol, checklists, and visual examples\nto promote consistent annotation of hepatocystic anatomy and CVS criteria. We\nbelieve that sharing annotation guidelines can help build trustworthy\nmulticentric datasets for assessing generalizability of performance, thus\naccelerating the clinical translation of deep learning models for surgical\nvideo analysis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we analyze the spectra of the preconditioned matrices arising\nfrom discretized multi-dimensional Riesz spatial fractional diffusion\nequations. The finite difference method is employed to approximate the\nmulti-dimensional Riesz fractional derivatives, which will generate symmetric\npositive definite ill-conditioned multi-level Toeplitz matrices. The\npreconditioned conjugate gradient method with a preconditioner based on the\nsine transform is employed to solve the resulting linear system. Theoretically,\nwe prove that the spectra of the preconditioned matrices are uniformly bounded\nin the open interval (1/2,3/2) and thus the preconditioned conjugate gradient\nmethod converges linearly. The proposed method can be extended to multi-level\nToeplitz matrices generated by functions with zeros of fractional order. Our\ntheoretical results fill in a vacancy in the literature. Numerical examples are\npresented to demonstrate our new theoretical results in the literature and show\nthe convergence performance of the proposed preconditioner that is better than\nother existing preconditioners.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A full understanding of high-mass star formation requires the study of one of\nthe most elusive components of the energy balance in the interstellar medium:\nmagnetic fields. We report ALMA 1.2 mm, high-resolution (700 au) dust\npolarization and molecular line observations of the rotating hot molecular core\nembedded in the high-mass star-forming region IRAS 18089-1732. The dust\ncontinuum emission and magnetic field morphology present spiral-like features\nresembling a whirlpool. The velocity field traced by the H13CO+ (J=3-2)\ntransition line reveals a complex structure with spiral filaments that are\nlikely infalling and rotating, dragging the field with them. We have modeled\nthe magnetic field and find that the best model corresponds to a weakly\nmagnetized core with a mass-to-magnetic-flux ratio (lambda) of 8.38. The\nmodeled magnetic field is dominated by a poloidal component, but with an\nimportant contribution from the toroidal component that has a magnitude of 30%\nof the poloidal component. Using the Davis-Chandrasekhar-Fermi method, we\nestimate a magnetic field strength of 3.5 mG. At the spatial scales accessible\nto ALMA, an analysis of the energy balance of the system indicates that gravity\noverwhelms turbulence, rotation, and the magnetic field. We show that high-mass\nstar formation can occur in weakly magnetized environments, with gravity taking\nthe dominant role.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The associations between emergent physical phenomena (e.g.,\nsuperconductivity) and orbital, charge, and spin degrees of freedom of $3d$\nelectrons are intriguing in transition metal compounds. Here, we successfully\nmanipulate the superconductivity of spinel oxide Li$_{1\\pm\nx}$Ti$_2$O$_{4-\\delta}$ (LTO) by ionic liquid gating. A dome-shaped\nsuperconducting phase diagram is established, where two insulating phases are\ndisclosed both in heavily electron-doping and hole-doping regions. The\nsuperconductor-insulator transition (SIT) in the hole-doping region can be\nattributed to the loss of Ti valence electrons. In the electron-doping region,\nLTO exhibits an unexpected SIT instead of a metallic behavior despite an\nincrease in carrier density. Furthermore, a thermal hysteresis is observed in\nthe normal state resistance curve, suggesting a first-order phase transition.\nWe speculate that the SIT and the thermal hysteresis stem from the enhanced\n$3d$ electron correlations and the formation of orbital ordering by comparing\nthe transport and structural results of LTO with the other spinel oxide\nsuperconductor MgTi$_2$O$_4$, as well as analysing the electronic structure by\nfirst-principles calculations. Further comprehension of the detailed interplay\nbetween superconductivity and orbital ordering would contribute to the\nrevealing of unconventional superconducting pairing mechanism.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Renewable energy forecasting is attaining greater importance due to its\nconstant increase in contribution to the electrical power grids. Solar energy\nis one of the most significant contributors to renewable energy and is\ndependent on solar irradiation. For the effective management of electrical\npower grids, forecasting models that predict solar irradiation, with high\naccuracy, are needed. In the current study, Machine Learning techniques such as\nLinear Regression, Extreme Gradient Boosting and Genetic Algorithm Optimization\nare used to forecast solar irradiation. The data used for training and\nvalidation is recorded from across three different geographical stations in the\nUnited States that are part of the SURFRAD network. A Global Horizontal Index\n(GHI) is predicted for the models built and compared. Genetic Algorithm\nOptimization is applied to XGB to further improve the accuracy of solar\nirradiation prediction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The ability to generate high-fidelity synthetic data is crucial when\navailable (real) data is limited or where privacy and data protection standards\nallow only for limited use of the given data, e.g., in medical and financial\ndata-sets. Current state-of-the-art methods for synthetic data generation are\nbased on generative models, such as Generative Adversarial Networks (GANs).\nEven though GANs have achieved remarkable results in synthetic data generation,\nthey are often challenging to interpret.Furthermore, GAN-based methods can\nsuffer when used with mixed real and categorical variables.Moreover, loss\nfunction (discriminator loss) design itself is problem specific, i.e., the\ngenerative model may not be useful for tasks it was not explicitly trained for.\nIn this paper, we propose to use a probabilistic model as a synthetic data\ngenerator. Learning the probabilistic model for the data is equivalent to\nestimating the density of the data. Based on the copula theory, we divide the\ndensity estimation task into two parts, i.e., estimating univariate marginals\nand estimating the multivariate copula density over the univariate marginals.\nWe use normalising flows to learn both the copula density and univariate\nmarginals. We benchmark our method on both simulated and real data-sets in\nterms of density estimation as well as the ability to generate high-fidelity\nsynthetic data\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article, we introduce a framework for quantum state tomography of\nqutrits by projective measurements. The framework is based on photon-counting\nwith measurement results distorted due to the Poisson noise and dark counts.\nTwo different measurement schemes are investigated numerically and compared in\nterms of their efficiency for distinct numbers of photons per measurement. The\naccuracy of state reconstruction is quantified by figures of merit which are\npresented on graphs versus the amount of noise.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sleeping problems have become one of the major diseases all over the world.\nTo tackle this issue, the basic tool used by specialists is the Polysomnogram,\nwhich is a collection of different signals recorded during sleep. After its\nrecording, the specialists have to score the different signals according to one\nof the standard guidelines. This process is carried out manually, which can be\nhighly time-consuming and very prone to annotation errors. Therefore, over the\nyears, many approaches have been explored in an attempt to support the\nspecialists in this task. In this paper, an approach based on convolutional\nneural networks is presented, where an in-depth comparison is performed in\norder to determine the convenience of using more than one signal simultaneously\nas input. Additionally, the models were also used as parts of an ensemble model\nto check whether any useful information can be extracted from signal processing\na single signal at a time which the dual-signal model cannot identify. Tests\nhave been performed by using a well-known dataset called expanded sleep-EDF,\nwhich is the most commonly used dataset as the benchmark for this problem. The\ntests were carried out with a leave-one-out cross-validation over the patients,\nwhich ensures that there is no possible contamination between training and\ntesting. The resulting proposal is a network smaller than previously published\nones, but which overcomes the results of any previous models on the same\ndataset. The best result shows an accuracy of 92.67\\% and a Cohen's Kappa value\nover 0.84 compared to human experts.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In many applications, we collect independent samples from interconnected\npopulations. These population distributions share some latent structure, so it\nis advantageous to jointly analyze the samples. One effective way to connect\nthe distributions is the semiparametric density ratio model (DRM). A key\ningredient in the DRM is that the log density ratios are linear combinations of\nprespecified functions; the vector formed by these functions is called the\nbasis function. A sensible basis function can often be chosen based on\nknowledge of the context, and DRM-based inference is effective even if the\nbasis function is imperfect. However, a data-adaptive approach to the choice of\nbasis function remains an interesting and important research problem. We\npropose an approach based on the classical functional principal component\nanalysis (FPCA). Under some conditions, we show that this approach leads to\nconsistent basis function estimation. Our simulation results show that the\nproposed adaptive choice leads to an efficiency gain. We use a real-data\nexample to demonstrate the efficiency gain and the ease of our approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Monocular 3D object detection is an important task in autonomous driving. It\ncan be easily intractable where there exists ego-car pose change w.r.t. ground\nplane. This is common due to the slight fluctuation of road smoothness and\nslope. Due to the lack of insight in industrial application, existing methods\non open datasets neglect the camera pose information, which inevitably results\nin the detector being susceptible to camera extrinsic parameters. The\nperturbation of objects is very popular in most autonomous driving cases for\nindustrial products. To this end, we propose a novel method to capture camera\npose to formulate the detector free from extrinsic perturbation. Specifically,\nthe proposed framework predicts camera extrinsic parameters by detecting\nvanishing point and horizon change. A converter is designed to rectify\nperturbative features in the latent space. By doing so, our 3D detector works\nindependent of the extrinsic parameter variations and produces accurate results\nin realistic cases, e.g., potholed and uneven roads, where almost all existing\nmonocular detectors fail to handle. Experiments demonstrate our method yields\nthe best performance compared with the other state-of-the-arts by a large\nmargin on both KITTI 3D and nuScenes datasets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Incremental learning represents a crucial task in aerial image processing,\nespecially given the limited availability of large-scale annotated datasets. A\nmajor issue concerning current deep neural architectures is known as\ncatastrophic forgetting, namely the inability to faithfully maintain past\nknowledge once a new set of data is provided for retraining. Over the years,\nseveral techniques have been proposed to mitigate this problem for image\nclassification and object detection. However, only recently the focus has\nshifted towards more complex downstream tasks such as instance or semantic\nsegmentation. Starting from incremental-class learning for semantic\nsegmentation tasks, our goal is to adapt this strategy to the aerial domain,\nexploiting a peculiar feature that differentiates it from natural images,\nnamely the orientation. In addition to the standard knowledge distillation\napproach, we propose a contrastive regularization, where any given input is\ncompared with its augmented version (i.e. flipping and rotations) in order to\nminimize the difference between the segmentation features produced by both\ninputs. We show the effectiveness of our solution on the Potsdam dataset,\noutperforming the incremental baseline in every test. Code available at:\nhttps://github.com/edornd/contrastive-distillation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We argue that black holes admit vortex structure. This is based both on a\ngraviton-condensate description of a black hole as well as on a correspondence\nbetween black holes and generic objects with maximal entropy compatible with\nunitarity, so-called saturons. We show that due to vorticity, a $Q$-ball-type\nsaturon of a calculable renormalisable theory obeys the same extremality bound\non the spin as the black hole. Correspondingly, a black hole with extremal spin\nemerges as a graviton condensate with vorticity. Next, we show that in the\npresence of mobile charges, the global vortex traps a magnetic flux of the\ngauge field. This can have macroscopically-observable consequences. For\ninstance, the most powerful jets observed in active galactic nuclei can\npotentially be accounted for. As a signature, such emissions can occur even\nwithout a magnetized accretion disk surrounding the black hole. The flux\nentrapment can provide an observational window to various hidden sectors, such\nas millicharged dark matter.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study problems with multiple missing covariates and partially observed\nresponses. We develop a new framework to handle complex missing covariate\nscenarios via inverse probability weighting, regression adjustment, and a\nmultiply-robust procedure. We apply our framework to three classical problems:\nthe Cox model from survival analysis, missing response, and binary treatment\nfrom causal inference. We also discuss how to handle missing covariates in\nthese scenarios, and develop associated identifying theories and asymptotic\ntheories. We apply our procedure to simulations and an Alzheimer's disease\ndataset and obtain meaningful results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, online linear regression in environments corrupted by\nnon-Gaussian noise (especially heavy-tailed noise) is addressed. In such\nenvironments, the error between the system output and the label also does not\nfollow a Gaussian distribution and there might exist abnormally large error\nsamples (or outliers) which mislead the learning process. The main challenge is\nhow to keep the supervised learning problem least affected by these unwanted\nand misleading outliers. In recent years, an information theoretic algorithm\nbased on Renyi's entropy, called minimum error entropy (MEE), has been employed\nto take on this issue. However, this minimization might not result in a desired\nestimator inasmuch as entropy is shift-invariant, i.e., by minimizing the error\nentropy, error samples may not be necessarily concentrated around zero. In this\npaper, a quantization technique is proposed by which not only aforementioned\nneed of setting errors around the origin in MEE is addressed, but also major\noutliers are rejected from MEE-based learning and MEE performance is improved\nfrom convergence rate, steady state misalignment, and testing error points of\nview.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We theoretically propose and study in-plane anisotropic acoustic phonon\npolaritons (APhPs) based on a layered structure consisting of a monolayer (or\nfew layers) {\\alpha}-phase molybdenum trioxide ({\\alpha}-MoO3) sandwiched\nbetween two metal layers. We find that the APhPs in the proposed sandwiched\nstructures are a canalization (highly directional) electromagnetic mode\npropagating along with the layers and at the same time exhibit extreme\nelectromagnetic-field confinement surpassing any other type of phonon-polariton\nmodes. When a double layer of {\\alpha}-MoO3 is sandwiched by two Au layers,\ntwisting the two {\\alpha}-MoO3 layers can adjust the interlayer polaritonic\ncoupling and thus manipulate the in-plane propagation of the highly confined\nAPhPs. Our results illustrate that the metal-MoO3-metal sandwiched structures\nare a promising platform for light guiding and manipulation at ultimate scale.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  An {\\em almost p-K\\\"ahler manifold} is a triple $(M,J,\\Omega)$, where $(M,J)$\nis an almost complex manifold of real dimension $2n$ and $\\Omega$ is a closed\nreal tranverse $(p,p)$-form on $(M,J)$, where $1\\leq p\\leq n$. When $J$ is\nintegrable, almost $p$-K\\\"ahler manifolds are called $p$-{\\em K\\\"ahler\nmanifolds}. We produce families of almost $p$-K\\\"ahler structures\n$(J_t,\\Omega_t)$ on $\\C^3$, $\\C^4$, and on the real torus $\\mathbb{T}^6$,\narising as deformations of K\\\"ahler structures $(J_0,g_0,\\omega_0)$, such that\nthe almost complex structures $J_t$ cannot be locally compatible with any\nsymplectic form for $t\\neq 0$. Furthermore, examples of special compact\nnilmanifolds with and without almost $p$-K\\\"ahler structures are presented.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we propose and evaluate a new reinforcement learning method,\nCOMPact Experience Replay (COMPER), which uses temporal difference learning\nwith predicted target values based on recurrence over sets of similar\ntransitions, and a new approach for experience replay based on two transitions\nmemories. Our objective is to reduce the required number of experiences to\nagent training regarding the total accumulated rewarding in the long run. Its\nrelevance to reinforcement learning is related to the small number of\nobservations that it needs to achieve results similar to that obtained by\nrelevant methods in the literature, that generally demand millions of video\nframes to train an agent on the Atari 2600 games. We report detailed results\nfrom five training trials of COMPER for just 100,000 frames and about 25,000\niterations with a small experiences memory on eight challenging games of Arcade\nLearning Environment (ALE). We also present results for a DQN agent with the\nsame experimental protocol on the same games set as the baseline. To verify the\nperformance of COMPER on approximating a good policy from a smaller number of\nobservations, we also compare its results with that obtained from millions of\nframes presented on the benchmark of ALE.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The development of pathological speech systems is currently hindered by the\nlack of a standardised objective evaluation framework. In this work, (1) we\nutilise existing detection and analysis techniques to propose a general\nframework for the consistent evaluation of synthetic pathological speech. This\nframework evaluates the voice quality and the intelligibility aspects of speech\nand is shown to be complementary using our experiments. (2) Using our proposed\nevaluation framework, we develop and test a dysarthric voice conversion system\n(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\nshow that the developed system is able to synthesise dysarthric speech with\ndifferent levels of speech intelligibility.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The backup control barrier function (CBF) was recently proposed as a\ntractable formulation that guarantees the feasibility of the CBF quadratic\nprogramming (QP) via an implicitly defined control invariant set. The control\ninvariant set is based on a fixed backup policy and evaluated online by forward\nintegrating the dynamics under the backup policy. This paper is intended as a\ntutorial of the backup CBF approach and a comparative study to some benchmarks.\nFirst, the backup CBF approach is presented step by step with the underlying\nmath explained in detail. Second, we prove that the backup CBF always has a\nrelative degree 1 under mild assumptions. Third, the backup CBF approach is\ncompared with benchmarks such as Hamilton Jacobi PDE and Sum-of-Squares on the\ncomputation of control invariant sets, which shows that one can obtain a\ncontrol invariant set close to the maximum control invariant set under a good\nbackup policy for many practical problems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We formulate a general, signature-independent form of the law of the excluded\nmiddle and prove that a logic is semisimple if and only if it enjoys this law,\nprovided that it satisfies a weak form of the so-called inconsistency lemma of\nRaftery. We then show that this equivalence can be used to provide simple\nsyntactic proofs of the theorems of Kowalski and Kracht characterizing the\nsemisimple varieties of FLew-algebras and Boolean algebras with operators, and\nto extend them to FLe-algebras and Heyting algebras with operators. Moreover,\nunder stronger assumptions this correspondence works at the level of individual\nmodels: the semisimple models of such a logic are precisely those which satisfy\nan axiomatic form of the law of the excluded middle, and a Glivenko-like\nconnection obtains between the logic and its extension by the axiom of the\nexcluded middle. This in particular subsumes the well-known Glivenko theorems\nrelating intuitionistic and classical logic and the modal logics S4 and S5. As\na consequence, we also obtain a description of the subclassical substructural\nlogics which are Glivenko related to classical logic.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the multicritical behavior of the three-dimensional Z_2 gauge\nHiggs model, at the multicritical point (MCP) of its phase diagram, where one\nfirst-order transition line and two continuous Ising-like transition lines\nmeet. The duality properties of the model determine some features of the\nmulticritical behavior at the MCP located along the self-dual line. Moreover,\nwe argue that the system develops a multicritical XY behavior at the MCP, which\nis controlled by the stable XY fixed point of the three-dimensional\nmulticritical Landau-Ginzburg-Wilson field theory with two competing scalar\nfields associated with the continuous Z_2 transition lines meeting at the MCP.\nThis implies an effective enlargement of the symmetry of the multicritical\nmodes at the MCP, to the continuous group O(2). We also provide some numerical\nresults to support the multicritical XY scenario.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Prepolarized Magnetic Resonance Imaging (PMRI) is a long-established\ntechnique conceived to counteract the loss in signal-to-noise ratio (SNR)\ninherent to low-field MRI systems. When it comes to hard biological tissues and\nsolid-state matter, PMRI is severely restricted by their ultra-short\ncharacteristic relaxation times. Here we demonstrate that efficient hard tissue\nprepolarization is within reach with a special-purpose 0.26 T scanner designed\nfor dental MRI and equipped with suitable high-power electronics. We have\ncharacterized the performance of a 0.5 T prepolarizer module which can be\nswitched on and off in just 200 us. To that end, we have used resin, dental and\nbone samples, all with T1 times in the order of 20 ms at our field strength.\nThe measured SNR enhancement is in good agreement with a simple theoretical\nmodel, and small deviations in extreme regimes can be attributed to mechanical\nvibrations due to the magnetic interaction between the prepolarization and main\nmagnets. Finally, we argue that these results can be applied to clinical dental\nimaging, opening the door to replacing hazardous X-ray systems with low-field\nPMRI scanners.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper formalizes connections between stability of polynomials and\nconvergence rates of Markov Chain Monte Carlo (MCMC) algorithms. We prove that\nif a (multivariate) partition function is nonzero in a region around a real\npoint $\\lambda$ then spectral independence holds at $\\lambda$. As a\nconsequence, for Holant-type problems (e.g., spin systems) on bounded-degree\ngraphs, we obtain optimal $O(n\\log n)$ mixing time bounds for the single-site\nupdate Markov chain known as the Glauber dynamics. Our result significantly\nimproves the running time guarantees obtained via the polynomial interpolation\nmethod of Barvinok (2017), refined by Patel and Regts (2017).\n  There are a variety of applications of our results. In this paper, we focus\non Holant-type (i.e., edge-coloring) problems, including weighted edge covers\nand weighted even subgraphs. For the weighted edge cover problem (and several\nnatural generalizations) we obtain an $O(n\\log{n})$ sampling algorithm on\nbounded-degree graphs. The even subgraphs problem corresponds to the\nhigh-temperature expansion of the ferromagnetic Ising model. We obtain an\n$O(n\\log{n})$ sampling algorithm for the ferromagnetic Ising model with a\nnonzero external field on bounded-degree graphs, which improves upon the\nclassical result of Jerrum and Sinclair (1993) for this class of graphs. We\nobtain further applications to antiferromagnetic two-spin models on line\ngraphs, weighted graph homomorphisms, tensor networks, and more.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  An Attempt is made to study the non-Hermitian effect on the topological\nquantum criticality and also in the physics of Majroana zero mode (MZMs). In\nthis work, the effects and modifications done by the non-Hermitian factor\n$\\gamma$ on the topological phases, criticality and also in the MZMs is\nstudied. We use the zero mode solutions (ZMS) to construct the phase diagram.\nWe find a correspondence between Hermitian and non-Hermitian model Hamiltonian.\nThe MZMs appear at criticality and has a stability dependence on the new\npassage created because of the introduction of non-Hermiticity. The\nmulticritical points are also studied to understand their nature under the\ninfluence of non-Hemiticity. We also study the effect of non-Hermiticity on the\ntopological phases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  LHC collisions can act as a source of photons in the initial state. This\nmechanism plays an important role in the production of particles with\nelectroweak couplings, and a precise account of photon-initiated (PI)\nproduction at the LHC is a key ingredient in the LHC precision physics\nprogramme. I will discuss the possibility of modelling PI processes directly\nvia the structure function approach. This can provide percent level precision\nin the production cross sections, and is therefore well positioned to account\nfor LHC precision requirements. This formalism in addition allows one to make\nuse of another useful feature of photons, namely that they are colour-singlet\nand can often be emitted elastically (or quasi-elastically) from the proton. I\nwill discuss recent work on applications of the structure function approach to\nprecision calculations of PI production in the inclusive mode, and to\n'exclusive' processes with rapidity gaps, which can provide a unique probe of\nthe Standard Model and physics beyond it.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A recent strand of research in structural proof theory aims at exploring the\nnotion of analytic calculi (i.e. those calculi that support general and modular\nproof-strategies for cut elimination), and at identifying classes of logics\nthat can be captured in terms of these calculi. In this context, Wansing\nintroduced the notion of proper display calculi as one possible design\nframework for proof calculi in which the analiticity desiderata are realized in\na particularly transparent way. Recently, the theory of properly displayable\nlogics (i.e. those logics that can be equivalently presented with some proper\ndisplay calculus) has been developed in connection with generalized Sahlqvist\ntheory (aka unified correspondence). Specifically, properly displayable logics\nhave been syntactically characterized as those axiomatized by analytic\ninductive axioms, which can be equivalently and algorithmically transformed\ninto analytic structural rules so that the resulting proper display calculi\nenjoy a set of basic properties: soundness, completeness, conservativity, cut\nelimination and subformula property. In this context, the proof that the given\ncalculus is complete w.r.t. the original logic is usually carried out\nsyntactically, i.e. by showing that a (cut free) derivation exists of each\ngiven axiom of the logic in the basic system to which the analytic structural\nrules algorithmically generated from the given axiom have been added. However,\nso far this proof strategy for syntactic completeness has been implemented on a\ncase-by-case base, and not in general. In this paper, we address this gap by\nproving syntactic completeness for properly displayable logics in any normal\n(distributive) lattice expansion signature. Specifically, we show that for\nevery analytic inductive axiom a cut free derivation can be effectively\ngenerated which has a specific shape, referred to as pre-normal form.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we study the category $\\mathcal{O}$ over the hyperalgebra of a\nreductive algebraic group in positive characteristics. For any locally closed\nsubset $\\mathcal{K}$ of weights we define a subquotient\n$\\mathcal{O}_{[\\mathcal{K}]}$ of $\\mathcal{O}$. It has the property that its\nsimple objects are parametrized by elements in $\\mathcal{K}$. We then show that\n$\\mathcal{O}_{[\\mathcal{K}]}$ is equivalent to\n$\\mathcal{O}_{[\\mathcal{K}+p^l\\gamma]}$ for any dominant weight $\\gamma$ if\n$l>0$ is an integer such that $\\mathcal{K}\\cap (\\mathcal{K}+p^l\\eta)=\\emptyset$\nfor all dominant weights $\\eta$. This allows one, for example, to restrict\nattention to subquotients inside the dominant (or the antidominant) chamber.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Cold boot attacks inspect the corrupted random access memory soon after the\npower has been shut down. While most of the bits have been corrupted, many\nbits, at random locations, have not. Since the keys in many encryption schemes\nare being expanded in memory into longer keys with fixed redundancies, the keys\ncan often be restored. In this work, we combine a novel cryptographic variant\nof a deep error correcting code technique with a modified SAT solver scheme to\napply the attack on AES keys. Even though AES consists of Rijndael S-box\nelements, that are specifically designed to be resistant to linear and\ndifferential cryptanalysis, our method provides a novel formalization of the\nAES key scheduling as a computational graph, which is implemented by a neural\nmessage passing network. Our results show that our methods outperform the state\nof the art attack methods by a very large margin.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sentence embedding methods offer a powerful approach for working with short\ntextual constructs or sequences of words. By representing sentences as dense\nnumerical vectors, many natural language processing (NLP) applications have\nimproved their performance. However, relatively little is understood about the\nlatent structure of sentence embeddings. Specifically, research has not\naddressed whether the length and structure of sentences impact the sentence\nembedding space and topology. This paper reports research on a set of\ncomprehensive clustering and network analyses targeting sentence and\nsub-sentence embedding spaces. Results show that one method generates the most\nclusterable embeddings. In general, the embeddings of span sub-sentences have\nbetter clustering properties than the original sentences. The results have\nimplications for future sentence embedding models and applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A new method is suggested for the extraction of $u$-quark and $d$-quark\ntransversity distributions, using single spin asymmetry (SSA) data in\nsemi-inclusive deep inelastic scattering (SIDIS) processes, where they couple\nto the Collins or the di-hadron fragmentation functions. We discuss a recent\nsuggestion to extract the transversity distribution using the concept of\ndifference asymmetries and their ratios, which avoids the requirement of\nCollins function. We suggest new measurements, involving ratios of polarized\ncross-sections, that would directly probe the ratio $h_1^{d_v}/h_1^{u_v}$. We\nalso show some numerical estimates.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We predict the average effect of Medicaid expansion on the non-elderly adult\nuninsurance rate among states that did not expand Medicaid in 2014 as if they\nhad expanded their Medicaid eligibility requirements. Using American Community\nSurvey data aggregated to the region level, we estimate this effect by finding\nweights that approximately reweights the expansion regions to match the\ncovariate distribution of the non-expansion regions. Existing methods to\nestimate balancing weights often assume that the covariates are measured\nwithout error and do not account for dependencies in the outcome model. Our\ncovariates have random noise that is uncorrelated with the outcome errors and\nour outcome model has state-level random effects inducing dependence between\nregions. To correct for the bias induced by the measurement error, we propose\ngenerating our weights on a linear approximation to the true covariates, using\nan idea from measurement error literature known as \"regression-calibration\"\n(see, e.g., Carroll (2006)). This requires auxiliary data to estimate the\nvariability of the measurement error. We also modify the Stable Balancing\nWeights objective proposed by Zubizaretta (2015)) to reduce the variance of our\nestimator when the model errors follow our assumed correlation structure. We\nshow that these approaches outperform existing methods when attempting to\npredict observed outcomes during the pre-treatment period. Using this method we\nestimate that Medicaid expansion would have caused a -2.33 (-3.54, -1.11)\npercentage point change in the adult uninsurance rate among states that did not\nexpand Medicaid.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Lexical inference in context (LIiC) is the task of recognizing textual\nentailment between two very similar sentences, i.e., sentences that only differ\nin one expression. It can therefore be seen as a variant of the natural\nlanguage inference task that is focused on lexical semantics. We formulate and\nevaluate the first approaches based on pretrained language models (LMs) for\nthis task: (i) a few-shot NLI classifier, (ii) a relation induction approach\nbased on handcrafted patterns expressing the semantics of lexical inference,\nand (iii) a variant of (ii) with patterns that were automatically extracted\nfrom a corpus. All our approaches outperform the previous state of the art,\nshowing the potential of pretrained LMs for LIiC. In an extensive analysis, we\ninvestigate factors of success and failure of our three approaches.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper presents a machine learning framework (GP-NODE) for Bayesian\nsystems identification from partial, noisy and irregular observations of\nnonlinear dynamical systems. The proposed method takes advantage of recent\ndevelopments in differentiable programming to propagate gradient information\nthrough ordinary differential equation solvers and perform Bayesian inference\nwith respect to unknown model parameters using Hamiltonian Monte Carlo sampling\nand Gaussian Process priors over the observed system states. This allows us to\nexploit temporal correlations in the observed data, and efficiently infer\nposterior distributions over plausible models with quantified uncertainty.\nMoreover, the use of sparsity-promoting priors such as the Finnish Horseshoe\nfor free model parameters enables the discovery of interpretable and\nparsimonious representations for the underlying latent dynamics. A series of\nnumerical studies is presented to demonstrate the effectiveness of the proposed\nGP-NODE method including predator-prey systems, systems biology, and a\n50-dimensional human motion dynamical system. Taken together, our findings put\nforth a novel, flexible and robust workflow for data-driven model discovery\nunder uncertainty. All code and data accompanying this manuscript are available\nonline at \\url{https://github.com/PredictiveIntelligenceLab/GP-NODEs}.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop a model of inter-temporal and intra-temporal price discrimination\nby monopoly airlines to study the ability of different discriminatory pricing\nmechanisms to increase efficiency and the associated distributional\nimplications. To estimate the model, we use unique data from international\nairline markets with flight-level variation in prices across time, cabins, and\nmarkets and information on passengers' reasons for travel and time of purchase.\nThe current pricing practice yields approximately 77% of the first-best\nwelfare. The source of this inefficiency arises primarily from private\ninformation about passenger valuations, not dynamic uncertainty about demand.\nWe also find that if airlines could discriminate between business and leisure\npassengers, total welfare would improve at the expense of business passenger\nsurplus. Also, replacing the current pricing that involves screening passengers\nacross cabin classes with offering a single cabin class has minimal effect on\ntotal welfare.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we study the problem of multiple stochastic agents interacting\nin a dynamic game scenario with continuous state and action spaces. We define a\nnew notion of stochastic Nash equilibrium for boundedly rational agents, which\nwe call the Entropic Cost Equilibrium (ECE). We show that ECE is a natural\nextension to multiple agents of Maximum Entropy optimality for single agents.\nWe solve both the \"forward\" and \"inverse\" problems for the multi-agent ECE\ngame. For the forward problem, we provide a Riccati algorithm to compute\nclosed-form ECE feedback policies for the agents, which are exact in the\nLinear-Quadratic-Gaussian case. We give an iterative variant to find locally\nECE feedback policies for the nonlinear case. For the inverse problem, we\npresent an algorithm to infer the cost functions of the multiple interacting\nagents given noisy, boundedly rational input and state trajectory examples from\nagents acting in an ECE. The effectiveness of our algorithms is demonstrated in\na simulated multi-agent collision avoidance scenario, and with data from the\nINTERACTION traffic dataset. In both cases, we show that, by taking into\naccount the agents' game theoretic interactions using our algorithm, a more\naccurate model of agents' costs can be learned, compared with standard inverse\noptimal control methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modeling the extragalactic astroparticle skies involves reconstructing the 3D\ndistribution of the most extreme sources in the Universe. Full-sky tomographic\nsurveys at near-infrared wavelengths have already enabled the astroparticle\ncommunity to bind the density of sources of astrophysical neutrinos and\nultra-high cosmic rays (UHECRs), constrain the distribution of binary\nblack-hole mergers and identify some of the components of the extragalactic\ngamma-ray background. This contribution summarizes the efforts of cleaning and\ncomplementing the catalogs developed by the gravitational-wave and\nnear-infrared communities, in order to obtain a cosmographic view on stellar\nmass ($M_*$) and star formation rate (SFR). Unprecedented cosmography is\noffered by a sample of about 400,000 galaxies within 350 Mpc, with a 50-50\nratio of spectroscopic and photometric distances, $M_*$, SFR and corrections\nfor incompleteness with increasing distance and decreasing Galactic latitude.\nThe inferred 3D distribution of $M_*$ and SFR is consistent with Cosmic Flows.\nThe $M_*$ and SFR densities converge towards values compatible with deep-field\nobservations beyond 100 Mpc, suggesting a close-to-isotropic distribution of\nmore distant sources. In addition to highlighting relevant applications for the\nfour astroparticle communities, this contribution explores the distribution of\n$B$-fields at Mpc scales deduced from the 3D distribution of matter, which is\nbelieved to be crucial in shaping the ultra-high-energy sky. These efforts\nprovide a new basis for modeling UHECR anisotropies, which bodes well for the\nidentification of their long-sought sources.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Generalized Locally Toeplitz (GLT) sequences of matrices have been\noriginated from the study of certain partial differential equations. To be more\nprecise, such matrix sequences arise when we numerically approximate some\npartial differential equations by discretization. The study of the asymptotic\nspectral behaviour of GLT sequence is very important in analysing the solution\nof corresponding partial differential equations. The approximating classes of\nsequences (a.c.s) and the spectral symbols are important notions in this\nconnection. Recently, G. Barbarino obtained some additional results regarding\nthe theoretical aspects of such notions. He obtained the completeness of the\nspace of matrix sequences with respect to pseudo metric a.c.s. Also, he\nidentified the space of GLT sequences with the space of measurable functions.\nIn this article, we follow the same research line and obtain various results\nconnecting the sub-algebras of matrix sequence spaces and sub-algebras of\nfunction spaces. In some cases, these are identifications as Banach spaces and\nsome of them are Banach algebra identifications. In the process, we also prove\nthat the convergence notions in the sense of eigenvalue/singular value\nclustering are equivalent to the convergence with respect to the metrics\nintroduced here. These convergence notions are related to the study of\npreconditioners in the case of matrix/operator sequences. Finally, as an\napplication of our main results, we establish a Korovkin-type result in the\nsetting of GLT sequences.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep learning based models have dominated the current landscape of production\nrecommender systems. Furthermore, recent years have witnessed an exponential\ngrowth of the model scale--from Google's 2016 model with 1 billion parameters\nto the latest Facebook's model with 12 trillion parameters. Significant quality\nboost has come with each jump of the model capacity, which makes us believe the\nera of 100 trillion parameters is around the corner. However, the training of\nsuch models is challenging even within industrial scale data centers. This\ndifficulty is inherited from the staggering heterogeneity of the training\ncomputation--the model's embedding layer could include more than 99.99% of the\ntotal model size, which is extremely memory-intensive; while the rest neural\nnetwork is increasingly computation-intensive. To support the training of such\nhuge models, an efficient distributed training system is in urgent need. In\nthis paper, we resolve this challenge by careful co-design of both the\noptimization algorithm and the distributed system architecture. Specifically,\nin order to ensure both the training efficiency and the training accuracy, we\ndesign a novel hybrid training algorithm, where the embedding layer and the\ndense neural network are handled by different synchronization mechanisms; then\nwe build a system called Persia (short for parallel recommendation training\nsystem with hybrid acceleration) to support this hybrid training algorithm.\nBoth theoretical demonstration and empirical study up to 100 trillion\nparameters have conducted to justified the system design and implementation of\nPersia. We make Persia publicly available (at\nhttps://github.com/PersiaML/Persia) so that anyone would be able to easily\ntrain a recommender model at the scale of 100 trillion parameters.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantitative susceptibility mapping (QSM) is an MRI phase-based\npost-processing method that quantifies tissue magnetic susceptibility\ndistributions. However, QSM acquisitions are relatively slow, even with\nparallel imaging. Incoherent undersampling and compressed sensing\nreconstruction techniques have been used to accelerate traditional\nmagnitude-based MRI acquisitions; however, most do not recover the full phase\nsignal due to its non-convex nature. In this study, a learning-based Deep\nComplex Residual Network (DCRNet) is proposed to recover both the magnitude and\nphase images from incoherently undersampled data, enabling high acceleration of\nQSM acquisition. Magnitude, phase, and QSM results from DCRNet were compared\nwith two iterative and one deep learning methods on retrospectively\nundersampled acquisitions from six healthy volunteers, one intracranial\nhemorrhage and one multiple sclerosis patients, as well as one prospectively\nundersampled healthy subject using a 7T scanner. Peak signal to noise ratio\n(PSNR), structural similarity (SSIM) and region-of-interest susceptibility\nmeasurements are reported for numerical comparisons. The proposed DCRNet method\nsubstantially reduced artifacts and blurring compared to the other methods and\nresulted in the highest PSNR and SSIM on the magnitude, phase, local field, and\nsusceptibility maps. It led to 4.0% to 8.8% accuracy improvements in deep grey\nmatter susceptibility than some existing methods, when the acquisition was\naccelerated four times. The proposed DCRNet also dramatically shortened the\nreconstruction time by nearly 10 thousand times for each scan, from around 80\nhours using conventional approaches to only 30 seconds.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this note we give asymptotic estimates for the volume growth associated to\nsuitable infinite graphs. Our main application is to give an asymptotic\nestimate for volume growth associated to translation surfaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Emotion recognition (ER) is an important task in Natural Language Processing\n(NLP), due to its high impact in real-world applications from health and\nwell-being to author profiling, consumer analysis and security. Current\napproaches to ER, mainly classify emotions independently without considering\nthat emotions can co-exist. Such approaches overlook potential ambiguities, in\nwhich multiple emotions overlap. We propose a new model \"SpanEmo\" casting\nmulti-label emotion classification as span-prediction, which can aid ER models\nto learn associations between labels and words in a sentence. Furthermore, we\nintroduce a loss function focused on modelling multiple co-existing emotions in\nthe input sentence. Experiments performed on the SemEval2018 multi-label\nemotion data over three language sets (i.e., English, Arabic and Spanish)\ndemonstrate our method's effectiveness. Finally, we present different analyses\nthat illustrate the benefits of our method in terms of improving the model\nperformance and learning meaningful associations between emotion classes and\nwords in the sentence.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Solar grade silicon (SoG-Si) is a key material for the development of\ncrystalline silicon photovoltaics (PV), which is expected to reach the\ntera-watt level in the next years and around 50TW in 2050. Upgraded\nmetallurgical grade silicon (UMG-Si) has already demonstrated to be a viable\nalternative to standard polysilicon in terms of cost and quality. This study\npresents the life cycle assessment (LCA) of UMG obtained by the FerroSolar\nprocess. Moreover, it shows the environmental impacts of PV modules and\nelectricity generation based on this material. For this, an exhaustive review\nof the life cycle inventory (LCI) of PV value chain, from metallurgical grade\nsilicon (MG-Si) down to electricity generation, has been carried out updating\ninputs for all processes. The Balance of System (BoS) has also been updated\nwith real state of the art data for a fixed open ground large PV site (100 MW).\nTwo different electricity mixes, with low and high carbon intensities, have\nbeen considered. The results reveal that for PV electricity generation using\nUMG instead of polysilicon leads to an overall reduction of Climate change (CC)\nemissions of over 20%, along with an improvement of the Energy Payback Time\n(EPBT) of 25%, achieving significantly low values, 12 gCO2eq / kWhe and 0.52\nyears, respectively. Moreover, it is shown that UMG silicon feedstock is not\nthe main contributor to the carbon and energy footprint of the produced\nelectricity, leaving the first place to PV module manufacturing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Android malware detection is a significat problem that affects billions of\nusers using millions of Android applications (apps) in existing markets. This\npaper proposes PetaDroid, a framework for accurate Android malware detection\nand family clustering on top of static analyses. PetaDroid automatically adapts\nto Android malware and benign changes over time with resilience to common\nbinary obfuscation techniques. The framework employs novel techniques\nelaborated on top of natural language processing (NLP) and machine learning\ntechniques to achieve accurate, adaptive, and resilient Android malware\ndetection and family clustering. PetaDroid identifies malware using an ensemble\nof convolutional neural network (CNN) on proposed Inst2Vec features. The\nframework clusters the detected malware samples into malware family groups\nutilizing sample feature digests generated using deep neural auto-encoder. For\nchange adaptation, PetaDroid leverages the detection confidence probability\nduring deployment to automatically collect extension datasets and periodically\nuse them to build new malware detection models. Besides, PetaDroid uses\ncode-fragment randomization during the training to enhance the resiliency to\ncommon obfuscation techniques. We extensively evaluated PetaDroid on multiple\nreference datasets. PetaDroid achieved a high detection rate (98-99% f1-score)\nunder different evaluation settings with high homogeneity in the produced\nclusters (96%). We conducted a thorough quantitative comparison with\nstate-of-the-art solutions MaMaDroid, DroidAPIMiner, MalDozer, in which\nPetaDroid outperforms them under all the evaluation settings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modeling equity in the allocation of scarce resources is a fast-growing\nconcern in the humanitarian logistics field. The Gini coefficient is one of the\nmost widely recognized measures of inequity and it was originally characterized\nby means of the Lorenz curve, which is a mathematical function that links the\ncumulative share of income to rank-ordered groups in a population. So far,\nhumanitarian logistics models that have approached equity using the Gini\ncoefficient do not actually optimize its original formulation, but use\nalternative definitions that do not necessarily replicate that original Gini\nmeasure. In this paper, we derive the original Gini coefficient via the Lorenz\ncurve to optimize the effectiveness-equity trade-off in a humanitarian\nlocation-allocation problem. We also propose new valid inequalities based on an\nupper-bounding Lorenz curve to tighten the linear relaxation of our model and\ndevelop a clustering-based construction of the Lorenz curve that requires fewer\nadditional constraints and variables than the original one. The computational\nstudy, based on the floods and landslides in Rio de Janeiro state, Brazil,\nreveals that while alternative Gini definitions have interesting properties,\nthey can generate vastly different decisions compared to the original Gini\ncoefficient. In addition, viewed from the perspective of the original Gini\ncoefficient, these decisions can be significantly less equitable.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Despite the accomplishments of Generative Adversarial Networks (GANs) in\nmodeling data distributions, training them remains a challenging task. A\ncontributing factor to this difficulty is the non-intuitive nature of the GAN\nloss curves, which necessitates a subjective evaluation of the generated output\nto infer training progress. Recently, motivated by game theory, duality gap has\nbeen proposed as a domain agnostic measure to monitor GAN training. However, it\nis restricted to the setting when the GAN converges to a Nash equilibrium. But\nGANs need not always converge to a Nash equilibrium to model the data\ndistribution. In this work, we extend the notion of duality gap to proximal\nduality gap that is applicable to the general context of training GANs where\nNash equilibria may not exist. We show theoretically that the proximal duality\ngap is capable of monitoring the convergence of GANs to a wider spectrum of\nequilibria that subsumes Nash equilibria. We also theoretically establish the\nrelationship between the proximal duality gap and the divergence between the\nreal and generated data distributions for different GAN formulations. Our\nresults provide new insights into the nature of GAN convergence. Finally, we\nvalidate experimentally the usefulness of proximal duality gap for monitoring\nand influencing GAN training.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We define weighted mean summability method of double sequences in\nintuitionistic fuzzy normed spaces($IFNS$), and obtain necessary and sufficient\nTauberian conditions under which convergence of double sequences in $IFNS$\nfollows from their weighted mean summability. This study reveals also Tauberian\nresults for some known summation methods in the special cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $G$ be a graph and $r\\in\\mathbb{N}$. The matching Kneser graph\n$\\textsf{KG}(G, rK_2)$ is a graph whose vertex set is the set of $r$-matchings\nin $G$ and two vertices are adjacent if their corresponding matchings are\nedge-disjoint. In [Alishahi, M. and Hajiabolhassan, H., On the Chromatic Number\nof Matching Kneser Graphs, Combin. Probab. and Comput. 29 (2020), no. 1,\n1--21.] it was conjectured that for any connected graph $G$ and positive\ninteger $r\\geq 2$, the chromatic number of $\\textsf{KG}(G, rK_2)$ is equal to\n$|E(G)|-\\textsf{ex}(G,rK_2)$, where $\\textsf{ex}(G,rK_2)$ denotes the largest\nnumber of edges in $G$ avoiding a matching of size $r$. In this note, we show\nthat the conjecture is not true for snarks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We apply the dynamical diquark model to predict the spectrum of hidden-charm\npentaquark states in both unflavored and open-strange sectors. Using only\nHamiltonian parameters introduced in the tetraquark $S$- and $P$-wave\nmultiplets, the model naturally produces the level spacing supported by the\nmost recent LHCb results for $P_c$ structures. Furthermore, using model inputs\nobtained from data of hidden-charm, open-strange tetraquarks ($Z_{cs}$), we\npredict the spectrum of $P_{cs}$ states, including the recently observed\n$P_{cs}(4459)$. We find all pentaquark candidates observed to date to belong to\nthe $1P$ multiplet and hence have positive parity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The most metal-deficient stars hold important clues about the early build-up\nand chemical evolution of the Milky Way, and carbon-enhanced metal-poor (CEMP)\nstars are of special interest. However, little is known about CEMP stars in the\nGalactic bulge. In this paper, we use the large spectroscopic sample of\nmetal-poor stars from the Pristine Inner Galaxy Survey (PIGS) to identify CEMP\nstars ([C/Fe] > +0.7) in the bulge region and to derive a CEMP fraction. We\nidentify 96 new CEMP stars in the inner Galaxy, of which 62 are very metal-poor\n([Fe/H] < -2.0); this is more than a ten-fold increase compared to the seven\npreviously known bulge CEMP stars. The cumulative fraction of CEMP stars in\nPIGS is $42^{\\,+14\\,}_{\\,-13} \\%$ for stars with [Fe/H] < -3.0, and decreases\nto $16^{\\,+3\\,}_{\\,-3} \\%$ for [Fe/H] < -2.5 and $5.7^{\\,+0.6\\,}_{\\,-0.5} \\%$\nfor [Fe/H] < -2.0. The PIGS inner Galaxy CEMP fraction for [Fe/H] < -3.0 is\nconsistent with the halo fraction found in the literature, but at higher\nmetallicities the PIGS fraction is substantially lower. While this can partly\nbe attributed to a photometric selection bias, such bias is unlikely to fully\nexplain the low CEMP fraction at higher metallicities. Considering the typical\ncarbon excesses and metallicity ranges for halo CEMP-s and CEMP-no stars, our\nresults point to a possible deficiency of both CEMP-s and CEMP-no stars\n(especially the more metal-rich) in the inner Galaxy. The former is potentially\nrelated to a difference in the binary fraction, whereas the latter may be the\nresult of a fast chemical enrichment in the early building blocks of the inner\nGalaxy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  One of the fundamental task in graph data mining is to find a planted\ncommunity(dense subgraph), which has wide application in biology, finance, spam\ndetection and so on. For a real network data, the existence of a dense subgraph\nis generally unknown. Statistical tests have been devised to testing the\nexistence of dense subgraph in a homogeneous random graph. However, many\nnetworks present extreme heterogeneity, that is, the degrees of nodes or\nvertexes don't concentrate on a typical value. The existing tests designed for\nhomogeneous random graph are not straightforwardly applicable to the\nheterogeneous case. Recently, scan test was proposed for detecting a dense\nsubgraph in heterogeneous(inhomogeneous) graph(\\cite{BCHV19}). However, the\ncomputational complexity of the scan test is generally not polynomial in the\ngraph size, which makes the test impractical for large or moderate networks. In\nthis paper, we propose a polynomial-time test that has the standard normal\ndistribution as the null limiting distribution. The power of the test is\ntheoretically investigated and we evaluate the performance of the test by\nsimulation and real data example.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The vision-based relative localization can provide effective feedback for the\ncooperation of aerial swarm and has been widely investigated in previous works.\nHowever, the limited field of view (FOV) inherently restricts its performance.\nTo cope with this issue, this letter proposes a novel distributed active\nvision-based relative localization framework and apply it to formation control\nin aerial swarms. Inspired by bird flocks in nature, we devise graph-based\nattention planning (GAP) to improve the observation quality of the active\nvision in the swarm. Then active detection results are fused with onboard\nmeasurements from Ultra-WideBand (UWB) and visual-inertial odometry (VIO) to\nobtain real-time relative positions, which further improve the formation\ncontrol performance of the swarm. Simulations and experiments demonstrate that\nthe proposed active vision system outperforms the fixed vision system in terms\nof estimation and formation accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Anomaly detection techniques are growing in importance at the Large Hadron\nCollider (LHC), motivated by the increasing need to search for new physics in a\nmodel-agnostic way. In this work, we provide a detailed comparative study\nbetween a well-studied unsupervised method called the autoencoder (AE) and a\nweakly-supervised approach based on the Classification Without Labels (CWoLa)\ntechnique. We examine the ability of the two methods to identify a new physics\nsignal at different cross sections in a fully hadronic resonance search. By\nconstruction, the AE classification performance is independent of the amount of\ninjected signal. In contrast, the CWoLa performance improves with increasing\nsignal abundance. When integrating these approaches with a complete background\nestimate, we find that the two methods have complementary sensitivity. In\nparticular, CWoLa is effective at finding diverse and moderately rare signals\nwhile the AE can provide sensitivity to very rare signals, but only with\ncertain topologies. We therefore demonstrate that both techniques are\ncomplementary and can be used together for anomaly detection at the LHC.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate detection prospects of the gravitational-wave background (GWB)\nthat originates from the merging of compact objects formed by the collapse of\npopulation III stars. Younger population I/II stars lead to a GWB in the\nLIGO/Virgo frequency band at the inspiral phase, while population III stars\nwould likely show up at the later merger and ringdown phases. We show that,\nusing a network of third-generation detectors, we may be able to separate a\npopulation I/II signal from a population III one, provided we can subtract\nindividual coalescence events. A detection of a population III GWB could reveal\nimportant information, such as the average redshifted total mass.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The subtrees and BC-subtrees (subtrees where any two leaves are at even\ndistance apart) have been extensively studied in recent years. Such structures,\nunder special constraints on degrees, have applications in many fields. Through\nan approach based on generating functions, we present recursive algorithms for\nenumerating various subtrees and BC-subtrees of maximum degree $\\leq k$ in\ntrees. The algorithms are illustrated through detailed examples. We also\nbriefly discuss, in trees, the densities of subtrees (resp.~BC-subtrees) of\nmaximum degree $\\leq k(\\geq 2)$ among all subtrees (resp.~BC-subtrees).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Observing the magnetic fields of low-mass interacting galaxies tells us how\nthey have evolved over cosmic time and their importance in galaxy evolution. We\nhave measured the Faraday rotation of 80 extra-galactic radio sources behind\nthe Small Magellanic Cloud (SMC) using the CSIRO Australia Telescope Compact\nArray (ATCA) with a frequency range of 1.4 -- 3.0 GHz. Both the sensitivity of\nour observations and the source density are an order of magnitude improvement\non previous Faraday rotation measurements of this galaxy. The SMC generally\nproduces negative rotation measures (RMs) after accounting for the Milky Way\nforeground contribution, indicating that it has a mean coherent line-of-sight\nmagnetic field strength of $-0.3\\pm0.1\\mu$G, consistent with previous findings.\nWe detect signatures of magnetic fields extending from the north and south of\nthe Bar of the SMC. The random component of the SMC magnetic field has a\nstrength of $\\sim 5\\mu$G with a characteristic size-scale of magneto-ionic\nturbulence $< 250$ pc, making the SMC like other low-mass interacting galaxies.\nThe magnetic fields of the SMC and Magellanic Bridge appear similar in\ndirection and strength, hinting at a connection between the two fields as part\nof the hypothesised `pan-Magellanic' magnetic field.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Densities of states for simple (sc) and base-centered (bcc) cubic lattices\nwith account of nearest and next-nearest neighbour hopping integrals $t$ and\n$t'$ are investigated in detail. It is shown that at values of $\\tau \\equiv\nt'/t = \\tau_\\ast$, corresponding to the change of isoenergetic surface\ntopology, the formation of van Hove $\\bf k$ lines takes place. At small\ndeviation from these special values, the weakly dispersive spectrum in the\nvicinity of van Hove lines is replaced by a weak $\\bf k$-dependence in the\nvicinity of few van Hove points which possess huge masses proportional to\n$|\\tau - \\tau_\\ast|^{-1}$. The singular contributions to the density of states\noriginating from van Hove points and lines are considered, as well as the\nchange in the topology of isoenergetic surfaces in the $\\bf k$-space with the\nvariation of $\\tau$. Closed analytical expressions for density of states as a\nfunction of energy and $\\tau$ in terms of elliptic integrals, and power-law\nasymptotics at $\\tau = \\tau_\\ast$ are obtained. Besides the case of sc lattice\nwith small $\\tau$ (maximum of density of states corresponds to energy level of\nX $\\bf k$-point), maximal value of the density of states is always achieved at\nenergies corresponding to \\textit{inner} $\\bf k$-points of the Brillouin zone\npositioned in high-symmetry directions, and not at zone faces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Inverse First Ionization Potential (FIP) Effect, the depletion in coronal\nabundance of elements like Fe, Mg, and Si that are ionized in the solar\nchromosphere relative to those that are neutral, has been identified in several\nsolar flares. We give a more detailed discussion of the mechanism of\nfractionation by the ponderomotive force associated with magnetohydrodynamic\nwaves, paying special attention to the conditions in which Inverse FIP\nfractionation arises in order to better understand its relation to the usual\nFIP Effect, i.e. the enhancement of coronal abundance of Fe, Mg, Si, etc. The\nFIP Effect is generated by parallel propagating Alfv\\'en waves, with either\nphotospheric, or more likely coronal, origins. The Inverse FIP Effect arises as\nupward propagating fast mode waves with an origin in the photosphere or below,\nrefract back downwards in the chromosphere where the Alfv\\'en speed is\nincreasing with altitude. We give a more physically motivated picture of the\nFIP fractionation, based on the wave refraction around inhomogeneities in the\nsolar atmosphere, and inspired by previous discussions of analogous phenomena\nin the optical trapping of particles by laser beams. We apply these insights to\nmodeling the fractionation and find good agreement with the observations of\nKatsuda et al. (2020; arXiv:2001.10643) and Dennis et al. (2015;\narXiv:1503.01602).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Life, services and activities within cities have commonly been studied by\nseparate disciplines, each one independent from the others. One such approach\nis the computer simulation, which enables in-depth modelling and cost-effective\nevaluation of city phenomena. However, the adoption of integrated city\nsimulations faces several barriers, such as managerial, social, and technical,\ndespite its potential to support city planning and policymaking. This paper\nintroduces the City Physiology: a new conceptual framework to facilitate the\nintegration of city layers when designing holistic simulators. The physiology\nis introduced and applied through a process of three steps. Firstly, a\nliterature review is offered in order to study the terminology and the progress\nalready made towards integrated modelling of different urban systems. Secondly,\ninteractions between urban systems are extracted from the approaches studied\nbefore. Finally, the pipeline to carry out the integration strategy is\ndescribed. In addition to providing a conceptual tool for holistic simulations,\nthe framework enables the discovery of new research lines generated by\npreviously unseen connections between city layers. Being an open framework,\navailable to all researchers to use and broaden, the authors of this paper\nenvisage that it will be a valuable resource in establishing an exact science\nof cities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce a numerical method to determine the Hamiltonian of Mean Force\n(HMF) Gibbs state for a quantum system strongly coupled to a reservoir. The\nmethod adapts the Time Evolving Matrix Product Operator (TEMPO) algorithm to\nimaginary time propagation. By comparing the real-time and imaginary-time\npropagation for a generalized spin-boson model, we confirm that the HMF Gibbs\nstate correctly predicts the steady state. We show that the numerical dynamics\nmatch the polaron master equation at strong coupling. We illustrate the\npotential of the imaginary-time TEMPO approach by exploring reservoir-induced\nentanglement between qubits.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A modified Hayward metric of magnetically charged black hole space-time based\non rational nonlinear electrodynamics with the Lagrangian ${\\cal L} = -{\\cal\nF}/(1+2\\beta{\\cal F})$ is considered. We introduce the fundamental length,\ncharacterizing quantum gravity effects. If the fundamental length vanishes the\ngeneral relativity coupling to rational nonlinear electrodynamics is recovered.\nWe obtain corrections to the Reissner--Nordstr\\\"{o}m solution as the radius\napproaches infinity. The metric possesses a de Sitter core without\nsingularities as $r\\rightarrow 0$. The Hawking temperature and the heat\ncapacity are calculated. It was shown that phase transitions occur and black\nholes are thermodynamically stable at some event horizon radii. We demonstrate\nthat curvature invariants are bounded and the limiting curvature conjecture\ntakes place.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The test loss of well-trained neural networks often follows precise power-law\nscaling relations with either the size of the training dataset or the number of\nparameters in the network. We propose a theory that explains and connects these\nscaling laws. We identify variance-limited and resolution-limited scaling\nbehavior for both dataset and model size, for a total of four scaling regimes.\nThe variance-limited scaling follows simply from the existence of a\nwell-behaved infinite data or infinite width limit, while the\nresolution-limited regime can be explained by positing that models are\neffectively resolving a smooth data manifold. In the large width limit, this\ncan be equivalently obtained from the spectrum of certain kernels, and we\npresent evidence that large width and large dataset resolution-limited scaling\nexponents are related by a duality. We exhibit all four scaling regimes in the\ncontrolled setting of large random feature and pretrained models and test the\npredictions empirically on a range of standard architectures and datasets. We\nalso observe several empirical relationships between datasets and scaling\nexponents: super-classing image tasks does not change exponents, while changing\ninput distribution (via changing datasets or adding noise) has a strong effect.\nWe further explore the effect of architecture aspect ratio on scaling\nexponents.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  M-stars are the most common hosts of planetary systems in the Galaxy.\nProtoplanetary disks around M-stars thus offer a prime opportunity to study the\nchemistry of planet-forming environments. We present an ALMA survey of\nmolecular line emission toward a sample of five protoplanetary disks around\nM4-M5 stars (FP Tau, J0432+1827, J1100-7619, J1545-3417, and Sz 69). These\nobservations can resolve chemical structures down to tens of AU. Molecular\nlines of $^{12}$CO, $^{13}$CO, C$^{18}$O, C$_2$H, and HCN are detected toward\nall five disks. Lines of H$_2$CO and DCN are detected toward 2/5 and 1/5 disks,\nrespectively. For disks with resolved C$^{18}$O, C$_2$H, HCN, and H$_2$CO\nemission, we observe substructures similar to those previously found in disks\naround solar-type stars (e.g., rings, holes, and plateaus). C$_2$H and HCN\nexcitation conditions estimated interior to the pebble disk edge for the bright\ndisk J1100-7619 are consistent with previous measurements around solar-type\nstars. The correlation previously found between C$_2$H and HCN fluxes for\nsolar-type disks extends to our M4-M5 disk sample, but the typical C$_2$H/HCN\nratio is higher for the M4-M5 disk sample. This latter finding is reminiscent\nof the hydrocarbon enhancements found by previous observational infrared\nsurveys in the innermost ($<$10AU) regions of M-star disks, which is intriguing\nsince our disk-averaged fluxes are heavily influenced by flux levels in the\noutermost disk, exterior to the pebble disk edge. Overall, most of the\nobservable chemistry at 10-100AU appears similar for solar-type and M4-M5\ndisks, but hydrocarbons may be more abundant around the cooler stars.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Pano3D is a new benchmark for depth estimation from spherical panoramas. It\naims to assess performance across all depth estimation traits, the primary\ndirect depth estimation performance targeting precision and accuracy, and also\nthe secondary traits, boundary preservation, and smoothness. Moreover, Pano3D\nmoves beyond typical intra-dataset evaluation to inter-dataset performance\nassessment. By disentangling the capacity to generalize to unseen data into\ndifferent test splits, Pano3D represents a holistic benchmark for $360^o$ depth\nestimation. We use it as a basis for an extended analysis seeking to offer\ninsights into classical choices for depth estimation. This results in a solid\nbaseline for panoramic depth that follow-up works can build upon to steer\nfuture progress.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This note provides a simple proof for the equality between the normalized\nvolume of a convex polytope with $m$ vertices and the mixed volume of $m$\nsimplices and thus shows the seemingly restrictive problem of computing mixed\nvolume of simplices is still at least as hard as computing volumes of convex\npolytopes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we focus on the coverage probability of a double-intelligent\nreflecting surface (IRS) assisted wireless network and study the impact of\nmultiplicative beamforming gain and correlated Rayleigh fading. In particular,\nwe obtain a novel closed-form expression of the coverage probability of a\nsingle-input single-output (SISO) system assisted by two large IRSs while being\ndependent on the corresponding arbitrary reflecting beamforming matrices (RBMs)\nand large-scale statistics in terms of correlation matrices. Taking advantage\nof the large-scale statistics, i.e., statistical channel state information\n(CSI), we perform optimization of the RBMs of both IRSs once per several\ncoherence intervals rather than at each interval. Hence, we achieve a reduction\nof the computational complexity, otherwise increased in multi-IRS-assisted\nnetworks during their RBM optimization. Numerical results validate the\nanalytical expressions even for small IRSs, confirm enhanced performance over\nthe conventional single-IRS counterpart, and reveal insightful properties.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Berenstein and Kazhdan's theory of geometric crystals gives rise to two\ncommuting families of geometric crystal operators acting on the space of\ncomplex $m \\times n$ matrices. These are birational actions, which we view as a\ncrystal-theoretic analogue of the usual action of ${\\rm SL}_m \\times {\\rm\nSL}_n$ on $m \\times n$ matrices. We prove that the field of rational invariants\n(and ring of polynomial invariants) of each family of geometric crystal\noperators is generated by a set of algebraically independent polynomials, which\nare generalizations of the elementary symmetric polynomials in $m$ (or $n$)\nvariables. We also give a set of algebraically independent generators for the\nintersection of these fields, and we explain how these fields are situated\ninside the larger fields of geometric $R$-matrix invariants, which were studied\nby Lam and the third-named author under the name loop symmetric functions. The\nkey tool in our proof is the geometric RSK correspondence of Noumi and Yamada,\nwhich we show to be an isomorphism of geometric crystals.\n  In an appendix jointly written with Thomas Lam, we prove the fundamental\ntheorem of loop symmetric functions, which says that the polynomial invariants\nof the geometric $R$-matrix are generated by the loop elementary symmetric\nfunctions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We formulate general definitions of semi-classical gauge transformations for\nnoncommutative gauge theories in general backgrounds of string theory, and give\nnovel explicit constructions using techniques based on symplectic embeddings of\nalmost Poisson structures. In the absence of fluxes the gauge symmetries close\na Poisson gauge algebra and their action is governed by a $P_\\infty$-algebra\nwhich we construct explicitly from the symplectic embedding. In curved\nbackgrounds they close a field dependent gauge algebra governed by an\n$L_\\infty$-algebra which is not a $P_\\infty$-algebra. Our technique produces\nnew all orders constructions which are significantly simpler compared to\nprevious approaches, and we illustrate its applicability in several examples of\ninterest in noncommutative field theory and gravity. We further show that our\nsymplectic embeddings naturally define a $P_\\infty$-structure on the exterior\nalgebra of differential forms on a generic almost Poisson manifold, which\ngeneralizes earlier constructions of differential graded Poisson algebras, and\nsuggests a new approach to defining noncommutative gauge theories beyond the\ngauge sector and the semi-classical limit based on $A_\\infty$-algebras.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Dynamically excited objects within the Kuiper belt show a bimodal\ndistribution in their surface colors, and these differing surface colors may be\na tracer of where these objects formed. In this work we explore radial color\ndistributions in the primordial planetesimal disk and implications for the\npositions of ice line/color transitions within the Kuiper belt's progenitor\npopulations. We combine a full dynamical model of the Kuiper belt's evolution\ndue to Neptune's migration with precise surface colors measured by the Colours\nof the Outer Solar System Origins Survey in order to examine the true color\nratios within the Kuiper belt and the ice lines within the primordial disk. We\ninvestigate the position of a dominant, surface color changing ice-line, with\ntwo possible surface color layouts within the initial disk; (1) inner neutral\nsurfaces and outer red, and (2) inner red surfaces and outer neutral. We\nperformed simulations with a primordial disk that truncates at 30 au. By\nradially stepping the color transition out through 0.5 au intervals we show\nthat both disk configurations are consistent with the observed color fraction.\nFor an inner neutral, outer red primordial disk we find that the color\ntransition can be at $28^{+2}_{-3}$ au at a 95% confidence level. For an inner\nred, outer neutral primordial disk the color transition can be at\n$27^{+3}_{-3}$ au at a 95% confidence level.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The aim of Quantum Fisher Cosmology is to use the quantum Fisher information\nabout pure de Sitter states to derive model independent observational\nconsequences of the existence of a primordial phase of the Universe of de\nSitter accelerated expansion. These quantum features are encoded in a scale\ndependent quantum cosmological tilt that defines what we can call the de Sitter\nuniversality class. The experimental predictions are: i) A phase transition\nfrom red into blue tilt at a scale order $k= 1$ Mpc$^{-1}$ that naturally\nsolves the cosmological trans-Planckian problem, ii) A spectral index for\ncurvature fluctuations at CMB scales $k= 0.05$ Mpc$^{-1}$ equal to $0.0328$,\niii) A tilt running at scale $k=0.002$ Mpc$^{-1}$ equal to $-0.0019$, iv) An\nenhancement of the amplitude of CMB peaks for extremely high multipoles ($l >\n10^5$) that can provide a natural mechanism for primordial black hole formation\nas a source of dark matter, v) A lack of power at scales of $8$ Mpc with\nrespect to the CMB scale that can explain the $\\sigma_8$ tension.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Solid-state single-photon emitters (SPE) are a basis for emerging\ntechnologies such as quantum communication and quantum sensing. SPE based on\nfluorescent point defects are ubiquitous in semiconductors and insulators, and\nnew systems with desirable properties for quantum information science may exist\namongst the vast number of unexplored defects. However, the characterization of\nnew SPE typically relies on time-consuming techniques for identifying point\nsource emitters by eye in photoluminescence (PL) images. This manual strategy\nis a bottleneck for discovering new SPE, motivating a more efficient method for\ncharacterizing emitters in PL images. Here we present a quantitative method\nusing image analysis and regression fitting to automatically identify Gaussian\nemitters in PL images and classify them according to their stability, shape,\nand intensity relative to the background. We demonstrate efficient emitter\nclassification for SPEs in nanodiamond arrays and hexagonal boron nitride\nflakes. Adaptive criteria detect SPE in both samples despite variation in\nemitter intensity, stability, and background features. The detection criteria\ncan be tuned for specific material systems and experimental setups to\naccommodate the diverse properties of SPE.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a magnetic implementation of a thermodynamic computing fabric.\nMagnetic devices within computing cores harness thermodynamics through its\nvoltage-controlled thermal stability; while the evolution of network states is\nguided by the spin-orbit-torque effect. We theoretically derive the dynamics of\nthe cores and show that the computing fabric can successfully compute ground\nstates of a Boltzmann Machine. Subsequently, we demonstrate the physical\nrealization of these devices based on a CoFeB-MgO magnetic tunnel junction\nstructure. The results of this work pave the path towards the realization of\nhighly efficient, high-performance thermodynamic computing hardware. Finally,\nthis paper will also give a perspective of computing beyond thermodynamic\ncomputing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we study the existence, uniqueness and comparison theorems for\nsolutions of one-dimensional reflected backward stochastic differential\nequations (RBSDEs) with one continuous obstacle and backward stochastic\ndifferential equations (BSDEs). The generators of such RBSDEs and BSDEs have a\nquadratic growth in $z$, with the quadratic term taking the form $f(y)|z|^2$,\nwhere the function $f$ is defined on an open interval $D$ and local integral.\nOur proofs mainly depend on a transformation based on $f$ and the domination\narguments based on RBSDEs. As applications, we give a probabilistic\ninterpretation of an obstacle problem for a quadratic PDE with continuous\ncoefficient, and study an optimal stopping problem for the payoff of American\noptions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop the complete composite theory of gravity, in which the gauge\nvector fields of the Yang-Mills theory with Lorentz symmetry group are\nexpressed in terms of the tetrad variables obtained from the decomposition of a\nmetric. A key element of a compelling formulation of composite gravity are\nrefined coordinate conditions that offer a natural coupling of the\ngravitational field to matter and ensure the closest relationship to general\nrelativity. The composite theory of gravity is presented from three different\nperspectives highlighting its intuitive interpretation, its relationship to\ngeneral relativity and its canonical Hamiltonian formulation, where the latter\nclarifies the structure of the heavily constrained theory and provides the\nstarting point for its quantization. The main physical ingredient of the theory\nis an anisotropic velocity-momentum relation, or tensorial mass, described by a\nmetric. We discuss the static isotropic solution in great detail because it\nprovides the background for the high-precision tests to be passed by an\nalternative theory of gravity and for the understanding of black holes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Many municipalities and large organizations have fleets of vehicles that need\nto be coordinated for tasks such as garbage collection or infrastructure\ninspection. Motivated by this need, this paper focuses on the common subproblem\nin which a team of vehicles needs to plan coordinated routes to patrol an area\nover iterations while minimizing temporally and spatially dependent costs. In\nparticular, at a specific location (e.g., a vertex on a graph), we assume the\ncost grows linearly in expectation with an unknown rate, and the cost is reset\nto zero whenever any vehicle visits the vertex (representing the robot\nservicing the vertex). We formulate this problem in graph terminology and call\nit Team Orienteering Coverage Planning with Uncertain Reward (TOCPUR). We\npropose to solve TOCPUR by simultaneously estimating the accumulated cost at\nevery vertex on the graph and solving a novel variant of the Team Orienteering\nProblem (TOP) iteratively, which we call the Team Orienteering Coverage Problem\n(TOCP). We provide the first mixed integer programming formulation for the\nTOCP, as a significant adaptation of the original TOP. We introduce a new\nbenchmark consisting of hundreds of randomly generated graphs for comparing\ndifferent methods. We show the proposed solution outperforms both the exact TOP\nsolution and a greedy algorithm. In addition, we provide a demo of our method\non a team of three physical robots in a real-world environment.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A graph $G$ on $n$ vertices of diameter $D$ is called $H$-palindromic if\n$\\alpha(G,k) = \\alpha(G,D-k)$ for all $k=0, 1, \\dots, \\left\n\\lfloor{\\frac{D}{2}}\\right \\rfloor$, where $\\alpha(G,k)$ is the number of\nunordered pairs of vertices at distance $k$. Quantities $\\alpha(G,k)$ form\ncoefficients of the Hosoya polynomial. In 1999, Caporossi, Dobrynin, Gutman and\nHansen showed that there are exactly five $H$-palindromic trees of even\ndiameter and conjectured that there are no such trees of odd diameter. We prove\nthis conjecture for bipartite graphs. An infinite family of $H$-palindromic\ntrees of diameter $6$ is also constructed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper develops a theory of colimit sketches \"with constructions\" in\nhigher category theory, formalising the input to the ubiquitous procedure of\nadjoining specified \"constructible\" colimits to a category such that specified\n\"relation\" colimits are enforced (or preserved). From a more technical\nstandpoint, sketches are a way to describe dense functors using techniques from\nthe homotopy theory of diagrams.\n  We establish basic properties of diagrams in an infinity-category C as a\nmodel for presheaves on C and Bousfield localisations thereof, discuss\nextensions of functors and adjunctions, and equivalences of sets of diagrams.\nWe introduce categories of presheaves which are \"constructible in one step\" by\na set of diagrams and explore, via well-known examples, when constructible\ncocompletion is idempotent, i.e. when any iterated construction can be\ncompleted in one step.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Due to the pervasive diffusion of personal mobile and IoT devices, many\n``smart environments'' (e.g., smart cities and smart factories) will be, among\nothers, generators of huge amounts of data. Currently, this is typically\nachieved through centralised cloud-based data analytics services. However,\naccording to many studies, this approach may present significant issues from\nthe standpoint of data ownership, and even wireless network capacity. One\npossibility to cope with these shortcomings is to move data analytics closer to\nwhere data is generated. In this paper, we tackle this issue by proposing and\nanalyzing a distributed learning framework, whereby data analytics are\nperformed at the edge of the network, i.e., on locations very close to where\ndata is generated. Specifically, in our framework, partial data analytics are\nperformed directly on the nodes that generate the data, or on nodes close by\n(e.g., some of the data generators can take this role on behalf of subsets of\nother nodes nearby). Then, nodes exchange partial models and refine them\naccordingly. Our framework is general enough to host different analytics\nservices. In the specific case analysed in the paper, we focus on a learning\ntask, considering two distributed learning algorithms. Using an activity\nrecognition and a pattern recognition task, both on reference datasets, we\ncompare the two learning algorithms between each other and with a central cloud\nsolution (i.e., one that has access to the complete datasets). Our results show\nthat using distributed machine learning techniques, it is possible to\ndrastically reduce the network overhead, while obtaining performance comparable\nto the cloud solution in terms of learning accuracy. The analysis also shows\nwhen each distributed learning approach is preferable, based on the specific\ndistribution of the data on the nodes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper critically examines arguments against independence, a measure of\ngroup fairness also known as statistical parity and as demographic parity. In\nrecent discussions of fairness in computer science, some have maintained that\nindependence is not a suitable measure of group fairness. This position is at\nleast partially based on two influential papers (Dwork et al., 2012, Hardt et\nal., 2016) that provide arguments against independence. We revisit these\narguments, and we find that the case against independence is rather weak. We\nalso give arguments in favor of independence, showing that it plays a\ndistinctive role in considerations of fairness. Finally, we discuss how to\nbalance different fairness considerations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the damped Schr\\\"odinger semigroup $e^{-it \\frac{d^2}{dx^2}}$ on\nthe tadpole graph ${\\mathcal R}$. We first give a careful spectral analysis and\nan appropriate decomposition of the kernel of the resolvent. As a consequence\nand by showing that the generalized eigenfunctions form a Riesz basis of some\nsubspace of $L^2({\\mathcal R})$, we prove that the corresponding energy decay\nexponentially.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For the past six years, researchers in genetic programming and other program\nsynthesis disciplines have used the General Program Synthesis Benchmark Suite\nto benchmark many aspects of automatic program synthesis systems. These\nproblems have been used to make notable progress toward the goal of general\nprogram synthesis: automatically creating the types of software that human\nprogrammers code. Many of the systems that have attempted the problems in the\noriginal benchmark suite have used it to demonstrate performance improvements\ngranted through new techniques. Over time, the suite has gradually become\noutdated, hindering the accurate measurement of further improvements. The field\nneeds a new set of more difficult benchmark problems to move beyond what was\npreviously possible.\n  In this paper, we describe the 25 new general program synthesis benchmark\nproblems that make up PSB2, a new benchmark suite. These problems are curated\nfrom a variety of sources, including programming katas and college courses. We\nselected these problems to be more difficult than those in the original suite,\nand give results using PushGP showing this increase in difficulty. These new\nproblems give plenty of room for improvement, pointing the way for the next six\nor more years of general program synthesis research.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Vision-based depth estimation is a key feature in autonomous systems, which\noften relies on a single camera or several independent ones. In such a\nmonocular setup, dense depth is obtained with either additional input from one\nor several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which\nsuffer from scale-ambiguity and infinite-depth problems. In this paper, we\npropose a new alternative of densely estimating metric depth by combining a\nmonocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of\ntoday's automotive-grade mass-produced laser scanners. Inspired by recent\nself-supervised methods, we introduce a novel framework, called LiDARTouch, to\nestimate dense depth maps from monocular images with the help of ``touches'' of\nLiDAR, i.e., without the need for dense ground-truth depth. In our setup, the\nminimal LiDAR input contributes on three different levels: as an additional\nmodel's input, in a self-supervised LiDAR reconstruction objective function,\nand to estimate changes of pose (a key component of self-supervised depth\nestimation architectures). Our LiDARTouch framework achieves new state of the\nart in self-supervised depth estimation on the KITTI dataset, thus supporting\nour choices of integrating the very sparse LiDAR signal with other visual\nfeatures. Moreover, we show that the use of a few-beam LiDAR alleviates scale\nambiguity and infinite-depth issues that camera-only methods suffer from. We\nalso demonstrate that methods from the fully-supervised depth-completion\nliterature can be adapted to a self-supervised regime with a minimal LiDAR\nsignal.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Security in the Internet of Things (IoT) requires ways to regularly update\nfirmware in the field. These demands ever increase with new, agile concepts\nsuch as security as code and should be considered a regular operation. Hosting\nmassive firmware roll-outs present a crucial challenge for the constrained\nwireless environment. In this paper, we explore how information-centric\nnetworking can ease reliable firmware updates. We start from the recent\nstandards developed by the IETF SUIT working group and contribute a system that\nallows for a timely discovery of new firmware versions by using\ncryptographically protected manifest files. Our design enables a cascading\nfirmware roll-out from a gateway towards leaf nodes in a low-power multi-hop\nnetwork. While a chunking mechanism prepares firmware images for typically\nlow-sized maximum transmission units (MTUs), an early Denial-of-Service (DoS)\ndetection prevents the distribution of tampered or malformed chunks. In\nexperimental evaluations on a real-world IoT testbed, we demonstrate feasible\nstrategies with adaptive bandwidth consumption and a high resilience to\nconnectivity loss when replicating firmware images into the IoT edge.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Differential privacy is among the most prominent techniques for preserving\nprivacy of sensitive data, oweing to its robust mathematical guarantees and\ngeneral applicability to a vast array of computations on data, including\nstatistical analysis and machine learning. Previous work demonstrated that\nconcrete implementations of differential privacy mechanisms are vulnerable to\nstatistical attacks. This vulnerability is caused by the approximation of real\nvalues to floating point numbers. This paper presents a practical solution to\nthe finite-precision floating point vulnerability, where the inverse transform\nsampling of the Laplace distribution can itself be inverted, thus enabling an\nattack where the original value can be retrieved with non-negligible advantage.\nThe proposed solution has the advantages of being generalisable to any\ninfinitely divisible probability distribution, and of simple implementation in\nmodern architectures. Finally, the solution has been designed to make side\nchannel attack infeasible, because of inherently exponential, in the size of\nthe domain, brute force attacks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A spin-1/2 Heisenberg model on honeycomb lattice is investigated by doing\ntriplon analysis and quantum Monte Carlo calculations. This model, inspired by\nCu$_2$(pymca)$_3$(ClO$_4$), has three different antiferromagnetic exchange\ninteractions ($J_A$, $J_B$, $J_C$) on three different sets of nearest-neighbour\nbonds which form a kagome superlattice. While the model is bipartite and\nunfrustrated, its quantum phase diagram is found to be dominated by a quantum\nparamagnetic phase that is best described as a spin-gapped hexagonal-singlet\nstate. The N\\'eel antiferromagnetic order survives only in a small region\naround $J_A=J_B=J_C$. The magnetization produced by external magnetic field is\nfound to exhibit plateaus at 1/3 and 2/3 of the saturation value, or at 1/3\nalone, or no plateaus. Notably, the plateaus exist only inside a bounded region\nwithin the hexagonal-singlet phase. This study provides a clear understanding\nof the spin-gapped behaviour and magnetization plateaus observed in\nCu$_2$(pymca)$_3$(ClO$_4$), and also predicts the possible disappearance of 2/3\nplateau under pressure.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The core of self-supervised learning for pre-training language models\nincludes pre-training task design as well as appropriate data augmentation.\nMost data augmentations in language model pre-training are context-independent.\nA seminal contextualized augmentation was recently proposed in ELECTRA and\nachieved state-of-the-art performance by introducing an auxiliary generation\nnetwork (generator) to produce contextualized data augmentation for the\ntraining of a main discrimination network (discriminator). This design,\nhowever, introduces extra computation cost of the generator and a need to\nadjust the relative capability between the generator and the discriminator. In\nthis paper, we propose a self-augmentation strategy (SAS) where a single\nnetwork is utilized for both regular pre-training and contextualized data\naugmentation for the training in later epochs. Essentially, this strategy\neliminates a separate generator and uses the single network to jointly conduct\ntwo pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced\nToken Detection) heads. It avoids the challenge to search for an appropriate\nsize of the generator, which is critical to the performance as evidenced in\nELECTRA and its subsequent variant models. In addition, SAS is a general\nstrategy that can be seamlessly combined with many new techniques emerging\nrecently or in the future, such as the disentangled attention mechanism from\nDeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other\nstate-of-the-art models in the GLUE tasks with similar or less computation\ncost.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Developing shopping experiences that delight the customer requires businesses\nto understand customer taste. This work reports a method to learn the shopping\npreferences of frequent shoppers to an online gift store by combining ideas\nfrom retail analytics and statistical learning with sparsity. Shopping activity\nis represented as a bipartite graph. This graph is refined by applying\nsparsity-based statistical learning methods. These methods are interpretable\nand reveal insights about customers' preferences as well as products driving\nrevenue to the store.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Cosmological particle production by a time-dependent scalar field is common\nin cosmology. We focus on the mechanism of asymmetry production when\ninteraction explicitly violates symmetry and its motion is rapid enough to\ncreate particles by itself. Combining the exact WKB analysis and the\nLandau-Zener transition, we point out that perturbation before the\nnon-perturbative analysis may drastically change the structure of the Stokes\nlines of the theory. The Exact WKB can play an important role in avoiding such\ndiscrepancies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose that cuprate superconductors are in the vicinity of a spontaneous\nd-wave type Fermi surface symmetry breaking, often called a d-wave Pomeranchuk\ninstability. This idea is explored by means of a comprehensive study of\nmagnetic excitations within the slave-boson mean-field theory of the t-J model.\nWe can naturally understand the pronounced xy anisotropy of magnetic\nexcitations in untwinned YBa_{2}Cu_{3}O_{y} and the sizable change of\nincommensurability of magnetic excitations at the transition temperature to the\nlow-temperature tetragonal lattice structure in La_{2-x}Ba_{x}CuO_{4}. In\naddition, the present theoretical framework allows the understanding of the\nsimilarities and differences of magnetic excitations in Y-based and La-based\ncuprates.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Few-shot learning aims to generalize to novel classes with only a few samples\nwith class labels. Research in few-shot learning has borrowed techniques from\ntransfer learning, metric learning, meta-learning, and Bayesian methods. These\nmethods also aim to train models from limited training samples, and while\nencouraging performance has been achieved, they often fail to generalize to\nnovel domains. Many of the existing meta-learning methods rely on training data\nfor which the base classes are sampled from the same domain as the novel\nclasses used for meta-testing. However, in many applications in the industry,\nsuch as document classification, collecting large samples of data for\nmeta-learning is infeasible or impossible. While research in the field of the\ncross-domain few-shot learning exists, it is mostly limited to computer vision.\nTo our knowledge, no work yet exists that examines the use of few-shot learning\nfor classification of semi-structured documents (scans of paper documents)\ngenerated as part of a business workflow (forms, letters, bills, etc.). Here\nthe domain shift is significant, going from natural images to the\nsemi-structured documents of interest. In this work, we address the problem of\nfew-shot document image classification under domain shift. We evaluate our work\nby extensive comparisons with existing methods. Experimental results\ndemonstrate that the proposed method shows consistent improvements on the\nfew-shot classification performance under domain shift.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent years have seen a rapidly escalating demand for battery technologies\ncapable of storing more energy, charging more quickly and having longer usable\nlifetimes, driven largely by increased electrification of transport and by\ngrid-scale energy storage systems. This has led to the development of many\npromising new electrode materials for high-rate lithium ion batteries. In order\nto rationalise and improve upon material performance, it is crucial to\nunderstand the fundamental ion-intercalation and degradation mechanisms\noccurring during realistic battery operation, on the nano- to meso-scale. Here\nwe apply a straightforward laboratory-based operando optical scattering\nmicroscopy method to study micron-sized rods of the high-rate anode material\nNb$_{14}$W$_3$O$_{44}$ during cycling at rates of up to 30C. We directly\nvisualise an elongation of the rods, which, by comparison with ensemble X-ray\ndiffraction, allows us to determine the state of charge (SOC) of the individual\nparticle. A continuous change in scattering intensity with SOC is also seen,\nenabling observation of a non-equilibrium kinetic phase separation within\nindividual particles. Phase field modelling (informed by pulsed-field-gradient\nnuclear magnetic resonance and electrochemical experiments) is used to verify\nthe kinetic origin of this separation, which arises from a dependence of the\nLi-ion diffusion coefficient upon SOC. Finally, we witness how such\nintra-particle SOC heterogeneity can lead to particle cracking; we follow the\ncycling behaviour of the resultant fragments, and show that they may become\nelectrically disconnected from the electrode. These results demonstrate the\npower of optical scattering microscopy to track rapid non-equilibrium\nprocesses, often occurring over less than 1 minute, which would be inaccessible\nwith established characterisation techniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This review is dedicated to two-dimensional sigma models with flag manifold\ntarget spaces, which are generalizations of the familiar $CP^{n-1}$ and\nGrassmannian models. They naturally arise in the description of continuum\nlimits of spin chains, and their phase structure is sensitive to the values of\nthe topological angles, which are determined by the representations of spins in\nthe chain. Gapless phases can in certain cases be explained by the presence of\ndiscrete 't Hooft anomalies in the continuum theory. We also discuss integrable\nflag manifold sigma models, which provide a generalization of the theory of\nintegrable models with symmetric target spaces. These models, as well as their\ndeformations, have an alternative equivalent formulation as bosonic Gross-Neveu\nmodels, which proves useful for demonstrating that the deformed geometries are\nsolutions of the renormalization group (Ricci flow) equations, as well as for\nthe analysis of anomalies and for describing potential couplings to fermions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we analyze the spatio-temporal economic properties of\nmulti-product supply chains. Specifically, we interpret the supply chain as a\ncoordinated market in which stakeholders (suppliers, consumers, and providers\nof transportation, storage, and transformation services) bid into a market that\nis cleared by an independent entity to obtain allocations and prices. The\nproposed model provides a general graph representation of spatio-temporal\nproduct transport that helps capture geographical transport, time delays, and\nstorage (temporal transport) in a unified and compact manner. This\nrepresentation allows us to establish fundamental economic properties for the\nsupply chain (revenue adequacy, cost recovery, and competitiveness) and to\nestablish bounds for space-time prices. To illustrate the concepts, we consider\na case study in which organic waste is used for producing biogas and\nelectricity. Our market model shows that incentives for waste storage emerge\nfrom electricity demand dynamics and illustrates how space-time price dynamics\nfor waste and derived products emerge from geographical transport and storage.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a simple and efficient image classification architecture based on\ndeep multiple instance learning, and apply it to the challenging task of caries\ndetection in dental radiographs. Technically, our approach contributes in two\nways: First, it outputs a heatmap of local patch classification probabilities\ndespite being trained with weak image-level labels. Second, it is amenable to\nlearning from segmentation labels to guide training. In contrast to existing\nmethods, the human user can faithfully interpret predictions and interact with\nthe model to decide which regions to attend to. Experiments are conducted on a\nlarge clinical dataset of $\\sim$38k bitewings ($\\sim$316k teeth), where we\nachieve competitive performance compared to various baselines. When guided by\nan external caries segmentation model, a significant improvement in\nclassification and localization performance is observed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Scene graph generation (SGG) aims to capture a wide variety of interactions\nbetween pairs of objects, which is essential for full scene understanding.\nExisting SGG methods trained on the entire set of relations fail to acquire\ncomplex reasoning about visual and textual correlations due to various biases\nin training data. Learning on trivial relations that indicate generic spatial\nconfiguration like 'on' instead of informative relations such as 'parked on'\ndoes not enforce this complex reasoning, harming generalization. To address\nthis problem, we propose a novel framework for SGG training that exploits\nrelation labels based on their informativeness. Our model-agnostic training\nprocedure imputes missing informative relations for less informative samples in\nthe training data and trains a SGG model on the imputed labels along with\nexisting annotations. We show that this approach can successfully be used in\nconjunction with state-of-the-art SGG methods and improves their performance\nsignificantly in multiple metrics on the standard Visual Genome benchmark.\nFurthermore, we obtain considerable improvements for unseen triplets in a more\nchallenging zero-shot setting.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present sensitivity projections for discovering a heavy resonance decaying\nto electron and muon pairs and for probing the charged lepton non-universality\nin such decays at the HL-LHC and FCC-hh. The analysis takes into account the\nexpected differences in the reconstruction efficiencies and the dilepton mass\nresolutions for dielectron and dimuon final states. We demonstrate how the\nanalyses at HL-LHC naturally paves the way for a FCC-hh machine thereby\nunderlining its importance.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  If we are ever to move beyond the study of isolated special cases in\ntheoretical neuroscience, we need to develop more general theories of neural\ncircuits over a given neural model. The present paper considers this challenge\nin the context of continuous-time recurrent neural networks (CTRNNs), a simple\nbut dynamically-universal model that has been widely utilized in both\ncomputational neuroscience and neural networks. Here we extend previous work on\nthe parameter space structure of codimension-1 local bifurcations in CTRNNs to\ninclude codimension-2 local bifurcation manifolds. Specifically, we derive the\nnecessary conditions for all generic local codimension-2 bifurcations for\ngeneral CTRNNs, specialize these conditions to circuits containing from one to\nfour neurons, illustrate in full detail the application of these conditions to\nexample circuits, derive closed-form expressions for these bifurcation\nmanifolds where possible, and demonstrate how this analysis allows us to find\nand trace several global codimension-1 bifurcation manifolds that originate\nfrom the codimension-2 bifurcations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We construct product formulas for exponentials of commutators and explore\ntheir applications. First, we directly construct a third-order product formula\nwith six exponentials by solving polynomial equations obtained using the\noperator differential method. We then derive higher-order product formulas\nrecursively from the third-order formula. We improve over previous recursive\nconstructions, reducing the number of gates required to achieve the same\naccuracy. In addition, we demonstrate that the constituent linear terms in the\ncommutator can be included at no extra cost. As an application, we show how to\nuse the product formulas in a digital protocol for counterdiabatic driving,\nwhich increases the fidelity for quantum state preparation. We also discuss\napplications to quantum simulation of one-dimensional fermion chains with\nnearest- and next-nearest-neighbor hopping terms, and two-dimensional\nfractional quantum Hall phases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we study a model for monolayer molybdenum disulfide with\nincluding the intravalley and intervalley electron-electron interaction. We\nsolve the model at a self-consistent mean-field level and get three solutions\n$L_{0}$, $L_{+}$ and $L_{-}$. As for $L_{0}$, the spin polarizations are\nopposite at $\\textbf{K}$ and $\\textbf{K}^{\\prime}$ valley and the total\nmagnetization is zero. $L_{\\pm}$ describe two degenerate spin-polarized states,\nand the directions of polarization are opposite for the states of $L_{+}$ and\n$L_{-}$. Based on these results, the ground state can be deduced to be spin\npolarized in domains in which their particular states can be randomly described\nby $L_{+}$ or $L_{-}$. Therefore, a zero net magnetization is induced for zero\nexternal magnetic field $\\mathbf{B}$, but a global ferromagnetic ground state\nfor a nonzero $\\mathbf{B}$. We estimate the size of domains as several\nnanometers. As the increase of the chemical potential, the ground state changes\nbetween $L_{0}$ and $L_{\\pm}$, indicating first order phase transitions at the\nborders, which is coincident with the observation of photoluminescence\nexperiments in the absence of the external magnetic field [J. G. Roch $\\it{et}$\n$\\it{al.}$, Phys. Rev. Lett. ${\\bf 124}$, 187602 (2020)].\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study a 2D potential flow of an ideal fluid with a free surface with\ndecaying conditions at infinity. By using the conformal variables approach, we\nstudy a particular solution of Euler equations having a pair of square-root\nbranch points in the conformal plane, and find that the analytic continuation\nof the fluid complex potential and conformal map define a flow in the entire\ncomplex plane, excluding a vertical cut between the branch points. The expanded\ndomain is called the \"virtual\" fluid, and it contains a vortex sheet whose\ndynamics is equivalent to the equations of motion posed at the free surface.\nThe equations of fluid motion are analytically continued to both sides of the\nvertical branch cut (the vortex sheet), and additional time-invariants\nassociated with the topology of conformal plane and Kelvin's theorem for\nvirtual fluid are explored. We called them \"winding\" and virtual circulation.\nThis result can be generalized to a system of many cuts connecting many branch\npoints, and resulting in a pair of invariants for each pair of branch points.\nWe develop an asymptotic theory that shows how a solution originating from a\nsingle vertical cut forms a singularity at the free surface in infinite time,\nthe rate of singularity approach is double-exponential, and supercedes the\nprevious result of the short branch cut theory with finite time singularity\nformation. The present work offers a new look at fluid dynamics with free\nsurface by unifying the problem of motion of vortex sheets, and the problem of\n2D water waves. A particularly interesting question that arises in this context\nis whether instabilities of the virtual vortex sheet are related to breaking of\nsteep ocean waves when gravity effects are included.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. Estimating\ncausal parameters is challenging in the presence of network interference of\nunknown form, as each individual may be influenced by both close individuals\nand distant ones in complex ways. Noncompliance with treatment assignment\nfurther complicates this problem, and prior methods dealing with network\nspillovers but disregarding the noncompliance issue may underestimate the\neffect of the treatment receipt on the outcome. To estimate meaningful causal\nparameters, we introduce a new concept of exposure mapping, which summarizes\npotentially complicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effect and the average causal effect for compliers, while\nexplicitly considering the possibility of misspecification of exposure mapping.\nBased on our identification results, we develop nonparametric estimation\nprocedures via inverse probability weighting. Their asymptotic properties,\nincluding consistency and asymptotic normality, are investigated using an\napproximate neighborhood interference framework, which is convenient for\ndealing with unknown forms of spillovers between individuals. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The use of lattice tensor representations is explored to investigate the\nlattice Landau gauge gluon propagator for the pure SU(3) Yang-Mills gauge\ntheory in 4D. The analysis of several tensor bases allows to quantify the\ncompleteness of the various tensor bases considered, the deviations of the\nlattice results from the continuum theory due to the lattice artefacts and\nestimate the theoretical uncertainty in the propagator. Furthermore, our\nanalysis tests continuum based relations with the lattice data and show that\nthe lattice Landau gauge gluon propagator is described by a unique form factor,\nas in the continuum formulation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we find a class of special inclusions that have the same\nproperty with respect to second order linear partial differential equations as\nholds for ellipsoids. That is, in the simplest case and in physical terms,\nconstant magnetization of the inclusion implies constant magnetic field on the\ninclusion. The special inclusions are found as solutions of a simple\nvariational inequality. This variational inequality allows us to prescribe the\nconnectivity and periodicity properties of the inclusions. For example we find\nperiodic arrays of inclusions in two and three dimensions for which constant\nmagnetization of the inclusions implies constant magnetic field on the\ninclusions. The volume fraction of the inclusions can be any number between\nzero and one. We find such inclusions with any finite number of components and\ncomponents that are multiply connected. These special inclusions enjoy many\nuseful properties with respect to homogenization and energy minimization. For\nexample, we use them to give new results on a) the effective properties of\ntwo-phase composites and b) optimal bounds and optimal microstructures for\ntwo-phase composites.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep learning (DL) inverse techniques have increased the speed of artificial\nelectromagnetic material (AEM) design and improved the quality of resulting\ndevices. Many DL inverse techniques have succeeded on a number of AEM design\ntasks, but to compare, contrast, and evaluate assorted techniques it is\ncritical to clarify the underlying ill-posedness of inverse problems. Here we\nreview state-of-the-art approaches and present a comprehensive survey of deep\nlearning inverse methods and invertible and conditional invertible neural\nnetworks to AEM design. We produce easily accessible and rapidly implementable\nAEM design benchmarks, which offers a methodology to efficiently determine the\nDL technique best suited to solving different design challenges. Our\nmethodology is guided by constraints on repeated simulation and an easily\nintegrated metric, which we propose expresses the relative ill-posedness of any\nAEM design problem. We show that as the problem becomes increasingly ill-posed,\nthe neural adjoint with boundary loss (NA) generates better solutions faster,\nregardless of simulation constraints. On simpler AEM design tasks, direct\nneural networks (NN) fare better when simulations are limited, while geometries\npredicted by mixture density networks (MDN) and conditional variational\nauto-encoders (VAE) can improve with continued sampling and re-simulation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The traveltime of compressional (P) and shear (S) waves have proven essential\nin many applications of earthquake and exploration seismology. An accurate and\nefficient traveltime computation for P and S waves is crucial for the success\nof these applications. However, solutions to the Eikonal equation with a\ncomplex phase velocity field in anisotropic media is challenging. The Eikonal\nequation is a first-order, hyperbolic, nonlinear partial differential equation\n(PDE) that represents the high-frequency asymptotic approximation of the wave\nequation. The fast marching and sweeping methods are commonly used due to their\nefficiency in numercally solving Eikonal equation. However, these methods\nsuffer from numerical inaccuracy in anisotropic media with sharp heterogeneity,\nirregular surface topography and complex phase velocity fields. This study\npresents a new method to solving the Eikonal equation by employing the\nperidynamic differential operator (PDDO). The PDDO provides the nonlocal form\nof the Eikonal equation by introducing an internal length parameter (horizon)\nand a weight function with directional nonlocality. The operator is immune to\ndiscontinuities in the form sharp changes in field or model variables and\ninvokes the direction of traveltime in a consistent manner. The weight function\ncontrols the degree of association among points within the horizon. Solutions\nare constructed in a consistent manner without upwind assumptions through\nsimple discretization. The capability of this approach is demonstrated by\nconsidering different types of Eikonal equations on complex velocity models in\nanisotropic media. The examples demonstrate its unconditional numerical\nstability and results compare well with the reference solutions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A large set of recent experiments has been exploring topological transport in\nbosonic systems, e.g. of photons or phonons. In the vast majority,\ntime-reversal symmetry is preserved, and band structures are engineered by a\nsuitable choice of geometry, to produce topologically nontrivial bandgaps in\nthe vicinity of high-symmetry points. However, this leaves open the possibility\nof large-quasimomentum backscattering, destroying the topological protection.\nUp to now, it has been unclear what precisely are the conditions where this\neffect can be sufficiently suppressed. In the present work, we introduce a\ncomprehensive semiclassical theory of tunneling transitions in momentum space,\ndescribing backscattering for one of the most important system classes, based\non the valley Hall effect. We predict that even for a smooth domain wall\neffective scattering centres develop at locations determined by both the local\nslope of the wall and the energy. Moreover, our theory provides a quantitative\nanalysis of the exponential suppression of the overall reflection amplitude\nwith increasing domain wall smoothness.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We generalize the well-known primal-dual algorithm proposed by Chambolle and\nPock for saddle point problems, and improve the condition for ensuring its\nconvergence. The improved convergence-guaranteeing condition is effective for\nthe generic setting, and it is shown to be optimal. It also allows us to\ndiscern larger step sizes for the resulting subproblems, and thus provides a\nsimple and universal way to improve numerical performance of the original\nprimal-dual algorithm. In addition, we present a structure-exploring heuristic\nto further relax the convergence-guaranteeing condition for some specific\nsaddle point problems, which could yield much larger step sizes and hence\nsignificantly better performance. Effectiveness of this heuristic is\nnumerically illustrated by the classic assignment problem.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In recent years, the metaverse has attracted enormous attention from around\nthe world with the development of related technologies. The expected metaverse\nshould be a realistic society with more direct and physical interactions, while\nthe concepts of race, gender, and even physical disability would be weakened,\nwhich would be highly beneficial for society. However, the development of\nmetaverse is still in its infancy, with great potential for improvement.\nRegarding metaverse's huge potential, industry has already come forward with\nadvance preparation, accompanied by feverish investment, but there are few\ndiscussions about metaverse in academia to scientifically guide its\ndevelopment. In this paper, we highlight the representative applications for\nsocial good. Then we propose a three-layer metaverse architecture from a macro\nperspective, containing infrastructure, interaction, and ecosystem. Moreover,\nwe journey toward both a historical and novel metaverse with a detailed\ntimeline and table of specific attributes. Lastly, we illustrate our\nimplemented blockchain-driven metaverse prototype of a university campus and\ndiscuss the prototype design and insights.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Data-driven character animation techniques rely on the existence of a\nproperly established model of motion, capable of describing its rich context.\nHowever, commonly used motion representations often fail to accurately encode\nthe full articulation of motion, or present artifacts. In this work, we address\nthe fundamental problem of finding a robust pose representation for motion\nmodeling, suitable for deep character animation, one that can better constrain\nposes and faithfully capture nuances correlated with skeletal characteristics.\nOur representation is based on dual quaternions, the mathematical abstractions\nwith well-defined operations, which simultaneously encode rotational and\npositional orientation, enabling a hierarchy-aware encoding, centered around\nthe root. We demonstrate that our representation overcomes common motion\nartifacts, and assess its performance compared to other popular\nrepresentations. We conduct an ablation study to evaluate the impact of various\nlosses that can be incorporated during learning. Leveraging the fact that our\nrepresentation implicitly encodes skeletal motion attributes, we train a\nnetwork on a dataset comprising of skeletons with different proportions,\nwithout the need to retarget them first to a universal skeleton, which causes\nsubtle motion elements to be missed. We show that smooth and natural poses can\nbe achieved, paving the way for fascinating applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Heavily pre-trained transformers for language modelling, such as BERT, have\nshown to be remarkably effective for Information Retrieval (IR) tasks,\ntypically applied to re-rank the results of a first-stage retrieval model. IR\nbenchmarks evaluate the effectiveness of retrieval pipelines based on the\npremise that a single query is used to instantiate the underlying information\nneed. However, previous research has shown that (I) queries generated by users\nfor a fixed information need are extremely variable and, in particular, (II)\nneural models are brittle and often make mistakes when tested with modified\ninputs. Motivated by those observations we aim to answer the following\nquestion: how robust are retrieval pipelines with respect to different\nvariations in queries that do not change the queries' semantics? In order to\nobtain queries that are representative of users' querying variability, we first\ncreated a taxonomy based on the manual annotation of transformations occurring\nin a dataset (UQV100) of user-created query variations. For each\nsyntax-changing category of our taxonomy, we employed different automatic\nmethods that when applied to a query generate a query variation. Our\nexperimental results across two datasets for two IR tasks reveal that retrieval\npipelines are not robust to these query variations, with effectiveness drops of\n$\\approx20\\%$ on average. The code and datasets are available at\nhttps://github.com/Guzpenha/query_variation_generators.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-user MIMO enhances throughput by simultaneously transmitting/receiving\nparallel data streams to/from a group of users. However, the throughput\nanalysis does not ac-count for variable data traffic. The research objective of\nthis project is to analyze the effect of variable traffic as it will change the\nsystem behavior. The two characteristic features of variable traffic viz.\npacket size variation and traffic burstiness have been considered for the\nanalysis in this project. Their effects have been studied individually to\nprovide an insight into the problem. Through simulations, we show that in\ncertain scenarios the system performance of MU-MIMO deteriorates significantly\ndue to both packet size variations and traffic burstiness individually even\nunder ideal conditions with respect to channel variations, channel correlation,\nmobility, etc. Furthermore, we show that under bursty traffic with higher\noffered load the aggregate throughput first re-mains steady until a certain\npeak to average rate ratio and then deteriorates linearly instead of\nexponentially. Also, un-der high amount of traffic burstiness, the throughputs\nare in-dependent of the aggregation rates. A thorough analysis is provided to\nexplain these two phenomena. This is the first research work that considers the\nimpact of variable traffic on the performance of MU-MIMO. Therefore, the\nimplications for MAC protocol design are significant.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper proposes an unsupervised cross-modality domain adaptation approach\nbased on pixel alignment and self-training. Pixel alignment transfers ceT1\nscans to hrT2 modality, helping to reduce domain shift in the training\nsegmentation model. Self-training adapts the decision boundary of the\nsegmentation network to fit the distribution of hrT2 scans. Experiment results\nshow that PAST has outperformed the non-UDA baseline significantly, and it\nreceived rank-2 on CrossMoDA validation phase Leaderboard with a mean Dice\nscore of 0.8395.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Hierarchical Mobility Label Based Network (HMLBN) is a new approach to the\nnetwork layer mobility management problem that relies on MPLS-aware control\nplane and MPLS-based forwarding plane to provide IP mobility support for IPv4\nand IPv6 mobile hosts and routers while being able to ensure optimal traffic\ndelivery between the communicating devices. The hierarchical system is capable\nof both macro- and micro-mobility support without the use of Mobile IP and its\nderivatives thus eliminating the user and network facing performance penalties\nassociated with triangular routing and bi-directional tunneling. This paper\npresents a system model and provides performance analysis for H-MLBN and\ncompares its performance with the Mobile IP based schemes. The results indicate\nsignificant performance improvements in the forwarding plane traffic delivery\nas well as the control plane network update costs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Future high-energy $e^+e^-$ colliders will provide some of the most precise\ntests of the Standard Model. Statistical uncertainties on electroweak precision\nobservables and triple gauge couplings are expected to improve by orders of\nmagnitude over current measurements. This provides a new challenge in\naccurately assessing and minimising the impact of systematic uncertainties.\nBeam polarisation may hold a unique potential to isolate and determine the size\nof systematic effects. So far, studies have mainly focused on the statistical\nimprovements from beam polarisation. This study aims to assess, for the first\ntime, its impact on systematic uncertainties. A combined fit of precision\nobservables, such as chiral fermion couplings and anomalous triple gauge\ncouplings, together with experimental systematic effects is performed on\ngenerator-level differential distribution of 2-fermion and 4-fermion\nfinal-states. Different configurations of available beam polarisations and\nluminosities are tested with and without systematic effects, and will be\ndiscussed in the context of the existing projections on fermion and gauge boson\ncouplings from detailed experimental simulations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we prove the cohomological Lichtenbaum conjecture of abelian\nextensions of imaginary quadratic fields up to a finite set of bad primes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Video Question Answering is a task which requires an AI agent to answer\nquestions grounded in video. This task entails three key challenges: (1)\nunderstand the intention of various questions, (2) capturing various elements\nof the input video (e.g., object, action, causality), and (3) cross-modal\ngrounding between language and vision information. We propose Motion-Appearance\nSynergistic Networks (MASN), which embed two cross-modal features grounded on\nmotion and appearance information and selectively utilize them depending on the\nquestion's intentions. MASN consists of a motion module, an appearance module,\nand a motion-appearance fusion module. The motion module computes the\naction-oriented cross-modal joint representations, while the appearance module\nfocuses on the appearance aspect of the input video. Finally, the\nmotion-appearance fusion module takes each output of the motion module and the\nappearance module as input, and performs question-guided fusion. As a result,\nMASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA\ndatasets. We also conduct qualitative analysis by visualizing the inference\nresults of MASN. The code is available at\nhttps://github.com/ahjeongseo/MASN-pytorch.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  One weakness of machine learning algorithms is the poor ability of models to\nsolve new problems without forgetting previously acquired knowledge. The\nContinual Learning (CL) paradigm has emerged as a protocol to systematically\ninvestigate settings where the model sequentially observes samples generated by\na series of tasks. In this work, we take a task-agnostic view of continual\nlearning and develop a hierarchical information-theoretic optimality principle\nthat facilitates a trade-off between learning and forgetting. We discuss this\nprinciple from a Bayesian perspective and show its connections to previous\napproaches to CL. Based on this principle, we propose a neural network layer,\ncalled the Mixture-of-Variational-Experts layer, that alleviates forgetting by\ncreating a set of information processing paths through the network which is\ngoverned by a gating policy. Due to the general formulation based on generic\nutility functions, we can apply this optimality principle to a large variety of\nlearning problems, including supervised learning, reinforcement learning, and\ngenerative modeling. We demonstrate the competitive performance of our method\nin continual supervised learning and in continual reinforcement learning.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present archival Giant Metrewave Radio Telescope (GMRT) observations of\ntwo exoplanetary systems, $\\tau$ Bo\\\"otis, and 55 Cancri, at 610 MHz and 150\nMHz, respectively. Theoretical models predict these systems to have some of the\nhighest expected flux densities at radio wavelengths. Both $\\tau$ Bo\\\"otis and\n55 Cancri have been previously observed at low frequency ($\\sim$ 30 MHz) with\nLow-Frequency Array (LOFAR) (Turner et al. 2020). $\\tau$ Bo\\\"otis shows\ntentative signatures of circularly polarized emission at 30 MHz, while no\nemission was detected from 55 Cancri. We do not detect radio emission from both\nthe systems, but the GMRT observations set $3\\sigma$ upper limits of 0.6 mJy at\n610 MHz for $\\tau$ Bo\\\"otis and 4.6 mJy at 150 MHz for 55 Cancri. The\nsensitivity achieved at 610 MHz in these observations is comparable to some of\nthe deepest images of an exoplanet field.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $p \\colon Y \\to X$ be a finite, regular cover of finite graphs with\nassociated deck group $G$, and consider the first homology $H_1(Y;\\mathbb{C})$\nof the cover as a $G$-representation. The main contribution of this article is\nto broaden the correspondence and dictionary between the representation theory\nof the deck group $G$ on the one hand, and topological properties of homology\nclasses in $H_1(Y;\\mathbb{C})$ on the other hand. We do so by studying certain\nsubrepresentations in the $G$-representation $H_1(Y;\\mathbb{C})$.\n  The homology class of a lift of a primitive element in $\\pi_1(X)$ spans an\ninduced subrepresentation in $H_1(Y;\\mathbb{C})$, and we show that this\nproperty is never sufficient to characterize such homology classes if $G$ is\nAbelian. We study $H_1^{\\textrm{comm}}(Y;\\mathbb{C}) \\leq H_1(Y;\\mathbb{C})$ --\nthe subrepresentation spanned by homology classes of lifts of commutators of\nprimitive elements in $\\pi_1(X)$. Concretely, we prove that the span of such a\nhomology class is isomorphic to the quotient of two induced representations.\nFurthermore, we construct examples of finite covers with\n$H_1^{\\textrm{comm}}(Y;\\mathbb{C}) \\neq \\ker(p_*)$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We obtain the statistical entropy of a scalar field on the Schwarzschild\nblack hole in holographic massive gravity by considering corrections on the\ndensity of quantum states to all orders in the Planck length from a generalized\nuncertainty principle (GUP). As a result, we find not only the generalized\nBekenstein-Hawking entropy depending on holographically massive gravitons\nwithout any artificial cutoff, but also new additional correction terms, which\nare proportional to surface gravity. Moreover, we also observe that all order\nGUP corrected entropy is improved to have smaller GUP parameter $\\lambda$ than\nthe previous results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recently, it has been shown that in graded systems, thermal rectification\n(TR) effect may remain in the thermodynamical limit. Here, by taking the\none-dimensional rotor lattice as an illustrating model, we investigate how the\ngraded structure may affect the TR efficiency. In particular, we consider the\ncase where the interaction is assigned with nonlinear polynomial functions. It\nis found that TR is robust in the thermodynamical limit and meanwhile its\nefficiency may considerably depend on the details of the graded structure. This\nfinding suggests that it is possible to enhance the TR effect by taking into\naccount the nonlinear graded structure even in large systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The event sequence of many diverse systems is represented as a sequence of\ndiscrete events in a continuous space. Examples of such an event sequence are\nearthquake aftershock events, financial transactions, e-commerce transactions,\nsocial network activity of a user, and the user's web search pattern. Finding\nsuch an intricate pattern helps discover which event will occur in the future\nand when it will occur. A Hawkes process is a mathematical tool used for\nmodeling such time series discrete events. Traditionally, the Hawkes process\nuses a critical component for modeling data as an intensity function with a\nparameterized kernel function. The Hawkes process's intensity function involves\ntwo components: the background intensity and the effect of events' history.\nHowever, such parameterized assumption can not capture future event\ncharacteristics using past events data precisely due to bias in modeling kernel\nfunction. This paper explores the recent advancement using novel deep\nlearning-based methods to model kernel function to remove such parametrized\nkernel function. In the end, we will give potential future research directions\nto improve modeling using the Hawkes process.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a detailed spectroscopic analysis of a galaxy at $z \\simeq 4.88$\nthat is, by chance, magnified $\\sim 30 \\times$ by gravitational lensing. Only\nthree sources at $z \\gtrsim 5$ are known with such high magnification. This\nparticular source has been shown to exhibit widespread, high equivalent width\nCIV $\\lambda$ 1549 {\\AA} emission, implying it is a unique example of a\nmetal-poor galaxy with a hard radiation field, likely representing the galaxy\npopulation responsible for cosmic reionisation. Using UV nebular line ratio\ndiagnostics, VLT/X-shooter observations rule out strong AGN activity,\nindicating a stellar origin of the hard radiation field instead. We present a\nnew detection of [NeIII] $\\lambda$ 3870 {\\AA} and use the [NeIII]/[OII] line\nratio to constrain the ionisation parameter and gas-phase metallicity. Closely\nrelated to the commonly used [OIII]/[OII] ratio, our [NeIII]/[OII] measurement\nshows this source is similar to local \"Green Pea\" galaxies and Lyman-continuum\nleakers. It furthermore suggests this galaxy is more metal poor than expected\nfrom the Fundamental Metallicity Relation, possibly as a consequence of excess\ngas accretion diluting the metallicity. Finally, we present the highest\nredshift detection of MgII $\\lambda$ 2796 {\\AA}, observed at high equivalent\nwidth in emission, in contrast to more evolved systems predominantly exhibiting\nMgII absorption. Strong MgII emission has been observed in most $z \\sim 0$\nLyman-continuum leakers known and has recently been proposed as an indirect\ntracer of escaping ionising radiation. In conclusion, this strongly lensed\ngalaxy, observed just 300 Myr after reionisation ends, enables testing of\nobservational diagnostics proposed to constrain the physical properties of\ndistant galaxies in the $\\mathit{JWST}$/ELT era.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We provide a generalization of pseudo-Frobenius numbers of numerical\nsemigroups to the context of the simplicial affine semigroups. In this way, we\ncharacterize the Cohen-Macaulay type of the simplicial affine semigroup ring\n$\\mathbb{K}[S]$.\n  We define the type of $S$, $\\operatorname{type}$, in terms of some Ap\\'ery\nsets of $S$ and show that it coincides with the Cohen-Macaulay type of the\nsemigroup ring, when $\\mathbb{K}[S]$ is Cohen-Macaulay. If $\\mathbb{K}[S]$ is a\n$d$-dimensional Cohen-Macaulay ring of embedding dimension at most $d+2$, then\n$\\operatorname{type}\\leq 2$. Otherwise, $\\operatorname{type}$ might be\narbitrary large and it has no upper bound in terms of the embedding dimension.\nFinally, we present a generating set for the conductor of $S$ as an ideal of\nits normalization.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  These are the first 50 issues of This Week's Finds of Mathematical Physics,\nfrom January 19, 1993 to March 12, 1995. These issues focus on quantum gravity,\ntopological quantum field theory, knot theory, and applications of\n$n$-categories to these subjects. However, there are also digressions into Lie\nalgebras, elliptic curves, linear logic and other subjects. They were typeset\nin 2020 by Tim Hosgood. If you see typos or other problems please report them.\n(I already know the cover page looks weird).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The $^3{\\rm H}(d,n)^4{\\rm He}$ reaction is of significant interest in nuclear\nastrophysics and nuclear applications. It is an important, early step in\nbig-bang nucleosynthesis and a key process in nuclear fusion reactors. We use\none- and two-level $R$-matrix approximations to analyze data on the cross\nsection for this reaction at center-of-mass energies below 215 keV. We\ncritically examine the data sets using a Bayesian statistical model that allows\nfor both common-mode and additional point-to-point uncertainties. We use Markov\nChain Monte Carlo sampling to evaluate this $R$-matrix-plus-statistical model\nand find two-level $R$-matrix results that are stable with respect to\nvariations in the channel radii. The $S$ factor at 40 keV evaluates to\n$25.36(19)$ MeV b (68% credibility interval). We discuss our Bayesian analysis\nin detail and provide guidance for future applications of Bayesian methods to\n$R$-matrix analyses. We also discuss possible paths to further reduction of the\n$S$-factor uncertainty.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a deterministic finite automaton and its implementation with at most\none single fault, that we can test on a set of inputs, we provide an algorithm\nto find a test set that guarantees finding whether the fault exists.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It is noted that effective very recently, the orbital motion of Encke's comet\nhas become affected by a very slight nongravitational deceleration. Soon after\nJ. F. Encke established in the early 19th century that the comet was returning\nto perihelion every 3.3 years, he also discovered that the object was notorious\nfor returning to perihelion a little earlier than predicted by the Newtonian\ntheory. The acceleration persisted over a period of two centuries, but its rate\nwas gradually decreasing, Generations of cometary astronomers were curious to\nknow whether or not the comet would eventually move in purely gravitational\norbit. A model based on the assumption of a precession of the comet's nucleus,\nwhich predicted that the acceleration would change into a deceleration, was not\npublished until 1979. This transition has now been documented by two\nindependent, highly-accurate orbit determinations. The era of the comet's\npersevering nongravitational acceleration is finally over.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $S_{g,n}$ be an oriented surface of genus $g$ with $n$ punctures, where\n$2g-2+n>0$ and $n>0$. Any ideal triangulation of $S_{g,n}$ induces a global\nparametrization of the Teichm\\\"uller space $\\mathcal{T}_{g,n}$ called the\nshearing coordinates. We study the asymptotics of the number of the mapping\nclass group orbits with respect to the standard Euclidean norm of the shearing\ncoordinates. The result is based on the works of Mirzakhani.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider networked sources that generate update messages with a defined\nrate and we investigate the age of that information at the receiver. Typical\napplications are in cyber-physical systems that depend on timely sensor\nupdates. We phrase the age of information in the min-plus algebra of the\nnetwork calculus. This facilitates a variety of models including wireless\nchannels and schedulers with random cross-traffic, as well as sources with\nperiodic and random updates, respectively. We show how the age of information\ndepends on the network service where, e.g., outages of a wireless channel cause\ndelays. Further, our analytical expressions show two regimes depending on the\nupdate rate, where the age of information is either dominated by congestive\ndelays or by idle waiting. We find that the optimal update rate strikes a\nbalance between these two effects.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The paper is proposing a methodology for modeling a gate-level netlist using\na Graph Convolutional Network (GCN). The model predicts the overall functional\nde-rating factors of sequential elements of a given circuit. In the preliminary\nphase of the work, the important goal is making a GCN which able to take a\ngate-level netlist as input information after transforming it into the\nProbabilistic Bayesian Graph in the form of Graph Modeling Language (GML). This\npart enables the GCN to learn the structural information of netlist in graph\ndomains. In the second phase of the work, the modeled GCN trained with the a\nfunctional de-rating factor of a very low number of individual sequential\nelements (flip-flops). The third phase includes understanding of GCN models\naccuracy to model an arbitrary circuit netlist. The designed model was\nvalidated for two circuits. One is the IEEE 754 standard double precision\nfloating point adder and the second one is the 10-Gigabit Ethernet MAC\nIEEE802.3 standard. The predicted results compared to the standard fault\ninjection campaign results of the error called Single EventUpset (SEU). The\nvalidated results are graphically pictured in the form of the histogram and\nsorted probabilities and evaluated with the Confidence Interval (CI) metric\nbetween the predicted and simulated fault injection results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate a set of techniques for RNN Transducers (RNN-Ts) that were\ninstrumental in lowering the word error rate on three different tasks\n(Switchboard 300 hours, conversational Spanish 780 hours and conversational\nItalian 900 hours). The techniques pertain to architectural changes, speaker\nadaptation, language model fusion, model combination and general training\nrecipe. First, we introduce a novel multiplicative integration of the encoder\nand prediction network vectors in the joint network (as opposed to additive).\nSecond, we discuss the applicability of i-vector speaker adaptation to RNN-Ts\nin conjunction with data perturbation. Third, we explore the effectiveness of\nthe recently proposed density ratio language model fusion for these tasks. Last\nbut not least, we describe the other components of our training recipe and\ntheir effect on recognition performance. We report a 5.9% and 12.5% word error\nrate on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation\nand a 12.7% WER on the Mozilla CommonVoice Italian test set.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that polynomials do not belong to the reproducing kernel Hilbert\nspace of infinitely differentiable translation-invariant kernels whose spectral\nmeasures have moments corresponding to a determinate moment problem. Our proof\nis based on relating this question to the problem of best linear estimation in\ncontinuous time one-parameter regression models with a stationary error process\ndefined by the kernel. In particular, we show that the existence of a sequence\nof estimators with variances converging to $0$ implies that the regression\nfunction cannot be an element of the reproducing kernel Hilbert space. This\nquestion is then related to the determinacy of the Hamburger moment problem for\nthe spectral measure corresponding to the kernel.\n  In the literature it was observed that a non-vanishing constant function does\nnot belong to the reproducing kernel Hilbert space associated with the Gaussian\nkernel (see Corollary 4.44 in Steinwart and Christmann, 2008). Our results\nprovide a unifying view of this phenomenon and show that the mentioned result\ncan be extended for arbitrary polynomials and a broad class of\ntranslation-invariant kernels.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recently, Robotic Cooking has been a very promising field. To execute a\nrecipe, a robot has to recognize different objects and their states. Contrary\nto object recognition, state identification has not been explored that much.\nBut it is very important because different recipe might require different state\nof an object. Moreover, robotic grasping depends on the state. Pretrained model\nusually perform very well in this type of tests. Our challenge was to handle\nthis problem without using any pretrained model. In this paper, we have\nproposed a CNN and trained it from scratch. The model is trained and tested on\nthe dataset from cooking state recognition challenge. We have also evaluated\nthe performance of our network from various perspective. Our model achieves\n65.8% accuracy on the unseen test dataset.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modularity of neural networks -- both biological and artificial -- can be\nthought of either structurally or functionally, and the relationship between\nthese is an open question. We show that enforcing structural modularity via\nsparse connectivity between two dense sub-networks which need to communicate to\nsolve the task leads to functional specialization of the sub-networks, but only\nat extreme levels of sparsity. With even a moderate number of interconnections,\nthe sub-networks become functionally entangled. Defining functional\nspecialization is in itself a challenging problem without a universally agreed\nsolution. To address this, we designed three different measures of\nspecialization (based on weight masks, retraining and correlation) and found\nthem to qualitatively agree. Our results have implications in both neuroscience\nand machine learning. For neuroscience, it shows that we cannot conclude that\nthere is functional modularity simply by observing moderate levels of\nstructural modularity: knowing the brain's connectome is not sufficient for\nunderstanding how it breaks down into functional modules. For machine learning,\nusing structure to promote functional modularity -- which may be important for\nrobustness and generalization -- may require extremely narrow bottlenecks\nbetween modules.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Graph network science is becoming increasingly popular, notably in big-data\nperspective where understanding individual entities for individual functional\nroles is complex and time consuming. It is likely when a set of genes are\nregulated by a set of genetic variants, the genes set is recruited for a common\nor related functional purpose. Grouping and extracting communities from network\nof associations becomes critical to understand system complexity, thus\nprioritizing genes for dis-ease and functional associations. Workload is\nreduced when studying entities one at a time. For this, we present GraphBreak,\na suite of tools for community detection application, such as for gene\nco-expression, protein interaction, regulation network, etc.Although developed\nfor use case of eQTLs regulatory genomic net-work community study -- results\nshown with our analysis with sample eQTL data. Graphbreak can be deployed for\nother studies if input data has been fed in requisite format, including but not\nlimited to gene co-expression networks, protein-protein interaction network,\nsignaling pathway and metabolic network. Graph-Break showed critical use case\nvalue in its downstream analysis for disease association of communities\ndetected. If all independent steps of community detection and analysis are a\nstep-by-step sub-part of the algorithm, GraphBreak can be considered a new\nalgorithm for community based functional characterization. Combination of\nvarious algorithmic implementation modules into a single script for this\npurpose illustrates GraphBreak novelty. Compared to other similar tools, with\nGraphBreak we can better detect communities with over-representation of its\nmember genes for statistical association with diseases, therefore target genes\nwhich can be prioritized for drug-positioning or drug-re-positioning as the\ncase be.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a robust learning algorithm to detect and handle collisions in 3D\ndeforming meshes. Our collision detector is represented as a bilevel deep\nautoencoder with an attention mechanism that identifies colliding mesh\nsub-parts. We use a numerical optimization algorithm to resolve penetrations\nguided by the network. Our learned collision handler can resolve collisions for\nunseen, high-dimensional meshes with thousands of vertices. To obtain stable\nnetwork performance in such large and unseen spaces, we progressively insert\nnew collision data based on the errors in network inferences. We automatically\nlabel these data using an analytical collision detector and progressively\nfine-tune our detection networks. We evaluate our method for collision handling\nof complex, 3D meshes coming from several datasets with different shapes and\ntopologies, including datasets corresponding to dressed and undressed human\nposes, cloth simulations, and human hand poses acquired using multiview capture\nsystems. Our approach outperforms supervised learning methods and achieves\n$93.8-98.1\\%$ accuracy compared to the groundtruth by analytic methods.\nCompared to prior learning methods, our approach results in a $5.16\\%-25.50\\%$\nlower false negative rate in terms of collision checking and a $9.65\\%-58.91\\%$\nhigher success rate in collision handling.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that a constant factor approximation of the shortest and closest\nlattice vector problem in any norm can be computed in time $2^{0.802\\, n}$.\nThis contrasts the corresponding $2^n$ time, (gap)-SETH based lower bounds for\nthese problems that even apply for small constant approximation. For both\nproblems, $\\mathrm{SVP}$ and $\\mathrm{CVP}$, we reduce to the case of the\nEuclidean norm. A key technical ingredient in that reduction is a twist of\nMilman's construction of an $M$-ellipsoid which approximates any symmetric\nconvex body $K$ with an ellipsoid $\\mathcal{E}$ so that $2^{\\varepsilon n}$\ntranslates of a constant scaling of $\\mathcal{E}$ can cover $K$ and vice versa.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the coupled dynamics of quantized vortices and normal fluid in\nsuperfluid $^4$He at finite temperatures using a numerical approach based on\nthe vortex filament model (VFM) and lattice Boltzmann method (LBM). The LBM\nallows us to simulate a fluid flow with only local operations, i .e., rather\nthan solving the Navier--Stokes (NS) equations directly; a fluid flow is\nconsidered a convection of mesoscopic particles between sites on a lattice\ngrid. Although the two-fluid nature of He II makes its flow complex, the\nparticle-like treatment of the normal fluid in the LBM significantly reduces\nthe complexity. We confirm, by comparing to results obtained with direct NS\nsimulations, that the proposed numerical approach reproduces characteristic\nflow structures. We also demonstrate that the proposed computational approach\nis suitable for a thermal counterflow simulation with a solid boundary to\nelucidate a thermal boundary layer near a heater in a closed channel.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We experimentally measure a three-dimensional (3D) granular system's\nreversibility under cyclic compression. We image the grains using a\nrefractive-index-matched fluid, then analyze the images using the artificial\nintelligence of variational autoencoders. These techniques allow us to track\nall the grains' translations and 3D rotations with accuracy sufficient to infer\nsliding and rolling displacements. Our observations reveal unique roles played\nby 3D rotational motions in granular flows. We find that rotations and\ncontact-point motion dominate the dynamics in the bulk, far from the\nperturbation's source. Furthermore, we determine that 3D rotations are\nirreversible under cyclic compression. Consequently, contact-point sliding,\nwhich is dissipative, accumulates throughout the cycle. Using numerical\nsimulations whose accuracy our experiment supports, we discover that much of\nthe dissipation occurs in the bulk, where grains rotate more than they\ntranslate. Our observations suggest that the analysis of 3D rotations is needed\nfor understanding granular materials' unique and powerful ability to absorb and\ndissipate energy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We use the theory of topological modular forms to constrain bosonic\nholomorphic CFTs, which can be viewed as $(0,1)$ SCFTs with trivial\nright-moving supersymmetric sector. A conjecture by Segal, Stolz and Teichner\nrequires the constant term of the partition function to be divisible by\nspecific integers determined by the central charge. We verify this constraint\nin large classes of physical examples, and rule out the existence of an\ninfinite set of extremal CFTs, including those with central charges $c=48, 72,\n96$ and $120$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Complex interconnections between information technology and digital control\nsystems have significantly increased cybersecurity vulnerabilities in smart\ngrids. Cyberattacks involving data integrity can be very disruptive because of\ntheir potential to compromise physical control by manipulating measurement\ndata. This is especially true in large and complex electric networks that often\nrely on traditional intrusion detection systems focused on monitoring network\ntraffic. In this paper, we develop an online detection algorithm to detect and\nlocalize covert attacks on smart grids. Using a network system model, we\ndevelop a theoretical framework by characterizing a covert attack on a\ngenerator bus in the network as sparse features in the state-estimation\nresiduals. We leverage such sparsity via a regularized linear regression method\nto detect and localize covert attacks based on the regression coefficients. We\nconduct a comprehensive numerical study on both linear and nonlinear system\nmodels to validate our proposed method. The results show that our method\noutperforms conventional methods in both detection delay and localization\naccuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  State estimation is critical to control systems, especially when the states\ncannot be directly measured. This paper presents an approximate optimal filter,\nwhich enables to use policy iteration technique to obtain the steady-state gain\nin linear Gaussian time-invariant systems. This design transforms the optimal\nfiltering problem with minimum mean square error into an optimal control\nproblem, called Approximate Optimal Filtering (AOF) problem. The equivalence\nholds given certain conditions about initial state distributions and policy\nformats, in which the system state is the estimation error, control input is\nthe filter gain, and control objective function is the accumulated estimation\nerror. We present a policy iteration algorithm to solve the AOF problem in\nsteady-state. A classic vehicle state estimation problem finally evaluates the\napproximate filter. The results show that the policy converges to the\nsteady-state Kalman gain, and its accuracy is within 2 %.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The scattering of two and more particles at low energies is described by the\nso called effective-range expansion. The leading terms of this expansion are\nthe scattering length and effective range. The analytic expressions for both of\nthe aforementioned scattering parameters are presented for the inverse-power\npotential and the Woods-Saxon potential. A technique for calculating the\napproximate scattering parameters is proposed. Approximate analytic formulas\nrepresenting the scattering length and effective range are obtained for the\nYukawa potential. The corresponding figures demonstrate a few interesting\nfeatures of the effective range. All analytic formulas, both exact and\napproximate, were verified by comparing with the corresponding results obtained\nby direct numerical calculations. Wolfram Mathematica is heavily used. The\npresented results can be used with advantage in the fields of nuclear physics,\natomic and molecular physics, quantum chemistry and many others.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the search for life in the cosmos, NASA's Transiting Exoplanet Survey\nSatellite (TESS) mission has already monitored about 74% of the sky for\ntransiting extrasolar planets, including potentially habitable worlds. However,\nTESS only observed a fraction of the stars long enough to be able to find\nplanets like Earth. We use the primary mission data - the first two years of\nobservations - and identify 4,239 stars within 210pc that TESS observed long\nenough to see 3 transits of an exoplanet that receives similar irradiation to\nEarth: 738 of these stars are located within 30pc. We provide reliable stellar\nparameters from the TESS Input Catalog that incorporates Gaia DR2 and also\ncalculate the transit depth and radial velocity semi-amplitude for an\nEarth-analog planet. Of the 4,239 stars in the Revised TESS HZ Catalog, 9 are\nknown exoplanet hosts - GJ 1061, GJ 1132, GJ 3512, GJ 685, Kepler-42, LHS 1815,\nL98-59, RR Cae, TOI 700 - around which TESS could identify additional\nEarth-like planetary companions. 37 additional stars host yet unconfirmed TESS\nObjects of Interest: three of these orbit in the habitable zone - TOI 203, TOI\n715, and TOI 2298. For a subset of 614 of the 4,239 stars, TESS has observed\nthe star long enough to be able to observe planets throughout the full\ntemperate, habitable zone out to the equivalent of Mars' orbit. Thus, the\nRevised TESS Habitable Zone Catalog provides a tool for observers to prioritize\nstars for follow-up observation to discover life in the cosmos. These stars are\nthe best path towards the discovery of habitable planets using the TESS mission\ndata.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article a theoretical framework for problems involving fractional\nequations of hyperbolic type arising in the theory of viscoelasticity is\npresented. Based on the Galerkin method, a variational problem of the\nfractionary viscoelasticity is studied. An appropriate functional setting is\nintroduced in order to establish the existence, uniqueness and a priori\nestimates for weak solutions. This framework is developed in close concordance\nwith important physical quantities of the theory of viscoelasticity\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In 1978 Sakoda and Sipser raised the question of the cost, in terms of size\nof representations, of the transformation of two-way and one-way\nnondeterministic automata into equivalent two-way deterministic automata.\nDespite all the attempts, the question has been answered only for particular\ncases (e.g., restrictions of the class of simulated automata or of the class of\nsimulating automata). However the problem remains open in the general case, the\nbest-known upper bound being exponential. We present a new approach in which\nunrestricted nondeterministic finite automata are simulated by deterministic\nmodels extending two-way deterministic finite automata, paying a polynomial\nincrease of size only. Indeed, we study the costs of the conversions of\nnondeterministic finite automata into some variants of one-tape deterministic\nTuring machines working in linear time, namely Hennie machines, weight-reducing\nTuring machines, and weight-reducing Hennie machines. All these variants are\nknown to share the same computational power: they characterize the class of\nregular languages.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article, we study the quantum field theoretic generalization of the\nCaldeira-Leggett model in general curved space-time considering interactions\nbetween two scalar fields in a classical gravitational background. The\nthermalization phenomena is then studied from the obtained de Sitter solution\nusing quantum quench from one scalar field model obtained from path integrated\neffective action in Euclidean signature. We consider an instantaneous quench in\nthe time-dependent mass protocol of the field of our interest. We find that the\ndynamics of the field post-quench can be described in terms of the state of the\ngeneralized Calabrese-Cardy (gCC) form and computed the different types of\ntwo-point correlation functions in this context. We explicitly found the\nconserved charges of $W_{\\infty}$ algebra that represents the gCC state after a\nquench in de Sitter space and found it to be significantly different from the\nflat space-time results. We extend our study for the different two-point\ncorrelation functions not only considering the pre-quench state as the ground\nstate, but also a squeezed state. We found that irrespective of the pre-quench\nstate, the post quench state can be written in terms of the gCC state showing\nthat the subsystem of our interest thermalizes in de Sitter space. Furthermore,\nwe provide a general expression for the two-point correlators and explicitly\nshow the thermalization process by considering a thermal Generalized Gibbs\nensemble (GGE). Finally, from the equal time momentum dependent counterpart of\nthe obtained results for the two-point correlators, we have studied the hidden\nfeatures of the power spectra and studied its consequences for different\nchoices of the quantum initial conditions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The distcomp command is introduced and illustrated. The command assesses\nwhether or not two distributions differ at each possible value while\ncontrolling the probability of any false positive, even in finite samples.\nSyntax and the underlying methodology (from Goldman and Kaplan, 2018) are\ndiscussed. Multiple examples illustrate the distcomp command, including\nrevisiting the experimental data of Gneezy and List (2006) and the regression\ndiscontinuity design of Cattaneo, Frandsen, and Titiunik (2015).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present 48 types of solutions to the anomaly cancellation conditions of\nlocal Abelian extensions of the Standard Model (SM) with right-handed singlet\nchiral fermions. At least two of them acquire effective light Dirac neutrino\nmasses, while the others get heavy masses from the spontaneous symmetry\nbreaking of the local Abelian symmetry, forming a dark sector with\nmulti-component and multi-generational fermionic dark matter. The corresponding\neffective Dirac neutrino mass operator can be realized at tree-level or\nradiatively by introducing extra scalars, and in some cases after imposing\nextra scotogenic conditions. The Dirac Zee model with Dirac fermionic dark\nmatter is presented as an example of model where the neutrino and dark matter\nphenomenology are basically independent of each other.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a stream of graph edges from a dynamic graph, how can we assign anomaly\nscores to edges and subgraphs in an online manner, for the purpose of detecting\nunusual behavior, using constant time and memory? For example, in intrusion\ndetection, existing work seeks to detect either anomalous edges or anomalous\nsubgraphs, but not both. In this paper, we first extend the count-min sketch\ndata structure to a higher-order sketch. This higher-order sketch has the\nuseful property of preserving the dense subgraph structure (dense subgraphs in\nthe input turn into dense submatrices in the data structure). We then propose 4\nonline algorithms that utilize this enhanced data structure, which (a) detect\nboth edge and graph anomalies; (b) process each edge and graph in constant\nmemory and constant update time per newly arriving edge, and; (c) outperform\nstate-of-the-art baselines on 4 real-world datasets. Our method is the first\nstreaming approach that incorporates dense subgraph search to detect graph\nanomalies in constant memory and time.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Despite great success on many machine learning tasks, deep neural networks\nare still vulnerable to adversarial samples. While gradient-based adversarial\nattack methods are well-explored in the field of computer vision, it is\nimpractical to directly apply them in natural language processing due to the\ndiscrete nature of text. To bridge this gap, we propose a general framework to\nadapt existing gradient-based methods to craft textual adversarial samples. In\nthis framework, gradient-based continuous perturbations are added to the\nembedding layer and are amplified in the forward propagation process. Then the\nfinal perturbed latent representations are decoded with a mask language model\nhead to obtain potential adversarial samples. In this paper, we instantiate our\nframework with \\textbf{T}extual \\textbf{P}rojected \\textbf{G}radient\n\\textbf{D}escent (\\textbf{TPGD}). We conduct comprehensive experiments to\nevaluate our framework by performing transfer black-box attacks on BERT,\nRoBERTa and ALBERT on three benchmark datasets. Experimental results\ndemonstrate our method achieves an overall better performance and produces more\nfluent and grammatical adversarial samples compared to strong baseline methods.\nAll the code and data will be made public.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The quality of a superconductor-normal metal-superconductor Josephson\njunction (JJ) depends crucially on the transparency of the\nsuperconductor-normal metal (S/N) interface. We demonstrate a technique for\nfabricating planar JJs with perfect S/N interfaces. The technique utilizes a\nstrong inverse proximity effect discovered in Al/V$_5$S$_8$ bilayers, by which\nthe Al layer is driven into the resistive state. The highly transparent S/N\nhomointerface and the peculiar normal metal enable the flow of Josephson\nsupercurrent across a 2.9 $\\mu$m long weak link. Moreover, our JJ exhibits a\ngiant critical current and a large product of the critical current and the\nnormal state resistance.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Unsupervised Domain Adaptation (UDA) for re-identification (re-ID) is a\nchallenging task: to avoid a costly annotation of additional data, it aims at\ntransferring knowledge from a domain with annotated data to a domain of\ninterest with only unlabeled data. Pseudo-labeling approaches have proven to be\neffective for UDA re-ID. However, the effectiveness of these approaches heavily\ndepends on the choice of some hyperparameters (HP) that affect the generation\nof pseudo-labels by clustering. The lack of annotation in the domain of\ninterest makes this choice non-trivial. Current approaches simply reuse the\nsame empirical value for all adaptation tasks and regardless of the target data\nrepresentation that changes through pseudo-labeling training phases. As this\nsimplistic choice may limit their performance, we aim at addressing this issue.\nWe propose new theoretical grounds on HP selection for clustering UDA re-ID as\nwell as method of automatic and cyclic HP tuning for pseudo-labeling UDA\nclustering: HyPASS. HyPASS consists in incorporating two modules in\npseudo-labeling methods: (i) HP selection based on a labeled source validation\nset and (ii) conditional domain alignment of feature discriminativeness to\nimprove HP selection based on source samples. Experiments on commonly used\nperson re-ID and vehicle re-ID datasets show that our proposed HyPASS\nconsistently improves the best state-of-the-art methods in re-ID compared to\nthe commonly used empirical HP setting.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Texture in dense or porous ceramics can enhance their functional and\nstructural properties. Current methods for texturation employ anisotropic\nparticles as starting powders and processes that drive their orientation into\nspecific directions. Using ultra-low magnetic fields combined with slip\ncasting, it is possible to purposely orient magnetically responsive particles\nin any direction. When those particles are suspended with nanoparticles,\ntemplated grain growth occurs during sintering to yield a textured ceramic.\nYet, the final grains are usually micrometric, leading weak mechanical\nproperties. Here, we explore how to tune the grain orientation size using\nmagnetic slip casting and templated grain growth. Our strategy consists in\nchanging the size of the anisotropic powders and ensuring that magnetic\nalignment and densification occurs. The obtained ceramics featured a large\nrange of grain anisotropy, with submicrometric thickness and anisotropic\nproperties. Textured ceramics with tunable grains dimensions and porosity are\npromising for filtering, biomedical or composite applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We describe topologically ordered and fracton ordered states on novel\ngeometries which do not have an underlying manifold structure. Using tree\ngraphs such as the $k$-coordinated Bethe lattice ${\\cal B}(k)$ and a hypertree\ncalled the $(k,n)$-hyper-Bethe lattice ${\\cal HB}(k,n)$ consisting of\n$k$-coordinated hyperlinks (defined by $n$ sites), we construct\nmultidimensional arboreal arenas such as ${\\cal B}(k_1) \\square {\\cal B}(k_2)$\nby the notion of a graph Cartesian product $\\square$. We study various quantum\nsystems such as the ${\\mathbb Z}_2$ gauge theory, generalized quantum Ising\nmodels (GQIM), the fractonic X-cube model, and related X-cube gauge theory\ndefined on these arenas. Even the simplest ${\\mathbb Z}_2$ gauge theory on a 2d\narboreal arena is fractonic -- the monopole excitation is immobile. The X-cube\nmodel on a 3d arboreal arena is fully fractonic, all multipoles are rendered\nimmobile. We obtain variational ground state phase diagrams of these gauge\ntheories. Further, we find an intriguing class of dualities in arboreal arenas\nas illustrated by the ${\\mathbb Z}_2$ gauge theory defined on ${\\cal B}(k_1)\n\\square {\\cal B}(k_2)$ being dual to a GQIM defined on ${\\cal HB}(2,k_1)\n\\square {\\cal HB}(2,k_2)$. Finally, we discuss different classes of topological\nand fracton orders on arboreal arenas. We find three distinct classes of\narboreal toric code orders on 2d arboreal arenas, those that occur on ${\\cal\nB}(2) \\square {\\cal B}(2)$, ${\\cal B}(k) \\square {\\cal B}(2), k >2$, and ${\\cal\nB}(k_1) \\square {\\cal B}(k_2)$, $k_1,k_2>2$. Likewise, four classes of X-cube\nfracton orders are found in 3d arboreal arenas -- those on ${\\cal\nB}(2)\\square{\\cal B}(2)\\square {\\cal B}(2)$, ${\\cal B}(k) \\square {\\cal\nB}(2)\\square {\\cal B}(2), k>2$, ${\\cal B}(k_1) \\square {\\cal B}(k_2) \\square\n{\\cal B}(2), k_1,k_2 >2$, and ${\\cal B}(k_1) \\square {\\cal B}(k_2) \\square\n{\\cal B}(k_3), k_1,k_2,k_3 >2$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Nano-graphene /polymer composites can functionas pressure induced\nelectro-switches, at concentrations around their conductivity percolation\nthreshold. Close to the critical point, the pressure dependence of the electron\ntunneling through the polymer barrier separating nanon-graphenes results from\nthecompetition among shorteningof the tunneling length and the increase of the\npolymer's polarizability. Such switching behaviorwas recentlyobserved\ninpolyvinyl alcohol (PVA) loaded withnano-graphene platelets (NGPs). In this\nwork, PVA is blended withh alpha-poly(vinylidene fluoride) (PVdF)and NGPs.\nCoaxial mechanical stress and electric field render the nano-composite\npiezoelectric. We investigate the influence of heterogeneity, thermal\nproperties, phase transitions and kinetic processes occurring in the polymer\nmatrix on the macroscopicelectrical conductivity and interfacial polarization\nin casted specimens. Furthermore, the effect of electro-activity of PVdF grains\non the electric and thermal properties are comparatively studied. Broadband\nDielectricspectroscopy is employed to resolve and inspect electron transport\nand trapping with respect to thermal transitions and kineticprocessestraced via\nDifferential Scanning Calorimetry. The harmonic electric field applied during a\nBDS sweep induces volume modifications of the electro-active PVdF grains,\nwhile, electro-activity of PVdF grains can disturb the internal electric field\nthat free (or bound) electric. The dc conductivity and dielectric relaxation\nwas found to exhibit weakdependencies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A robot working in human-centric environments needs to know which kind of\nobjects exist in the scene, where they are, and how to grasp and manipulate\nvarious objects in different situations to help humans in everyday tasks.\nTherefore, object recognition and grasping are two key functionalities for such\nrobots. Most state-of-the-art tackles object recognition and grasping as two\nseparate problems while both use visual input. Furthermore, the knowledge of\nthe robot is fixed after the training phase. In such cases, if the robot faces\nnew object categories, it must retrain from scratch to incorporate new\ninformation without catastrophic interference. To address this problem, we\npropose a deep learning architecture with augmented memory capacities to handle\nopen-ended object recognition and grasping simultaneously. In particular, our\napproach takes multi-views of an object as input and jointly estimates\npixel-wise grasp configuration as well as a deep scale- and rotation-invariant\nrepresentation as outputs. The obtained representation is then used for\nopen-ended object recognition through a meta-active learning technique. We\ndemonstrate the ability of our approach to grasp never-seen-before objects and\nto rapidly learn new object categories using very few examples on-site in both\nsimulation and real-world settings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Embedded systems acquire information about the real world from sensors and\nprocess it to make decisions and/or for transmission. In some situations, the\nrelationship between the data and the decision is complex and/or the amount of\ndata to transmit is large (e.g. in biologgers). Artificial Neural Networks\n(ANNs) can efficiently detect patterns in the input data which makes them\nsuitable for decision making or compression of information for data\ntransmission. However, ANNs require a substantial amount of energy which\nreduces the lifetime of battery-powered devices. Therefore, the use of Spiking\nNeural Networks can improve such systems by providing a way to efficiently\nprocess sensory data without being too energy-consuming. In this work, we\nintroduce a low-powered neuron model called Integrate-and-Fire which exploits\nthe charge and discharge properties of the capacitor. Using parallel and series\nRC circuits, we developed a trainable neuron model that can be expressed in a\nrecurrent form. Finally, we trained its simulation with an artificially\ngenerated dataset of dog postures and implemented it as hardware that showed\npromising energetic properties. This paper is the full text of the research,\npresented at the 20th International Conference on Artificial Intelligence and\nSoft Computing Web System (ICAISC 2021)\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce the mixed-norm amalgam spaces\n$(L^{\\vec{p}},L^{\\vec{s}})(\\mathbb{R}^n)$ and\n$(L^{\\vec{p}},L^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$, and show their some basic\nproperties. In addition, we find the predual\n$\\mathcal{H}(\\vec{p}',\\vec{s}\\,',\\alpha')$ of mixed-norm amalgam spaces\n$(L^{\\vec{p}},\\ell^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$ by the dual spaces\n$(L^{\\vec{p}'},\\ell^{\\vec{s}\\,'})(\\mathbb{R}^n)$ of\n$(L^{\\vec{p}},\\ell^{\\vec{s}})(\\mathbb{R}^n)$, where\n$(L^{\\vec{p}},L^{\\vec{s}})(\\mathbb{R}^n)=(L^{\\vec{p}},\\ell^{\\vec{s}})(\\mathbb{R}^n)$\nand\n$(L^{\\vec{p}},L^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)=(L^{\\vec{p}},\\ell^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$.\nThen, we study the strong-type estimates for fractional integral operators\n$I_{\\gamma}$ on mixed-norm amalgam spaces\n$(L^{\\vec{p}},L^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$. And, the strong-type\nestimates of linear commutators $[b,I_{\\gamma}]$ generated by $b\\in\nBMO(\\mathbb{R}^n)$ and $I_{\\gamma}$ on mixed-norm amalgam spaces\n$(L^{\\vec{p}},L^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$ are established as well.\nFurthermore, based on the dual theorem, the characterization of\n$BMO(\\mathbb{R}^n)$ by the boundedness of $[b,I_\\gamma]$ from\n$(L^{\\vec{p}},L^{\\vec{s}})^{\\alpha}(\\mathbb{R}^n)$ to\n$(L^{\\vec{q}},L^{\\vec{s}})^{\\beta}(\\mathbb{R}^n)$ is given, which is a new\nresult even for the classical amalgam spaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum state tomography is the conventional method used to characterize\ndensity matrices for general quantum states. However, the data acquisition time\ngenerally scales linearly with the dimension of the Hilbert space, hindering\nthe possibility of dynamic monitoring of a high-dimensional quantum system.\nHere, we demonstrate a direct tomography protocol to measure density matrices\nof photons in the position basis through the use of a polarization-resolving\ncamera, where the dimension of density matrices can be as large as\n580$\\times$580 in our experiment. The use of the polarization-resolving camera\nenables parallel measurements in the position and polarization basis and as a\nresult, the data acquisition time of our protocol does not increase with the\ndimension of the Hilbert space and is solely determined by the camera exposure\ntime (on the order of 10 ms). Our method is potentially useful for the\nreal-time monitoring of the dynamics of quantum states and paves the way for\nthe development of high-dimensional, time-efficient quantum metrology\ntechniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Thin fiber networks are widely represented in nature and can be found in\nman-made materials such as paper and packaging. The strength of such materials\nis an intricate subject due to inherited randomness and size-dependencies.\nDirect fiber-level numerical simulations can provide insights into the role of\nthe constitutive components of such networks, their morphology, and\narrangements on the strength of the products made of them. However, direct\nmechanical simulation of randomly generated large and thin fiber networks is\ncharacterized by overwhelming computational costs. Herein, a stochastic\nconstitutive model for predicting the random mechanical response of isotropic\nthin fiber networks of arbitrary size is presented. The model is based on\nstochastic volume elements (SVEs) with SVE size-specific deterministic and\nstochastic constitutive law parameters. The randomness in the network is\ndescribed by the spatial fields of the uniaxial strain and strength to failure,\nformulated using multivariate kernel functions and approximate univariate\nprobability density functions. The proposed stochastic continuum approach shows\ngood agreement when compared to direct numerical simulation with respect to\nmechanical response. Furthermore, strain localization patterns matched the one\nobserved in direct simulations, which suggests an accurate prediction of the\nfailure location. This work demonstrates that the proposed stochastic\nconstitutive model can be used to predict the response of random isotropic\nfiber networks of arbitrary size.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent advances in genomic sequencing technology have resulted in an\nabundance of genome sequence data. Despite the progress in interpreting those\ndata, there remains a broad scope for their translation into clinical and\nsocietal benefits. Loss-of-function variations in the human genome can be\ncausal in disease development. Precise identification of such variations and\npathogenicity prediction may lead to better drug targeting, among other\nbenefits. Machine learning comes across as a promising method for its proven\npredictive ability. We have curated a novel dataset for the classification of\nLOF variants using high-quality databases of genetic variation. We trained and\nvalidated seven different classification algorithms using the new dataset to\nclassify the variants as Benign, Pathogenic and Likely pathogenic. We recorded\nthe best overall performance using the XG-Boost algorithm with an F1-score of\n0.88 on the test set. We observed fair performance on Pathogenic samples with\nhigh recall and moderate precision and subpar performance on Likely pathogenic\nclass, albeit with moderate precision. Overall, the encouraging results make\nour final model a promising candidate for further real-world tests.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a simple method for automatic speech recognition (ASR) by\nfine-tuning BERT, which is a language model (LM) trained on large-scale\nunlabeled text data and can generate rich contextual representations. Our\nassumption is that given a history context sequence, a powerful LM can narrow\nthe range of possible choices and the speech signal can be used as a simple\nclue. Hence, comparing to conventional ASR systems that train a powerful\nacoustic model (AM) from scratch, we believe that speech recognition is\npossible by simply fine-tuning a BERT model. As an initial study, we\ndemonstrate the effectiveness of the proposed idea on the AISHELL dataset and\nshow that stacking a very simple AM on top of BERT can yield reasonable\nperformance.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The O(2) model in Euclidean space-time is the zero-gauge-coupling limit of\nthe compact scalar quantum electrodynamics. We obtain a dual representation of\nit called the charge representation. We study the quantum phase transition in\nthe charge representation with a truncation to ``spin $S$,\" where the quantum\nnumbers have an absolute value less than or equal to $S$. The charge\nrepresentation preserves the gapless-to-gapped phase transition even for the\nsmallest spin truncation $S = 1$. The phase transition for $S = 1$ is an\ninfinite-order Gaussian transition with the same critical exponents $\\delta$\nand $\\eta$ as the Berezinskii-Kosterlitz-Thouless (BKT) transition, while there\nare true BKT transitions for $S \\ge 2$. The essential singularity in the\ncorrelation length for $S = 1$ is different from that for $S \\ge 2$. The\nexponential convergence of the phase-transition point is studied in both\nLagrangian and Hamiltonian formulations. We discuss the effects of replacing\nthe truncated $\\hat{U}^{\\pm} = \\exp(\\pm i \\hat{\\theta})$ operators by the spin\nladder operators $\\hat{S}^{\\pm}$ in the Hamiltonian. The marginal operators\nvanish at the Gaussian transition point for $S = 1$, which allows us to extract\nthe $\\eta$ exponent with high accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the problem of online learning in Linear Quadratic Control\nsystems whose state transition and state-action transition matrices $A$ and $B$\nmay be initially unknown. We devise an online learning algorithm and provide\nguarantees on its expected regret. This regret at time $T$ is upper bounded (i)\nby $\\widetilde{O}((d_u+d_x)\\sqrt{d_xT})$ when $A$ and $B$ are unknown, (ii) by\n$\\widetilde{O}(d_x^2\\log(T))$ if only $A$ is unknown, and (iii) by\n$\\widetilde{O}(d_x(d_u+d_x)\\log(T))$ if only $B$ is unknown and under some mild\nnon-degeneracy condition ($d_x$ and $d_u$ denote the dimensions of the state\nand of the control input, respectively). These regret scalings are minimal in\n$T$, $d_x$ and $d_u$ as they match existing lower bounds in scenario (i) when\n$d_x\\le d_u$ [SF20], and in scenario (ii) [lai1986]. We conjecture that our\nupper bounds are also optimal in scenario (iii) (there is no known lower bound\nin this setting).\n  Existing online algorithms proceed in epochs of (typically exponentially)\ngrowing durations. The control policy is fixed within each epoch, which\nconsiderably simplifies the analysis of the estimation error on $A$ and $B$ and\nhence of the regret. Our algorithm departs from this design choice: it is a\nsimple variant of certainty-equivalence regulators, where the estimates of $A$\nand $B$ and the resulting control policy can be updated as frequently as we\nwish, possibly at every step. Quantifying the impact of such a\nconstantly-varying control policy on the performance of these estimates and on\nthe regret constitutes one of the technical challenges tackled in this paper.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We formulate a conjecture from graph theory that is equivalent to\nNash-solvability of the finite two-person shortest path games with positive\nlocal costs. For the three-person games such conjecture fails.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent demand for distributed software had led to a surge in popularity in\nactor-based frameworks. However, even with the stylized message passing model\nof actors, writing correct distributed software is still difficult. We present\nour work on linearizability checking in DS2, an integrated framework for\nspecifying, synthesizing, and testing distributed actor systems. The key\ninsight of our approach is that often subcomponents of distributed actor\nsystems represent common algorithms or data structures (e.g.\\ a distributed\nhash table or tree) that can be validated against a simple sequential model of\nthe system. This makes it easy for developers to validate their concurrent\nactor systems without complex specifications. DS2 automatically explores the\nconcurrent schedules that system could arrive at, and it compares observed\noutput of the system to ensure it is equivalent to what the sequential\nimplementation could have produced. We describe DS2's linearizability checking\nand test it on several concurrent replication algorithms from the literature.\nWe explore in detail how different algorithms for enumerating the model\nschedule space fare in finding bugs in actor systems, and we present our own\nrefinements on algorithms for exploring actor system schedules that we show are\neffective in finding bugs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Counters are the fundamental building block of many data sketching schemes,\nwhich hash items to a small number of counters and account for collisions to\nprovide good approximations for frequencies and other measures. Most existing\nmethods rely on fixed-size counters, which may be wasteful in terms of space,\nas counters must be large enough to eliminate any risk of overflow. Instead,\nsome solutions use small, fixed-size counters that may overflow into secondary\nstructures.\n  This paper takes a different approach. We propose a simple and general method\ncalled SALSA for dynamic re-sizing of counters and show its effectiveness.\nSALSA starts with small counters, and overflowing counters simply merge with\ntheir neighbors. SALSA can thereby allow more counters for a given space,\nexpanding them as necessary to represent large numbers. Our evaluation\ndemonstrates that, at the cost of a small overhead for its merging logic, SALSA\nsignificantly improves the accuracy of popular schemes (such as Count-Min\nSketch and Count Sketch) over a variety of tasks. Our code is released as\nopen-source [1].\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Chatbots are designed to carry out human-like conversations across different\ndomains, such as general chit-chat, knowledge exchange, and persona-grounded\nconversations. To measure the quality of such conversational agents, a dialogue\nevaluator is expected to conduct assessment across domains as well. However,\nmost of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are\nnot designed for multi-domain evaluation. We are motivated to design a general\nand robust framework, MDD-Eval, to address the problem. Specifically, we first\ntrain a teacher evaluator with human-annotated data to acquire a rating skill\nto tell good dialogue responses from bad ones in a particular domain and then,\nadopt a self-training strategy to train a new evaluator with teacher-annotated\nmulti-domain data, that helps the new evaluator to generalize across multiple\ndomains. MDD-Eval is extensively assessed on six dialogue evaluation\nbenchmarks. Empirical results show that the MDD-Eval framework achieves a\nstrong performance with an absolute improvement of 7% over the state-of-the-art\nADMs in terms of mean Spearman correlation scores across all the evaluation\nbenchmarks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a method that efficiently learns distributions over articulation\nmodel parameters directly from depth images without the need to know\narticulation model categories a priori. By contrast, existing methods that\nlearn articulation models from raw observations typically only predict point\nestimates of the model parameters, which are insufficient to guarantee the safe\nmanipulation of articulated objects. Our core contributions include a novel\nrepresentation for distributions over rigid body transformations and\narticulation model parameters based on screw theory, von Mises-Fisher\ndistributions, and Stiefel manifolds. Combining these concepts allows for an\nefficient, mathematically sound representation that implicitly satisfies the\nconstraints that rigid body transformations and articulations must adhere to.\nLeveraging this representation, we introduce a novel deep learning based\napproach, DUST-net, that performs category-independent articulation model\nestimation while also providing model uncertainties. We evaluate our approach\non several benchmarking datasets and real-world objects and compare its\nperformance with two current state-of-the-art methods. Our results demonstrate\nthat DUST-net can successfully learn distributions over articulation models for\nnovel objects across articulation model categories, which generate point\nestimates with better accuracy than state-of-the-art methods and effectively\ncapture the uncertainty over predicted model parameters due to noisy inputs.\nProject webpage: https://pearl-utexas.github.io/DUST-net/\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  When implementing a non-continuous controller for a cyber-physical system, it\nmay happen that the evolution of the closed-loop system is not anymore\npiecewise differentiable along the trajectory, mainly due to conditional\nstatements inside the controller. This may lead to some unwanted chattering\neffects than may damage the system. This behavior is difficult to observe even\nin simulation. In this paper, we propose an interval approach to characterize\nthe sliding surface which corresponds to the set of all states such that the\nstate trajectory may jump indefinitely between two distinct behaviors. We show\nthat the recent notion of thick sets will allows us to compute efficiently an\nouter approximation of the sliding surface of a given class of hybrid system\ntaking into account all set-membership uncertainties. An application to the\nverification of the controller of a child swing is considered to illustrate the\nprinciple of the approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  With the onset of COVID-19 pandemic, social media has rapidly become a\ncrucial communication tool for information generation, dissemination, and\nconsumption. In this scoping review, we selected and examined peer-reviewed\nempirical studies relating to COVID-19 and social media during the first\noutbreak starting in November 2019 until May 2020. From an analysis of 81\nstudies, we identified five overarching public health themes concerning the\nrole of online social platforms and COVID-19. These themes focused on: (i)\nsurveying public attitudes, (ii) identifying infodemics, (iii) assessing mental\nhealth, (iv) detecting or predicting COVID-19 cases, (v) analyzing government\nresponses to the pandemic, and (vi) evaluating quality of health information in\nprevention education videos. Furthermore, our review highlights the paucity of\nstudies on the application of machine learning on social media data related to\nCOVID-19 and a lack of studies documenting real-time surveillance developed\nwith social media data on COVID-19. For COVID-19, social media can play a\ncrucial role in disseminating health information as well as tackling infodemics\nand misinformation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we present a new method for 3D face reconstruction from\nsparse-view RGB images. Unlike previous methods which are built upon 3D\nmorphable models (3DMMs) with limited details, we leverage an implicit\nrepresentation to encode rich geometric features. Our overall pipeline consists\nof two major components, including a geometry network, which learns a\ndeformable neural signed distance function (SDF) as the 3D face representation,\nand a rendering network, which learns to render on-surface points of the neural\nSDF to match the input images via self-supervised optimization. To handle\nin-the-wild sparse-view input of the same target with different expressions at\ntest time, we propose residual latent code to effectively expand the shape\nspace of the learned implicit face representation as well as a novel\nview-switch loss to enforce consistency among different views. Our experimental\nresults on several benchmark datasets demonstrate that our approach outperforms\nalternative baselines and achieves superior face reconstruction results\ncompared to state-of-the-art methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Learning from imbalanced data is among the most challenging areas in\ncontemporary machine learning. This becomes even more difficult when considered\nthe context of big data that calls for dedicated architectures capable of\nhigh-performance processing. Apache Spark is a highly efficient and popular\narchitecture, but it poses specific challenges for algorithms to be implemented\nfor it. While oversampling algorithms are an effective way for handling class\nimbalance, they have not been designed for distributed environments. In this\npaper, we propose a holistic look on oversampling algorithms for imbalanced big\ndata. We discuss the taxonomy of oversampling algorithms and their mechanisms\nused to handle skewed class distributions. We introduce a Spark library with 14\nstate-of-the-art oversampling algorithms implemented and evaluate their\nefficacy via extensive experimental study. Using binary and multi-class massive\ndata sets, we analyze the effectiveness of oversampling algorithms and their\nrelationships with different types of classifiers. We evaluate the trade-off\nbetween accuracy and time complexity of oversampling algorithms, as well as\ntheir scalability when increasing the size of data. This allows us to gain\ninsight into the usefulness of specific components of oversampling algorithms\nfor big data, as well as formulate guidelines and recommendations for designing\nfuture resampling approaches for massive imbalanced data. Our library can be\ndownloaded from https://github.com/fsleeman/spark-class-balancing.git.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Spectral decomposition of matrices is a recurring and important task in\napplied mathematics, physics and engineering. Many application problems require\nthe consideration of matrices of size three with spectral decomposition over\nthe real numbers. If the functional dependence of the spectral decomposition on\nthe matrix elements has to be preserved, then closed-form solution approaches\nmust be considered. Existing closed-form expressions are based on the use of\nprincipal matrix invariants which suffer from a number of deficiencies when\nevaluated in the framework of finite precision arithmetic. This paper\nintroduces an alternative form for the computation of the involved matrix\ninvariants (in particular the discriminant) in terms of sum-of-products\nexpressions as function of the matrix elements. We prove and demonstrate by\nnumerical examples that this alternative approach leads to increased floating\npoint accuracy, especially in all important limit cases (e.g. eigenvalue\nmultiplicity). It is believed that the combination of symbolic algorithms with\nthe accuracy improvements presented in this paper can serve as a powerful\nbuilding block for many engineering tasks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  On 14th September 2020, the Royal Astronomical Society made an official\nstatement coupled with a webminar on the discovery of phosphine on Venus.\nSingle-line millimetre-waveband spectral detections of phosphine (with a\nsignal-to-noise ratio of $\\approx$ 15$\\sigma$) from the JCMT and ALMA\ntelescopes indicated a phosphine abundance of 20 ppb (parts per billion), 1000\ntimes more than that on the Earth. Phosphine is an important biomarker and\nimmediate speculation in the media about indicators of life being found on\nVenus followed. This article presents an analysis of the study and the results\non the observation of the spectral absorption feature of phosphine in the\nclouds of Venus, thus implying as a potential biosignature. If phosphine is\nproduced through biotic, as opposed to abiotic pathways, the discovery could\nimply a significant biomass in the Venusian atmosphere. The discovery led to a\nmajor controversy with criticism of the analysis and results and responses to\nit. The issue remains unresolved, leading to a fresh interest in the study of\nVenus including ground-based observations as well as space-probes that can\nanswer these questions conclusively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the free data in the Fefferman-Graham expansion of asymptotically\nEinstein metrics with non-zero cosmological constant. We prove that if\n$\\mathscr{I}$ is conformally flat, the rescaled Weyl tensor at $\\mathscr{I}$\nagrees up to a constant with the free data at $\\mathscr{I}$ , namely the\ntraceless part of the $n$-th order coefficient of the expansion. In the\nnon-conformally flat case, the rescaled Weyl tensor is generically divergent at\n$\\mathscr{I}$ but one can still extract the free data in terms of the\ndifference of the Weyl tensors of suitably constructed metrics, in full\ngenerality when the spacetime dimension $D$ is even and provided the so-called\nobstruction tensor at $\\mathscr{I}$ is identically zero when $D$ is odd. These\nresults provide a geometric definition of the data, particularly relevant for\nthe asymptotic Cauchy problem of even dimensional Einstein metrics with\npositive $\\Lambda$ and also for the odd dimensional analytic case\nirrespectively of the sign of $\\Lambda$. We establish a Killing initial data\nequation at spacelike $\\mathscr{I}$ in all dimension for analytic data. These\nresults are used to find a geometric characterization of the Kerr-de Sitter\nmetrics in all dimensions in terms of its geometric data at null infinity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  GW200115 was the second merger of a black hole and a neutron star confidently\ndetected through gravitational waves. Inference on the signal allows for a\nlarge black hole spin misaligned with the orbital angular momentum, but shows\nlittle support for aligned spin values. We show that this is a natural\nconsequence of measuring the parameters of a black hole -- neutron star binary\nwith non-spinning components while assuming the priors used in the\nLIGO-Virgo-KAGRA analysis. We suggest that, a priori, a non-spinning binary is\nmore consistent with current astrophysical understanding.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Numerous malware families rely on domain generation algorithms (DGAs) to\nestablish a connection to their command and control (C2) server. Counteracting\nDGAs, several machine learning classifiers have been proposed enabling the\nidentification of the DGA that generated a specific domain name and thus\ntriggering targeted remediation measures. However, the proposed\nstate-of-the-art classifiers are based on deep learning models. The black box\nnature of these makes it difficult to evaluate their reasoning. The resulting\nlack of confidence makes the utilization of such models impracticable. In this\npaper, we propose EXPLAIN, a feature-based and contextless DGA multiclass\nclassifier. We comparatively evaluate several combinations of feature sets and\nhyperparameters for our approach against several state-of-the-art classifiers\nin a unified setting on the same real-world data. Our classifier achieves\ncompetitive results, is real-time capable, and its predictions are easier to\ntrace back to features than the predictions made by the DGA multiclass\nclassifiers proposed in related work.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a new class of robust and Fisher-consistent estimators for mixture\nmodels. These estimators can be used to construct robust model-based clustering\nprocedures. We study in detail the case of multivariate normal mixtures and\npropose a procedure that uses S estimators of multivariate location and\nscatter. We develop an algorithm to compute the estimators and to build the\nclusters which is quite similar to the EM algorithm. An extensive Monte Carlo\nsimulation study shows that our proposal compares favorably with other robust\nand non robust model-based clustering procedures. We apply ours and alternative\nprocedures to a real data set and again find that the best results are obtained\nusing our proposal.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Univariate Weibull distribution is a well-known lifetime distribution and has\nbeen widely used in reliability and survival analysis. In this paper, we\nintroduce a new family of bivariate generalized Weibull (BGW) distributions,\nwhose univariate marginals are exponentiated Weibull distribution. Different\nstatistical quantiles like marginals, conditional distribution, conditional\nexpectation, product moments, correlation and a measure component reliability\nare derived. Various measures of dependence and statistical properties along\nwith ageing properties are examined. Further, the copula associated with BGW\ndistribution and its various important properties are also considered. The\nmethods of maximum likelihood and Bayesian estimation are employed to estimate\nunknown parameters of the model. A Monte Carlo simulation and real data study\nare carried out to demonstrate the performance of the estimators and results\nhave proven the effectiveness of the distribution in real-life situations\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  J-PAS will soon start imaging 8000 deg2 of the northern sky with its unique\nset of 56 filters (R $\\sim$ 60). Before, we observed 1 deg2 on the AEGIS field\nwith an interim camera with all the J-PAS filters. With this data (miniJPAS),\nwe aim at proving the scientific potential of J-PAS to identify and\ncharacterize the galaxy populations with the goal of performing galaxy\nevolution studies across cosmic time. Several SED-fitting codes are used to\nconstrain the stellar population properties of a complete flux-limited sample\n(rSDSS <= 22.5 AB) of miniJPAS galaxies that extends up to z = 1. We find\nconsistent results on the galaxy properties derived from the different codes,\nindependently of the galaxy spectral-type or redshift. For galaxies with\nSNR>=10, we estimate that the J-PAS photometric system allows to derive stellar\npopulation properties with a precision that is equivalent to that obtained with\nspectroscopic surveys of similar SNR. By using the dust-corrected (u-r)\ncolour-mass diagram, a powerful proxy to characterize galaxy populations, we\nfind that the fraction of red and blue galaxies evolves with cosmic time, with\nred galaxies being $\\sim$ 38% and $\\sim$ 18% of the whole population at z = 0.1\nand z = 0.5, respectively. At all redshifts, the more massive galaxies belong\nto the red sequence and these galaxies are typically older and more metal rich\nthan their counterparts in the blue cloud. Our results confirm that with J-PAS\ndata we will be able to analyze large samples of galaxies up to z $\\sim$ 1,\nwith galaxy stellar masses above of log(M$_*$/M$_{\\odot}$) $\\sim$ 8.9, 9.5, and\n9.9 at z = 0.3, 0.5, and 0.7, respectively. The SFH of a complete sub-sample of\ngalaxies selected at z $\\sim$ 0.1 with log(M$_*$/M$_{\\odot}$) > 8.3 constrain\nthe cosmic evolution of the star formation rate density up to z $\\sim$ 3 in\ngood agreement with results from cosmological surveys.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the manuscript, we are interested in using kinetic theory to better\nunderstand the time evolution of wealth distribution and their large scale\nbehavior such as the evolution of inequality (e.g. Gini index). We investigate\nthree type of dynamics denoted unbiased, poor-biased and rich-biased dynamics.\nAt the particle level, one agent is picked randomly based on its wealth and one\nof its dollar is redistributed among the population. Proving the so-called\npropagation of chaos, we identify the limit of each dynamics as the number of\nindividual approaches infinity using both coupling techniques [48] and\nmartingale-based approach [36]. Equipped with the limit equation, we identify\nand prove the convergence to specific equilibrium for both the unbiased and\npoor-biased dynamics. In the rich-biased dynamics however, we observe a more\ncomplex behavior where a dispersive wave emerges. Although the dispersive wave\nis vanishing in time, its also accumulates all the wealth leading to a Gini\napproaching 1 (its maximum value). We characterize numerically the behavior of\ndispersive wave but further analytic investigation is needed to derive such\ndispersive wave directly from the dynamics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the holographic bound utilizing a homogeneous, isotropic, and\nnon-relativistic neutral hydrogen gas present in the de Sitter space.\nConcretely, we propose to employ de Sitter holography intertwined with quantum\ndeformation of the hydrogen atom using the framework of quantum groups.\nParticularly, the $\\mathcal U_q(so(4))$ quantum algebra is used to construct a\nfinite-dimensional Hilbert space of the hydrogen atom. As a consequence of the\nquantum deformation of the hydrogen atom, we demonstrate that the Rydberg\nconstant is dependent on the de Sitter radius, $L_\\Lambda$. This feature is\nthen extended to obtain a finite-dimensional Hilbert space for the full set of\nall hydrogen atoms in the de Sitter universe. We then show that the dimension\nof the latter Hilbert space satisfies the holographic bound. We further show\nthat the mass of a hydrogen atom $m_\\text{atom}$, the total number of hydrogen\natoms at the universe, $N$, and the retrieved dimension of the Hilbert space of\nneutral hydrogen gas, $\\text{Dim}{\\mathcal H}_\\text{bulk}$, are related to the\nde Sitter entropy, $S_\\text{dS}$, the Planck mass, $m_\\text{Planck}$, the\nelectron mass, $m_\\text{e}$, and the proton mass $m_\\text{p}$, by\n$m_\\text{atom}\\simeq m_\\text{Planck}S_\\text{dS}^{-\\frac{1}{6}}$, $N\\simeq\nS_\\text{dS}^\\frac{2}{3}$ and $\\text{Dim}{\\mathcal\nH}_\\text{bulk}=2^{\\frac{m_\\text{e}}{m_\\text{p}}\\alpha^2S_\\text{dS}}$,\nrespectively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Edge computing has emerged as a prospective paradigm to meet ever-increasing\ncomputation demands in Mobile Target Tracking Wireless Sensor Networks\n(MTT-WSN). This paradigm can offload time-sensitive tasks to sink nodes to\nimprove computing efficiency. Nevertheless, it is difficult to execute dynamic\nand critical tasks in the MTT-WSN network. Besides, the network cannot ensure\nconsecutive tracking due to the limited energy. To address the problems, this\npaper proposes a new hierarchical target tracking structure based on Edge\nIntelligence (EI) technology. The structure integrates the computing resource\nof both mobile nodes and edge servers to provide efficient computation\ncapability for real-time target tracking. Based on the proposed structure, we\nformulate an energy optimization model with the constrains of system execution\nlatency and trajectory prediction accuracy. Moreover, we propose a long-term\ndynamic resource allocation algorithm to obtain the optimal resource allocation\nsolution for the ac- curate and consecutive tracking. Simulation results\ndemonstrate that our algorithm outperforms the deep Q-learning over 14.5% in\nterms of system energy consumption. It can also obtain a significant\nenhancement in tracking accuracy compared with the non-cooperative scheme.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Entanglement entropy (EE) in critical quantum spin chains described by 1+1D\nconformal field theories contains signatures of the universal characteristics\nof the field theory. Boundaries and defects in the spin chain give rise to\nuniversal contributions in the EE. In this work, we analyze these universal\ncontributions for the critical Ising and XXZ spin chains for different\nconformal boundary conditions and defects. For the spin chains with boundaries,\nwe use the boundary states for the corresponding continuum theories to compute\nthe subleading contribution to the EE analytically and provide supporting\nnumerical computation for the spin chains. Subsequently, we analyze the\nbehavior of EE in the presence of conformal defects for the two spin chains and\ndescribe the change in both the leading logarithmic and subleading terms in the\nEE.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article, we study a fractional control problem that models the\nmaximization of the profits obtained by exploiting a certain resource whose\ndynamics are governed by the fractional logistic equation. Due to the\nsingularity of this problem, we develop different resolution techniques, both\nfor the classical case and for the fractional case. In the last section we\nperform several numerical simulations to make a comparison between both cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper studies a distributionally robust portfolio optimization model\nwith a cardinality constraint for limiting the number of invested assets. We\nformulate this model as a mixed-integer semidefinite optimization (MISDO)\nproblem by means of the moment-based uncertainty set of probability\ndistributions of asset returns. To exactly solve large-scale problems, we\npropose a specialized cutting-plane algorithm that is based on bilevel\noptimization reformulation. We prove the finite convergence of the algorithm.\nWe also apply a matrix completion technique to lower-level SDO problems to make\ntheir problem sizes much smaller. Numerical experiments demonstrate that our\ncutting-plane algorithm is significantly faster than the state-of-the-art MISDO\nsolver SCIP-SDP. We also show that our portfolio optimization model can achieve\ngood investment performance compared with the conventional mean-variance model.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The 4D 4-point scattering amplitude of massless scalars via a massive\nexchange is expressed in a basis of conformal primary particle wavefunctions.\nThis celestial amplitude is expanded in a basis of 2D conformal partial waves\non the unitary principal series, and then rewritten as a sum over 2D conformal\nblocks via contour deformation. The conformal blocks include intermediate\nexchanges of spinning light-ray states, as well as scalar states with positive\ninteger conformal weights. The conformal block prefactors are found as expected\nto be quadratic in the celestial OPE coefficients.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we extend the work of Brenner and Sung [Math. Comp. 59,\n321--338 (1992)] and present a regularity estimate for the elastic equations in\nconcave domains. Based on the regularity estimate we prove that the constants\nin the error estimates of the nonconforming Crouzeix-Raviart element\napproximations for the elastic equations/eigenvalue problem are independent of\nthe Lame constant, which means the nonconforming Crouzeix-Raviart element\napproximations are locking-free. We also establish two kinds of two-grid\ndiscretization schemes for the elastic eigenvalue problem and analyze that when\nthe mesh sizes of the coarse grid and fine grid satisfy some relationship, the\nresulting solutions can achieve optimal accuracy. Numerical examples are\nprovided to show the efficiency of two-grid schemes for the elastic eigenvalue\nproblem.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Accurately describing and detecting 2D and 3D keypoints is crucial to\nestablishing correspondences across images and point clouds. Despite a plethora\nof learning-based 2D or 3D local feature descriptors and detectors having been\nproposed, the derivation of a shared descriptor and joint keypoint detector\nthat directly matches pixels and points remains under-explored by the\ncommunity. This work takes the initiative to establish fine-grained\ncorrespondences between 2D images and 3D point clouds. In order to directly\nmatch pixels and points, a dual fully convolutional framework is presented that\nmaps 2D and 3D inputs into a shared latent representation space to\nsimultaneously describe and detect keypoints. Furthermore, an ultra-wide\nreception mechanism in combination with a novel loss function are designed to\nmitigate the intrinsic information variations between pixel and point local\nregions. Extensive experimental results demonstrate that our framework shows\ncompetitive performance in fine-grained matching between images and point\nclouds and achieves state-of-the-art results for the task of indoor visual\nlocalization. Our source code will be available at [no-name-for-blind-review].\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the cohomology of Lie superalgebras for the full complex of forms:\nsuperforms, pseudoforms and integral forms. We use the technique of spectral\nsequences to abstractly compute the Chevalley-Eilenberg cohomology. We first\nfocus on the superalgebra $\\mathfrak{osp}(2|2)$ and show that there exist\nnon-empty cohomology spaces among pseudoforms related to sub-superalgebras. We\nthen extend some classical theorems by Koszul, as to include pseudoforms and\nintegral forms. Further, we conjecture that the algebraic Poincar\\'e duality\nextends to Lie superalgebras, as long as all the complexes of forms are taken\ninto account and we prove that this holds true for $\\mathfrak{osp}(2|2)$. We\nfinally construct the cohomology representatives explicitly by using a\ndistributional realisation of pseudoforms and integral forms. On one hand,\nthese results show that the cohomology of Lie superalgebras is actually larger\nthan expected, whereas one restricts to superforms only; on the other hand, we\nshow the emergence of completely new cohomology classes represented by\npseudoforms. These classes realise as integral form classes of\nsub-superstructures.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Cayley graphs of finite groups are known to provide several examples of\nfamilies of expanders, and some of them are Ramanujan graphs. Babai studied\nisospectral non-isomorphic Cayley graphs of the dihedral groups. Lubotzky,\nSamuels and Vishne proved that there are isospectral non-isomorphic Cayley\ngraphs of $\\mathrm{PSL}_d(\\mathbb F_q)$ for every $d\\geq 5$ ($d \\neq 6$) and\nprime power $q> 2$. In this article, we focus on three variants of Cayley\ngraphs, viz., the Cayley sum graphs, the twisted Cayley graphs, and the twisted\nCayley sum graphs. We prove the existence of non-isomorphic expander families\nof bounded degree, whose spectra are related by the values of certain\ncharacters. We also provide several new examples of expander families, and\nexamples of non-expanders and Ramanujan graphs formed by these three variants.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  End-to-end models are favored in automatic speech recognition (ASR) because\nof their simplified system structure and superior performance. Among these\nmodels, Transformer and Conformer have achieved state-of-the-art recognition\naccuracy in which self-attention plays a vital role in capturing important\nglobal information. However, the time and memory complexity of self-attention\nincreases squarely with the length of the sentence. In this paper, a\nprob-sparse self-attention mechanism is introduced into Conformer to sparse the\ncomputing process of self-attention in order to accelerate inference speed and\nreduce space consumption. Specifically, we adopt a Kullback-Leibler divergence\nbased sparsity measurement for each query to decide whether we compute the\nattention function on this query. By using the prob-sparse attention mechanism,\nwe achieve impressively 8% to 45% inference speed-up and 15% to 45% memory\nusage reduction of the self-attention module of Conformer Transducer while\nmaintaining the same level of error rate.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we propose a novel fault attack termed as Single Event\nTransient Fault Analysis (SETFA) attack, which is well suited for hardware\nimplementations. The proposed approach pinpoints hotspots in the cypher's Sbox\ncombinational logic circuit that significantly reduce the key entropy when\nsubjected to faults. ELEPHANT is a parallel authenticated encryption and\nassociated data (AEAD) scheme targeted to hardware implementations, a finalist\nin the Lightweight cryptography (LWC) competition launched by NIST. In this\nwork, we investigate vulnerabilities of ELEPHANT against fault analysis. We\nobserve that the use of 128-bit random nonce makes it resistant against many\ncryptanalysis techniques like differential, linear, etc., and their variants.\nHowever, the relaxed nature of Statistical Fault Analysis (SFA) methods makes\nthem widely applicable in restrictive environments. We propose a SETFA-based\nkey recovery attack on Elephant. We performed Single experiments with random\nplaintexts and keys, on Dumbo, a Sponge-based instance of the Elephant-AEAD\nscheme. Our proposed approach could recover the secret key in 85-250\nciphertexts. In essence, this work investigates new vulnerabilities towards\nfault analysis that may require to be addressed to ensure secure computations\nand communications in IoT scenarios.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For many novel applications, such as patient-specific computer-aided surgery,\nconventional solution techniques of the underlying nonlinear problems are\nusually computationally too expensive and are lacking information about how\ncertain can we be about their predictions. In the present work, we propose a\nhighly efficient deep-learning surrogate framework that is able to accurately\npredict the response of bodies undergoing large deformations in real-time. The\nsurrogate model has a convolutional neural network architecture, called U-Net,\nwhich is trained with force-displacement data obtained with the finite element\nmethod. We propose deterministic and probabilistic versions of the framework.\nThe probabilistic framework utilizes the Variational Bayes Inference approach\nand is able to capture all the uncertainties present in the data as well as in\nthe deep-learning model. Based on several benchmark examples, we show the\npredictive capabilities of the framework and discuss its possible limitations\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In most practical adaptive signal processing systems, e.g., active noise\ncontrol, active vibration control, and acoustic echo cancellation, substantial\nnonlinearities that cannot be neglected exist. In this paper, we analyze the\nbehaviors of an adaptive system in which the output of the adaptive filter has\nthe clipping saturation-type nonlinearity by a statistical-mechanical method.\nWe discuss the dynamical and steady-state behaviors of the adaptive system by\nasymptotic analysis, steady-state analysis, and numerical calculation. As a\nresult, it has become clear that the saturation value has the critical point at\nwhich the system's mean-square stability and instability switch. The obtained\ntheory well explains the strange behaviors around the critical point observed\nin the computer simulation. Finally, the exact value of the critical point is\nalso derived.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Ride-pooling has become an important service option offered by ride-hailing\nplatforms as it serves multiple trip requests in a single ride. By leveraging\ncustomer data, connected vehicles, and efficient assignment algorithms,\nride-pooling can be a critical instrument to address driver shortages and\nmitigate the negative externalities of ride-hailing operations. Recent\nliterature has focused on computationally intensive optimization-based methods\nthat maximize system throughput or minimize vehicle miles. However, individual\ncustomers may experience substantial service quality degradation due to the\nconsequent waiting and detour time. In contrast, this paper examines heuristic\nmethods for real-time ride-pooling assignments that are highly scalable and\neasily computable. We propose a restricted subgraph method and compare it with\nother existing heuristic and optimization-based matching algorithms using a\nvariety of metrics. By fusing multiple sources of trip and network data in New\nYork City, we develop a flexible, agent-based simulation platform to test these\nstrategies on different demand levels and examine how they affect both the\ncustomer experience and the ride-hailing platform. Our results find a trade-off\namong heuristics between throughput and customer matching time. We show that\nour proposed ride-pooling strategy maintains system performance while limiting\ntrip delays and improving customer experience. This work provides insight for\npolicymakers and ride-hailing operators about the performance of simpler\nheuristics and raises concerns about prioritizing only specific platform\nmetrics without considering service quality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Human assistive robotics have the potential to help the elderly and\nindividuals living with disabilities with their Activities of Daily Living\n(ADL). Robotics researchers focus on assistive tasks from the perspective of\nvarious control schemes and motion types. Health research on the other hand\nfocuses on clinical assessment and rehabilitation, arguably leaving important\ndifferences between the two domains. In particular, little is known\nquantitatively on which ADLs are typically carried out in a persons everyday\nenvironment - at home, work, etc. Understanding what activities are frequently\ncarried out during the day can help guide the development and prioritization of\nrobotic technology for in-home assistive robotic deployment. This study targets\nseveral lifelogging databases, where we compute (i) ADL task frequency from\nlong-term low sampling frequency video and Internet of Things (IoT) sensor\ndata, and (ii) short term arm and hand movement data from 30 fps video data of\ndomestic tasks. Robotics and health care communities have differing terms and\ntaxonomies for representing tasks and motions. In this work, we derive and\ndiscuss a robotics-relevant taxonomy from quantitative ADL task and motion data\nin attempt to ameliorate taxonomic differences between the two communities. Our\nquantitative results provide direction for the development of better assistive\nrobots to support the true demands of the healthcare community.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Future Moon bases will likely be constructed using resources mined from the\nsurface of the Moon. The difficulty of maintaining a human workforce on the\nMoon and communications lag with Earth means that mining will need to be\nconducted using collaborative robots with a high degree of autonomy. In this\npaper, we describe our solution for Phase 2 of the NASA Space Robotics\nChallenge, which provided a simulated lunar environment in which teams were\ntasked to develop software systems to achieve autonomous collaborative robots\nfor mining on the Moon. Our 3rd place and innovation award winning solution\nshows how machine learning-enabled vision could alleviate major challenges\nposed by the lunar environment towards autonomous space mining, chiefly the\nlack of satellite positioning systems, hazardous terrain, and delicate robot\ninteractions. A robust multi-robot coordinator was also developed to achieve\nlong-term operation and effective collaboration between robots.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the Open Data Portal Germany (OPAL) project, a pipeline of the following\ndata refinement steps has been developed: requirements analysis, data\nacquisition, analysis, conversion, integration and selection. 800,000 datasets\nin DCAT format have been produced.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present ALMA observations of a small but statistically complete sample of\ntwelve 250 micron selected galaxies at $z=0.35$ designed to measure their dust\nsubmillimeter continuum emission as well as their CO(1-0) and atomic carbon\n[CI](3P1-3P0) spectral lines. This is the first sample of galaxies with global\nmeasures of all three $H_2$-mass tracers and which show star formation rates\n(4-26 Msun yr$^{-1}$) and infra-red luminosities ($1-6\\times10^{11}$ Lsun)\ntypical of star forming galaxies in their era. We find a surprising diversity\nof morphology and kinematic structure; one-third of the sample have evidence\nfor interaction with nearby smaller galaxies, several sources have disjoint\ndust and gas morphology. Moreover two galaxies have very high $L_{CI}/L_{CO}$\nratios for their global molecular gas reservoirs; if confirmed, such extreme\nintensity ratios in a sample of dust selected, massive star forming galaxies\npresents a challenge to our understanding of ISM. Finally, we use the emission\nof the three molecular gas tracers, to determine the carbon abundance,\n$X_{ci}$, and CO-$\\rm{H_2}$ conversion $\\alpha_{co}$ in our sample, using a\nweak prior that the gas-to-dust ratio is similar to that of the Milky Way for\nthese massive and metal rich galaxies. Using a likelihood method which\nsimultaneously uses all three gas tracer measurements, we find mean values and\nerrors on the mean of\n$\\alpha_{co}=3.0\\pm0.5\\,\\rm{Msun\\,(K\\,kms^{-1}\\,pc^2)^{-1}}$ and\n$X_{ci}=1.6\\pm0.1\\times 10^{-5}$ (or $\\alpha_{ci}=18.8\\,K kms^{-1}\\,pc^2\n(Msun)^{-1}$) and $\\delta_{GDR}=128\\pm16$ (or\n$\\alpha_{850}=5.9\\times10^{12}\\,\\rm{W\\,Hz^{-1}\\, Msun^{-1}}$), where our\nstarting assumption is that these metal rich galaxies have an average\ngas-to-dust ratio similar to that of the Milky Way centered on\n$\\delta_{GDR}=135$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The wide spread of click-and-mortar model offers an opportunity to consider a\nuniversal commercial interaction method. However, issues like privacy\nprotection resist the widespread acceptance. Traditional SET and SSL protocols\nare designed using Public Key Infrastructure (PKI) where extensive computations\nare carried out. Our aim is here to design a protocol to secure any type of\ncommercial interaction for online platform, which also considers mobile\nplatform. Therefore, our focus is on reducing heavy computations and making the\noverall procedure faster. A Secured Click and Mortar Commercial Interaction\n(SCMCI) protocol is proposed here to improve the performance of the commercial\ninteraction procedure through replacing time consuming public key encryption\nand decryption algorithms by hybrid logic including the use of symmetric key.\nComparative analysis has been done with traditional SET protocol using cryptool\nto prove the efficiency of the protocol.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In two-way time-of-arrival (TOA) systems, a user device (UD) obtains its\nposition by round-trip communications to a number of anchor nodes (ANs) at\nknown locations. The objective function of the maximum likelihood (ML) method\nfor two-way TOA localization is nonconvex. Thus, the widely-adopted\nGauss-Newton iterative method to solve the ML estimator usually suffers from\nthe local minima problem. In this paper, we convert the original estimator into\na convex problem by relaxation, and develop a new semidefinite programming\n(SDP) based localization method for moving UDs, namely SDP-M. Numerical result\ndemonstrates that compared with the iterative method, which often fall into\nlocal minima, the SDP-M always converge to the global optimal solution and\nsignificantly reduces the localization error by more than 40%. It also has\nstable localization accuracy regardless of the UD movement, and outperforms the\nconventional method for stationary UDs, which has larger error with growing UD\nvelocity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This is an unconventional review article on spectral problems in black hole\nperturbation theory. Our purpose is to explain how to apply various known\ntechniques in quantum mechanics to such spectral problems. The article includes\nanalytical/numerical treatments, semiclassical perturbation theory, the\n(uniform) WKB method and useful mathematical tools: Borel summations, Pad\\'e\napproximants, etc. The article is not comprehensive, but rather looks into a\nfew examples from various points of view. The techniques in this article are\nwidely applicable to many other examples.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper summarizes our efforts to aid human reasoning when verification\nfails through the use of two distinct Formalization Integrated Development\nEnvironments (F-IDEs) that we have developed. Both environments are modular and\nfacilitate reasoning about the full behavior of object-based code. The first\nenvironment, referred to as the web-IDE, has been used for several years to\nteach aspects of formal specification and verification, including why and where\nverification conditions (VCs) arise and how to use them when verification\nfails. The second F-IDE, RESOLVE Studio, remains experimental, but is a more\nfully-fledged environment backed by a sequent-based VC generator that produces\nVCs with fewer extraneous givens. While the environments and VC generation\ntechniques are necessarily language specific, the principles of alternative VC\ngeneration methods, F-IDE features, and observations about their impact on\nnovices and experienced users are more generally applicable.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  As 3D object detection on point clouds relies on the geometrical\nrelationships between the points, non-standard object shapes can hinder a\nmethod's detection capability. However, in safety-critical settings, robustness\nto out-of-domain and long-tail samples is fundamental to circumvent dangerous\nissues, such as the misdetection of damaged or rare cars. In this work, we\nsubstantially improve the generalization of 3D object detectors to\nout-of-domain data by deforming point clouds during training. We achieve this\nwith 3D-VField: a novel data augmentation method that plausibly deforms objects\nvia vector fields learned in an adversarial fashion. Our approach constrains 3D\npoints to slide along their sensor view rays while neither adding nor removing\nany of them. The obtained vectors are transferable, sample-independent and\npreserve shape and occlusions. Despite training only on a standard dataset,\nsuch as KITTI, augmenting with our vector fields significantly improves the\ngeneralization to differently shaped objects and scenes. Towards this end, we\npropose and share CrashD: a synthetic dataset of realistic damaged and rare\ncars, with a variety of crash scenarios. Extensive experiments on KITTI, Waymo,\nour CrashD and SUN RGB-D show the generalizability of our techniques to\nout-of-domain data, different models and sensors, namely LiDAR and ToF cameras,\nfor both indoor and outdoor scenes. Our CrashD dataset is available at\nhttps://crashd-cars.github.io.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A mapping $\\alpha : V(G) \\to V(H)$ from the vertex set of one graph $G$ to\nanother graph $H$ is an isometric embedding if the shortest path distance\nbetween any two vertices in $G$ equals the distance between their images in\n$H$. Here, we consider isometric embeddings of a weighted graph $G$ into\nunweighted Hamming graphs, called Hamming embeddings, when $G$ satisfies the\nproperty that every edge is a shortest path between its endpoints. Using a\nCartesian product decomposition of $G$ called its pseudofactorization, we show\nthat every Hamming embedding of $G$ may be partitioned into Hamming embeddings\nfor each irreducible pseudofactor graph of $G$, which we call its canonical\npartition. This implies that $G$ permits a Hamming embedding if and only if\neach of its irreducible pseudofactors is Hamming embeddable. This result\nextends prior work on unweighted graphs that showed that an unweighted graph\npermits a Hamming embedding if and only if each irreducible pseudofactor is a\ncomplete graph. When a graph $G$ has nontrivial pseudofactors, determining\nwhether $G$ has a Hamming embedding can be simplified to checking embeddability\nof two or more smaller graphs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It has been recently demonstrated that both, a classical Schwarzschild black\nhole (BH), and a dense concentration of self-gravitating fermionic dark matter\n(DM) placed at the Galaxy centre, can explain the precise astrometric data\n(positions and radial velocities) of the S-stars orbiting SgrA*. This result\nencompasses the 17 best resolved S-stars, and includes the test of general\nrelativistic effects such as the gravitational redshift in the S2-star. In\naddition, the DM model features another remarkable result: the dense core of\nfermions is the central region of a continuous density distribution of DM whose\ndiluted halo explains the Galactic rotation curve. In this Letter, we\ncomplement the above findings by analyzing in both models the relativistic\nperiapsis precession of the S2-star orbit. While the Schwarzschild BH scenario\npredicts a unique prograde precession for S2, in the DM scenario it can be\neither retrograde or prograde, depending on the amount of DM mass enclosed\nwithin the S2 orbit, which in turn is a function of the DM fermion mass. We\nshow that all the current and publicly available data of S2 can not\ndiscriminate between the two models, but upcoming S2 astrometry close to next\napocentre passage could potentially establish if SgrA* is governed by a\nclassical BH or by a quantum DM system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Acoustic echo cancellation (AEC), noise suppression (NS) and automatic gain\ncontrol (AGC) are three often required modules for real-time communications\n(RTC). This paper proposes a neural network supported algorithm for RTC, namely\nNN3A, which incorporates an adaptive filter and a multi-task model for residual\necho suppression, noise reduction and near-end speech activity detection. The\nproposed algorithm is shown to outperform both a method using separate models\nand an end-to-end alternative. It is further shown that there exists a\ntrade-off in the model between residual suppression and near-end speech\ndistortion, which could be balanced by a novel loss weighting function. Several\npractical aspects of training the joint model are also investigated to push its\nperformance to limit.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We compare different formulations of the generalized uncertainty principle\nthat have an underlying algebraic structure. We show that the formulation by\nKempf, Mangano and Mann (KMM) [Phys. Rev. D 52 (1995)], quite popular for\nphenomenological studies, satisfies the Jacobi identities only for spin zero\nparticles. In contrast, the formulation proposed earlier by one of us (MM)\n[Phys. Lett. B 319 (1993)] has an underlying algebraic structure valid for\nparticles of all spins, and is in this sense more fundamental. The latter is\nalso much more constrained, resulting into only two possible solutions, one\nexpressing the existence of a minimum length, and the other expressing a form\nof quantum-to-classical transition. We also discuss how this more stringent\nalgebraic formulation has an intriguing physical interpretation in terms of a\ndiscretized time at the Planck scale.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We explore the interaction between two genetic incompatibilities\n(underdominant loci in diploid organisms) in a population occupying a\none-dimensional space. We derive a system of partial differential equations\ndescribing the dynamics of allele frequencies and linkage disequilibrium\nbetween the two loci, and use a quasi-linkage equilibrium approximation in\norder to reduce the number of variables. We investigate the solutions of this\nsystem and demonstrate the existence of a solution in which the two clines in\nallele frequency remain stacked together. In the case of asymmetric\nincompatibilities (i.e. when one homozygote is favored over the other at each\nlocus), these stacked clines propagate in the form of a traveling wave. We\nobtain an approximation for the speed of this wave which, in particular, is\ndecreased by recombination between the two loci but is always larger than the\nspeed of \"one cline alone\".\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In a recent work, a method for the magnetic resonance (MR) measurement of the\ntrue diffusion propagator was introduced, which was subsequently implemented\nand validated for free diffusion on a benchtop MR scanner. Here, we provide a\nbrief theoretical description of the method and discuss various experimental\nregimes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This article is devoted to the features that were under development between\nRoboCup 2019 Sydney and RoboCup 2021 Worldwide. These features include\nvision-related matters, such as detection and localization, mechanical and\nalgorithmic novelties. Since the competition was held virtually, the\nsimulation-specific features are also considered in the article. We give an\noverview of the approaches that were tried out along with the analysis of their\npreconditions, perspectives and the evaluation of their performance.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Decentralized nonconvex optimization has received increasing attention in\nrecent years in machine learning due to its advantages in system robustness,\ndata privacy, and implementation simplicity. However, three fundamental\nchallenges in designing decentralized optimization algorithms are how to reduce\ntheir sample, communication, and memory complexities. In this paper, we propose\na \\underline{g}radient-\\underline{t}racking-based \\underline{sto}chastic\n\\underline{r}ecursive \\underline{m}omentum (GT-STORM) algorithm for efficiently\nsolving nonconvex optimization problems. We show that to reach an\n$\\epsilon^2$-stationary solution, the total number of sample evaluations of our\nalgorithm is $\\tilde{O}(m^{1/2}\\epsilon^{-3})$ and the number of communication\nrounds is $\\tilde{O}(m^{-1/2}\\epsilon^{-3})$, which improve the\n$O(\\epsilon^{-4})$ costs of sample evaluations and communications for the\nexisting decentralized stochastic gradient algorithms. We conduct extensive\nexperiments with a variety of learning models, including non-convex logistical\nregression and convolutional neural networks, to verify our theoretical\nfindings. Collectively, our results contribute to the state of the art of\ntheories and algorithms for decentralized network optimization.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Aims: We present a detailed X-ray study of the recently discovered supernova\nremnant (SNR) G53.41+0.03 that follows up and further expands on the previous,\nlimited analysis of archival data covering a small portion of the SNR. Methods:\nWith the new dedicated 70ks XMM-Newton observation we investigate the\nmorphological structure of the SNR in X-rays, search for a presence of a young\nneutron star and characterise the plasma conditions in the selected regions by\nmeans of spectral fitting. Results: The first full view of SNR G53.41+0.03\nshows an X-ray emission region well aligned with the reported half-shell radio\nmorphology. We find three distinct regions of the remnant that vary in\nbrightness and hardness of the spectra, and are all best characterised by a hot\nplasma model in a non-equilibrium ionisation state. Of the three regions, the\nbrightest one contains the most mature plasma, with ionisation age $\\tau\n\\approx 4\\times10^{10}$s cm$^{-3}$ (where $\\tau = n_e t$), a lower electron\ntemperature of kT$_\\mathrm{e} \\approx 1$ keV and the highest estimated gas\ndensity of n$_\\mathrm{H}\\approx 0.87$ cm$^{-3}$. The second, fainter but\nspectrally harder, region reveals a younger plasma ($\\tau \\approx\n1.7\\times10^{10}$s cm$^{-3}$) with higher temperature (kT$_\\mathrm{e} \\approx\n2$ keV) and two to three times lower density (n$_\\mathrm{H}\\approx 0.34$\ncm$^{-3}$). The third region is very faint, but we identify spectroscopically\nthe presence of a hot plasma.Employing several methods for age estimation, we\nfind the remnant to be $t \\approx 1000-5000$ yrs old, confirming the earlier\nreports of a relatively young age. The environment of the remnant also contains\na number of point sources, of which most are expected to be positioned in the\nforeground. Of the two point sources in the geometrical centre of the remnant\none is consistent with the characteristics of a young neutron star.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Engineering optical emission from two dimensional, transition metal\ndichalcogenides (TMDs) materials such as Tungsten disulphide (WS2) has\nimplications in creating and understanding nanophotonic sources. One of the\nchallenges in controlling the optical emission from 2D materials is to achieve\nnarrow angular spread using a simple photonic geometry. In this paper, we study\nhow the photoluminescence of a monolayer WS2 can be controlled when coupled to\nfilm coupled microsphere dielectric antenna. Specifically, by employing Fourier\nplane microscopy and spectroscopic techniques, we quantify the wavevector\ndistribution in the momentum space. As a result, we show beaming of the WS2\nphotoluminescence with angular divergence of {\\theta}1/2 = 4.6{\\deg}.\nFurthermore, the experimental measurements have been supported by\nthree-dimensional numerical simulations. We envisage that the discussed results\ncan be generalized to a variety of nanophotonic 2D materials, and can be\nharnessed in nonlinear and quantum technology.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Text classification has seen an increased use in both academic and industry\nsettings. Though rule based methods have been fairly successful, supervised\nmachine learning has been shown to be most successful for most languages, where\nmost research was done on English. In this article, the success of lexicon\nanalysis, support vector machines, and extreme gradient boosting for the task\nof text classification and sentiment analysis are evaluated in Turkish and a\npretrained transformer based classifier is proposed, outperforming previous\nmethods for Turkish text classification. In the context of text classification,\nall machine learning models proposed in the article are domain-independent and\ndo not require any task-specific modifications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Machine Learning (ML) has the potential to accelerate discovery of new\nmaterials and shed light on useful properties of existing materials. A key\ndifficulty when applying ML in Materials Science is that experimental datasets\nof material properties tend to be small. In this work we show how material\ndescriptors can be learned from the structures present in large scale datasets\nof material simulations; and how these descriptors can be used to improve the\nprediction of an experimental property, the energy of formation of a solid. The\nmaterial descriptors are learned by training a Graph Neural Network to regress\nsimulated formation energies from a material's atomistic structure. Using these\nlearned features for experimental property predictions outperforms existing\nmethods that are based solely on chemical composition. Moreover, we find that\nthe advantage of our approach increases as the generalization requirements of\nthe task are made more stringent, for example when limiting the amount of\ntraining data or when generalizing to unseen chemical spaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Under holographic prescription for Schwinger-Keldysh closed time contour for\nnon-equilibrium system, we consider fluctuation effect of the order parameter\nin a holographic superconductor model. Near the critical point, we derive the\ntime-dependent Ginzburg-Landau effective action governing dynamics of the\nfluctuating order parameter. In a semi-analytical approach, the time-dependent\nGinzburg-Landau action is computed up to quartic order of the fluctuating order\nparameter, and first order in time derivative.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Ferromagnetic spin valves offer the key building blocks to integrate giant-\nand tunneling-magnetoresistance effects into spintronics devices. Starting from\na generalized Blonder-Tinkham-Klapwijk approach, we theoretically investigate\nthe impact of interfacial Rashba and Dresselhaus spin-orbit couplings on the\ntunneling conductance, and thereby the magnetoresistance characteristics, of\nferromagnet/superconductor/ferromagnet spin-valve junctions embedding thin\nsuperconducting spacers between the either parallel or antiparallel magnetized\nferromagnets. We focus on the unique interplay between usual electron\ntunnelings-that fully determine the magnetoresistance in the normal-conducting\nstate-and the peculiar Andreev reflections in the superconducting state. In the\npresence of interfacial spin-orbit couplings, special attention needs to be\npaid to the spin-flip (\"unconventional\") Andreev-reflection process that is\nexpected to induce superconducting triplet correlations in proximitized\nregions. As a transport signature of these triplet pairings, we detect\nconductance double peaks around the singlet-gap energy, reflecting the\ncompetition between the singlet and an additionally emerging triplet gap; the\nlatter is an effective superconducting gap that can be ascribed to the\nformation of triplet Cooper pairs through interfacial spin-flip scatterings\n(i.e., to the generation of an effective triplet-pairing term in the order\nparameter). We thoroughly analyze the Andreev reflections' role in connection\nwith superconducting magnetoresistance phenomena, and eventually unravel huge\nconductance and magnetoresistance magnetoanisotropies-easily exceeding their\nnormal-state counterparts by several orders of magnitude-as another\nexperimentally accessible fingerprint of unconventional Andreev reflections.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  { The ground-state to ground-state electron-capture $Q$ value of $^{159}$Dy\n($3/2^-$) has been measured directly utilizing the double Penning trap mass\nspectrometer JYFLTRAP. A value of 364.73(19)~keV was obtained from a\nmeasurement of the cyclotron frequency ratio of the decay parent $^{159}$Dy and\nthe decay daughter $^{159}$Tb ions using the novel phase-imaging ion-cyclotron\nresonance technique. The $Q$ values for allowed Gamow-Teller transition to\n$5/2^-$ and the third-forbidden unique transition to $11/2^+$ state with\nexcitation energies of 363.5449(14)~keV and 362.050(40)~keV in $^{159}$Tb were\ndetermined to be 1.18(19) keV and 2.68(19) keV, respectively. The\nhigh-precision $Q$ value of transition $3/2^-\\to 5/2^-$ from this work,\nrevealing itself as the lowest electron-capture $Q$ value, is utilized to\nunambiguously characterise all the possible lines that are present in its\nelectron capture spectrum. { We performed atomic many-body calculations for\nboth transitions to determine electron-capture probabilities from various\natomic orbitals, and found an order of magnitude enhancement in the event rates\nnear the end-point of energy spectrum in the transition to the $5/2^-$ nuclear\nexcited state, which can become very interesting once the experimental\nchallenges of identifying decays into excited states are overcome. The\ntransition to the $11/2^+$ state is strongly suppressed and found unsuitable\nfor measuring the neutrino mass. These results show that the electron capture\nin the $^{159}$Dy atom, going to the $5/2^-$ state of the $^{159}$Tb nucleus,\n%\\textcolor{red} {is a new candidate which may open the way to determine the\nelectron-neutrino mass in the sub-eV region by studying EC. Further\nexperimental feasibility studies, including coincidence measurements with\nrealistic detectors, will be of great interest.} }\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The large enhancement of the primordial power spectrum of the curvature\nperturbation can seed the formation of primordial black hole, that can play as\na dark matter component in the Universe. In multi-filed inflation models, the\ncurved trajectory of the scalar fields in the field space can generate a peak\nin the power spectrum on small scales due to the existence of the isocurvature\nperturbation. Here we show that a potential can be reconstructed from a given\npower spectrum, which is made of a scale-invariant one on large scales and the\nother function with a peak on small scales. In multi-field inflation models the\nreconstructed potential may not be unique and we can find different potentials\nfrom a given power spectrum.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Topological lattices have recently generated a great deal of interest based\non the unique mechanical properties rooted in their topological polarization,\nincluding the ability to support localized modes at certain floppy edges. The\nstudy of these systems has been predominantly restricted to the realm of\nin-plane mechanics, to which many topological effects are germane. In this\nstudy, we stretch this paradigm by exploring the possibility to export certain\ntopological attributes to the flexural wave behavior of thin lattice sheets. To\ncouple the topological modes to the out-of-plane response, we assemble a\nbilayer lattice by stacking a thick topological kagome layer onto a thin\ntwisted kagome lattice. The band diagram reveals the existence of modes whose\nout-of-plane character is controlled by the edge modes of the topological\nlayer, a behavior elucidated via simulations and confirmed via laser vibrometer\nexperiments on a bilayer prototype specimen. These results open an alternative\ndirection for topological mechanics whereby flexural waves are controlled by\nthe in-plane topology, leading to potential applications for flexural wave\ndevices with engineered polarized response.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Trustworthiness is typically regarded as a desirable feature of national\nidentification systems (NISs); but the variegated nature of the trustor\ncommunities associated with such systems makes it difficult to see how a single\nsystem could be equally trustworthy to all actual and potential trustors. This\nworry is accentuated by common theoretical accounts of trustworthiness.\nAccording to such accounts, trustworthiness is relativized to particular\nindividuals and particular areas of activity, such that one can be trustworthy\nwith regard to some individuals in respect of certain matters, but not\ntrustworthy with regard to all trustors in respect of every matter. The present\narticle challenges this relativistic approach to trustworthiness by outlining a\nnew account of trustworthiness, dubbed the expectation-oriented account. This\naccount allows for the possibility of an absolutist (or one-place) approach to\ntrustworthiness. Such an account, we suggest, is the approach that best\nsupports the effort to develop NISs. To be trustworthy, we suggest, is to\nminimize the error associated with trustor expectations in situations of social\ndependency (commonly referred to as trust situations), and to be trustworthy in\nan absolute sense is to assign equal value to all expectation-related errors in\nall trust situations. In addition to outlining the features of the\nexpectation-oriented account, we describe some of the implications of this\naccount for the design, development, and management of trustworthy NISs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We construct in ZFC a countably compact group without non-trivial convergent\nsequences of size $2^{\\mathfrak{c}}$, answering a question of Bellini,\nRodrigues and Tomita. We also construct in ZFC a selectively pseudocompact\ngroup which is not countably pracompact, showing that these two properties are\nnot equivalent in the class of topological groups. Using the same technique, we\nconstruct a group which has all powers selectively pseudocompact but is not\ncountably pracompact, assuming the existence of a selective ultrafilter.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present an approach to cosmology in which the Universe learns its own\nphysical laws. It does so by exploring a landscape of possible laws, which we\nexpress as a certain class of matrix models. We discover maps that put each of\nthese matrix models in correspondence with both a gauge/gravity theory and a\nmathematical model of a learning machine, such as a deep recurrent, cyclic\nneural network. This establishes a correspondence between each solution of the\nphysical theory and a run of a neural network. This correspondence is not an\nequivalence, partly because gauge theories emerge from $N \\rightarrow \\infty $\nlimits of the matrix models, whereas the same limits of the neural networks\nused here are not well-defined. We discuss in detail what it means to say that\nlearning takes place in autodidactic systems, where there is no supervision. We\npropose that if the neural network model can be said to learn without\nsupervision, the same can be said for the corresponding physical theory. We\nconsider other protocols for autodidactic physical systems, such as\noptimization of graph variety, subset-replication using self-attention and\nlook-ahead, geometrogenesis guided by reinforcement learning, structural\nlearning using renormalization group techniques, and extensions. These\nprotocols together provide a number of directions in which to explore the\norigin of physical laws based on putting machine learning architectures in\ncorrespondence with physical theories.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the problem of simultaneous scheduling and resource allocation of\nan incoming flow of requests to a set of computing units. By representing each\ncomputing unit as a node, we model the overall system as a multi-queue scheme.\nInspired by congestion control approaches in communication networks, we propose\nan AIMD-like (additive increase multiplicative decrease) admission control\npolicy that is stable irrespective of the total number of nodes and AIMD\nparameters. The admission policy allows us to establish an event-driven\ndiscrete model, triggered by a locally identifiable enabling condition.\nSubsequently, we propose a decentralized resource allocation strategy via a\nsimple nonlinear state feedback controller, guaranteeing global convergence to\na bounded set in finite time. Last, we reveal the connection of these\nproperties with Quality of Service specifications, by calculating local queuing\ntime via a simple formula consistent with Little's Law.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we focus on the duo ring property via quasinilpotent elements\nwhich gives a new kind of generalizations of commutativity. We call this kind\nof ring qnil-duo. Firstly, some properties of quasinilpotents in a ring are\nprovided. Then the set of quasinilpotents is applied to the duo property of\nrings, in this perspective, we introduce and study right (resp., left) qnil-duo\nrings. We show that this concept is not left-right symmetric. Among others it\nis proved that if the Hurwitz series ring $H(R; \\alpha)$ is right qnil-duo,\nthen $R$ is right qnil-duo. Every right qnil-duo ring is abelian. A right\nqnil-duo exchange ring has stable range 1.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Graph data completion is a fundamentally important issue as data generally\nhas a graph structure, e.g., social networks, recommendation systems, and the\nInternet of Things. We consider a graph where each node has a data matrix,\nrepresented as a \\textit{graph-tensor} by stacking the data matrices in the\nthird dimension. In this paper, we propose a \\textit{Convolutional Graph-Tensor\nNet} (\\textit{Conv GT-Net}) for the graph data completion problem, which uses\ndeep neural networks to learn the general transform of graph-tensors. The\nexperimental results on the ego-Facebook data sets show that the proposed\n\\textit{Conv GT-Net} achieves significant improvements on both completion\naccuracy (50\\% higher) and completion speed (3.6x $\\sim$ 8.1x faster) over the\nexisting algorithms.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this note, we discuss the ellipticity of the single layer boundary\nintegral operator for the wave equation in one space dimension. This result not\nonly generalizes the well-known ellipticity of the energetic boundary integral\nformulation in $L^2$, but it also turns out to be a particular case of a recent\nresult on the inf-sup stability of boundary integral operators for the wave\nequation. Instead of the time derivative in the energetic formulation, we use a\nmodified Hilbert transformation, which allows us to stay in Sobolev spaces of\nthe same order. This results in the applicability of standard boundary element\nerror estimates, which are confirmed by numerical results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove a concentration phenomenon on the empirical eigenvalue distribution\n(EED) of the principal submatrix in a random hermitian matrix whose\ndistribution is invariant under unitary conjugacy; for example, this class\nincludes GUE (Gaussian Unitary Ensemble) and Wishart matrices. More precisely,\nif the EED of the whole matrix converges to some deterministic probability\nmeasure $\\mathfrak{m}$, then its fluctuation from the EED of the principal\nsubmatrix, after a rescaling, concentrates at the Rayleigh measure (in general,\na Schwartz distribution) associated with $\\mathfrak{m}$ by the Markov--Krein\ncorrespondence. For the proof, we use the moment method with Weingarten\ncalculus and free probability. At some stage of calculations, the proof\nrequires a relation between the moments of the Rayleigh measure and free\ncumulants of $\\mathfrak{m}$. This formula is more or less known, but we provide\na different proof by observing a combinatorial structure of non-crossing\npartitions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum assembly languages are machine-independent languages that\ntraditionally describe quantum computation in the circuit model. Open quantum\nassembly language (OpenQASM 2) was proposed as an imperative programming\nlanguage for quantum circuits based on earlier QASM dialects. In principle, any\nquantum computation could be described using OpenQASM 2, but there is a need to\ndescribe a broader set of circuits beyond the language of qubits and gates. By\nexamining interactive use cases, we recognize two different timescales of\nquantum-classical interactions: real-time classical computations that must be\nperformed within the coherence times of the qubits, and near-time computations\nwith less stringent timing. Since the near-time domain is adequately described\nby existing programming frameworks, we choose in OpenQASM 3 to focus on the\nreal-time domain, which must be more tightly coupled to the execution of\nquantum operations. We add support for arbitrary control flow as well as\ncalling external classical functions. In addition, we recognize the need to\ndescribe circuits at multiple levels of specificity, and therefore we extend\nthe language to include timing, pulse control, and gate modifiers. These new\nlanguage features create a multi-level intermediate representation for circuit\ndevelopment and optimization, as well as control sequence implementation for\ncalibration, characterization, and error mitigation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We derive a simple proof, based on information theoretic inequalities, of an\nupper bound on the largest rates of $q$-ary $\\overline{2}$-separable codes that\nimproves recent results of Wang for any $q\\geq 13$. For the case $q=2$, we\nrecover a result of Lindstr\\\"om, but with a much simpler derivation. The method\neasily extends to give bounds on $B_2$ codes which, although not improving on\nWang's results, use much simpler tools and might be useful for future\napplications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that if we factor the long exact sequence in cohomology of a cofiber\nsequence of spectra into short exact sequences, then the $d_2$-differential in\nthe Adams spectral sequence of any one term is related in a precise way to\nYoneda composition with the 2-extension given by the complementary terms in the\nlong exact sequence. We use this to give a complete analysis of the Adams\nspectral sequence for the connective image-of-$J$ spectrum, finishing a\ncalculation that was begun by D. Davis in 1975.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Shape resonances in H$_2$, produced as reaction intermediates in the\nphotolysis of H$_2$S precursor molecules, are measured in a half-collision\napproach. Before desintegrating into two ground state H atoms, the reaction is\nquenched by two-photon Doppler-free excitation to the F electronically excited\nstate of H$_2$. For $J=13,15,17,19$ and 21, resonances with lifetimes in the\nrange of nano to milliseconds were observed with an accuracy of 30~MHz\n(1.4~mK). The experimental resonance positions are found to be in excellent\nagreement with theoretical predictions when nonadiabatic and quantum\nelectrodynamical corrections are included. This is the first time such effects\nare observed in collisions between neutral atoms. From the potential energy\ncurve of the H$_2$ molecule, now tested at high accuracy over a wide range of\ninternuclear separations, the s-wave scattering length for singlet H(1s)+H(1s)\nscattering is determined at $a = 0.2735^{39}_{31}~a_0$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we introduce a \\textit{Bi-level OPTimization} (BiOPT)\nframework for minimizing the sum of two convex functions, where both can be\nnonsmooth. The BiOPT framework involves two levels of methodologies. At the\nupper level of BiOPT, we first regularize the objective by a $(p+1)$th-order\nproximal term and then develop the generic inexact high-order proximal-point\nscheme and its acceleration using the standard estimation sequence technique.\nAt the lower level, we solve the corresponding $p$th-order proximal auxiliary\nproblem inexactly either by one iteration of the $p$th-order tensor method or\nby a lower-order non-Euclidean composite gradient scheme with the complexity\n$\\mathcal{O}(\\log \\tfrac{1}{\\varepsilon})$, for the accuracy parameter\n$\\varepsilon>0$. Ultimately, if the accelerated proximal-point method is\napplied at the upper level, and the auxiliary problem is handled by a\nnon-Euclidean composite gradient scheme, then we end up with a $2q$-order\nmethod with the convergence rate $\\mathcal{O}(k^{-(p+1)})$, for $q=\\lfloor p/2\n\\rfloor$, where $k$ is the iteration counter.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Bitcoin uses blockchain technology to maintain transactions order and\nprovides probabilistic guarantee to prevent double-spending, assuming that an\nattacker's computational power does not exceed %50 of the network power. In\nthis paper, we design a novel bribery attack and show that this guarantee can\nbe hugely undermined. Miners are assumed to be rational in this setup and they\nare given incentives that are dynamically calculated. In this attack, the\nadversary misuses the Bitcoin protocol to bribe miners and maximize their\ngained advantage. We will reformulate the bribery attack to propose a general\nmathematical foundation upon which we build multiple strategies. We show that,\nunlike Whale Attack, these strategies are practical. If the rationality\nassumption holds, this shows how vulnerable blockchain-based systems like\nBitcoin are. We suggest a soft fork on Bitcoin to fix this issue at the end.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Spin Hall effect, an electric generation of spin current, allows for\nefficient control of magnetization. Recent theory revealed that orbital Hall\neffect creates orbital current, which can be much larger than spin Hall-induced\nspin current. However, orbital current cannot directly exert a torque on a\nferromagnet, requiring a conversion process from orbital current to spin\ncurrent. Here, we report two effective methods of the conversion through\nspin-orbit coupling engineering, which allows us to unambiguously demonstrate\norbital-current-induced spin torque, or orbital Hall torque. We find that\norbital Hall torque is greatly enhanced by introducing either a rare-earth\nferromagnet Gd or a Pt interfacial layer with strong spin-orbit coupling in\nCr/ferromagnet structures, indicating that the orbital current generated in Cr\nis efficiently converted into spin current in the Gd or Pt layer. Furthermore,\nwe show that the orbital Hall torque can facilitate the reduction of switching\ncurrent of perpendicular magnetization in spin-orbit-torque-based spintronic\ndevices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove that the Borel space of torsion-free Abelian groups with domain\n$\\omega$ is Borel complete, i.e., the isomorphism relation on this Borel space\nis as complicated as possible, as an isomorphism relation. This solves a\nlong-standing open problem in descriptive set theory, which dates back to the\nseminal paper on Borel reducibility of Friedman and Stanley from 1989.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Solar energy is now the cheapest form of electricity in history.\nUnfortunately, significantly increasing the grid's fraction of solar energy\nremains challenging due to its variability, which makes balancing electricity's\nsupply and demand more difficult. While thermal generators' ramp rate -- the\nmaximum rate that they can change their output -- is finite, solar's ramp rate\nis essentially infinite. Thus, accurate near-term solar forecasting, or\nnowcasting, is important to provide advance warning to adjust thermal generator\noutput in response to solar variations to ensure a balanced supply and demand.\nTo address the problem, this paper develops a general model for solar\nnowcasting from abundant and readily available multispectral satellite data\nusing self-supervised learning. Specifically, we develop deep auto-regressive\nmodels using convolutional neural networks (CNN) and long short-term memory\nnetworks (LSTM) that are globally trained across multiple locations to predict\nraw future observations of the spatio-temporal data collected by the recently\nlaunched GOES-R series of satellites. Our model estimates a location's future\nsolar irradiance based on satellite observations, which we feed to a regression\nmodel trained on smaller site-specific solar data to provide near-term solar\nphotovoltaic (PV) forecasts that account for site-specific characteristics. We\nevaluate our approach for different coverage areas and forecast horizons across\n25 solar sites and show that our approach yields errors close to that of a\nmodel using ground-truth observations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The data from the event horizon telescope have provided a novel view of the\nvicinity of the horizon of a black hole (BH), by imaging the region around the\nlight-ring. They have also raised hopes for measuring in the near future,\nfeatures of the image (or the shadow) related to higher order effects of\nphotons traveling in these regions, such as the appearance of higher order\nbright rings. While the prospect of measuring these fine features of Kerr BHs\nis very interesting in itself, there are some even more intriguing prospects\nfor observing novel features of possible non-Kerr objects, in the case that the\nsubjects of our images are not the BH solutions of GR. In the hope of\nsufficient resolution being available in the future, we explore in this work\nthe structure and properties of null geodesics around a Hartle-Thorne spacetime\nthat includes a deformation from the Kerr spacetime characterised by the\nquadrupole deformation $\\delta q$. These spacetimes have been found to exhibit\na bifurcation of the equatorial light-ring to two off-equatorial light-rings in\na range of $\\delta q$s and spin parameters. In addition to this, there is a\nrange of parameters where both the equatorial and the off-equatorial\nlight-rings are present. This results in the formation of a pocket that can\ntrap photons. We investigate the properties of these trapped orbits and find\nthat chaotic behaviour emerges. Some of these chaotic orbits are additionally\nfound to be \"sticky\" and get trapped close to periodic orbits for long times.\nWe also explore how these novel features affect the shadow and find that the\noff-equatorial light-rings produce distinctive features that deform its\ncircular shape, while the chaotic behaviour associated to the pocket creates\nfeatures with fractal structure. These results are shown to be quite general,\nextending to higher order Hartle-Thorne spacetimes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Biological age is an important sociodemographic factor in studies on academic\ncareers (research productivity, scholarly impact, and collaboration patterns).\nIt is assumed that the academic age, or the time elapsed from the first\npublication, is a good proxy for biological age. In this study, we analyze the\nlimitations of the proxy in academic career studies, using as an example the\nentire population of Polish academic scientists visible in the last decade in\nglobal science and holding at least a PhD (N = 20,569). The proxy works well\nfor science, technology, engineering, mathematics, and medicine (STEMM)\ndisciplines; however, for non-STEMM disciplines (particularly for humanities\nand social sciences), it has a dramatically worse performance. This negative\nconclusion is particularly important for systems that have only become recently\nvisible in global academic journals. The micro-level data suggest a delayed\nparticipation of social scientists and humanists in global science networks,\nwith practical implications for predicting biological age from academic age. We\ncalculate correlation coefficients, present contingency analysis of academic\ncareer stages with academic positions and age groups, and create a linear\nmultivariate regression model. Our research suggests that in scientifically\ndeveloping countries, academic age as a proxy for biological age should be used\nmore cautiously than in advanced countries: ideally, it should be used only for\nSTEMM disciplines.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper concerns the micro-local and qualitative analysis of the\nfractional Zener wave equation. The classical and Gevrey-type wave front sets\nof the fundamental solution are determined, and questions on dispersion,\ndissipation, wave propagation speed and wave packet shape are addressed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce new methods to numerically construct for the first time\nstationary axisymmetric black hole solutions in Einstein-aether theory and\nstudy their properties. The key technical challenge is to impose regularity at\nthe spin-2, 1, and 0 wave mode horizons. Interestingly we find the metric\nhorizon, and various wave mode horizons, are not Killing horizons, having null\ngenerators to which no linear combination of Killing vectors is tangent, and\nwhich spiral from pole to equator or vice versa. Existing phenomenological\nconstraints result in two regions of coupling parameters where the theory is\nviable and some couplings are large; region I with a large twist coupling and\nregion II with also a (somewhat) large expansion coupling. Currently these\nconstraints do not include tests from strong field dynamics, such as\nobservations of black holes and their mergers. Given the large aether\ncoupling(s) one might expect such dynamics to deviate significantly from\ngeneral relativity, and hence to further constrain the theory. Here we argue\nthis is not the case, since for these parameter regions solutions exist where\nthe aether is \"painted\" onto a metric background that is very close to that of\ngeneral relativity. This painting for region I is approximately independent of\nthe large twist coupling, and for region II is also approximately independent\nof the large expansion coupling and normal to a maximal foliation of the\nspacetime. We support this picture analytically for weak fields, and\nnumerically for rotating black hole solutions, which closely approximate the\nKerr metric.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To evaluate the stress level and damage of a reinforced concrete containment\nwall and its reaction to pressure variations, we implemented successive\nultrasonic experiments on the exterior surface of the containment wall in the\ngusset area for three consecutive years. During each experiment, the pressure\ninside the containment wall increased gradually from 0 MPa to 0.43 MPa and then\ndecreased back to 0 Mpa.From the analysis of the ultrasonic coda waves obtained\nin the multiple scattering regime, we performed Coda Wave Interferometry to\ncalculate the apparent velocity changes in the structure (denoted by $dV/V_a$)\nand Coda Wave Decorrelation (DC) measurements to produce 3D cartographies of\nstress and crack distribution. From three source-receiver pairs, located at the\ntop, middle and bottom of the experimental region, we observe that coda waves\ndilate, shrink and remain almost unchanged, respectively. This corresponds to\nthe decreasing, increasing and invariant pressure inside the concrete. The\ncomparison of three years' results demonstrates that the variation of $dV/V_a$\nand DC under the same pressure test increases through the years, which\nindicates the progressive deterioration and aging of the concrete. From a large\ncollection of source-receiver pairs at different times, the spatial-temporal\nvariations of $dV/V_a$ and DC are then used to produce a map of the structural\nvelocity and scattering changes, respectively. We observe a decreasing velocity\non the top part and an increasing in the middle one, which is in line with the\n$dV/V_a$ analysis. The reconstructed scattering changes (or structural changes)\nhighlight the active region during the inflation-deflation procedure,\ncorresponding to the opening and closing (and sometimes the development) of\ncracks. The larger magnitude in 2019 than in 2017 indicates the increasing\ndamage in the concrete.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In cellular reprogramming, almost all epigenetic memories of differentiated\ncells are erased by the overexpression of few genes, regaining pluripotency,\npotentiality for differentiation. Considering the interplay between oscillatory\ngene expression and slower epigenetic modifications, such reprogramming is\nperceived as an unintuitive, global attraction to the unstable manifold of a\nsaddle, which represents pluripotency. The universality of this scheme is\nconfirmed by the repressilator model, and by gene regulatory networks randomly\ngenerated and those extracted from embryonic stem cells.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Impressive progress in the control of atomically thin crystals is now\nenabling the realization of gated structures in which two electrodes are\nseparated by atomic scale distances. The electrical capacitance of these\nstructures is determined by phenomena that are not relevant in capacitors with\nlarger electrode separation. With the aim to analyze these phenomena, we use\nlinear response theory to develop a systematic description of capacitance for\ntwo coupled electron liquids, accounting for the wave nature of electrons, as\nwell as for the effect of both intra and interlayer Coulomb interactions. Our\ntheory leads to a general expression for the electrical capacitance in terms of\nboth intra and interlayer electronic polarizabilities. The intralayer\npolarizability is directly related to the conventional expression for the\nquantum capacitance, whereas the interlayer polarizability term accounts for\ninteraction-induced correlations between charges hosted by opposite capacitor\nplates. We refer to this latter term as to the cross quantum capacitance. We\ndiscuss the implications of the general expression for the capacitance, show\nthat it leads to established results when the effect of interlayer correlations\nis negligible, and that the intra and interlayer polarizabilities play a\ncomparable role for capacitors with very small electrode separation. Using two\ndifferent approaches, we calculate the capacitance in specific cases, and find\nthat the interlayer polarizability can be either positive or negative, so that\nthe cross quantum capacitance can either increase or decrease the total\ncapacitance. We conclude by showing that the cross quantum capacitance term can\nlead to a non-monotonic evolution of the total capacitance with increasing\nseparation between the capacitor plates, which would represent an unambiguous\nmanifestation of the cross quantum capacitance if observed experimentally.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We Use the method of linearly independent polynomials to derive an upper\nbound for the cardinality of a spherical s-distance set F where the sum of\ndistinct inner products of any two elements from F is zero. Our result\ngeneralizes the well-known Gerzon's bound for the cardinality of an equiangular\nspherical set to a significantly broader class of spherical s-distance sets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Nas \\'ultimas d\\'ecadas houve forte mudan\\c{c}a no perfil das\npublica\\c{c}\\~oes em an\\'alise econ\\^omica do direito e nos m\\'etodos\nemp\\'iricos mais utilizados. Por\\'em, nos pr\\'oximos anos a mudan\\c{c}a pode\nser maior e mais r\\'apida, exigindo adapta\\c{c}\\~ao dos pesquisadores da\n\\'area. Neste cap\\'itulo analiso algumas tend\\^encias recentes de mudan\\c{c}a e\noportunidades futuras que se avizinham a partir avan\\c{c}os nas bases de dados,\nestat\\'istica, computa\\c{c}\\~ao e no arcabou\\c{c}o regulat\\'orio dos pa\\'ises.\nAvan\\c{c}o a hip\\'otese de que expans\\~ao de objetos e m\\'etodos favorecer\\'a\nequipes de pesquisa maiores e interdisciplinares e apresento evid\\^encias\ncircunstanciais a partir de dados bibliom\\'etricos de que isso j\\'a vem\nacontecendo no Journal of Law and Economics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We give a simple quantitative condition, involving the \"mapping content\" of\nAzzam--Schul, that implies that a Lipschitz map from a Euclidean space to a\nmetric space must be close to factoring through a tree. Using results of\nAzzam--Schul and the present authors, this gives simple checkable conditions\nfor a Lipschitz map to have a large piece of its domain on which it behaves\nlike an orthogonal projection. The proof involves new lower bounds and\ncontinuity statements for mapping content, and relies on a \"qualitative\"\nversion of the main theorem recently proven by Esmayli--Haj{\\l}asz.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This work is devoted to deriving the Onsager-Machlup action functional for a\nclass of stochastic differential equations with (non-Gaussian) L\\'{e}vy process\nas well as Brownian motion in high dimensions. This is achieved by applying the\nGirsanov transformation for probability measures and then by a path\nrepresentation. The Poincar\\'{e} lemma is essential to handle such path\nrepresentation problem in high dimensions. We provide a sufficient condition on\nthe vector field such that this path representation holds in high dimensions.\nMoreover, this Onsager-Machlup action functional may be considered as the\nintegral of a Lagrangian. Finally, by a variational principle, we investigate\nthe most probable transition pathways analytically and numerically.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Secure ultra-reliable low-latency communication (URLLC) has been recently\ninvestigated with the fundamental limits of finite block length (FBL) regime in\nmind. Analysis has revealed that when eavesdroppers outnumber BS antennas or\nenjoy a more favorable channel condition compared to the legitimate users, base\nstation (BS) transmit power should increase exorbitantly to meet quality of\nservice (QoS) constraints. Channel-induced impairments such as shadowing and/or\nblockage pose a similar challenge. These practical considerations can\ndrastically limit secure URLLC performance in FBL regime. Deployment of an\nintelligent reflecting surface (IRS) can endow such systems with much-needed\nresiliency and robustness to satisfy stringent latency, availability, and\nreliability requirements. We address this problem and propose a joint design of\nIRS platform and secure URLLC network. We minimize the total BS transmit power\nby simultaneously designing the beamformers and artificial noise at the BS and\nphase-shifts at the IRS, while guaranteeing the required number of securely\ntransmitted bits with the desired packet error probability, information\nleakage, and maximum affordable delay. The proposed optimization problem is\nnon-convex and we apply block coordinate descent and successive convex\napproximation to iteratively solve a series of convex sub-problems instead. The\nproposed algorithm converges to a sub-optimal solution in a few iterations and\nattains substantial power saving and robustness compared to baseline schemes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The $^{23}$Na($\\alpha,p$)$^{26}$Mg reaction has been identified as having a\nsignificant impact on the nucleosynthesis of several nuclei between Ne and Ti\nin type-Ia supernovae, and of $^{23}$Na and $^{26}$Al in massive stars. The\nreaction has been subjected to renewed experimental interest recently,\nmotivated by high uncertainties in early experimental data and in the\nstatistical Hauser-Feshbach models used in reaction rate compilations. Early\nexperiments were affected by target deterioration issues and unquantifiable\nuncertainties. Three new independent measurements instead are utilizing inverse\nkinematics and Rutherford scattering monitoring to resolve this. In this work\nwe present directly measured angular distributions of the emitted protons to\neliminate a discrepancy in the assumptions made in the recent reaction rate\nmeasurements, which results in cross sections differing by a factor of 3. We\nderive a new combined experimental reaction rate for the\n$^{23}$Na($\\alpha,p$)$^{26}$Mg reaction with a total uncertainty of 30% at\nrelevant temperatures. Using our new $^{23}$Na($\\alpha,p$)$^{26}$Mg rate, the\n$^{26}$Al and $^{23}$Na production uncertainty is reduced to within 8%. In\ncomparison, using the factor of 10 uncertainty previously recommended by the\nrate compilation STARLIB, $^{26}$Al and $^{23}$Na production was changing by\nmore than a factor of 2. In type-Ia supernova conditions, the impact on\nproduction of $^{23}$Na is constrained to within 15%.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Motivated by the recent discovery of superconductivity in the pentalayer\nnickelate Nd$_6$Ni$_5$O$_{12}$ [Nature Materials 10.1038], we calculate its\nelectronic structure and superconducting critical temperature. We find that\nelectronic correlations are essential for pushing Nd$_6$Ni$_5$O$_{12}$ into the\nsuperconducting doping range as they shift the electron pockets above the Fermi\nenergy. As a consequence, Nd$_6$Ni$_5$O$_{12}$ can be described with a single\n$d_{x^2-y^2}$ orbital per Ni. Instead, for the bilayer nickelate\nNd$_3$Ni$_2$O$_6$ we find correlations to drive the system into a three-orbital\nregime also involving the Ni $d_{xz,yz}$ states. We suggest, however, that\nsingle-orbital physics with optimal doping can be restored by substituting 60%\nof the trivalent Nd or La by tetravalent Zr.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that Cooper pairing can occur intrinsically away from the Fermi\nsurface in $j=3/2$ superconductors with strong spin-orbit coupling and equally\ncurved bands in the normal state. In contrast to conventional pairing between\nspin-$1/2$ electrons, we derive that pairing can happen between inter-band\nelectrons having different magnetic quantum numbers, for instance, $m_j=1/2$\nand $m_j=3/2$. Such superconducting correlations manifest themselves by a pair\nof indirect gap-like structures at finite excitation energies. An observable\nsignature of this exotic pairing is the emergence of a pair of symmetric\nsuperconducting coherence peaks in the density of states at finite energies.\nMoreover, the angular-momentum-resolved density of states in the presence of a\nperturbative Zeeman field reflects the $m_j$ composition of the Cooper pairs.\nWe argue that such finite-energy pairing is a generic feature of $j=3/2$\nsuperconductors, both in presence and absence of inversion symmetry.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In contrast to the generic object, aerial targets are often non-axis aligned\nwith arbitrary orientations having the cluttered surroundings. Unlike the\nmainstreamed approaches regressing the bounding box orientations, this paper\nproposes an effective adaptive points learning approach to aerial object\ndetection by taking advantage of the adaptive points representation, which is\nable to capture the geometric information of the arbitrary-oriented instances.\nTo this end, three oriented conversion functions are presented to facilitate\nthe classification and localization with accurate orientation. Moreover, we\npropose an effective quality assessment and sample assignment scheme for\nadaptive points learning toward choosing the representative oriented reppoints\nsamples during training, which is able to capture the non-axis aligned features\nfrom adjacent objects or background noises. A spatial constraint is introduced\nto penalize the outlier points for roust adaptive learning. Experimental\nresults on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD\nand DIOR-R, demonstrate the efficacy of our proposed approach. The source code\nis availabel at: https://github.com/LiWentomng/OrientedRepPoints.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep networks can learn to accurately recognize objects of a category by\ntraining on a large number of annotated images. However, a meta-learning\nchallenge known as a low-shot image recognition task comes when only a few\nimages with annotations are available for learning a recognition model for one\ncategory. The objects in testing/query and training/support images are likely\nto be different in size, location, style, and so on. Our method, called\nCascaded Feature Matching Network (CFMN), is proposed to solve this problem. We\ntrain the meta-learner to learn a more fine-grained and adaptive deep distance\nmetric by focusing more on the features that have high correlations between\ncompared images by the feature matching block which can align associated\nfeatures together and naturally ignore those non-discriminative features. By\napplying the proposed feature matching block in different layers of the\nfew-shot recognition network, multi-scale information among the compared images\ncan be incorporated into the final cascaded matching feature, which boosts the\nrecognition performance further and generalizes better by learning on\nrelationships. The experiments for few-shot learning on two standard datasets,\n\\emph{mini}ImageNet and Omniglot, have confirmed the effectiveness of our\nmethod. Besides, the multi-label few-shot task is first studied on a new data\nsplit of COCO which further shows the superiority of the proposed feature\nmatching network when performing few-shot learning in complex images. The code\nwill be made publicly available.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the adversarial robustness of CNNs from the perspective of\nchannel-wise activations. By comparing \\textit{non-robust} (normally trained)\nand \\textit{robustified} (adversarially trained) models, we observe that\nadversarial training (AT) robustifies CNNs by aligning the channel-wise\nactivations of adversarial data with those of their natural counterparts.\nHowever, the channels that are \\textit{negatively-relevant} (NR) to predictions\nare still over-activated when processing adversarial data. Besides, we also\nobserve that AT does not result in similar robustness for all classes. For the\nrobust classes, channels with larger activation magnitudes are usually more\n\\textit{positively-relevant} (PR) to predictions, but this alignment does not\nhold for the non-robust classes. Given these observations, we hypothesize that\nsuppressing NR channels and aligning PR ones with their relevances further\nenhances the robustness of CNNs under AT. To examine this hypothesis, we\nintroduce a novel mechanism, i.e., \\underline{C}hannel-wise\n\\underline{I}mportance-based \\underline{F}eature \\underline{S}election (CIFS).\nThe CIFS manipulates channels' activations of certain layers by generating\nnon-negative multipliers to these channels based on their relevances to\npredictions. Extensive experiments on benchmark datasets including CIFAR10 and\nSVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying\nCNNs. \\url{https://github.com/HanshuYAN/CIFS}\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper focuses on the encoding and decoding of Construction D' coding\nlattices that can be used with shaping lattices for power-constrained channels.\nTwo encoding methods and a decoding algorithm for Construction D' lattices are\ngiven. A design of quasi-cyclic low-density parity-check (QC-LDPC) codes to\nform Construction D' lattices is presented. This allows construction of nested\nlattice codes which are good for coding, good for shaping, and have low\ncomplexity encoding and decoding. Numerical results of using $E_8$, $BW_{16}$\nand Leech lattices for shaping a Construction D' lattice indicate that the\nshaping gains 0.65 dB, 0.86 dB and 1.03 dB are preserved, respectively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A vertex coloring is called distinguishing if the identity is the only\nautomorphism that can preserve it. The distinguishing number of a graph is the\nminimum number of colors required for such a coloring. The distinguishing\nthreshold of a graph $G$ is the minimum number of colors $k$ required that any\narbitrary $k$-coloring of $G$ is distinguishing. We prove a statement that\ngives a necessary and sufficient condition for a vertex coloring of the\nCartesian product to be distinguishing. Then we use it to calculate the\ndistinguishing threshold of a Cartesian product graph. Moreover, we calculate\nthe number of non-equivalent distinguishing colorings of grids.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Balanced hypergraph partitioning is a classical NP-hard optimization problem\nwith applications in various domains such as VLSI design, simulating quantum\ncircuits, optimizing data placement in distributed databases or minimizing\ncommunication volume in high-performance computing. Engineering parallel\nheuristics for this problem is a topic of recent research. Most of them are\nnon-deterministic though. In this work, we design and implement a highly\nscalable deterministic algorithm in the state-of-the-art parallel partitioning\nframework Mt-KaHyPar. On our extensive set of benchmark instances, it achieves\nsimilar partition quality and performance as a comparable but non-deterministic\nconfiguration of Mt-KaHyPar and outperforms the only other existing parallel\ndeterministic algorithm BiPart regarding partition quality, running time and\nparallel speedups.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show polynomial-time quantum algorithms for the following problems:\n  (*) Short integer solution (SIS) problem under the infinity norm, where the\npublic matrix is very wide, the modulus is a polynomially large prime, and the\nbound of infinity norm is set to be half of the modulus minus a constant.\n  (*) Learning with errors (LWE) problem given LWE-like quantum states with\npolynomially large moduli and certain error distributions, including bounded\nuniform distributions and Laplace distributions.\n  (*) Extrapolated dihedral coset problem (EDCP) with certain parameters.\n  The SIS, LWE, and EDCP problems in their standard forms are as hard as\nsolving lattice problems in the worst case. However, the variants that we can\nsolve are not in the parameter regimes known to be as hard as solving\nworst-case lattice problems. Still, no classical or quantum polynomial-time\nalgorithms were known for the variants of SIS and LWE we consider. For EDCP,\nour quantum algorithm slightly extends the result of Ivanyos et al. (2018).\n  Our algorithms for variants of SIS and EDCP use the existing quantum\nreductions from those problems to LWE, or more precisely, to the problem of\nsolving LWE given LWE-like quantum states. Our main contribution is solving LWE\ngiven LWE-like quantum states with interesting parameters using a filtering\ntechnique.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a compact, three-dimensional, real-analytic Lorentzian manifold\n$(M,g)$, we prove that the identity component of the conformal group preserves\na metric in the conformal class $[g]$, or $(M,g)$ is conformally flat.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sparse representation of real-life images is a very effective approach in\nimaging applications, such as denoising. In recent years, with the growth of\ncomputing power, data-driven strategies exploiting the redundancy within\npatches extracted from one or several images to increase sparsity have become\nmore prominent. This paper presents a novel image denoising algorithm\nexploiting such an image-dependent basis inspired by the quantum many-body\ntheory. Based on patch analysis, the similarity measures in a local image\nneighborhood are formalized through a term akin to interaction in quantum\nmechanics that can efficiently preserve the local structures of real images.\nThe versatile nature of this adaptive basis extends the scope of its\napplication to image-independent or image-dependent noise scenarios without any\nadjustment. We carry out a rigorous comparison with contemporary methods to\ndemonstrate the denoising capability of the proposed algorithm regardless of\nthe image characteristics, noise statistics and intensity. We illustrate the\nproperties of the hyperparameters and their respective effects on the denoising\nperformance, together with automated rules of selecting their values close to\nthe optimal one in experimental setups with ground truth not available.\nFinally, we show the ability of our approach to deal with practical images\ndenoising problems such as medical ultrasound image despeckling applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that if $u$ is a compactly supported distribution on the complex\nplane such that, for every pair of entire functions $f,g$, \\[ \\langle\nu,f\\overline{g}\\rangle=\\langle u,f\\rangle\\langle u,\\overline{g}\\rangle, \\] then\n$u$ is supported at a single point. As an application, we complete the\nclassification of all weighted Dirichlet spaces on the unit disk that are de\nBranges-Rovnyak spaces by showing that, for such spaces, the weight is\nnecessarily a superharmonic function.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Within condensed-matter systems, strong electronic interactions often lead to\nexotic quantum phases. A recent manifestation of this is the unexpected\nobservation of magnetic quantum oscillations and metallic thermal transport,\nboth properties of systems with Fermi surfaces of itinerant quasiparticles, in\nthe Kondo insulators SmB6 and YbB$_{12}$. To understand these phenomena, it is\ninformative to study their evolution as the energy gap of the Kondo-Insulator\nstate is closed by a large magnetic field. We show here that both the\nquantum-oscillation frequency and the cyclotron mass display a strong field\ndependence in the resulting high-field metallic state in $_{12}$. By tracking\nthe Fermi-surface area, we conclude that the same quasiparticle band gives rise\nto the quantum oscillations in both insulating and metallic states. These data\nare understood most simply using a two-fluid picture where unusual\nquasiparticles, contributing little or nothing to charge transport, coexist\nwith conventional fermions. In the metallic state this leads to a heavy-fermion\nbad metal with negligible magnetoresistance, relatively high resistivity and a\nvery large Kadowaki-Woods ratio, underlining the exotic nature of the fermion\nensemble inhabiting $_{12}$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Parameter estimation and the variable selection are two pioneer issues in\nregression analysis. While traditional variable selection methods require prior\nestimation of the model parameters, the penalized methods simultaneously carry\non parameter estimation and variable select. Therefore, penalized variable\nselection methods are of great interest and have been extensively studied in\nliterature. However, most of the papers in literature are only limited to the\nregression models with uncorrelated error terms and normality assumption. In\nthis study, we combine the parameter estimation and the variable selection in\nregression models with autoregressive error term by using different penalty\nfunctions under heavy tailed error distribution assumption. We conduct a\nsimulation study and a real data example to show the performance of the\nestimators.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a gravitational theory that interpolates between JT gravity, and a\ngravity theory with a fixed boundary Hamiltonian. For this, we consider a\nmatrix integral with the insertion of a Gaussian with variance $\\sigma^2$,\ncentered around a matrix $\\textsf{H}_0$. Tightening the Gaussian renders the\nmatrix integral less random, and ultimately it collapses the ensemble to one\nHamiltonian $\\textsf{H}_0$. This model provides a concrete setup to study\nfactorization, and what the gravity dual of a single member of the ensemble is.\nWe find that as $\\sigma^2$ is decreased, the JT gravity dilaton potential gets\nmodified, and ultimately the gravity theory goes through a series of phase\ntransitions, corresponding to a proliferation of extra macroscopic holes in the\nspacetime. Furthermore, we observe that in the Efetov model approach to random\nmatrices, the non-averaged factorizing theory is described by one simple saddle\npoint.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Waves of misery is a phenomenon where spikes of many node splits occur over\nshort periods of time in tree indexes. Waves of misery negatively affect the\nperformance of tree indexes in insertion-heavy workloads.Waves of misery have\nbeen first observed in the context of the B-tree, where these waves cause\nunpredictable index performance. In particular, the performance of search and\nindex-update operations deteriorate when a wave of misery takes place, but is\nmore predictable between the waves. This paper investigates the presence or\nlack of waves of misery in several R-tree variants, and studies the extent of\nwhich these waves impact the performance of each variant. Interestingly,\nalthough having poorer query performance, the Linear and Quadratic R-trees are\nfound to be more resilient to waves of misery than both the Hilbert and\nR*-trees. This paper presents several techniques to reduce the impact in\nperformance of the waves of misery for the Hilbert and R*-trees. One way to\neliminate waves of misery is to force node splits to take place at regular\ntimes before nodes become full to achieve deterministic performance. The other\nway is that upon splitting a node, do not split it evenly but rather at\ndifferent node utilization factors. This allows leaf nodes not to fill at the\nsame pace. We study the impact of two new techniques to mitigate waves of\nmisery after the tree index has been constructed, namely Regular Elective\nSplits (RES, for short) and Unequal Random Splits (URS, for short). Our\nexperimental investigation highlights the trade-offs in performance of the\nintroduced techniques and the pros and cons of each technique.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study multilevel fermions in an optical lattice described by the Hubbard\nmodel with on site SU($n$)-symmetric interactions. We show that in an\nappropriate parameter regime this system can be mapped onto a spin model with\nall-to-all SU($n$)-symmetric couplings. Raman pulses that address internal spin\nstates modify the atomic dispersion relation and induce spin-orbit coupling,\nwhich can act as a synthetic inhomogeneous magnetic field that competes with\nthe SU($n$) exchange interactions. We investigate the mean-field dynamical\nphase diagram of the resulting model as a function of $n$ and different initial\nconfigurations that are accessible with Raman pulses. Consistent with previous\nstudies for $n=2$, we find that for some initial states the spin model exhibits\ntwo distinct dynamical phases that obey simple scaling relations with $n$.\nMoreover, for $n>2$ we find that dynamical behavior can be highly sensitive to\ninitial intra-spin coherences. Our predictions are readily testable in current\nexperiments with ultracold alkaline-earth(-like) atoms.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We derive a sufficient condition for asymptotic flocking in the Cucker-Smale\nmodel with self-delay (also called reaction delay) and with non-symmetric\ninteraction weights. The condition prescribes smallness of the delay length\nrelative to the decay rate of the inter-agent communication weight. The proof\nis carried out by a bootstrapping argument combining a decay estimate for the\ngroup velocity diameter with a variant of the Gronwall-Halanay inequality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Observed clusters should be modelled by considering the distribution function\nto be a random variable that quantifies the degree of excitation of the\nsystem's normal modes. A system of canonical coordinates for the space of DFs\nis identified so DFs can be weighted in a consistent way.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, by using pinning impulse controller and hybrid controller\nrespectively, the research difficulties of asymptotic synchronization and\nfinite time cluster synchronization of time-varying delayed neural networks are\nstudied. On the ground of Lyapunov stability theorem and Lyapunov-Razumikhin\nmethod, a novel sufficient criterion on asymptotic cluster synchronization of\ntime-varying delayed neural networks is obtained. Utilizing Finite time\nstability theorem and hybrid control technology, a sufficient criterion on\nfinite-time cluster synchronization is also obtained. In order to deal with\ntime-varying delay and save control cost, pinning pulse control is introduced\nto promote the realization of asymptotic cluster synchronization. Following the\nidea of pinning control scheme, we design a progressive hybrid control to\npromote the realization of finite time cluster synchronization. Finally, an\nexample is given to illustrate the theoretical results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider random geometric graphs on the plane characterized by a\nnon-uniform density of vertices. In particular, we introduce a graph model\nwhere $n$ vertices are independently distributed in the unit disc with\npositions, in polar coordinates $(l,\\theta)$, obeying the probability density\nfunctions $\\rho(l)$ and $\\rho(\\theta)$. Here we choose $\\rho(l)$ as a normal\ndistribution with zero mean and variance $\\sigma\\in(0,\\infty)$ and\n$\\rho(\\theta)$ as an uniform distribution in the interval $\\theta\\in [0,2\\pi)$.\nThen, two vertices are connected by an edge if their Euclidian distance is less\nor equal than the connection radius $\\ell$. We characterize the topological\nproperties of this random graph model, which depends on the parameter set\n$(n,\\sigma,\\ell)$, by the use of the average degree $\\left\\langle k\n\\right\\rangle$ and the number of non-isolated vertices $V_\\times$; while we\napproach their spectral properties with two measures on the graph adjacency\nmatrix: the ratio of consecutive eigenvalue spacings $r$ and the Shannon\nentropy $S$ of eigenvectors. First we propose a heuristic expression for\n$\\left\\langle k(n,\\sigma,\\ell) \\right\\rangle$. Then, we look for the scaling\nproperties of the normalized average measure $\\left\\langle \\overline{X}\n\\right\\rangle$ (where $X$ stands for $V_\\times$, $r$ and $S$) over graph\nensembles. We demonstrate that the scaling parameter of $\\left\\langle\n\\overline{V_\\times} \\right\\rangle=\\left\\langle V_\\times \\right\\rangle/n$ is\nindeed $\\left\\langle k \\right\\rangle$; with $\\left\\langle \\overline{V_\\times}\n\\right\\rangle \\approx 1-\\exp(-\\left\\langle k \\right\\rangle)$. Meanwhile, the\nscaling parameter of both $\\left\\langle \\overline{r} \\right\\rangle$ and\n$\\left\\langle \\overline{S} \\right\\rangle$ is proportional to $n^{-\\gamma}\n\\left\\langle k \\right\\rangle$ with $\\gamma\\approx 0.16$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove quantitative stability of isometries on the first Heisenberg group\nwith sub-Riemannian geometry: every $ (1+ \\varepsilon)$-quasi-isometry of the\nJohn domain of the Heisenberg group $ \\mathbb {H} $ is close to some isometry\nwith order of closeness $ \\sqrt{\\varepsilon} + \\varepsilon $ in the uniform\nnorm and with order of closeness $ \\varepsilon $ in the Sobolev norm $L_2^1$.\nHomogeneous dilations show the asymptotic sharpness of the results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We give explicit formulas for the geodesic growth series of a Right Angled\nCoxeter Group (RACG) based on a link-regular graph that is 4-clique free, i.e.\nwithout tetrahedrons\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Topological defects in active liquid crystals can be confined by introducing\ngradients of activity. Here, we examine the dynamical behavior of two defects\nconfined by a sharp gradient of activity that separates an active circular\nregion and a surrounding passive nematic material. Continuum simulations are\nused to explain how the interplay among energy injection into the system,\nhydrodynamic interactions, and frictional forces governs the dynamics of\ntopologically required self-propelling $+1/2$ defects. Our findings are\nrationalized in terms of a phase diagram for the dynamical response of defects\nin terms of activity and frictional damping strength. Different regions of the\nunderlying phase diagram correspond to distinct dynamical modes, namely\nimmobile defects (ID), steady rotation of defects (SR), bouncing defects (TB),\nbouncing-cruising defects (BC), dancing defects (DA), and multiple defects with\nirregular dynamics (MD). These dynamic states raise the prospect of generating\nsynchronized defect arrays for microfluidic applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The performance of a small prototype of a cylindrical drift chamber (CDC)\nused in the COMET Phase-I experiment was studied by using an electron beam. The\nprototype chamber was constructed with alternating all-stereo wire\nconfiguration and operated with the He-iC$_{4}$H$_{10}$ (90/10) gas mixture\nwithout a magnetic field. The drift space-time relation, drift velocity,\nd$E$/d$x$ resolution, hit efficiency, and spatial resolution as a function of\ndistance from the wire were investigated. The average spatial resolution of 150\n$\\mu$m with the hit efficiency of 99% was obtained at applied voltages higher\nthan 1800 V. We have demonstrated that the design and gas mixture of the\nprototype match the operation of the COMET CDC.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Image resolution that has close relations with accuracy and computational\ncost plays a pivotal role in network training. In this paper, we observe that\nthe reduced image retains relatively complete shape semantics but loses\nextensive texture information. Inspired by the consistency of the shape\nsemantics as well as the fragility of the texture information, we propose a\nnovel training strategy named Temporally Resolution Decrement. Wherein, we\nrandomly reduce the training images to a smaller resolution in the time domain.\nDuring the alternate training with the reduced images and the original images,\nthe unstable texture information in the images results in a weaker correlation\nbetween the texture-related patterns and the correct label, naturally enforcing\nthe model to rely more on shape properties that are robust and conform to the\nhuman decision rule. Surprisingly, our approach greatly improves both the\ntraining and inference efficiency of convolutional neural networks. On ImageNet\nclassification, using only 33\\% calculation quantity (randomly reducing the\ntraining image to 112$\\times$112 within 90\\% epochs) can still improve\nResNet-50 from 76.32\\% to 77.71\\%. Superimposed with the strong training\nprocedure of ResNet-50 on ImageNet, our method achieves 80.42\\% top-1 accuracy\nwith saving 37.5\\% calculation overhead. To the best of our knowledge this is\nthe highest ImageNet single-crop accuracy on ResNet-50 under 224$\\times$224\nwithout extra data or distillation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The cosmology of the Twin Higgs requires the breaking of the $\\mathbb{Z}_2$\nsymmetry, but it is still an open question whether this breaking needs to be\nexplicit. In this paper, we study how the Mirror Twin Higgs could be modified\nto be compatible with current cosmological constraints without explicit\n$\\mathbb{Z}_2$ breaking. We first present a simple toy model that can realize\nbaryogenesis without explicit $\\mathbb{Z}_2$ breaking or reaching temperatures\nthat would lead to domain walls. The model can also either solve the\n$N_{\\text{eff}}$ problem and bring the abundance of mirror atoms to an allowed\nlevel or provide the correct dark matter abundance. We then present another\nsimple model that leads to mirror neutron dark matter and thus acceptable dark\nmatter self-interactions. We also include in appendix a series of results on\nenergy exchange between different sectors that might prove useful for other\ncosmological problems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $\\Gamma=q_1\\mathbb{Z}\\oplus q_2 \\mathbb{Z}\\oplus\\cdots\\oplus\nq_d\\mathbb{Z}$, where $q_l\\in \\mathbb{Z}_+$, $l=1,2,\\cdots,d$. Let $\\Delta+V$\nbe the discrete Schr\\\"odinger operator, where $\\Delta$ is the discrete\nLaplacian on $\\mathbb{Z}^d$ and the potential $V:\\mathbb{Z}^d\\to \\mathbb{R}$ is\n$\\Gamma$-periodic. We prove three rigidity theorems for discrete periodic\nSchr\\\"odinger operators in any dimension $d\\geq 3$:\n  (1) if at some energy level, Fermi varieties of the $\\Gamma$-periodic\npotential $V$ and the $\\Gamma$-periodic potential $Y$ are the same (this\nfeature is referred to as {\\it Fermi isospectrality} of $V$ and $Y$), and $Y $\nis a separable function, then $V$ is separable;\n  (2) if potentials $V$ and $Y$ are Fermi isospectral and both\n$V=\\bigoplus_{j=1}^rV_j$ and $Y=\\bigoplus_{j=1}^r Y_j$ are separable functions,\nthen, up to a constant, lower dimensional decompositions $V_j$ and $Y_j$ are\nFloquet isospectral, $j=1,2,\\cdots,r$;\n  (3) if a potential $V$ and the zero potential are Fermi isospectral, then $V$\nis zero.\n  In particular, all conclusions in (1), (2) and (3) hold if we replace the\nassumption \"Fermi isospectrality\" with a stronger assumption \"Floquet\nisospectrality\".\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep Neural Networks (DNNs) have achieved tremendous success for cognitive\napplications. The core operation in a DNN is the dot product between quantized\ninputs and weights. Prior works exploit the weight/input repetition that arises\ndue to quantization to avoid redundant computations in Convolutional Neural\nNetworks (CNNs). However, in this paper we show that their effectiveness is\nseverely limited when applied to Fully-Connected (FC) layers, which are\ncommonly used in state-of-the-art DNNs, as it is the case of modern Recurrent\nNeural Networks (RNNs) and Transformer models.\n  To improve energy-efficiency of FC computation we present CREW, a hardware\naccelerator that implements Computation Reuse and an Efficient Weight Storage\nmechanism to exploit the large number of repeated weights in FC layers. CREW\nfirst performs the multiplications of the unique weights by their respective\ninputs and stores the results in an on-chip buffer. The storage requirements\nare modest due to the small number of unique weights and the relatively small\nsize of the input compared to convolutional layers. Next, CREW computes each\noutput by fetching and adding its required products. To this end, each weight\nis replaced offline by an index in the buffer of unique products. Indices are\ntypically smaller than the quantized weights, since the number of unique\nweights for each input tends to be much lower than the range of quantized\nweights, which reduces storage and memory bandwidth requirements.\n  Overall, CREW greatly reduces the number of multiplications and provides\nsignificant savings in model memory footprint and memory bandwidth usage. We\nevaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x\nspeedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,\na state-of-art computation reuse technique, CREW achieves 2.10x speedup and\n2.08x energy savings on average.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Since most industrial control applications use PID controllers, PID tuning\nand anti-windup measures are significant problems. This paper investigates\ntuning the feedback gains of a PID controller via back-calculation and\nautomatic differentiation tools. In particular, we episodically use a cost\nfunction to generate gradients and perform gradient descent to improve\ncontroller performance. We provide a theoretical framework for analyzing this\nnon-convex optimization and establish a relationship between back-calculation\nand disturbance feedback policies. We include numerical experiments on linear\nsystems with actuator saturation to show the efficacy of this approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a prior probability density $p$ on a compact set $K$ we characterize\nthe probability distribution $q_{\\delta}^*$ on $K$ contained in a Wasserstein\nball $B_{\\delta}(\\mu)$ centered in a given discrete measure $\\mu$ for which the\nrelative-entropy $H(q,p)$ achieves its minimum. This characterization gives us\nan algorithm for computing such distributions efficiently\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Frustrated topological spin textures have unique properties that may enable\nnovel spintronic applications, such as helicity-based information storage and\ncomputing. Here, we report the statics and current-induced dynamics of\ntwo-dimensional (2D) pancake skyrmions in a stack of weakly coupled frustrated\nmagnetic monolayers, which form a three-dimensional (3D) skyrmion string. The\nBloch-type skyrmion string is energetically more stable than its N\\'eel-type\ncounterpart. It can be driven into translational motion by the dampinglike\nspin-orbit torque and shows the damping-dependent skyrmion Hall effect. Most\nnotably, the skyrmion string can be transformed to a dynamically stable bimeron\nstring by the dampinglike spin-orbit torque. The current-induced bimeron string\nrotates stably with respect to its center, which can spontaneously transform\nback to a skyrmion string when the current is switched off. Our results reveal\nunusual physical properties of 3D frustrated spin textures, and may open up\ndifferent possibilities for spintronic applications based on skyrmion and\nbimeron strings.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum criticality emerges from the collective behavior of many interacting\nquantum particles, often at the transition between different phases of matter.\nIt is one of the cornerstones of condensed matter physics, which we access on\nnoisy intermediate-scale (NISQ) quantum devices by leveraging a dynamically\ndriven phenomenon. We probe the critical properties of the one-dimensional\nquantum Ising model on a programmable superconducting quantum chip via a\nKibble-Zurek process, obtain scaling laws, and estimate critical exponents\ndespite inherent sources of errors on the hardware. In addition, we investigate\nhow the improvement of NISQ computers (more qubits, less noise) will\nconsolidate the computation of those universal physical properties. A\none-parameter noise model captures the effect of imperfections and reproduces\nthe experimental data. Its systematic study reveals that the noise, analogously\nto temperature, induces a new length scale in the system. We introduce and\nsuccessfully verify modified scaling laws, directly accounting for the noise\nwithout any prior knowledge. It makes data analyses for extracting physical\nproperties transparent to noise. By understanding how imperfect quantum\nhardware modifies the genuine properties of quantum states of matter, we\nenhance the power of NISQ processors considerably for addressing quantum\ncriticality and potentially other phenomena and algorithms.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A mixture of experts models the conditional density of a response variable\nusing a mixture of regression models with covariate-dependent mixture weights.\nWe extend the finite mixture of experts model by allowing the parameters in\nboth the mixture components and the weights to evolve in time by following\nrandom walk processes. Inference for time-varying parameters in richly\nparameterized mixture of experts models is challenging. We propose a sequential\nMonte Carlo algorithm for online inference and based on a tailored proposal\ndistribution built on ideas from linear Bayes methods and the EM algorithm. The\nmethod gives a unified treatment for mixtures with time-varying parameters,\nincluding the special case of static parameters. We assess the properties of\nthe method on simulated data and on industrial data where the aim is to predict\nsoftware faults in a continuously upgraded large-scale software project.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper initiates a novel research direction in the theory of Diophantine\nequations: define an appropriate version of the equation's size, order all\npolynomial Diophantine equations starting from the smallest ones, and then\nsolve the equations in that order. By combining a new computer-aided procedure\nwith human reasoning, we solved the Hilbert's tenth problem for all polynomial\nDiophantine equations of size less than $31$, where the size is defined in\n(Zidane, 2018). In addition, we solved this problem for all equations of size\nequal to $31$, with a single exception. Further, we solved the Hilbert's tenth\nproblem for all two-variable Diophantine equations of size less than $32$, all\nsymmetric equations of size less than $39$, all three-monomial equations of\nsize less than $45$, and, in each category, identified the explicit smallest\nequations for which the problem remains open. As a result, we derived a list of\nequations that are very simple to write down but which are apparently difficult\nto solve. As we know from the example of Fermat's Last Theorem, such equations\nhave a potential to stimulate the development of new methods in number theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We analyze the diversity gain for a distributed antenna subarray employing\northogonal frequency-division multiplexing (OFDM) in millimeter-wave (mm-Wave)\nmassive multiple-input multiple-output (MIMO) systems. We show that the\ndiversity gain depends on the number of transmitted data streams, the number of\nremote antenna units, and the number of propagation paths between RAUs.\nFurthermore, we show that by using bit-interleaved coded multiple beamforming\n(BICMB), one can achieve the maximum diversity gain in a distributed antenna\nsubarray system. The assumption in both scenarios is that the number of the\nantennas at the transmitter and the receiver are large enough and channel state\ninformation (CSI) is known at the transmitter and the receiver.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We devise a condition strictly between the existence of an $n$-ary and an\n$n{+}1$-ary near-unanimity term. We evaluate exactly the distributivity and\nmodularity levels implied by such a condition.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the structure of the zero set of a finite point charge electrical\nfield $F = (X,Y,Z)$ in $\\mathbb R^3$. Indeed, mostly we focus on a finite point\ncharge electrical field $F =(X,Y)$ in $\\mathbb R^2$. The well-known conjecture\nis that the zero set of $F = (X,Y)$ is finite. We show that this is true in a\nSpecial Case: when the point charges for $F = (X,Y)$ lie on a line. In\naddition, we give fairly complete structural information about the zero sets of\n$X$ and $Y$ for $F = (X,Y)$ in the Special Case. A highlight of the paper\nstates that in the Special Case the zero set of $F = (X,Y)$ contains at most\n$9M^24^M$ points, where $M$ is the number of point charges.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Despite significant improvements on the image generation performance of\nGenerative Adversarial Networks (GANs), generations with low visual fidelity\nstill have been observed. As widely used metrics for GANs focus more on the\noverall performance of the model, evaluation on the quality of individual\ngenerations or detection of defective generations is challenging. While recent\nstudies try to detect featuremap units that cause artifacts and evaluate\nindividual samples, these approaches require additional resources such as\nexternal networks or a number of training data to approximate the real data\nmanifold. In this work, we propose the concept of local activation, and devise\na metric on the local activation to detect artifact generations without\nadditional supervision. We empirically verify that our approach can detect and\ncorrect artifact generations from GANs with various datasets. Finally, we\ndiscuss a geometrical analysis to partially reveal the relation between the\nproposed concept and low visual fidelity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given a specification $\\varphi(X,Y)$ over inputs $X$ and output $Y$, defined\nover a background theory $\\mathbb{T}$, the problem of program synthesis is to\ndesign a program $f$ such that $Y=f(X)$ satisfies the specification $\\varphi$.\nOver the past decade, syntax-guided synthesis (SyGuS) has emerged as a dominant\napproach for program synthesis where in addition to the specification\n$\\varphi$, the end-user also specifies a grammar $L$ to aid the underlying\nsynthesis engine. This paper investigates the feasibility of synthesis\ntechniques without grammar, a sub-class defined as $\\mathbb{T}$-constrained\nsynthesis.\n  We show that $\\mathbb{T}$-constrained synthesis can be reduced to\nDQF($\\mathbb{T}$), i.e., to the problem of finding a witness of a Dependency\nQuantified Formula Modulo Theory. When the underlying theory is the theory of\nbitvectors, the corresponding DQF(BV) problem can be further reduced to\nDependency Quantified Boolean Formulas (DQBF). We rely on the progress in DQBF\nsolving to design DQBF-based synthesizers that outperform the domain-specific\nprogram synthesis techniques, thereby positioning DQBF as a core representation\nlanguage for program synthesis. Our empirical analysis shows that\n$\\mathbb{T}$-constrained synthesis can achieve significantly better performance\nthan syntax-guided approaches. Furthermore, the general-purpose DQBF solvers\nperform on par with domain-specific synthesis techniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This is a sequel to our paper \"Permute, Graph, Map, Derange\", involving\ndecomposable combinatorial labeled structures in the exp-log class of type\na=1/2, 1, 3/2, 2. As before, our approach is to establish how well existing\ntheory matches experimental data and to raise open questions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study non-abelian versions of the Mellin transformations, originally\nintroduced by Gabber-Loeser on complex affine tori. Our main result is a\ngeneralization to the non-abelian context and with arbitrary coefficients of\nthe t-exactness of Gabber-Loeser's Mellin transformation. As an intermediate\nstep, we obtain vanishing results for the Sabbah specialization functors. Our\nmain application is to construct new examples of duality spaces in the sense of\nBieri-Eckmann, generalizing results of Denham-Suciu.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Machine learning in medicine leverages the wealth of healthcare data to\nextract knowledge, facilitate clinical decision-making, and ultimately improve\ncare delivery. However, ML models trained on datasets that lack demographic\ndiversity could yield suboptimal performance when applied to the\nunderrepresented populations (e.g. ethnic minorities, lower social-economic\nstatus), thus perpetuating health disparity. In this study, we evaluated four\nclassifiers built to predict Hyperchloremia - a condition that often results\nfrom aggressive fluids administration in the ICU population - and compared\ntheir performance in racial, gender, and insurance subgroups. We observed that\nadding social determinants features in addition to the lab-based ones improved\nmodel performance on all patients. The subgroup testing yielded significantly\ndifferent AUC scores in 40 out of the 44 model-subgroup, suggesting disparities\nwhen applying ML models to social determinants subgroups. We urge future\nresearchers to design models that proactively adjust for potential biases and\ninclude subgroup reporting in their studies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Micro- and nano-scale systems driven by rapid changes in control parameters\n(control protocols) dissipate significant energy. In the fast-protocol limit,\nwe find that protocols that minimize dissipation at fixed duration are\nuniversally given by a two-step process, jumping to and from a point that\nbalances jump size with fast relaxation. Jump protocols could be exploited by\nmolecular machines or thermodynamic computing to improve energetic efficiency,\nand implemented in nonequilibrium free-energy estimation to improve accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Scientific claim verification is a unique challenge that is attracting\nincreasing interest. The SCIVER shared task offers a benchmark scenario to test\nand compare claim verification approaches by participating teams and consists\nin three steps: relevant abstract selection, rationale selection and label\nprediction. In this paper, we present team QMUL-SDS's participation in the\nshared task. We propose an approach that performs scientific claim verification\nby doing binary classifications step-by-step. We trained a BioBERT-large\nclassifier to select abstracts based on pairwise relevance assessments for each\n<claim, title of the abstract> and continued to train it to select rationales\nout of each retrieved abstract based on <claim, sentence>. We then propose a\ntwo-step setting for label prediction, i.e. first predicting \"NOT_ENOUGH_INFO\"\nor \"ENOUGH_INFO\", then label those marked as \"ENOUGH_INFO\" as either \"SUPPORT\"\nor \"CONTRADICT\". Compared to the baseline system, we achieve substantial\nimprovements on the dev set. As a result, our team is the No. 4 team on the\nleaderboard.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The transfer of angular momentum carried by photons into a microobject has\nbeen widely exploited to achieve the actuation of the microobject. However,\nthis scheme is fundamentally defective in nonliquid environments as a result of\nthe scale gap between friction forces ($\\mu$N) and optical forces (pN). To\nbypass this challenge, the researchers have recently proposed to take advantage\nof elastic waves based on opto-thermo-mechanical effects [1-4]. Grounded on\nthis insight, we here demonstrate and characterize the in-plane rotation of a\ngold nanoplate in its surface contacting with a microfiber, driven by\nnanosecond laser pulses, which has not been explored before. Furthermore, we\nexamine the underlying physical mechanisms and highlight the essential role of\nthe spatial gradient of optical absorption. The combined experimental and\ntheoretical results offer new insights into the study of the light-induced\nactuation of the microobjects in nonliquid environments, an emerging field far\nfrom being mature in both comprehensive understanding and practical\napplications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Internal Linear Combination (ILC) is widely used to extract the cosmic\nmicrowave background (CMB) signal from multi-frequency observation maps,\nespecially for Satellite experiments with quasi-full sky coverage. We extend\nILC method to CMB polarization map analysis with a small sky patch which is\nespecially typical for ground-based experiments, by combing ILC with a template\ncleaning method which can give pure $B$ map free from $EB$ leakage caused by\npartial sky coverage. The feature of our methods is that we do the ILC analysis\non pseudo-scalar $B$ maps, and the advantage is that it totally avoids the\nimpact of $EB$ leakage on ILC, so that it can improve the efficiency of\ncomponent separation dramatically. We demonstrate our methods with mock data of\na future ground-based experiment with a deep survey on a clean patch in the\nnorthern sky, and the results show that the level of foreground residual can be\nwell controlled, it biases the tensor to scalar ratio ($r$) at the order of\n$10^{-3}$ which is comparable to the statistical error by noise.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The velocity of dislocations is derived analytically to incorporate and\npredict the intriguing effects induced by the preferential solute segregation\nand Cottrell atmospheres in both two-dimensional and three-dimensional binary\nsystems of various crystalline symmetries. The corresponding mesoscopic\ndescription of defect dynamics is constructed through the amplitude formulation\nof the phase-field crystal model which has been shown to accurately capture\nelasticity and plasticity in a wide variety of systems. Modifications of the\nPeach-Koehler force as a result of solute concentration variations and\ncompositional stresses are presented, leading to interesting new predictions of\ndefect motion due to effects of Cottrell atmospheres. These include the\ndeflection of dislocation glide paths, the variation of climb speed and\ndirection, and the change or prevention of defect annihilation, all of which\nplay an important role in determining the fundamental behaviors of complex\ndefect network and dynamics. The analytic results are verified by numerical\nsimulations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the COVID-19 pandemic, refugees' access to information has become\nincreasingly important given the rapid change in the scientific and public\nhealth knowledge-base. However, this access is complicated by social distancing\nrequirements that disrupt traditional in-person communication. Many refugees\nmust then rely on alternative information sources to stay informed. Differences\nin media types and information sources in turn may be related to anxieties\narising from the virus and perceptions of others' adherence to recommended\nprotective behaviors. We examine these relationships with survey data from\n1,000 refugees living in both camps and non-camp settings in Kenya. Using logit\nmodels, we test relationships between information source and anxiety and the\neffect of these variables on refugees' expected behaviors of community members.\nOur primary contributions include the finding that information sources\nconsistently exacerbate (e.g., Facebook) or ameliorate (e.g., news from the\ninternet) different anxieties, or can have mixed effects (e.g., radio). We also\nfind that anxiety and information have significant impacts on refugees'\nexpectations of compliance by others and that, whether between camps or between\ncamps and non-camp locales, findings vary by location. Our results have\nimplications for refugee media and infectious disease anxiety scholarship as\nwell as for managing infectious disease response.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Neural text generation models are likely to suffer from the low-diversity\nproblem. Various decoding strategies and training-based methods have been\nproposed to promote diversity only by exploiting contextual features, but\nrarely do they consider incorporating syntactic structure clues. In this work,\nwe propose using linguistic annotation, i.e., part-of-speech (POS), to guide\nthe text generation. In detail, we introduce POS Guided Softmax to explicitly\nmodel two posterior probabilities: (i) next-POS, and (ii) next-token from the\nvocabulary of the target POS. A POS Guided Sampling strategy is further\nproposed to address the low-diversity problem by enriching the diversity of\nPOS. Extensive experiments and human evaluations show that, compared with\nexisting state-of-the-art methods, our POS Guided Softmax and Sampling (POSG)\ncan generate more diverse text while maintaining comparable quality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The large amount of deployed smart devices put tremendous traffic pressure on\nnetworks. Caching at the edge has been widely studied as a promising technique\nto solve this problem. To further improve the successful transmission\nprobability (STP) of cache-enabled cellular networks (CEN), we combine the\ncooperative transmission technique with CEN and propose a novel transmission\nscheme. Local channel state information (CSI) is introduced at each cooperative\nbase station (BS) to enhance the strength of the signal received by the user. A\ntight approximation for the STP of this scheme is derived using tools from\nstochastic geometry. The optimal content placement strategy of this scheme is\nobtained using a numerical method to maximize the STP. Simulation results\ndemonstrate the optimal strategy achieves significant gains in STP over several\ncomparative baselines with the proposed scheme.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove tight H\\\"olderian error bounds for all $p$-cones. Surprisingly, the\nexponents differ in several ways from those that have been previously\nconjectured; moreover, they illuminate $p$-cones as a curious example of a\nclass of objects that possess properties in 3 dimensions that they do not in 4\nor more. Using our error bounds, we analyse least squares problems with\n$p$-norm regularization, where our results enable us to compute the\ncorresponding KL exponents for previously inaccessible values of $p$. Another\napplication is a (relatively) simple proof that most $p$-cones are neither\nself-dual nor homogeneous. Our error bounds are obtained under the framework of\nfacial residual functions, and we expand it by establishing for general cones\nan optimality criterion under which the resulting error bound must be tight.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent estimates of the characteristics of Planet Nine have suggested that it\ncould be closer than originally assumed. Such a Planet Nine would also be\nbrighter than originally assumed, suggesting the possibility that it has\nalready been observed in wide-field moderate-depth surveys. We search for\nPlanet Nine in the Zwicky Transient Facility public archive and find no\ncandidates. Using known asteroids to calculate the magnitude limit of the\nsurvey, we find that we should have detected Planet Nine throughout most of the\nnorthern portion of its predicted orbit -- including within the galactic plane\n-- to a 95% detection efficiency of approximately $V=20.5$. To aid in\nunderstanding detection limits for this and future analyses, we present a\nfull-sky synthetic Planet Nine population drawn from a statistical sampling of\npredicted Planet Nine orbits. We use this reference population to estimate that\nthis survey rules out 56% of predicted Planet Nine phase space, and we\ndemonstrate how future analyses can use the same synthetic population to\ncontinue to constrain the amount of parameter space effectively searched for\nPlanet Nine.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Substantial increase in the use of Electronic Health Records (EHRs) has\nopened new frontiers for predictive healthcare. However, while EHR systems are\nnearly ubiquitous, they lack a unified code system for representing medical\nconcepts. Heterogeneous formats of EHR present a substantial barrier for the\ntraining and deployment of state-of-the-art deep learning models at scale. To\novercome this problem, we introduce Description-based Embedding, DescEmb, a\ncode-agnostic description-based representation learning framework for\npredictive modeling on EHR. DescEmb takes advantage of the flexibility of\nneural language understanding models while maintaining a neutral approach that\ncan be combined with prior frameworks for task-specific representation learning\nor predictive modeling. We tested our model's capacity on various experiments\nincluding prediction tasks, transfer learning and pooled learning. DescEmb\nshows higher performance in overall experiments compared to code-based\napproach, opening the door to a text-based approach in predictive healthcare\nresearch that is not constrained by EHR structure nor special domain knowledge.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Single quantum dots and other materials exhibit irregular switching between\non and off states; these on-off states follow power-law statistics giving rise\nto 1/f noise. We transfer this phenomenon (also referred to as on-off\nintermittency) to the generation and recombination (g-r) process in\nsemiconductor materials. In addition to g-r noise we obtain 1/f noise that can\nbe provided in the form of the Hooge relation. The predicted Hooge coefficient\ndepends on the parameters of the g-r noise and on the parameters of the\nintermittency. Due to the power-law distribution of the on-times, the\ncoefficient for intermittency shows a smooth dependence on time t. We also\nsuggest an alternative form of the 1/f noise formula by Hooge relating the 1/f\nnoise to the number of centers (such as donor or trap atoms) rather than to the\nnumber of charge carriers as defined by Hooge.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Learning and generalizing to novel concepts with few samples (Few-Shot\nLearning) is still an essential challenge to real-world applications. A\nprinciple way of achieving few-shot learning is to realize a model that can\nrapidly adapt to the context of a given task. Dynamic networks have been shown\ncapable of learning content-adaptive parameters efficiently, making them\nsuitable for few-shot learning. In this paper, we propose to learn the dynamic\nkernels of a convolution network as a function of the task at hand, enabling\nfaster generalization. To this end, we obtain our dynamic kernels based on the\nentire task and each sample and develop a mechanism further conditioning on\neach individual channel and position independently. This results in dynamic\nkernels that simultaneously attend to the global information whilst also\nconsidering minuscule details available. We empirically show that our model\nimproves performance on few-shot classification and detection tasks, achieving\na tangible improvement over several baseline models. This includes\nstate-of-the-art results on 4 few-shot classification benchmarks:\nmini-ImageNet, tiered-ImageNet, CUB and FC100 and competitive results on a\nfew-shot detection dataset: MS COCO-PASCAL-VOC.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For a fixed planar graph $H$, let\n$\\operatorname{\\mathbf{N}}_{\\mathcal{P}}(n,H)$ denote the maximum number of\ncopies of $H$ in an $n$-vertex planar graph. In the case when $H$ is a cycle,\nthe asymptotic value of $\\operatorname{\\mathbf{N}}_{\\mathcal{P}}(n,C_m)$ is\ncurrently known for $m\\in\\{3,4,5,6,8\\}$. In this note, we extend this list by\nestablishing $\\operatorname{\\mathbf{N}}_{\\mathcal{P}}(n,C_{10})\\sim(n/5)^5$ and\n$\\operatorname{\\mathbf{N}}_{\\mathcal{P}}(n,C_{12})\\sim(n/6)^6$. We prove this\nby answering the following question for $m\\in\\{5,6\\}$, which is interesting in\nits own right: which probability mass $\\mu$ on the edges of some clique\nmaximizes the probability that $m$ independent samples from $\\mu$ form an\n$m$-cycle?\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Fundamental astrophysical parameters have been derived for Be 55 open cluster\nbased on UBVI CCD photometric data, observed with the AZT-22 1.5m telescope at\nMaidanak Astronomical Observatory in Uzbekistan. The mean reddening is obtained\nas E(B-V)=1.77+-0.10 mag from early type members. The zero age main sequence\nfitting in the Q(VA)- Q0 diagrams indicates the distance modulus, (V0 -\nMV)=12.4+-0.20 mag (d=3.02+-0.28 kpc). This photometric distance is consistent\nwith the distances of Gaia EDR3 (d=3.09+-0.16 kpc) and period-luminosity\nrelation (d=2.78+-0.32 kpc) of its Cepheid S5 within the uncertainties. This\ndistance also locates the cluster near the Perseus spiral arm. The Geneva\nisochrone fittings to the Hertzsprung-Russell diagram and observational\ncolourmagnitude diagrams derive turn-off age, 85+-13 Myr, by taking care five\nred supergiants/bright giants. The possible inconsistences on the locations of\nthe bright giants with the rotating/non-rotating isochrones may be due to both\nthe age spread of stars in young open clusters and the diversity in rotational\nvelocities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Using Chandra observations, we derive the $Y_{\\rm X}$ proxy and associated\ntotal mass measurement, $M_{500}^{\\rm Y_X}$, for 147 clusters with $z \\leq\n0.35$ from the Planck Early Sunyaev-Zel'dovich catalog, and for 80 clusters\nwith $z \\leq 0.30$ from an X-ray flux-limited sample. We re-extract the Planck\n$Y_{\\rm SZ}$ measurements and obtain the corresponding mass proxy,\n$M_{500}^{\\rm SZ}$, from the full Planck mission maps, minimizing the Malmquist\nbias due to observational scatter. The masses re-extracted using the more\nprecise X-ray position and characteristic size agree with the published PSZ2\nvalues, but yield a significant reduction in the scatter (by a factor of two)\nin the $M_{500}^{\\rm SZ}$-$M_{500}^{\\rm X}$ relation. The slope is\n$0.93\\pm0.03$, and the median ratio, $M_{500}^{\\rm SZ}/M_{500}^{\\rm X}=\n0.91\\pm0.01$, is within the expectations from known X-ray calibration\nsystematics. The $Y_{\\rm SZ}/Y_{\\rm X}$ ratio is $0.88\\pm0.02$, in good\nagreement with predictions from cluster structure, and implying a low level of\nclumpiness. In agreement with the findings of the Planck Collaboration, the\nslope of the $Y_{\\rm SZ}$-$D_{\\rm A}^{-2} Y_{X}$ flux relation is significantly\nless than unity ($0.89\\pm0.01$). Using extensive simulations, we show that this\nresult is not due to selection effects, intrinsic scatter, or covariance\nbetween quantities. We demonstrate analytically that changing the $Y_{\\rm\nSZ}$-$Y_{X}$ relation from apparent flux to intrinsic properties results in a\nbest-fit slope that is closer to unity and increases the dispersion about the\nrelation. The redistribution resulting from this transformation implies that\nthe best fit parameters of the $M_{500}^{\\rm SZ}$-$M_{500}^{\\rm X}$ relation\nwill be sample-dependent.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Extreme events gain the attention of researchers due to their utmost\nimportance in various contexts ranging from finance to climatology. This brings\nsuch recurrent events to the limelight of attention in interdisciplinary\nresearch. A comprehensive review of recent progress is provided to capture\nrecent improvements in analyzing such very high-amplitude events from the point\nof view of dynamical systems and random walkers. We emphasize, in detail, the\nmechanisms responsible for the emergence of such events in complex systems.\nSeveral mechanisms that contribute to the occurrence of extreme events have\nbeen elaborated that investigate the sources of instabilities leading to them.\nIn addition, we discuss the prediction of extreme events from two different\ncontexts, using dynamical instabilities and data-based machine learning\nalgorithms. Tracking of instabilities in the phase space is not always feasible\nand precise knowledge of the dynamics of extreme events does not necessarily\nhelp in forecasting extreme events. Moreover, in most studies on\nhigh-dimensional systems, only a few degrees of freedom participate in extreme\nevents' formation. Thus, the notable inclusion of prediction through machine\nlearning is of enormous significance, particularly for those cases where the\ngoverning equations of the model are explicitly unavailable. Besides, random\nwalks on complex networks can represent several transport processes, and\nexceedances of the flux of walkers above a prescribed threshold may describe\nextreme events. We unveil the theoretical studies on random walkers with their\nenormous potential for applications in reducing extreme events. We cover the\npossible controlling strategies, which may be helpful to mitigate extreme\nevents in physical situations like traffic jams, heavy load of web requests,\ncompetition for shared resources, floods in the network of rivers, and many\nmore.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Dielectric-loaded accelerating (DLA) structures are being studied as an\nalternative to conventional disk-loaded copper structures to produce the high\naccelerating gradient. This paper presents the design, fabrication and\nlow-power RF measurement of an externally-powered X-band DLA structure with a\ndielectric constant epsilon_r=16.66 and a loss tangent tan_delta=0.0000343. A\ndielectric matching section for coupling the RF power from a circular waveguide\nto an X-band DLA structure consists of a very compact dielectric disk with a\nwidth of 2.035 mm and a tilt angle of 60 degree, resulting in a broadband\ncoupling at a low RF field which has the potential to survive in the high-power\nenvironment. Based on simulation studies, a prototype of the DLA structure was\nfabricated. Results from bench measurements and their comparison with design\nvalues are presented. The detailed analysis on the fabrication error which may\ncause the discrepancy between the RF measurements and simulations is also\ndiscussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Magnetic skyrmions and their anti-particles, the antiskyrmions, are stable\nmagnetic solitons existing down to the nanometer scale. Their stability and\nsize as well as the possibility to propel them by, e.g., electric currents make\nthem promising candidates for the use in memory devices, such as racetrack\nmemories. Skyrmions and antiskyrmions share those same advantages individually,\nbut may annihilate each other when they coexist in the same device. Yet,\ncombining them to represent logical bits of 0 (skyrmion) and 1 (antiskyrmion)\nin one device opens new possibilities to create new types of densely packed\nracetrack memory devices. For this, a controlled creation and annihilation\nprocedure, i.e., a writing or deletion operation, is necessary. Here, we\npropose a method to create arbitrary sequences of coexisting skyrmions and\nantiskyrmions by rotations of the magnetic moments at the edge of a rectangular\nslab. The skyrmions and antiskyrmions remain stable and do not annihilate each\nother.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a new, fast, and easy to use tool for modelling light and radial\nvelocity curves of close eclipsing binaries with built-in methods for solving\nan inverse problem. The main goal of ELISa (Eclipsing binary Learning and\nInteractive System) is to provide an acceptable compromise between\ncomputational speed and precision during the fitting of light curves and radial\nvelocities of eclipsing binaries. The package is entirely written in the Python\nprogramming language in a modular fashion, making it easy to install, modify,\nand run on various operating systems. ELISa implements Roche geometry and the\ntriangulation process to model a surface of the eclipsing binary components,\nwhere the surface parameters of each surface element are treated separately.\nSurface symmetries and approximations based on the similarity between surface\ngeometries were used to reduce the runtime during light curve calculation\nsignificantly. ELISa implements the least square trust region reflective\nalgorithm and Markov-chain Monte Carlo optimisation methods to provide the\nbuilt-in capability to determine parameters of the binary system from\nphotometric observations and radial velocities. The precision and speed of the\nlight curve generator were evaluated using various benchmarks. We conclude that\nELISa maintains an acceptable level of accuracy to analyse data from\nground-based and space-based observations, and it provides a significant\nreduction in computational time compared to the current widely used tools for\nmodelling eclipsing binaries.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we derive the boundary Schr\\\"{o}dinger (functional) equation\nfor the wave function of a quantum gravity system on a manifold with boundaries\ntogether with a new constraint equation defined on the timelike boundary. From\na detailed analysis of the gravity boundary condition on the spatial boundary,\nwe find that while the lapse and the shift functions are independent Lagrange\nmultipliers on the bulk, on the spatial boundary, these two are related;\nnamely, they are not independent. In the Hamiltonian ADM formalism, a new\nLagrange multiplier, solving the boundary conditions involving the lapse and\nthe shift functions evaluated on the spatial boundary, is introduced. The\nclassical equation of motion associated with this Lagrange multiplier turns out\nto be an identity when evaluated on a classical solution of Einstein's\nequations. On the other hand, its quantum counterpart is a constraint equation\ninvolving the gravitational degrees of freedom defined only on the boundary.\nThis constraint has not been taken into account before when studying the\nquantum gravity Schr\\\"{o}dinger evolution on manifolds with boundaries.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It is well known that sets of $p$-capacity zero are removable for bounded\n$p$-harmonic functions, but on metric spaces there are examples of removable\nsets of positive capacity. In this paper, we show that this can happen even on\nunweighted $\\mathbf{R}^n$ when $n > p$, although only in very special cases. A\ncomplete characterization of removable singularities for bounded\n$\\mathcal{A}$-harmonic functions on weighted $\\mathbf{R}^n$, $n \\ge 1$, is also\ngiven, where the weight is $p$-admissible. The same characterization is also\nshown to hold for bounded quasiharmonic functions on weighted $\\mathbf{R}^n$,\n$n \\ge 2$, as well as on unweighted $\\mathbf{R}$. For bounded\n$\\mathcal{A}$-superharmonic functions and bounded quasisuperharmonic functions\non weighted $\\mathbf{R}^n$, $n \\ge 2$, we show that relatively closed sets are\nremovable if and only if they have zero capacity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We explore the interactions between a quark anti-quark pair in a thermal\nmedium based on lattice QCD ensembles with $N_f = 2+1$ dynamical HISQ flavors.\nOur dataset spans the phenomenologically relevant temperature range between\nT=140MeV-2GeV based on lattice sizes $N_\\tau=10,12$ and $16$, with an aspect\nratio of $N_\\sigma/N_\\tau=4$. The peak position $\\Omega$ and the width $\\Gamma$\nof the spectral function of Wilson-line correlators in Coulomb gauge is\ncomputed. We assess the information content in the correlation functions and\ndeploy three complementary strategies to reconstruct spectral information:\nmodel fits, Pad\\'e approximation and the Bayesian BR method. Limitations of\neach approach are carefully assessed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Using the Feynman-Kac formula, a work fluctuation theorem for a Brownian\nparticle in a nonconfining potential, e.g., a potential well with finite depth,\nis derived. The theorem yields aninequality that puts a lower bound on the\naverage work needed to change the potential in time.In comparison to the\nJarzynski equality, which holds for confining potentials, an additional\ntermdescribing a form of energy related to the never ending diffusive expansion\nappears.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Process discovery aims to learn a process model from observed process\nbehavior. From a user's perspective, most discovery algorithms work like a\nblack box. Besides parameter tuning, there is no interaction between the user\nand the algorithm. Interactive process discovery allows the user to exploit\ndomain knowledge and to guide the discovery process. Previously, an incremental\ndiscovery approach has been introduced where a model, considered to be under\nconstruction, gets incrementally extended by user-selected process behavior.\nThis paper introduces a novel approach that additionally allows the user to\nfreeze model parts within the model under construction. Frozen sub-models are\nnot altered by the incremental approach when new behavior is added to the\nmodel. The user can thus steer the discovery algorithm. Our experiments show\nthat freezing sub-models can lead to higher quality models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Permutation Pattern Matching problem asks, given two permutations\n$\\sigma$ on $n$ elements and $\\pi$, whether $\\sigma$ admits a subsequence with\nthe same relative order as $\\pi$ (or, in the counting version, how many such\nsubsequences are there). This natural problem was shown by Bose, Buss and Lubiw\n[IPL 1998] to be NP-complete, and hence we should seek exact exponential time\nsolutions. The asymptotically fastest such solution up to date, by Berendsohn,\nKozma and Marx [IPEC 2019], works in $\\mathcal{O}(1.6181^n)$ time. We design a\nsimple and faster $\\mathcal{O}(1.415^{n})$ time algorithm for both the\ndetection and the counting version. We also prove that this is optimal among a\ncertain natural class of algorithms.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article, we study the whole theory of regularized learning for\ngeneralized data in Banach spaces including representer theorems, approximation\ntheorems, and convergence theorems. The generalized input data are composed of\nlinear functionals in the predual spaces of the Banach spaces to represent the\ndiscrete local information of different engineering and physics models. The\ngeneralized data and the multi-loss functions are used to compute the empirical\nrisks, and the regularized learning is to minimize the regularized empirical\nrisks over the Banach spaces. Even if the original problems are unknown or\nunformulated, then the exact solutions of the original problems are\napproximated globally by the regularized learning. In the proof of the\nconvergence theorems, the strong convergence condition is replaced to the weak\nconvergence condition with the additional checkable condition which is\nindependent of the original problems. The theorems of the regularized learning\ncan be used to solve many problems of machine learning such as support vector\nmachines and neural networks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Vienna today is one of the capitals for the research on foundations of\nquantum physics. In this paper we reconstruct the main historical steps of the\ndevelopment of modern physics in Vienna, with an emphasis on quantum\nfoundations. We show that the two main intuitive reasons, namely the influence\nof E. Schr\\\"odinger and the initiatives of A. Zeilinger in more recent years,\ncannot alone be held accountable for today's outstanding research landscape on\nfoundation of quantum mechanics in Vienna. We instead show that the connection\nbetween physics and philosophy in Vienna always had an exceptional strength,\nand that this played a major role in establishing the prolific field of quantum\nfoundations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The problem of finding the unique low dimensional decomposition of a given\nmatrix has been a fundamental and recurrent problem in many areas. In this\npaper, we study the problem of seeking a unique decomposition of a low rank\nmatrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation.\nSpecifically, we consider $Y = A X\\in \\mathbb{R}^{p\\times n}$ where the matrix\n$A\\in \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and\nthe matrix $X\\in \\mathbb{R}^{r\\times n}$ is element-wise sparse. We prove that\nthis sparse decomposition of $Y$ can be uniquely identified, up to some\nintrinsic signed permutation. Our approach relies on solving a nonconvex\noptimization problem constrained over the unit sphere. Our geometric analysis\nfor the nonconvex optimization landscape shows that any {\\em strict} local\nsolution is close to the ground truth solution, and can be recovered by a\nsimple data-driven initialization followed with any second order descent\nalgorithm. At last, we corroborate these theoretical results with numerical\nexperiments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The current paper addresses relevant network security vulnerabilities\nintroduced by network devices within the emerging paradigm of Internet of\nThings (IoT) as well as the urgent need to mitigate the negative effects of\nsome types of Distributed Denial of Service (DDoS) attacks that try to explore\nthose security weaknesses. We design and implement a Software-Defined Intrusion\nDetection System (IDS) that reactively impairs the attacks at its origin,\nensuring the normal operation of the network infrastructure. Our proposal\nincludes an IDS that automatically detects several DDoS attacks, and then as an\nattack is detected, it notifies a Software Defined Networking (SDN) controller.\nThe current proposal also downloads some convenient traffic forwarding\ndecisions from the SDN controller to network devices. The evaluation results\nsuggest that our proposal timely detects several types of cyber-attacks based\non DDoS, mitigates their negative impacts on the network performance, and\nensures the correct data delivery of normal traffic. Our work sheds light on\nthe programming relevance over an abstracted view of the network infrastructure\nto timely detect a Botnet exploitation, mitigate malicious traffic at its\nsource, and protect benign traffic.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We use the continuum micromagnetic framework to derive the formulas for\ncompact skyrmion lifetime due to thermal noise in ultrathin ferromagnetic films\nwith relatively weak interfacial Dzyaloshinskii-Moriya interaction. In the\nabsence of a saddle point connecting the skyrmion solution to the ferromagnetic\nstate, we interpret the skyrmion collapse event as ``capture by an absorber''\nat microscale. This yields an explicit Arrhenius collapse rate with both the\nbarrier height and the prefactor as functions of all the material parameters,\nas well as the dynamical paths to collapse.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the optical properties of the solar gravitational lens (SGL) while\ntreating the Sun as an extended, axisymmetric and rotating body. The\ngravitational field of the Sun is represented using a set of zonal harmonics.\nWe develop an analytical description of the intensity of light that is observed\nin the image plane in the strong interference region of a realistic SGL. This\nformalism makes it possible to model not only the point-spread function of\npoint sources, but also actual observables, images that form in the focal plane\nof an imaging telescope positioned in the image plane. Perturbations of the\nmonopole gravitational field of the Sun are dominated by the solar quadrupole\nmoment, which results in forming an astroid caustic on the image plane.\nConsequently, an imaging telescope placed inside the astroid caustic observes\nfour bright spots, forming the well-known pattern of an Einstein cross. The\nrelative intensities and positions of these spots change as the telescope is\nmoved in the image plane, with spots merging into bright arcs when the\ntelescope approaches the caustic boundary. Outside the astroid caustic, only\ntwo spots remain and the observed pattern eventually becomes indistinguishable\nfrom the imaging pattern of a monopole lens at greater distances from the\noptical axis. We present results from extensive numerical simulations, forming\nthe basis of our ongoing study of prospective exoplanet imaging with the SGL.\nThese results are also applicable to describe a large class of gravitational\nlensing scenarios involving axisymmetric lenses that can be represented using\nzonal harmonics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The present work proposes an approach for fluid-solid and contact interaction\nproblems including thermo-mechanical coupling and reversible phase transitions.\nThe solid field is assumed to consist of several arbitrarily-shaped,\nundeformable but mobile rigid bodies, that are evolved in time individually and\nallowed to get into mechanical contact with each other. The fluid field\ngenerally consists of multiple liquid or gas phases. All fields are spatially\ndiscretized using the method of smoothed particle hydrodynamics (SPH). This\napproach is especially suitable in the context of continually changing\ninterface topologies and dynamic phase transitions without the need for\nadditional methodological and computational effort for interface tracking as\ncompared to mesh- or grid-based methods. Proposing a concept for the\nparallelization of the computational framework, in particular concerning a\ncomputationally efficient evaluation of rigid body motion, is an essential part\nof this work. Finally, the accuracy and robustness of the proposed framework is\ndemonstrated by several numerical examples in two and three dimensions,\ninvolving multiple rigid bodies, two-phase flow, and reversible phase\ntransitions, with a focus on two potential application scenarios in the fields\nof engineering and biomechanics: powder bed fusion additive manufacturing\n(PBFAM) and disintegration of food boluses in the human stomach. The efficiency\nof the parallel computational framework is demonstrated by a strong scaling\nanalysis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A big part of achieving Artificial General Intelligence(AGI) is to build a\nmachine that can see and listen like humans. Much work has focused on designing\nmodels for image classification, video classification, object detection, pose\nestimation, speech recognition, etc., and has achieved significant progress in\nrecent years thanks to deep learning. However, understanding the world is not\nenough. An AI agent also needs to know how to talk, especially how to\ncommunicate with a human. While perception (vision, for example) is more common\nacross animal species, the use of complicated language is unique to humans and\nis one of the most important aspects of intelligence.\n  In this thesis, we focus on generating textual output given visual input. In\nChapter 3, we focus on generating the referring expression, a text description\nfor an object in the image so that a receiver can infer which object is being\ndescribed. We use a comprehension machine to directly guide the generated\nreferring expressions to be more discriminative. In Chapter 4, we introduce a\nmethod that encourages discriminability in image caption generation. We show\nthat more discriminative captioning models generate more descriptive captions.\nIn Chapter 5, we study how training objectives and sampling methods affect the\nmodels' ability to generate diverse captions. We find that a popular captioning\ntraining strategy will be detrimental to the diversity of generated captions.\nIn Chapter 6, we propose a model that can control the length of generated\ncaptions. By changing the desired length, one can influence the style and\ndescriptiveness of the captions. Finally, in Chapter 7, we rank/generate\ninformative image tags according to their information utility. The proposed\nmethod better matches what humans think are the most important tags for the\nimages.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The recent advances in the study of thermodynamics of microscopic processes\nhave driven the search for new developments in energy converters utilizing\nquantum effects. We here propose a universal framework to describe the\nthermodynamics of a quantum engine fueled by quantum projective measurements.\nStandard quantum thermal machines operating in a finite-time regime with a\ndriven Hamiltonian that does not commute in different times have the\nperformance decreased by the presence of coherence, which is associated with a\nlarger entropy production and irreversibility degree. However, we show that\nreplacing the standard hot thermal reservoir by a projective measurement\noperation with general basis in the Bloch sphere and controlling the basis\nangles suitably could improve the performance of the quantum engine as well as\ndecrease the entropy change during the measurement process. Our results go in\ndirection of a generalization of quantum thermal machine models where the fuel\ncomes from general sources beyond the standard thermal reservoir.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the saturation of defect density in an atomic Bose gas rapidly\ncooled into a superfluid phase. The number of quantum vortices, which are\nspontaneously created in the quenched gas, exhibits a Poissonian distribution\nnot only for a slow quench in the Kibble-Zurek (KZ) scaling regime but also for\na fast quench in which case the mean vortex number is saturated. This shows\nthat the saturation is not caused by destructive vortex collisions, but by the\nearly-time coarsening in an emerging condensate, which is further supported by\nthe observation that the condensate growth lags the quenching in the saturation\nregime. Our results demonstrate that the defect saturation is an effect beyond\nthe KZ mechanism, opening a path for studying critical phase transition\ndynamics using the defect number distribution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we calculate the next-to-leading order electroweak corrections\nto the process $H\\to e^+e^-Z\\to e^+e^-\\mu^+\\mu^-$ in the framework of\nsingle-pole approximation, in which the contributions of intermediate $Z$ boson\nin different polarizations are considered respectively. The total and\ndifferential decay widths versus various kinematic variables are presented.\nPhenomenologically, the corresponding cross sections in the experiment of Large\nHadron Collider (LHC) are evaluated.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The theory of 3-layer density stratified ideal fluids is examined with a view\ntowards its generalization to the n-layer case. The focus is on structural\nproperties, especially for the case of a rigid upper lid constraint. We show\nthat the long-wave dispersionless limit is a system of quasi-linear equations\nthat do not admit Riemann invariants. We equip the layer-averaged\none-dimensional model with a natural Hamiltonian structure, obtained with a\nsuitable reduction process from the continuous density stratification structure\nof the full two-dimensional equations proposed by Benjamin. For a a laterally\nunbounded fluid between horizontal rigid boundaries, the paradox about the\nnon-conservation of horizontal total momentum is revisited, and it is shown\nthat the pressure imbalances causing it can be intensified by three-layer\nsetups with respect to their two-layer counterparts. The generator of the\nx-translational symmetry in the n-layer setup is also identified by the\nappropriate Hamiltonian formalism. The Boussinesq limit and a family of special\nsolutions recently introduced by de Melo Virissimo and Milewski are also\ndiscussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present far- and near-ultraviolet absorption spectroscopy of the $\\sim$23\nMyr edge-on debris disk surrounding the A0V star $\\eta$ Telescopii, obtained\nwith the Hubble Space Telescope Space Telescope Imaging Spectrograph. We detect\nabsorption lines from C I, C II, O I, Mg II, Al II, Si II, S II, Mn II, Fe II,\nand marginally N I. The lines show two clear absorption components at\n$-22.7\\pm0.5$ km s$^{-1}$ and $-17.8\\pm0.7$ km s$^{-1}$, which we attribute to\ncircumstellar (CS) and interstellar (IS) gas, respectively. CO absorption is\nnot detected, and we find no evidence for star-grazing exocomets. The CS\nabsorption components are blueshifted by $-16.9\\pm2.6$ km s$^{-1}$ in the\nstar's reference frame, indicating that they are outflowing in a radiatively\ndriven disk wind. We find that the C/Fe ratio in the $\\eta$ Tel CS gas is\nsignificantly higher than the solar ratio, as is the case in the $\\beta$ Pic\nand 49 Cet debris disks. Unlike those disks, however, the measured C/O ratio in\nthe $\\eta$ Tel CS gas is consistent with the solar value. Our analysis shows\nthat because $\\eta$ Tel is an earlier type star than $\\beta$ Pic and 49 Cet,\nwith more substantial radiation pressure at the dominant C II transitions, this\nspecies cannot bind the CS gas disk to the star as it does for $\\beta$ Pic and\n49 Cet, resulting in the disk wind.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We generalise and improve some recent bounds for additive energies of modular\nroots. Our arguments use a variety of techniques, including those from additive\ncombinatorics, algebraic number theory and the geometry of numbers. We give\napplications of these results to new bounds on correlations between {\\it\nSali{\\'e}} sums and to a new equidistribution estimate for the set of modular\nroots of primes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We analyzed the largest Small Magellanic Cloud (SMC) cluster sample (32) with\nproper motions and radial velocity measurements, from which we obtained their\nspace velocity components. By adopting as a reference the recent best-fitted\nrotating disc of SMC star clusters as a function of the position angle, we\ncomputed their residual velocity vectors, and compared their magnitudes\n($\\Delta$V) with that of a cluster with residual velocity components equal to\nthe velocity dispersions along the three independent SMC rotating disc axes of\nmotion ($\\Delta$V = 60 km/s). We found that clusters that belong to SMC tidally\ninduced structures have $\\Delta$V > 50 km/s, which suggests that space\nvelocities of clusters in the process of escaping the rotating disc kinematics,\nare measurably different. Studied clusters pertaining to a northern branch of\nthe Magellanic Bridge, the main Magellanic Bridge, the Counter-Bridge and the\nWest halo give support to these findings. NGC 121, the oldest known SMC\ncluster, does not belong to any SMC tidal feature, and has $\\Delta$V = 64 km/s,\nslightly above the boundary between bound and kinematically perturbed clusters.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Melanin is a ubiquitous natural pigment that exhibits broadband absorption\nand high refractive index. Despite its widespread use in structural color\nproduction, how the absorbing material, melanin, affects the generated color is\nunknown. Using a combined molecular dynamics and finite-difference time-domain\ncomputational approach, this paper investigates structural color generation in\none-component melanin nanoparticle-based supra-assemblies (called supraballs)\nas well as binary mixtures of melanin and silica (non-absorbing)\nnanoparticle-based supraballs. Experimentally produced one-component melanin\nand one-component silica supraballs, with thoroughly characterized primary\nparticle characteristics using neutron scattering, produce reflectance profiles\nsimilar to the computational analogues, confirming that the computational\napproach correctly simulates both absorption and multiple scattering from the\nself-assembled nanoparticles. These combined approaches demonstrate that\nmelanin's broadband absorption increases the primary reflectance peak\nwavelength, increases saturation, and decreases lightness factor. In addition,\nthe dispersity of nanoparticle size more strongly influences the optical\nproperties of supraballs than packing fraction, as evidenced by production of a\nlarger range of colors when size dispersity is varied versus packing fraction.\nFor binary melanin and silica supraballs, the chemistry-based stratification\nallows for more diverse color generation and finer saturation tuning than does\nthe degree of mixing/demixing between the two chemistries.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Enriched versions of type A Schubert polynomials are constructed with\ncoefficients in a polynomial ring in variables $c_1, c_2, \\ldots$. Specializing\nthese variables to $0$ recovers the double Schubert polynomials of Lascoux and\nSch\\\"utzenberger; specializing them to certain power series recovers the\nback-stable double Schubert polynomials of Lam, Lee, and Shimozono;\nspecializing them to Schur Q-polynomials relates them to the type C double\nSchubert polynomials of Ikeda, Mihalcea, and Naruse. Many formulas for\nclassical Schubert polynomials generalize to this setting. They give, and are\ncharacterized by, formulas for degeneracy loci.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Generative adversarial networks (GANs) have shown significant potential in\nmodeling high dimensional distributions of image data, especially on\nimage-to-image translation tasks. However, due to the complexity of these\ntasks, state-of-the-art models often contain a tremendous amount of parameters,\nwhich results in large model size and long inference time. In this work, we\npropose a novel method to address this problem by applying knowledge\ndistillation together with distillation of a semantic relation preserving\nmatrix. This matrix, derived from the teacher's feature encoding, helps the\nstudent model learn better semantic relations. In contrast to existing\ncompression methods designed for classification tasks, our proposed method\nadapts well to the image-to-image translation task on GANs. Experiments\nconducted on 5 different datasets and 3 different pairs of teacher and student\nmodels provide strong evidence that our methods achieve impressive results both\nqualitatively and quantitatively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Using TESS photometry and Rozhen spectra of the Be/gamma-ray binaries MWC 148\nand MWC 656, we estimate the projected rotational velocity ($ {v} \\sin i$), the\nrotational period (P$_{\\rm rot}$), radius (R$_{\\rm 1}$), and inclination ($i$)\nof the mass donor. For MWC 148 we derive P$_{\\rm rot} = 1.10 \\pm 0.03$~d,\nR$_{\\rm 1}= 9.2 \\pm 0.5$~R$_\\odot$, $i = 40^\\circ \\pm 2^\\circ$, and $ {v} \\sin\ni =272 \\pm 5$~km~s$^{-1}$. For MWC 656 we obtain P$_{\\rm rot} = 1.12 \\pm\n0.03$~d, R$_{\\rm 1}= 8.8 \\pm 0.5$~R$_\\odot$, $i = 52^\\circ \\pm 3^\\circ$, and $\n{v} \\sin i =313 \\pm 3$~km~s$^{-1}$. For MWC 656 we also find that the rotation\nof the mass donor is coplanar with the orbital plane.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce AndroidEnv, an open-source platform for Reinforcement Learning\n(RL) research built on top of the Android ecosystem. AndroidEnv allows RL\nagents to interact with a wide variety of apps and services commonly used by\nhumans through a universal touchscreen interface. Since agents train on a\nrealistic simulation of an Android device, they have the potential to be\ndeployed on real devices. In this report, we give an overview of the\nenvironment, highlighting the significant features it provides for research,\nand we present an empirical evaluation of some popular reinforcement learning\nagents on a set of tasks built on this platform.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present an overview of the TREC-COVID Challenge, an information retrieval\n(IR) shared task to evaluate search on scientific literature related to\nCOVID-19. The goals of TREC-COVID include the construction of a pandemic search\ntest collection and the evaluation of IR methods for COVID-19. The challenge\nwas conducted over five rounds from April to July, 2020, with participation\nfrom 92 unique teams and 556 individual submissions. A total of 50 topics (sets\nof related queries) were used in the evaluation, starting at 30 topics for\nRound 1 and adding 5 new topics per round to target emerging topics at that\nstate of the still-emerging pandemic. This paper provides a comprehensive\noverview of the structure and results of TREC-COVID. Specifically, the paper\nprovides details on the background, task structure, topic structure, corpus,\nparticipation, pooling, assessment, judgments, results, top-performing systems,\nlessons learned, and benchmark datasets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Contrary to the common belief that light-induced halide ion segregation in a\nmixed halide al-loy occurs within the illuminated area, we find that the Br\nions released by light diffuse away from the area, which generates a\ncounter-balancing Coulombic force between the anion deficit and surplus region,\ntogether resulting in a macro/mesoscopic size anion ring surrounding the\ncenter, showing a photoluminescence ring. Upon removing the illumination, the\ndisplaced anions return to the illuminated area, and the restoring force leads\nto a damped ultra-low-frequency oscillatory ion motion, which may be the first\nobservation of an ionic plasma oscillation in solids.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Embedded non-volatile memory technologies such as resistive random access\nmemory (RRAM) and spin-transfer torque magnetic RAM (STT MRAM) are increasingly\nbeing researched for application in neuromorphic computing and hardware\naccelerators for AI. However, the stochastic write processes in these memory\ntechnologies affect their yield and need to be studied alongside process\nvariations, which drastically increase the complexity of yield analysis using\nthe Monte Carlo approach. Therefore, we propose an approach based on the\nFokker-Planck equation for modeling the stochastic write processes in STT MRAM\nand RRAM devices. Moreover, we show that our proposed approach can reproduce\nthe experimental results for both STT-MRAM and RRAM devices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Direct Delta Mush is a novel skinning deformation technique introduced by Le\nand Lewis (2019). It generalizes the iterative Delta Mush algorithm of\nMancewicz et al (2014), providing a direct solution with improved efficiency\nand control. Compared to Linear Blend Skinning, Direct Delta Mush offers better\nquality of deformations and ease of authoring at comparable performance.\nHowever, Direct Delta Mush does not handle non-rigid joint transformations\ncorrectly which limits its application for most production environments. This\npaper presents an extension to Direct Delta Mush that integrates the non-rigid\npart of joint transformations into the algorithm. In addition, the paper also\ndescribes practical considerations for computing the orthogonal component of\nthe transformation and stability issues observed during the implementation and\ntesting.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning\n(DL) methodology for the continuous representation of measurements. Unlike\ntraditional DL methods that learn a mapping from the measurements to the\ndesired image, CoIL trains a multilayer perceptron (MLP) to encode the complete\nmeasurement field by mapping the coordinates of the measurements to their\nresponses. CoIL is a self-supervised method that requires no training examples\nbesides the measurements of the test object itself. Once the MLP is trained,\nCoIL generates new measurements that can be used within a majority of image\nreconstruction methods. We validate CoIL on sparse-view computed tomography\nusing several widely-used reconstruction methods, including purely model-based\nmethods and those based on DL. Our results demonstrate the ability of CoIL to\nconsistently improve the performance of all the considered methods by providing\nhigh-fidelity measurement fields.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We analyse in detail the role of additional hadron-hadron interactions in\nelastic photon-initiated (PI) production at the LHC, both in $pp$ and heavy ion\ncollisions. We first demonstrate that the source of difference between our\npredictions and other results in the literature for PI muon pair production is\ndominantly due to an unphysical cut that is imposed in these latter results on\nthe dimuon-hadron impact parameter. We in addition show that this is\nexperimentally disfavoured by the shape of the muon kinematic distributions\nmeasured by ATLAS in ultraperipheral PbPb collisions. We then consider the\ntheoretical uncertainty due to the survival probability for no additional\nhadron-hadron interactions, and in particular the role this may play in the\ntendency for the predicted cross sections to lie somewhat above ATLAS data on\nPI muon pair production, in both $pp$ and PbPb collisions. This difference is\nrelatively mild, at the $\\sim 10\\%$ level, and hence a very good control over\nthe theory is clearly required. We show that this uncertainty is very small,\nand it is only by taking very extreme and rather unphysical variations in the\nmodelling of the survival factor that this tension can be removed. This\nunderlines the basic, rather model independent, point that a significant\nfraction of elastic PI scattering occurs for hadron-hadron impact parameters\nthat are simply outside the range of QCD interactions, and hence this sets a\nlower bound on the survival factor in any physically reasonable approach.\nFinally, other possible origins for this discrepancy are discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Rank-constrained spatial covariance matrix estimation (RCSCME) is a\nstate-of-the-art blind speech extraction method applied to cases where one\ndirectional target speech and diffuse noise are mixed. In this paper, we\nproposed a new algorithmic extension of RCSCME. RCSCME complements a deficient\none rank of the diffuse noise spatial covariance matrix, which cannot be\nestimated via preprocessing such as independent low-rank matrix analysis, and\nestimates the source model parameters simultaneously. In the conventional\nRCSCME, a direction of the deficient basis is fixed in advance and only the\nscale is estimated; however, the candidate of this deficient basis is not\nunique in general. In the proposed RCSCME model, the deficient basis itself can\nbe accurately estimated as a vector variable by solving a vector optimization\nproblem. Also, we derive new update rules based on the EM algorithm. We confirm\nthat the proposed method outperforms conventional methods under several noise\nconditions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop a calculus that gives an elementary approach to enumerate\npartition-like objects using an infinite upper-triangular number-theoretic\nmatrix. We call this matrix the Partition-Frequency Enumeration (PFE) matrix.\nThis matrix unifies a large number of results connecting number-theoretic\nfunctions to partition-type functions. The calculus is extended to arbitrary\ngenerating functions, and functions with Weierstrass products. As a by-product,\nwe recover (and extend) some well-known recurrence relations for many\nnumber-theoretic functions, including the sum of divisors function, Ramanujan's\n$\\tau$ function, sums of squares and triangular numbers, and for $\\zeta(2n)$,\nwhere $n$ is a positive integer. These include classical results due to Euler,\nEwell, Ramanujan, Lehmer and others. As one application, we embed Ramanujan's\nfamous congruences $p(5n+4)\\equiv 0$ (mod $5)$ and $\\tau(5n+5)\\equiv 0$ (mod\n$5)$ into an infinite family of such congruences.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Panel count data is common when the study subjects are exposed to recurrent\nevents, observed only at discrete time points. In this article, we consider the\nregression analysis of panel count data with multiple modes of recurrence. We\npropose a proportional mean model to estimate the effect of covariates on the\nunderlying counting process due to different modes of recurrence. The\nsimultaneous estimation of baseline cumulative mean functions and regression\nparameters of $(k>1)$ recurrence modes are studied in detail. Asymptotic\nproperties of the proposed estimators are also established. A Monte Carlo\nsimulation study is carried out to validate the finite sample behaviour of the\nproposed estimators. The methods are applied to a real data arising from skin\ncancer chemoprevention trial.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Jeans instability within the framework of post-Newtonian Boltzmann and\nPoisson equations are analyzed. The components of the energy-momentum tensor\nare calculated from a post-Newtonian Maxwell-J\\\"uttner distribution function.\nThe perturbations of the distribution function and gravitational potentials\nfrom their background states with the representation of the perturbations as\nplane waves lead to a dispersion relation with post-Newtonian corrections. The\ninfluence of the post-Newtonian approximation on the Jeans mass is determined\nand it was shown that the mass necessary for an overdensity to begin the\ngravitational collapse in the post-Newtonian theory is smaller than the one in\nthe Newtonian theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The methods of approximation, regularization and smoothing of trigonometric\ninterpolation splines are considered in the paper. It is shown that\ntrigonometric splines can be considered from two points of view - as a\ntrigonometric Fourier series and as discrete trigonometric Fourier series\naccording to certain systems of functions that are smoothness carriers. It is\nargued that with approximation and smoothing of trigonometric splines it is\nexpedient to consider as discrete rows, since their differential properties are\nstored.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $\\mathbb{C}\\mathsf{A}_n = \\mathbb{C}[S_2\\wr S_2 \\wr\\cdots \\wr S_2]$ be\nthe group algebra of $n$-step iterated wreath product. We prove some structural\nproperties of $\\mathsf{A}_n$ such as their centers, centralizers, right and\ndouble cosets. We apply these results to explicitly write down Mackey theorem\nfor groups $\\mathsf{A}_n$ and give a partial description of the natural\ntransformations between induction and restriction functors on the\nrepresentations of the iterated wreath product tower by computing certain hom\nspaces of the category of $\\displaystyle \\bigoplus_{m\\geq 0}(\\mathsf{A}_m,\n\\mathsf{A}_n)-$bimodules. A complete description of the category is an open\nproblem.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Robots excel at avoiding obstacles but struggle to traverse complex 3-D\nterrain with cluttered large obstacles. By contrast, insects like cockroaches\nexcel at doing so. Recent research in our lab elucidated how locomotor\ntransitions emerge from locomotor-environment interaction for diverse locomotor\nchallenges abstracted from complex 3-D terrain and the strategies to overcome\nthem. Here we built on these fundamental insights to develop a\ncockroach-inspired legged robot, Omni-Roach, that integrated these strategies\nto achieve multi-modal locomotion and provide a robophysical model to study the\ntrade-off between multi-functionality and performance. The robot was based on\nthe RHex design with six compliant legs and featured a rounded body with two\nwings that can open and a tail with pitch and yaw degrees of freedom. After two\ndevelopment and testing iterations, our robot was capable of overcoming all\nlocomotor challenges with a high performance and success rate. It traversed\ncluttered rigid pillars only 1.1x robot body width apart, a 2.5x hip height\nbump, a 0.75x body length gap, densely cluttered flexible beams only 65% body\nwidth apart, and self-righted within 4 seconds. Systematic beam traversal\nexperiments further revealed that a downward-pointing tail oscillating\nlaterally helps roll the body into beam gaps and break frictional and\ninterlocking contact to traverse. Our work highlights the usefulness of\nmulti-functional appendages and exaptation for large obstacle traversal.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove a conjecture of Sturmfels, Timme and Zwiernik on the ML-degrees of\nlinear covariance models in algebraic statistics. As in our previous works on\nlinear concentration models, the proof ultimately relies on the computation of\ncertain intersection numbers on the varieties of complete quadrics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A seminal result in game theory is von Neumann's minmax theorem, which states\nthat zero-sum games admit an essentially unique equilibrium solution. Classical\nlearning results build on this theorem to show that online no-regret dynamics\nconverge to an equilibrium in a time-average sense in zero-sum games. In the\npast several years, a key research direction has focused on characterizing the\nday-to-day behavior of such dynamics. General results in this direction show\nthat broad classes of online learning dynamics are cyclic, and formally\nPoincar\\'{e} recurrent, in zero-sum games. We analyze the robustness of these\nonline learning behaviors in the case of periodic zero-sum games with a\ntime-invariant equilibrium. This model generalizes the usual repeated game\nformulation while also being a realistic and natural model of a repeated\ncompetition between players that depends on exogenous environmental variations\nsuch as time-of-day effects, week-to-week trends, and seasonality.\nInterestingly, time-average convergence may fail even in the simplest such\nsettings, in spite of the equilibrium being fixed. In contrast, using novel\nanalysis methods, we show that Poincar\\'{e} recurrence provably generalizes\ndespite the complex, non-autonomous nature of these dynamical systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Pretraining language models with next-token prediction on massive text\ncorpora has delivered phenomenal zero-shot, few-shot, transfer learning and\nmulti-tasking capabilities on both generative and discriminative language\ntasks. Motivated by this success, we explore a Vector-quantized Image Modeling\n(VIM) approach that involves pretraining a Transformer to predict rasterized\nimage tokens autoregressively. The discrete image tokens are encoded from a\nlearned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple\nimprovements over vanilla VQGAN from architecture to codebook learning,\nyielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN\nfurther improves vector-quantized image modeling tasks, including\nunconditional, class-conditioned image generation and unsupervised\nrepresentation learning. When trained on ImageNet at \\(256\\times256\\)\nresolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception\nDistance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which\nobtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and\nunsupervised pretraining, we further evaluate the pretrained Transformer by\naveraging intermediate features, similar to Image GPT (iGPT). This\nImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy\nfrom 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL\nwhich is trained with extra web image data and larger model size.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The theoretical physicist and mathematician Aristophanes Dimakis passed away\non July 8, 2021, at the age of 68, in Athens, Greece. We briefly review his\nlife, career and scientific achievements.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The $l\\!=\\!+1.\\!\\!^\\circ3$ region in the Galactic center is characterized by\nmultiple shell-like structures and their extremely broad velocity widths. We\nrevisit the molecular superbubble hypothesis for this region, based on high\nresolution maps of CO {\\it J}=1--0, $^{13}$CO {\\it J}=1--0, H$^{13}$CN {\\it\nJ}=1--0, H$^{13}$CO$^{+}$ {\\it J}=1--0, SiO {\\it J}=2--1, and CS {\\it J}=2--1\nlines obtained from the Nobeyama radio observatory 45-m telescope, as well as\nCO {\\it J}=3--2 maps obtained from the James Clerk Maxwell telescope. We\nidentified eleven expanding shells with total kinetic energy and typical\nexpansion time $E_{\\rm kin}\\!\\sim\\! 10^{51.9}$ erg and $t_{\\rm exp}\\!\\sim\\!\n10^{4.9}$ yr, respectively. In addition, the $l\\!=\\!+1.\\!\\!^\\circ3$ region\nexhibited high SiO {\\it J}=2--1/H$^{13}$CN {\\it J}=1--0 and SiO {\\it\nJ}=2--1/H$^{13}$CO$^{+}$ {\\it J}=1--0 intensity ratios, indicating that the\nregion has experienced dissociative shocks in the past. These new findings\nconfirm the molecular superbubble hypothesis for the $l\\!=\\!+1.\\!\\!^\\circ3$\nregion. The nature of the embedded star cluster, which may have supplied 20--70\nsupernova explosions within 10$^5$ yr, is discussed. This work also show the\nimportance of compact broad-velocity-width features in searching for localized\nenergy sources hidden behind severe interstellar extinction and stellar\ncontamination.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This study aims to show the fundamental difference between logistic\nregression and Bayesian classifiers in the case of exponential and\nunexponential families of distributions, yielding the following findings.\nFirst, the logistic regression is a less general representation of a Bayesian\nclassifier. Second, one should suppose distributions of classes for the correct\nspecification of logistic regression equations. Third, in specific cases, there\nis no difference between predicted probabilities from correctly specified\ngenerative Bayesian classifier and discriminative logistic regression.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The concept of porous numbers is presented. A number $k$ which is not a\nmultiple of 10 is called {\\it porous} if every number $m$ with sum of digits =\n$k$ and $k$ a divisor of both $m$ and digit reversal of $m$ has a zero in its\ndigits. It is proved that 11, 37, 74, 101 and 121 are the only porous numbers\nsmaller than 1000.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Low excitation radio galaxies (LERGs) are weakly accreting active galactic\nnuclei (AGN) believed to be fuelled by radiatively inefficient accretion\nprocesses. Despite this, recent works have shown evidence for ionized and\nneutral hydrogen gas outflows in these galaxies. To investigate the potential\ndrivers of such outflows we select a sample of 802 LERGs using the Best &\nHeckman (2012) catalogue of radio galaxies. By modelling the [O III] $\\lambda\n5007$ profile in Sloan Digital Sky Survey spectra of a sample of 802 LERGs, we\ndetermine that the ionized outflows are present in $\\sim 1.5\\%$ of the\npopulation. Using $1.4~\\text{GHz}$ imaging from the Faint Images of the Radio\nSky at Twenty Centimeters survey we analyze the radio morphology of LERGs with\noutflows and find these to be consistent with the parent LERG population.\nHowever, we note that unlike the majority of the LERG population, those LERGs\nshowing outflows have Eddington scaled accretion rates close to $1\\%$. This is\nindicative that ionized outflows in LERGs are driven by the radiation pressure\nfrom the accretion disk of the AGN rather than the radio jets. We report\nspecific star formation rates in the range of $10^{-12} < \\text{sSFR} <\n10^{-9}~\\text{yr}^{-1}$. Moreover, we observe higher mass outflow rates of\n$7-150~M_{\\odot}~\\text{yr}^{-1}$ for these LERGs than luminous quasars for a\ngiven bolometric luminosity, which could possibly be due to the radio source in\nLERGs boosting the mass-loading. This scenario could indicate that these\noutflows could potentially drive feedback in LERGs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the problem of teaching via demonstrations in sequential\ndecision-making settings. In particular, we study how to design a personalized\ncurriculum over demonstrations to speed up the learner's convergence. We\nprovide a unified curriculum strategy for two popular learner models: Maximum\nCausal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy\nBehavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over\ndemonstrations based on a notion of difficulty scores computed w.r.t. the\nteacher's optimal policy and the learner's current policy. Compared to the\nstate of the art, our strategy doesn't require access to the learner's internal\ndynamics and still enjoys similar convergence guarantees under mild technical\nconditions. Furthermore, we adapt our curriculum strategy to the setting where\nno teacher agent is present using task-specific difficulty scores. Experiments\non a synthetic car driving environment and navigation-based environments\ndemonstrate the effectiveness of our curriculum strategy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Rare-earth ions are promising solid state systems to build light-matter\ninterfaces at the quantum level. This relies on their potential to show narrow\noptical homogeneous linewidths or, equivalently, long-lived optical quantum\nstates. In this letter, we report on europium molecular crystals that exhibit\nlinewidths in the 10s of kHz range, orders of magnitude narrower than other\nmolecular centers. We harness this property to demonstrate efficient optical\nspin initialization, coherent storage of light using an atomic frequency comb,\nand optical control of ion-ion interactions towards implementation of quantum\ngates. These results illustrate the utility of rare-earth molecular crystals as\na new platform for photonic quantum technologies that combines highly coherent\nemitters with the unmatched versatility in composition, structure, and\nintegration capability of molecular materials.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the regularity properties of the second order linear operator in\n$\\mathbb{R}^{N+1}$: \\begin{equation*} \\mathscr{L} u := \\sum_{j,k= 1}^{m}\na_{jk}\\partial_{x_j x_k}^2 u + \\sum_{j,k= 1}^{N} b_{jk}x_k \\partial_{x_j} u -\n\\partial_t u, \\end{equation*} where $A = \\left( a_{jk} \\right)_{j,k= 1, \\dots,\nm}, B= \\left( b_{jk} \\right)_{j,k= 1, \\dots, N}$ are real valued matrices with\nconstant coefficients, with $A$ symmetric and strictly positive. We prove that,\nif the operator $\\mathscr{L}$ satisfies H\\\"ormander's hypoellipticity\ncondition, and $f$ is a Dini continuous function, then the second order\nderivatives of the solution $u$ to the equation $\\mathscr{L} u = f$ are Dini\ncontinuous functions as well. We also consider the case of Dini continuous\ncoefficients $a_{jk}$'s. A key step in our proof is a Taylor formula for\nclassical solutions to $\\mathscr{L} u = f$ that we establish under minimal\nregularity assumptions on $u$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To obtain high-resolution depth maps, some previous learning-based multi-view\nstereo methods build a cost volume pyramid in a coarse-to-fine manner. These\napproaches leverage fixed depth range hypotheses to construct cascaded plane\nsweep volumes. However, it is inappropriate to set identical range hypotheses\nfor each pixel since the uncertainties of previous per-pixel depth predictions\nare spatially varying. Distinct from these approaches, we propose a Dynamic\nDepth Range Network (DDR-Net) to determine the depth range hypotheses\ndynamically by applying a range estimation module (REM) to learn the\nuncertainties of range hypotheses in the former stages. Specifically, in our\nDDR-Net, we first build an initial depth map at the coarsest resolution of an\nimage across the entire depth range. Then the range estimation module (REM)\nleverages the probability distribution information of the initial depth to\nestimate the depth range hypotheses dynamically for the following stages.\nMoreover, we develop a novel loss strategy, which utilizes learned dynamic\ndepth ranges to generate refined depth maps, to keep the ground truth value of\neach pixel covered in the range hypotheses of the next stage. Extensive\nexperimental results show that our method achieves superior performance over\nother state-of-the-art methods on the DTU benchmark and obtains comparable\nresults on the Tanks and Temples benchmark. The code is available at\nhttps://github.com/Tangshengku/DDR-Net.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recommender system is an applicable technique in most E-commerce commercial\nproduct technical designs. However, nearly all recommender system faces a\nchallenge called the cold-start problem. The problem is so notorious that\nalmost every industrial practitioner needs to resolve this issue when building\nrecommender systems. Most cold-start problem solvers need some kind of data\ninput as the starter of the system. On the other hand, many real-world\napplications place popular items or random items as recommendation results. In\nthis paper, we propose a new technique called ZeroMat that requries no input\ndata at all and predicts the user item rating data that is competitive in Mean\nAbsolute Error and fairness metric compared with the classic matrix\nfactorization with affluent data, and much better performance than random\nplacement.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study $p$-adic properties of the coherent cohomology of some automorphic\nsheaves on the Hilbert modular variety $X$ for a totally real field $F$ in the\ncase where the prime $p$ is totally split in $F$. More precisely, we develop\nhigher Hida theory \\`{a} la Pilloni, constructing, for $0\\leq q\\leq\n[F:\\mathbb{Q}]$, some modules $M^q$ which $p$-adically interpolate the ordinary\npart of the cohomology groups $H^q(X, \\underline{\\omega}^{\\kappa})$, varying\nthe weight $\\kappa$ of the automorphic sheaf.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Reconstructing the full quantum state of a many-body system requires the\nestimation of a number of parameters that grows exponentially with system size.\nNevertheless, there are situations in which one is only interested in a subset\nof these parameters and a full reconstruction is not needed. A paradigmatic\nexample is a scenario where one aims at determining all the reduced states only\nup to a given size. Overlapping tomography provides constructions to address\nthis problem with a number of product measurements much smaller than what is\nobtained when performing independent tomography of each reduced state. There\nare however many relevant physical systems with a natural notion of locality\nwhere one is mostly interested in the reduced states of neighboring particles.\nIn this work, we study this form of local overlapping tomography. First of all,\nwe show that, contrary to its full version, the number of product measurements\nneeded for local overlapping tomography does not grow with system size. Then,\nwe present strategies for qubit and fermionic systems in selected lattice\ngeometries. The developed methods find a natural application in the estimation\nof many-body systems prepared in current quantum simulators or quantum\ncomputing devices, where interactions are often local.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Convolution on 3D point clouds that generalized from 2D grid-like domains is\nwidely researched yet far from perfect. The standard convolution characterises\nfeature correspondences indistinguishably among 3D points, presenting an\nintrinsic limitation of poor distinctive feature learning. In this paper, we\npropose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels\nfor points according to their dynamically learned features. Compared with using\na fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud\nconvolutions, effectively and precisely capturing the diverse relations between\npoints from different semantic parts. Unlike popular attentional weight\nschemes, the proposed AdaptConv implements the adaptiveness inside the\nconvolution operation instead of simply assigning different weights to the\nneighboring points. Extensive qualitative and quantitative evaluations show\nthat our method outperforms state-of-the-art point cloud classification and\nsegmentation approaches on several benchmark datasets. Our code is available at\nhttps://github.com/hrzhou2/AdaptConv-master.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Protecting users from accessing malicious web sites is one of the important\nmanagement tasks for network operators. There are many open-source and\ncommercial products to control web sites users can access. The most traditional\napproach is blacklist-based filtering. This mechanism is simple but not\nscalable, though there are some enhanced approaches utilizing fuzzy matching\ntechnologies. Other approaches try to use machine learning (ML) techniques by\nextracting features from URL strings. This approach can cover a wider area of\nInternet web sites, but finding good features requires deep knowledge of trends\nof web site design. Recently, another approach using deep learning (DL) has\nappeared. The DL approach will help to extract features automatically by\ninvestigating a lot of existing sample data. Using this technique, we can build\na flexible filtering decision module by keep teaching the neural network module\nabout recent trends, without any specific expert knowledge of the URL domain.\nIn this paper, we apply a mechanical approach to generate feature vectors from\nURL strings. We implemented our approach and tested with realistic URL access\nhistory data taken from a research organization and data from the famous\narchive site of phishing site information, PhishTank.com. Our approach achieved\n2~3% better accuracy compared to the existing DL-based approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the problem of modifying a network topology in such a way as to\ndelay the propagation of a disease with minimal disruption of the network\ncapacity to reroute goods/items/passengers. We find an approximate solution to\nthe Susceptible-Infected-Susceptible (SIS) model, which constitutes a tight\nupper bound to its exact solution. This upper bound allows direct\nstructure-epidemic dynamic relations via the total communicability function.\nUsing this approach we propose a strategy to remove edges in a network that\nsignificantly delays the propagation of a disease across the network with\nminimal disruption of its capacity to deliver goods/items/passengers. We apply\nthis strategy to the analysis of the U.K. airport transportation network\nweighted by the number of passengers transported in the year 2003. We find that\nthe removal of all flights connecting four origin-destination pairs in the U.K.\ndelays the propagation of a disease by more than 300\\%, with a minimal\ndeterioration of the transportation capacity of this network. These time delays\nin the propagation of a disease represent an important non-pharmaceutical\nintervention to confront an epidemics, allowing for better preparations of the\nhealth systems, while keeping the economy moving with minimal disruptions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report the observation of the controlled expansion of a two-dimensional\nquantum gas confined onto a curved shell-shaped surface. We start from the\nellipsoidal geometry of a dressed quadrupole trap and introduce a novel gravity\ncompensation mechanism enabling to explore the full ellipsoid. The zero-point\nenergy of the transverse confinement manifests itself by the spontaneous\nemergence of an annular shape in the atomic distribution. The experimental\nresults are compared with the solution of the three-dimensional\nGross-Pitaevskii equation and with a two-dimensional semi-analytical model.\nThis work evidences how a hidden dimension can affect dramatically the embedded\nlow-dimensional system by inducing a change of topology.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Over the past few years, a tremendous growth of machine learning was brought\nabout by a significant increase in adoption of cloud-based services. As a\nresult, various solutions have been proposed in which the machine learning\nmodels run on a remote cloud provider. However, when such a model is deployed\non an untrusted cloud, it is of vital importance that the users' privacy is\npreserved. To this end, we propose Blind Faith -- a machine learning model in\nwhich the training phase occurs in plaintext data, but the classification of\nthe users' inputs is performed on homomorphically encrypted ciphertexts. To\nmake our construction compatible with homomorphic encryption, we approximate\nthe activation functions using Chebyshev polynomials. This allowed us to build\na privacy-preserving machine learning model that can classify encrypted images.\nBlind Faith preserves users' privacy since it can perform high accuracy\npredictions by performing computations directly on encrypted data.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We implement a user-extensible ad hoc connection between the Lean proof\nassistant and the computer algebra system Mathematica. By reflecting the syntax\nof each system in the other and providing a flexible interface for extending\ntranslation, our connection allows for the exchange of arbitrary information\nbetween the two systems.\n  We show how to make use of the Lean metaprogramming framework to verify\ncertain Mathematica computations, so that the rigor of the proof assistant is\nnot compromised. We also use Mathematica as an untrusted oracle to guide proof\nsearch in the proof assistant and interact with a Mathematica notebook from\nwithin a Lean session. In the other direction, we import and process Lean\ndeclarations from within Mathematica. The proof assistant library serves as a\ndatabase of mathematical knowledge that the CAS can display and explore.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26\nNLP models show that LS can largely reduce the number of queries usually by an\norder of magnitude to achieve high attack success rates. Further experiments\nshow that the adversarial examples crafted by LS usually have higher quality,\nexhibit better transferability, and can bring more robustness improvement to\nvictim models by adversarial training.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present numerical computations and analysis of atomic to molecular\n(HI-to-H$_2$) transitions in cool ($\\sim$100 K) low-metallicity dust-free\n(primordial) gas, in which molecule formation occurs via cosmic-ray driven\nnegative ion chemistry, and removal is by a combination of far-UV\nphotodissociation and cosmic-ray ionization and dissociation. For any gas\ntemperature, the behavior depends on the ratio of the Lyman-Werner (LW) band\nFUV intensity to gas density, $I_{\\rm LW}/n$, and the ratio of the cosmic-ray\nionization rate to the gas density, $\\zeta/n$. We present sets of HI-to-H$_2$\nabundance profiles for a wide range of $\\zeta/n$ and $I_{\\rm LW}/n$, for\ndust-free gas. We determine the conditions for which H$_2$ absorption line\nself-shielding in optically thick clouds enables a transition from atomic to\nmolecular form for ionization-driven chemistry. We also examine the effects of\ncosmic-ray energy losses on the atomic and molecular density profiles and\ntransition points. For a unit Galactic interstellar FUV field intensity\n($I_{\\rm LW}=1$) with LW flux $2.07\\times 10^7$ photons cm$^{-2}$ s$^{-1}$, and\na uniform cosmic-ray ionization rate $\\zeta=10^{-16}$ s$^{-1}$, an HI-to-H$_2$\ntransition occurs at a total hydrogen gas column density of $4\\times 10^{21}$\ncm$^{-2}$, within $3\\times 10^7$ yr, for a gas volume density of $n=10^6$\ncm$^{-3}$ at 100 K. For these parameters, the dust-free limit obtains for a\ndust-to-gas ratio Z$^\\prime_d \\lesssim 10^{-5}$, which may be reached for\noverall metallicities $Z^\\prime\\lesssim 0.01$ relative to Galactic solar\nvalues.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, the mass and pole residue of $ X(4685) $ state with spin-parity\n$J^P=1^+$ are computed by the QCD sum rule approaches up to operator dimension\nseven based on the diquark-antidiquark configuration. For its mass we get $\nm_{X_{cs}}= 4607^{+36}_{-22} $ MeV and pole residue $ \\lambda_{X_{cs}}=\n6.19^{+0.32}_{-0.24}\\times 10^{-2}~\\mathrm{GeV^5}$, which may be checked via\nother nonperturbative approaches as well as future experiments. As a by\nproduct, the mass of the hidden-bottom partner state of the $ X(4685) $ is\nextracted to be both around $ m_{X_{bs}}= (10604-10924) $ MeV and $\n\\lambda_{X_{bs}}= (42.2-53.7)\\times 10^{-2}~\\mathrm{GeV^5}$, which can be\nsearched in the $\\Upsilon \\phi $ invariant mass distribution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Aspect-based sentiment analysis (ABSA) aims to predict the sentiment\nexpressed in a review with respect to a given aspect. The core of ABSA is to\nmodel the interaction between the context and given aspect to extract the\naspect-related information. In prior work, attention mechanisms and dependency\ngraph networks are commonly adopted to capture the relations between the\ncontext and given aspect. And the weighted sum of context hidden states is used\nas the final representation fed to the classifier. However, the information\nrelated to the given aspect may be already discarded and adverse information\nmay be retained in the context modeling processes of existing models. This\nproblem cannot be solved by subsequent modules and there are two reasons:\nfirst, their operations are conducted on the encoder-generated context hidden\nstates, whose value cannot change after the encoder; second, existing encoders\nonly consider the context while not the given aspect. To address this problem,\nwe argue the given aspect should be considered as a new clue out of context in\nthe context modeling process. As for solutions, we design several aspect-aware\ncontext encoders based on different backbones: an aspect-aware LSTM and three\naspect-aware BERTs. They are dedicated to generate aspect-aware hidden states\nwhich are tailored for ABSA task. In these aspect-aware context encoders, the\nsemantics of the given aspect is used to regulate the information flow.\nConsequently, the aspect-related information can be retained and\naspect-irrelevant information can be excluded in the generated hidden states.\nWe conduct extensive experiments on several benchmark datasets with empirical\nanalysis, demonstrating the efficacies and advantages of our proposed\naspect-aware context encoders.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the luminosity suppression and its effect on the mass-radius\nrelation as well as cooling evolution of highly magnetised white dwarfs. Based\non the effect of magnetic field relative to gravitational energy, we suitably\nmodify our treatment of the radiative opacity, magnetostatic equilibrium and\ndegenerate core equation of state to obtain the structural properties of these\nstars. Although the Chandrasekhar mass limit is retained in the absence of\nmagnetic field and irrespective of the luminosity, strong central fields of\nabout $10^{14}\\, {\\rm G}$ can yield super-Chandrasekhar white dwarfs with\nmasses up to $1.9\\, M_{\\odot}$. Smaller white dwarfs tend to remain\nsuper-Chandrasekhar for sufficiently strong central fields even when their\nluminosity is significantly suppressed to $10^{-16}\\ L_{\\odot}$. Owing to the\ncooling evolution and simultaneous field decay over $10\\ {\\rm Gyr}$, the\nlimiting masses of small magnetised white dwarfs can fall to $1.5\\ M_{\\odot}$\nover time. However the majority of these systems still remain practically\nhidden throughout their cooling evolution because of their high fields and\ncorrespondingly low luminosities. Utilising the stellar evolution code\n$\\textit{STARS}$, we obtain close agreement with the analytical mass limit\nestimates and this suggests that our analytical formalism is physically\nmotivated. Our results argue that super-Chandrasekhar white dwarfs born due to\nstrong field effects may not remain so for long. This explains their apparent\nscarcity in addition to making them hard to detect because of their suppressed\nluminosities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We review the problem of the consistency between the observed values of the\nlunar recession from Lunar Laser Ranging (LLR) and of the increase of the\nlength-of-the-day (LOD). From observations of lunar occultations completed by\nrecent IERS data, we derive a variation rate of the LOD equal to 1.09 ms/cy\nfrom 1680 to 2020, which compares well with McCarthy and Babcock (1986) and\nSidorenkov (2005). This rate is lower than the mean rate of 1.78 ms/cy derived\nby Stephenson et al. (2016) on the basis of eclipses in the Antiquity and\nMiddle Age. The difference in the two observed rates starts at the epoch of a\nmajor change in the data accuracy with telescopic observations. The observed\nlunar recession appears too large when compared to the tidal slowing down of\nthe Earth determined from eclipses in the Antiquity and Middle Age and even\nmuch more when determined from lunar occultations and IERS data from 1680 to\n2020. With a proper account of the tidal effects and of the detailed studies on\nthe atmospheric effects, the melting from icefields, the changes of the sea\nlevel, the glacial isostatic adjustment, and the core-mantle coupling, we\nconclude that the long-standing problem of the presence or absence of a local\ncosmological expansion is still an open question.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Filtering packet traffic and rules of permit/denial of data packets into\nnetwork nodes are granted by facilitating Access Control Lists (ACL). This\npaper proposes a procedure of adding a link load threshold value to the access\ncontrol list rules option, which acts on the basis of threshold value. The\nultimate goal of this enhanced ACL is to avoid congestion in targeted\nsubnetworks. The link load threshold value allows to decide that packet traffic\nis rerouted by the router to avoid congestion, or packet drop happens on the\nbasis of packet priorities. The packet rerouting in case of high traffic loads,\nbased on new packet filtering procedure for congestion avoidance, will result\nin the reduction of the overall packet drop ratio, and of over-subscription in\ncongested subnetworks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Twisted atomic bilayers are emerging platforms for manipulating chiral\nlight-matter interaction at the extreme nanoscale, due to their inherent\nmagnetoelectric responses induced by the finite twist angle and quantum\ninterlayer coupling between the atomic layers. Recent studies have reported the\ndirect correspondence between twisted atomic bilayers and chiral metasurfaces,\nwhich features a chiral surface conductivity, in addition to the electric and\nmagnetic surface conductivities. However, far-field chiral optics in light of\nthese consitututive conductivities remains unexplored. Within the framework of\nthe full Maxwell equations, we find that the chiral surface conductivity can be\nexploited to realize perfect polarization transformation between linearly\npolarized light. Remarkably, such an exotic chiral phenomenon can occur either\nfor the reflected or transmitted light.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Ultracold gases of fermionic alkaline-earth (like) atoms are hopeful\ncandidates for the quantum simulation of many-body physics induced by magnetic\nimpurities (e.g., the Kondo physics), because there are spin-exchange\ninteractions (SEIs) between two atoms in the electronic ground ($^1$S$_0$) and\nmetastable ($^3$P) state, respectively. Nevertheless, this SEI cannot be tuned\nvia magnetic Feshbach resonance. In this work we propose three methods to\ncontrol the SEI between one atom in the $^1$S$_0$ state and another atom in the\n$^3$P$_2$ states or $^3$P$_2$-$^3$P$_0$ dressed states, with one or two laser\nbeams.These methods are based on the spin-dependent AC-Stark shifts of the\n$^3$P$_2$ states, or the $^3$P$_2$-$^3$P$_0$ Raman coupling. We show that due\nto the structure of alkaline-earth (like) atoms, the heating effects induced by\nthe laser beams of our methods are very weak. For instance, for ultracold Yb\natoms, AC-Stark-shift difference of variant spin states of the $^3$P$_2(F=3/2)$\nlevel, or the strength of the $^3$P$_2$-$^3$P$_0$ Raman coupling, could be of\nthe order of $(2\\pi)$MHz, while the heating rate (photon scattering rate) is\nonly of the order of Hz. As a result, the Feshbach resonances, with which one\ncan efficiently control the SEI by changing the laser intensity, may be induced\nby the laser beams with low-enough heating rate, even if the scattering lengths\nof the bare inter-atomic interaction are so small that being comparable with\nthe length scale associated with the van der Waals interaction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper proposes a very simple deterministic mathematical model, which, by\nusing a power-law, is a \\emph{non-integer power model} (or \\emph{fractional\npower model (FPM)}). Such a model, in non-integer power of time, namely $t^m$\nup to constants, enables representing at each day, with a good precision, the\ntotality of the contaminated individuals. Despite being enriched with knowledge\nthrough an internal structure based on a geometric sequence \"with variable\nratio\", the model (in its non-integer representation) has only three\nparameters, among which the non-integer power, $m$, that determines on its own,\naccording to its value, an aggravation or an improvement of the viral\nspreading. Its simplicity comes from the power-law, $t^m$, which simply\nexpresses the singular dynamics of the operator of non-integer differentiation\nor integration, of high parametric compactness, that governs diffusion\nphenomena and, as shown in this paper, the spreading phenomena by\ncontamination. The proposed model is indeed validated with the official data of\nMinistry of Health on the COVID-19 spreading. Used in prediction, it well\nenables justifying the choice of a lockdown, without which the spreading would\nhave highly worsened. The comparison of this model in $t^m$ with two known\nmodels having the same number of parameters, well shows that its\nrepresentativity of the real data is better or more general. Finally, in a more\nfundamental context and particularly in terms of complexity and simplicity, a\nself-filtering action enables showing the compatibility between the\n\\emph{internal complexity} that the internal structure and its stochastic\nbehavior present, and the \\emph{global simplicity} that the model in $t^m$\noffers in a deterministic manner: it is true that the non-integer power of a\npower-law is well a marker of complexity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the Hausdorff dimension of the divergence set on which the\npointwise convergence $\\lim_{t\\rightarrow 0} e^{it\\sqrt{-\\Delta}} f(x) = f(x)$\nfails when $f \\in H^s(\\mathbb R^d)$. We especially prove the conjecture raised\nby Barcel\\'o, Bennett, Carbery and Rogers \\cite{BBCR} for $d=3$, and improve\nthe previous results in higher dimensions $d\\ge4$. We also show that a\nStrichartz type estimate for $f\\to e^{it\\sqrt{-\\Delta}} f$ with the measure $\ndt\\,d\\mu(x)$ is essentially equivalent to the estimate for the spherical\naverage of $\\widehat \\mu$ which has been extensively studied for the Falconer\ndistance set problem. The equivalence provides shortcuts to the recent results\ndue to B. Liu and K. Rogers.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Photoexcitation and shaping of a propagating surface plasmon polariton (SPP)\non silver and gold microstructures are well established and lead to the\ndiscovery of the plasmonic spin-Hall effect recently. Whereas silver is often\nthe material of choice due to its exceptional low plasma frequency and weak\ndamping, similar observations have not been reported for ferromagnetic metals.\nIn this work, we report on propagating SPPs on Ni$_{80}$Fe$_{20}$\nmicrostructures imaged by photoemission electron microscopy (PEEM) in\ncombination with a tunable femtosecond laser system at MHz repetition rate.\nCircular dichroic (CD) images in threshold PEEM show clear edge-induced SPPs\nwith sub-micrometer wavelength and propagation length of about 3.5 $\\mu$m.\nAnalysis of the interference patterns as well as the coupling of the optical\nspin angular momentum to the observed fringe fields reveal propagation\ncharacteristics exclusive to evanescent waves and the presence of the plasmonic\nspin-Hall effect. Our work provides direct evidence that many materials with a\nhigh plasma frequency allow for excitation and observation of propagating SPPs\nat the dielectric/metal interface via CD PEEM imaging, enabling\nmagnetoplasmonic investigation of common ferromagnets on nanometer length and\nfemtosecond time scales.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Zbigniew Oziewicz was a pioneer of the 4D space-time approach to covariant\nrelative velocities. In the 1980s (according to private correspondence) he\ndiscovered two types of 4D relative velocities: binary and ternary, along with\nthe rules for adding them. They were first published in conference materials in\n2004, and the second time in a peer-reviewed journal in 2007. These physically\nlogical and mathematically precise concepts are so subtle that Oziewicz's\nnumerous preprints have yet to receive the recognition they deserve.\n  This work was planned to be a more review, but a thorough review of the\nlittle-known results was made in an original synthetic manner with numerous\ngeneralizations. The Part I presents the Oziewicz-\\'Swierk-Bol\\'os (and\nMatolsci or Bini-Carini-Jantzen) binary relative velocity and the\nOziewicz-Ungar-Dragan (also Celakoska-Chakmakov-Petrushevski on the basis of\nUrbantke, as well as Wyk) ternary relative velocity. The Einstein-Oziewicz and\nEinstein-Minkowski velocities, which are a four-dimensional generalization of\nEinstein velocities addition, also have a ternary character. This also applies\nto Oziewicz-Minkowski relative velocity, which is a time-like equivalent\n(generalization) of the canonical ternary velocity. It turns out that the\nLorentz transformation of velocity itself is already ternary anchored. This\nfact is explicitly revealed by the manifestly covariant Lorentz transformation\nof velocity, which is the main tool of the work.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A sensitive photoacoustic detection approach employing a silicon cantilever\nis investigated for power measurement of electromagnetic radiation. The\ntechnique which is actuated by pressure waves generated through\nradiation-induced heat, depicts high sensitivity for a considerably large\nspectral range from 325 nm to 1523 nm. The implemented method shows linear\nresponse in the measurement of radiation power from 15 nW to 6 mW,\ndemonstrating a dynamic range of almost six orders of magnitude. A numerical\nmodel has been developed to analyse and optimise the measurement sensitivity\nusing different dimensions of the cantilever which is one of the key components\nof the detection process. The numerical results are in good agreement with the\nexperimentally obtained frequency response of the detection process. The power\ndetection technique shows potential of finding future applications in the\ntechnologies that employ electromagnetic radiation detection for scientific\nstudies and industrial purposes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Visualization recommendation or automatic visualization generation can\nsignificantly lower the barriers for general users to rapidly create effective\ndata visualizations, especially for those users without a background in data\nvisualizations. However, existing rule-based approaches require tedious manual\nspecifications of visualization rules by visualization experts. Other machine\nlearning-based approaches often work like black-box and are difficult to\nunderstand why a specific visualization is recommended, limiting the wider\nadoption of these approaches. This paper fills the gap by presenting KG4Vis, a\nknowledge graph (KG)-based approach for visualization recommendation. It does\nnot require manual specifications of visualization rules and can also guarantee\ngood explainability. Specifically, we propose a framework for building\nknowledge graphs, consisting of three types of entities (i.e., data features,\ndata columns and visualization design choices) and the relations between them,\nto model the mapping rules between data and effective visualizations. A\nTransE-based embedding technique is employed to learn the embeddings of both\nentities and relations of the knowledge graph from existing\ndataset-visualization pairs. Such embeddings intrinsically model the desirable\nvisualization rules. Then, given a new dataset, effective visualizations can be\ninferred from the knowledge graph with semantically meaningful rules. We\nconducted extensive evaluations to assess the proposed approach, including\nquantitative comparisons, case studies and expert interviews. The results\ndemonstrate the effectiveness of our approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Wasserstein gradient flows provide a powerful means of understanding and\nsolving many diffusion equations. Specifically, Fokker-Planck equations, which\nmodel the diffusion of probability measures, can be understood as gradient\ndescent over entropy functionals in Wasserstein space. This equivalence,\nintroduced by Jordan, Kinderlehrer and Otto, inspired the so-called JKO scheme\nto approximate these diffusion processes via an implicit discretization of the\ngradient flow in Wasserstein space. Solving the optimization problem associated\nto each JKO step, however, presents serious computational challenges. We\nintroduce a scalable method to approximate Wasserstein gradient flows, targeted\nto machine learning applications. Our approach relies on input-convex neural\nnetworks (ICNNs) to discretize the JKO steps, which can be optimized by\nstochastic gradient descent. Unlike previous work, our method does not require\ndomain discretization or particle simulation. As a result, we can sample from\nthe measure at each time step of the diffusion and compute its probability\ndensity. We demonstrate our algorithm's performance by computing diffusions\nfollowing the Fokker-Planck equation and apply it to unnormalized density\nsampling as well as nonlinear filtering.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Expanding upon our previous study of competing critical phenomena in black\nhole formation, we numerically investigate the behavior of dominant exponents\nacross the boundary separating asymptotically dispersing and collapsing regions\nin a two-dimensional configuration space of initial data. We find that across\nthe Type II boundary section the dominant exponent remains constant, equal to\nthe reciprocal of Choptuik's well-known quasi-universal value, whereas across\nthe Type I section the exponent noticeably varies. We postulate that this\nchange reflects the existence of a third critical solution in addition to the\ntwo primary competing solutions, possibly another member of the family of\nmetastable soliton stars constituting the Type I attractor.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In network science complex systems are represented as a mathematical graphs\nconsisting of a set of nodes representing the components and a set of edges\nrepresenting their interactions. The framework of networks has led to\nsignificant advances in the understanding of the structure, formation and\nfunction of complex systems. Social and biological processes such as the\ndynamics of epidemics, the diffusion of information in social media, the\ninteractions between species in ecosystems or the communication between neurons\nin our brains are all actively studied using dynamical models on complex\nnetworks. In all of these systems, the patterns of connections at the\nindividual level play a fundamental role on the global dynamics and finding the\nmost important nodes allows one to better understand and predict their\nbehaviors. An important research effort in network science has therefore been\ndedicated to the development of methods allowing to find the most important\nnodes in networks. In this short entry, we describe network centrality measures\nbased on the notions of network traversal they rely on. This entry aims at\nbeing an introduction to this extremely vast topic, with many contributions\nfrom several fields, and is by no means an exhaustive review of all the\nliterature about network centralities.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Inference time, model size, and accuracy are three key factors in deep model\ncompression.\n  Most of the existing work addresses these three key factors separately as it\nis difficult to optimize them all at the same time.\n  For example, low-bit quantization aims at obtaining a faster model; weight\nsharing quantization aims at improving compression ratio and accuracy; and\nmixed-precision quantization aims at balancing accuracy and inference time. To\nsimultaneously optimize bit-width, model size, and accuracy, we propose pruning\nternary quantization (PTQ): a simple, effective, symmetric ternary quantization\nmethod. We integrate L2 normalization, pruning, and the weight decay term to\nreduce the weight discrepancy in the gradient estimator during quantization,\nthus producing highly compressed ternary weights. Our method brings the highest\ntest accuracy and the highest compression ratio. For example, it produces a\n939kb (49$\\times$) 2bit ternary ResNet-18 model with only 4\\% accuracy drop on\nthe ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\\times$) with\nonly 2.8\\% average precision drop. Our method is verified on image\nclassification, object detection/segmentation tasks with different network\nstructures such as ResNet-18, ResNet-50, and MobileNetV2.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The security and privacy concerns along with the amount of data that is\nrequired to be processed on regular basis has pushed processing to the edge of\nthe computing systems. Deploying advanced Neural Networks (NN), such as deep\nneural networks (DNNs) and spiking neural networks (SNNs), that offer\nstate-of-the-art results on resource-constrained edge devices is challenging\ndue to the stringent memory and power/energy constraints. Moreover, these\nsystems are required to maintain correct functionality under diverse security\nand reliability threats. This paper first discusses existing approaches to\naddress energy efficiency, reliability, and security issues at different system\nlayers, i.e., hardware (HW) and software (SW). Afterward, we discuss how to\nfurther improve the performance (latency) and the energy efficiency of Edge AI\nsystems through HW/SW-level optimizations, such as pruning, quantization, and\napproximation. To address reliability threats (like permanent and transient\nfaults), we highlight cost-effective mitigation techniques, like fault-aware\ntraining and mapping. Moreover, we briefly discuss effective detection and\nprotection techniques to address security threats (like model and data\ncorruption). Towards the end, we discuss how these techniques can be combined\nin an integrated cross-layer framework for realizing robust and\nenergy-efficient Edge AI systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We demonstrate that, hidden within one-layer randomly weighted neural\nnetworks, there exist subnetworks that can achieve impressive performance,\nwithout ever modifying the weight initializations, on machine translation\ntasks. To find subnetworks for one-layer randomly weighted neural networks, we\napply different binary masks to the same weight matrix to generate different\nlayers. Hidden within a one-layer randomly weighted Transformer, we find that\nsubnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed\npre-trained embedding layer, the previously found subnetworks are smaller than,\nbut can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained\nTransformer small/base on IWSLT14/WMT14. Furthermore, we demonstrate the\neffectiveness of larger and deeper transformers in this setting, as well as the\nimpact of different initialization methods. We released the source code at\nhttps://github.com/sIncerass/one_layer_lottery_ticket.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We perform 3D SPH simulations of warped, non-coplanar gravitationally\nunstable discs to show that as the warp propagates through the self-gravitating\ndisc, it heats up the disc rendering it gravitationally stable. Thus losing\ntheir spiral structure and appearing completely axisymmetric. In their youth,\nprotoplanetary discs are expected to be massive and self-gravitating, which\nresults in non-axisymmetric spiral structures. However recent observations of\nyoung protoplanetary discs with ALMA have revealed that discs with large-scale\nspiral structure are rarely observed in the midplane. Instead, axisymmetric\ndiscs with some also having ring & gap structures are more commonly observed.\nOur work invloving warps, non-coplanar disc structures that are expected to\ncommonly occur in young discs, potentially resolves this discrepancy between\nobservations and theoretical predictions. We demonstrate that they are able to\nsuppress the large-scale spiral structure of self-gravitating protoplanetary\ndiscs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In a recent work of Anderson and Hu, the authors constructed a measure that\nwas $p$-adic and $q$-adic doubling, for any primes $p$ and $q$, yet not\ndoubling. This work relied heavily on a developed number theory framework. Here\nwe develop this framework farther, which yields a measure that is $m$-adic and\n$n$-adic doubling for any coprime $m,n$, yet not doubling. Additionally we show\nseveral new applications to the intersection of weight and the function\nclasses.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove that quantum information propagates with a finite velocity in any\nmodel of interacting bosons whose (possibly time-dependent) Hamiltonian\ncontains spatially local single-boson hopping terms along with arbitrary local\ndensity-dependent interactions. More precisely, with density matrix $\\rho\n\\propto \\exp[-\\mu N]$ (with $N$ the total boson number), ensemble averaged\ncorrelators of the form $\\langle [A_0,B_r(t)]\\rangle $, along with\nout-of-time-ordered correlators, must vanish as the distance $r$ between two\nlocal operators grows, unless $t \\ge r/v$ for some finite speed $v$. In one\ndimensional models, we give a useful extension of this result that demonstrates\nthe smallness of all matrix elements of the commutator $[A_0,B_r(t)]$ between\nfinite density states if $t/r$ is sufficiently small. Our bounds are relevant\nfor physically realistic initial conditions in experimentally realized models\nof interacting bosons. In particular, we prove that $v$ can scale no faster\nthan linear in number density in the Bose-Hubbard model: this scaling matches\nprevious results in the high density limit. The quantum walk formalism\nunderlying our proof provides an alternative method for bounding quantum\ndynamics in models with unbounded operators and infinite-dimensional Hilbert\nspaces, where Lieb-Robinson bounds have been notoriously challenging to prove.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A discrete multidimensional system is the set of solutions to a system of\nlinear partial difference equations defined on the lattice $\\Z^n$. This paper\nshows that it is determined by a unique coarsest sublattice, in the sense that\nthe solutions of the system on this sublattice determine the solutions on\n$\\Z^n$; it is therefore the correct domain of definition of the discrete\nsystem. In turn, the defining sublattice is determined by a Galois group of\nsymmetries that leave invariant the equations defining the system. These\nresults find application in understanding properties of the system such as\ncontrollability and autonomy, and in its order reduction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the Euclidean $k$-Means problem we are given a collection of $n$ points\n$D$ in an Euclidean space and a positive integer $k$. Our goal is to identify a\ncollection of $k$ points in the same space (centers) so as to minimize the sum\nof the squared Euclidean distances between each point in $D$ and the closest\ncenter. This problem is known to be APX-hard and the current best approximation\nratio is a primal-dual $6.357$ approximation based on a standard LP for the\nproblem [Ahmadian et al. FOCS'17, SICOMP'20].\n  In this note we show how a minor modification of Ahmadian et al.'s analysis\nleads to a slightly improved $6.12903$ approximation. As a related result, we\nalso show that the mentioned LP has integrality gap at least\n$\\frac{16+\\sqrt{5}}{15}>1.2157$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Evolutionary game theory is a mathematical toolkit to analyse the\ninteractions that an individual agent has in a population and how the\ncomposition of strategies in this population evolves over time. While it can\nprovide neat solutions to simple problems, in more complicated situations where\nassumptions such as infinite population size may be relaxed, deriving analytic\nsolutions can be intractable. In this short paper, we present a game with\ncomplex interactions and examine how an agent-based model may be used as a\nheuristic technique to find evolutionarily stable states.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper is the fourth in a series presenting (galaxy morphology, and thus\ngalaxy formation)-dependent black hole mass, $M_{\\rm BH}$, scaling relations.\nWe have used a sample of 119 galaxies with directly-measured $M_{\\rm BH}$ and\nhost spheroid parameters obtained from multi-component decomposition of,\nprimarily, $3.6\\,\\mu$m Spitzer images. Here, we investigate the correlations\nbetween $M_{\\rm BH}$ and the projected luminosity density $\\mu$, the projected\nstellar mass density $\\Sigma$, and the deprojected (internal) stellar mass\ndensity $\\rho$, for various spheroid radii. We discover the predicted $M_{\\rm\nBH}$--$\\mu_{\\rm 0,sph}$ relation and present the first $M_{\\rm BH}$--$\\mu_{\\rm\ne, sph}$ and $M_{\\rm BH}$--$\\rho_{\\rm e,int, sph}$ diagrams displaying slightly\ndifferent (possibly curved) trends for early- and late-type galaxies (ETGs and\nLTGs) and an offset between ETGs with (fast-rotators, ES/S0) and without\n(slow-rotators, E) a disk. The scatter about various $M_{\\rm\nBH}$--$\\langle\\Sigma\\rangle_{\\rm R,sph}$ (and $\\langle\\rho\\rangle_{\\rm r,sph}$)\nrelations is shown to systematically decrease as the enclosing aperture (and\nvolume) increases, dropping from 0.69~dex when using the spheroid\n\\enquote{compactness}, $\\langle\\Sigma\\rangle_{\\rm 1kpc,sph}$, to 0.59~dex when\nusing $\\langle\\Sigma\\rangle_{\\rm 5kpc,sph}$. We also reveal that $M_{\\rm BH}$\ncorrelates with the internal density, $\\rho_{\\rm soi,sph}$, at the BH's\nsphere-of-influence radius, such that core-S\\'ersic (high S\\'ersic index, $n$)\nand (low-$n$) S\\'ersic galaxies define different relations with total rms\nscatters 0.21~dex and 0.77~dex, respectively.(Abridged)\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A spin current through a ferromagnet/heavy-metal interface may shrink due to\nthe spin-flip at the interface, resulting in the spin-memory loss. Here we\npropose a mechanism of the spin-memory loss. In contrast to other mechanisms\nbased on interfacial spin-orbit coupling, our mechanism is based on the bulk\nspin-orbit coupling in a heavy-metal. We demonstrate that the bulk spin-orbit\ncoupling induces the entanglement between the spin and orbital degrees of\nfreedom and this spin-orbital entanglement can give rise to sizable spin-flip\nat the interface even when the interfacial spin-orbit coupling is weak. Our\nmechanism emphasizes crucial roles of the atomic orbital degree of freedom and\ninduces the strong spin-memory loss near band crossing points between bands of\ndifferent orbital characters.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Virtual assistants such as Amazon's Alexa, Apple's Siri, Google Home, and\nMicrosoft's Cortana, are becoming ubiquitous in our daily lives and\nsuccessfully help users in various daily tasks, such as making phone calls or\nplaying music. Yet, they still struggle with playful utterances, which are not\nmeant to be interpreted literally. Examples include jokes or absurd requests or\nquestions such as, \"Are you afraid of the dark?\", \"Who let the dogs out?\", or\n\"Order a zillion gummy bears\". Today, virtual assistants often return\nirrelevant answers to such utterances, except for hard-coded ones addressed by\ncanned replies.\n  To address the challenge of automatically detecting playful utterances, we\nfirst characterize the different types of playful human-virtual assistant\ninteraction. We introduce a taxonomy of playful requests rooted in theories of\nhumor and refined by analyzing real-world traffic from Alexa. We then focus on\none node, personification, where users refer to the virtual assistant as a\nperson (\"What do you do for fun?\"). Our conjecture is that understanding such\nutterances will improve user experience with virtual assistants. We conducted a\nWizard-of-Oz user study and showed that endowing virtual assistant s with the\nability to identify humorous opportunities indeed has the potential to increase\nuser satisfaction. We hope this work will contribute to the understanding of\nthe landscape of the problem and inspire novel ideas and techniques towards the\nvision of giving virtual assistants a sense of humor.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Our experiments demonstrate that alloying the cubic-phase YbN into the\nwurtzite-phase AlN results in clear mechanical softening and enhanced\nelectromechanical coupling of AlN. First-principle calculations reproduce\nexperimental results well, and predict a maximum 270% increase in\nelectromechanical coupling coefficient caused by (1) an enhanced piezoelectric\nresponse induced by the local strain of Yb ions and (2) a structural\nflexibility of the (Yb,Al)N alloy. Extensive calculations suggest that the\nsubstitutional neighbor Yb-Yb pairs in wurtzite AlN are energetically stable\nalong $c$ axis, and avoid forming on the basal plane of wurtzite structure due\nto the repulsion between them, which explains that (Yb,Al)N films with high Yb\nconcentrations are difficult to fabricate in our sputtering experiments.\nMoreover, the neighbor Yb-Yb pair interactions also promote structural\nflexibility of (Yb,Al)N, and are considered a cause for mechanical softening of\n(Yb,Al)N.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop a new method of online inference for a vector of parameters\nestimated by the Polyak-Ruppert averaging procedure of stochastic gradient\ndescent (SGD) algorithms. We leverage insights from time series regression in\neconometrics and construct asymptotically pivotal statistics via random\nscaling. Our approach is fully operational with online data and is rigorously\nunderpinned by a functional central limit theorem. Our proposed inference\nmethod has a couple of key advantages over the existing methods. First, the\ntest statistic is computed in an online fashion with only SGD iterates and the\ncritical values can be obtained without any resampling methods, thereby\nallowing for efficient implementation suitable for massive online data. Second,\nthere is no need to estimate the asymptotic variance and our inference method\nis shown to be robust to changes in the tuning parameters for SGD algorithms in\nsimulation experiments with synthetic data.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A code is called $(n, k, r, t)$ information symbol locally repairable code\n\\big($(n, k, r, t)_i$ LRC\\big) if each information coordinate can be achieved\nby at least $t$ disjoint repair sets, containing at most $r$ other coordinates.\nThis paper considers a class of $(n, k, r, t)_i$ LRCs, where each repair set\ncontains exactly one parity coordinate. We explore the systematic code in terms\nof the standard parity check matrix. First, some structural features of the\nparity check matrix are proposed by showing some connections with the\nmembership matrix and the minimum distance optimality of the code. Next to\nthat, parity check matrix based proofs of various bounds associated with the\ncode are placed. In addition to this, we provide several constructions of\noptimal $(n, k, r, t)_i$ LRCs, with the help of two Cayley tables of a finite\nfield. Finally, we generalize a result of $q$-ary $(n, k, r)$ LRCs to $q$-ary\n$(n, k, r, t)$ LRCs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It is well known that numerical simulations of high-speed reacting flows, in\nthe framework of state-to-state formulations, are the most detailed but also\noften prohibitively computationally expensive. In this work, we start to\ninvestigate the possibilities offered by the use of machine learning methods\nfor state-to-state approaches to alleviate such burden.\n  In this regard, several tasks have been identified. Firstly, we assessed the\npotential of state-of-the-art data-driven regression models based on machine\nlearning to predict the relaxation source terms which appear in the right-hand\nside of the state-to-state Euler system of equations for a one-dimensional\nreacting flow of a N$_2$/N binary mixture behind a plane shock wave. It is\nfound that, by appropriately choosing the regressor and opportunely tuning its\nhyperparameters, it is possible to achieve accurate predictions compared to the\nfull-scale state-to-state simulation in significantly shorter times.\n  Secondly, we investigated different strategies to speed-up our in-house\nstate-to-state solver by coupling it with the best-performing pre-trained\nmachine learning algorithm. The embedding of machine learning methods into\nordinary differential equations solvers may offer a speed-up of several orders\nof magnitude but some care should be paid for how and where such coupling is\nrealized. Performances are found to be strongly dependent on the mutual nature\nof the interfaced codes.\n  Finally, we aimed at inferring the full solution of the state-to-state Euler\nsystem of equations by means of a deep neural network completely by-passing the\nuse of the state-to-state solver while relying only on data. Promising results\nsuggest that deep neural networks appear to be a viable technology also for\nthese tasks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  One of the major challenges in modern robotics is controlling\nmicromanipulation by active and adaptive materials. In the respiratory system,\nsuch actuation enables pathogen clearance by means of motile cilia. While\nvarious types of artificial cilia have been engineered recently, they often\ninvolve complex manufacturing protocols and focus on transporting liquids only.\nHere, we create soft magnetic carpets via an easy self-assembly route based on\nthe Rosensweig instability. These carpets can transport liquids but also solid\nobjects that are larger and heavier than the artificial cilia, using a\ncrowd-surfing effect. This amphibious transportation is locally and\nreconfigurably tuneable by simple micromagnets or advanced programmable\nmagnetic fields with a high degree of spatial resolution. We identify and model\ntwo surprising cargo reversal effects due to collective ciliary motion and\nnon-trivial elastohydrodynamics. While our active carpets are generally\napplicable to integrated control systems for transport, mixing and sorting,\nthese effects could also be exploited for microfluidic viscosimetry and\nelastometry.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work we investigate the behaviour of a human crowd in a cross-flow.\nWe first analyse the results of a set of controlled experiments in which\nsubjects were divided into two groups, in such a way to explore different\ndensity settings, and asked to walk through the crossing area. We study the\nresults of the experiment by analysing, along with traditional indicators such\nas density and velocity, also walking and body orientation, studying how these\nmicroscopic observables are influenced by density. Furthermore, we report a\npreliminary but quantitative analysis on the emergence of self-organising\nstripes in the crossing area.\n  We also try to reproduce the empirical results using a hierarchy of models,\nwhich differ in the details of the body shape (using a disk-shaped body vs a\nmore realistic elliptical shape) and in how collision avoiding is performed\n(using only information regarding \"centre of mass\" distance and velocity, or\nactually introducing body shape information). We verified that the most\ndetailed model (i.e., using body shape information and an elliptical body)\noutperforms in a significant way the simplest one (using only centre of mass\ndistance and velocity, and disk-shaped bodies). Furthermore, we observed that\nif elliptical bodies are introduced without using such information in collision\navoidance, the performance of the model is relatively poor. Nevertheless, the\ndifference between the different models is relevant only in describing the\n\"tails\" of the observable distributions, suggesting that the more complex\nmodels could be of practical use only for describing high density settings. We\nalso verified that \"stripe formation\" emerges in all models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we present an efficient and effective single-stage framework\n(DiverGAN) to generate diverse, plausible and semantically consistent images\naccording to a natural-language description. DiverGAN adopts two novel\nword-level attention modules, i.e., a channel-attention module (CAM) and a\npixel-attention module (PAM), which model the importance of each word in the\ngiven sentence while allowing the network to assign larger weights to the\nsignificant channels and pixels semantically aligning with the salient words.\nAfter that, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is\nintroduced to enable the linguistic cues from the sentence embedding to\nflexibly manipulate the amount of change in shape and texture, further\nimproving visual-semantic representation and helping stabilize the training.\nAlso, a dual-residual structure is developed to preserve more original visual\nfeatures while allowing for deeper networks, resulting in faster convergence\nspeed and more vivid details. Furthermore, we propose to plug a fully-connected\nlayer into the pipeline to address the lack-of-diversity problem, since we\nobserve that a dense layer will remarkably enhance the generative capability of\nthe network, balancing the trade-off between a low-dimensional random latent\ncode contributing to variants and modulation modules that use high-dimensional\nand textual contexts to strength feature maps. Inserting a linear layer after\nthe second residual block achieves the best variety and quality. Both\nqualitative and quantitative results on benchmark data sets demonstrate the\nsuperiority of our DiverGAN for realizing diversity, without harming quality\nand semantic consistency.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Data-intensive science is increasingly reliant on real-time processing\ncapabilities and machine learning workflows, in order to filter and analyze the\nextreme volumes of data being collected. This is especially true at the energy\nand intensity frontiers of particle physics where bandwidths of raw data can\nexceed 100 Tb/s of heterogeneous, high-dimensional data sourced from hundreds\nof millions of individual sensors. In this paper, we introduce a new\ndata-driven approach for designing and optimizing high-throughput data\nfiltering and trigger systems such as those in use at physics facilities like\nthe Large Hadron Collider (LHC). Concretely, our goal is to design a\ndata-driven filtering system with a minimal run-time cost for determining which\ndata event to keep, while preserving (and potentially improving upon) the\ndistribution of the output as generated by the hand-designed trigger system. We\nintroduce key insights from interpretable predictive modeling and\ncost-sensitive learning in order to account for non-local inefficiencies in the\ncurrent paradigm and construct a cost-effective data filtering and trigger\nmodel that does not compromise physics coverage.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a new mechanical probe to study the properties of superfluid\n$^3$He at microkelvin temperatures down to 100$\\mu$K. The setup consists of a\nset of coils for levitating a superconducting sphere and controlling its motion\nin a wide variety of regimes. In particular, the realisation of motion of a\nlevitating body at a uniform velocity presents both an experimental challenge\nand a promising direction into the study of the edge states in topological\nsuperfluid $^3$He-B. We include the theoretical study of the device stability\nand simulations to illustrate the capabilities of the control system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The warming climate is causing livestock to experience heat stress at an\nincreasing frequency. Holstein cows are particularly susceptible to heat stress\nbecause of their high metabolic rate. Heat stress negatively affects immune\nfunction, particularly with respect to the cell-mediated immune response, which\nleads to increased susceptibility to disease. Cattle identified as having\nenhanced immune response have lower incidence of disease. Therefore, the\nobjective of this study was to evaluate the impact of in vitro heat challenge\non blood mononuclear cells from dairy cattle, that had previously been ranked\nfor immune response, in terms of heat shock protein 70 concentration, nitric\noxide production, and cell proliferation. Bovine blood mononuclear cells, from\nHolstein dairy cattle previously ranked for immune response based on their\nestimated breeding values, were subjected to three heat treatments:\nthermoneutral, heat stress 1 and heat stress 2. Cells of each treatment were\nevaluated for heat shock protein 70, cell proliferation and nitric oxide\nproduction. Blood mononuclear cells from dairy cattle classified as high immune\nresponders, based on their estimated breeding values for antibody and\ncell-mediated responses, produced a significantly greater concentration of heat\nshock protein 70 under most heat stress treatments compared to average and low\nresponders, and greater cell-proliferation across all treatments. Similarly, a\ntrend was observed where high responders displayed greater nitric oxide\nproduction compared to average and low responders across heat treatments.\nOverall, these results suggest that blood mononuclear cells from high immune\nresponder dairy cows are more thermotolerant compared to average and low immune\nresponders\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Economic and financial theories and practice essentially deal with uncertain\nfuture. Humans encounter uncertainty in different kinds of activity, from\nsensory-motor control to dynamics in financial markets, what has been subject\nof extensive studies. Representation of uncertainty with normal or lognormal\ndistribution is a common feature of many of those studies. For example,\nproposed Bayessian integration of Gaussian multisensory input in the brain or\nlog-normal distribution of future asset price in renowned Black-Scholes-Merton\n(BSM) model for pricing contingent claims.\n  Standard deviation of log(future asset price) scaled by square root of time\nin the BSM model is called implied volatility. Actually, log(future asset\nprice) is not normally distributed and traders account for that to avoid\nlosses. Nevertheless the BSM formula derived under the assumption of constant\nvolatility remains a major uniform framework for pricing options in financial\nmarkets. I propose that one of the reasons for such a high popularity of the\nBSM formula could be its ability to translate uncertainty measured with implied\nvolatility into price in a way that is compatible with human intuition for\nmeasuring uncertainty.\n  The present study deals with mathematical relationship between uncertainty\nand the BSM implied volatility. Examples for a number of common probability\ndistributions are presented. Overall, this work proposes that representation of\nvarious probability distributions in terms of the BSM implied volatility\nprofile may be meaningful in both biological and financial worlds. Necessary\nbackground from financial mathematics is provided in the text.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The age of a supernova remnant (SNR) is, though undoubtedly one of the most\nimportant properties for study of its evolution, difficult to estimate reliably\nin most cases. In this study, we compare the dynamical and plasma ages of the\nSNRs and characteristic ages of their associated pulsars with the corresponding\nSNRs' ages that are generally thought to be reliable ($t_{\\rm r}$): historical\nand light-echo ages of the SNRs, kinematic ages of the ejecta knots and\nkinematic ages of the associated neutron stars (NS). The kinematic age of\nejecta knots or a NS is the time that they have taken to reach the current\npositions from the explosion center. We use all of the available 24 systems for\nwhich $t_{\\rm r}$ is already available (historical, light-echo, and ejecta\nkinematic ages) or measurable (NS kinematic age). We estimate the NS kinematic\nages for eight SNR-NS systems by determining quantitatively the geometric\ncenters of the SNR shells. The obtained $t_{\\rm r}$ ranges from 33 yr to\n$\\approx 400$ kyr. We find that the two SNR ages, dynamical and plasma ages,\nare consistent with $t_{\\rm r}$ within a factor of four, whereas the\ncharacteristic ages of the pulsars differ from $t_{\\rm r}$ by more than a\nfactor of four in some systems. Using the $t_{\\rm r}$ summarized in this work,\nwe present the initial spin periods of the associated pulsars, which are more\nstrictly constrained than the previous works, as well.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Kitaev model predicts that quantum spin liquids (QSLs) will form at low\ntemperatures under certain special conditions, and materials hosting the QSL\nstate are frequently sought. The layered honeycomb lattice material\n{\\alpha}-RuCl3 has emerged as a prime candidate for displaying the Kitaev QSL\nstate. Here we describe a new polymorph of RuI3 with a layered honeycomb\nlattice structure, synthesized at moderately high pressures and stable under\nambient conditions. Preliminary characterization reveals metallic, paramagnetic\nbehavior, the absence of long-range magnetic order down to 0.35 K and an\nunusually large T-linear contribution to the heat capacity at low temperatures.\nWe propose that this RuI3 phase, with a layered honeycomb lattice and strong\nspin-orbit coupling, provides a new route for the characterization of quantum\nmaterials.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate models of nonlinear quantum computation based on deterministic\npositive trace-preserving (PTP) channels and associated master equations. The\nmodels are defined in any finite Hilbert space, but the main results are for\ndimension $N = 2$. For every normalizable linear or nonlinear positive map\n$\\phi$ on bounded linear operators $X$, there is an associated normalized PTP\nchannel $ \\phi(X) / {\\rm tr}[\\phi(X)]$. Normalized PTP channels include unitary\nmean field theories, such as the Gross-Pitaevskii equation for interacting\nbosons, as well as models of linear and nonlinear dissipation. They classify\ninto 4 types, yielding 3 distinct forms of nonlinearity whose computational\npower we explore. In the qubit case these channels support Bloch ball torsion\nand other distortions studied previously, where it has been shown that such\nnonlinearity can be used to increase the separation between a pair of close\nqubit states, resulting in an exponential speedup for state discrimination.\nBuilding on this idea, we argue that this operation can be made robust to noise\nby using dissipation to induce a bifurcation to a phase where a pair of stable\nfixed points create an intrinisically fault-tolerant nonlinear state\ndiscriminator.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Clustering is a commonly used method for exploring and analysing data where\nthe primary objective is to categorise observations into similar clusters. In\nrecent decades, several algorithms and methods have been developed for\nanalysing clustered data. We notice that most of these techniques\ndeterministically define a cluster based on the value of the attributes,\ndistance, and density of homogenous and single-featured datasets. However,\nthese definitions are not successful in adding clear semantic meaning to the\nclusters produced. Evolutionary operators and statistical and\nmulti-disciplinary techniques may help in generating meaningful clusters. Based\non this premise, we propose a new evolutionary clustering algorithm (ECAStar)\nbased on social class ranking and meta-heuristic algorithms for stochastically\nanalysing heterogeneous and multiple-featured datasets. The ECAStar is\nintegrated with recombinational evolutionary operators, Levy flight\noptimisation, and some statistical techniques, such as quartiles and\npercentiles, as well as the Euclidean distance of the K-means algorithm.\nExperiments are conducted to evaluate the ECAStar against five conventional\napproaches: K-means (KM), K-meansPlusPlus (KMPlusPlus), expectation\nmaximisation (EM), learning vector quantisation (LVQ), and the genetic\nalgorithm for clusteringPlusPlus (GENCLUSTPlusPlus).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the limits of linear modeling of swarm behavior by characterizing\nthe inflection point beyond which linear models of swarm collective behavior\nbreak down. The problem we consider is a central place object gathering task.\nWe design a linear model which strives to capture the underlying dynamics of\nobject gathering in robot swarms from first principles, rather than extensively\nrelying on post-hoc model fitting. We evaluate our model with swarms of up to\n8,000 robots in simulation, demonstrating that it accurately captures\nunderlying swarm behavioral dynamics when the swarm can be approximated using\nthe mean-field model, and when it cannot, and finite-size effects are present.\nWe further apply our model to swarms exhibiting non-linear behaviors, and show\nthat it still provides accurate predictions in some scenarios, thereby\nestablishing better practical limits on linear modeling of swarm behaviors.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Science Data Systems (SDS) handle science data from acquisition through\nprocessing to distribution. They are deployed in the Cloud today, and the\nefficiency of Cloud instance utilization is critical to success. Conventional\nSDS are unable to take advantage of a cost-effective Amazon EC2 spot market,\nespecially for long-running tasks. Some of the difficulties found in current\npractice at NASA/JPL are: a lack of mechanism for app programmers to save\nvaluable partial results for future processing continuation, the heavy weight\nfrom using container-based (Singularity) sandboxes with more than 200,000\nOS-level files; and the gap between scientists developing algorithms/programs\non a laptop and the SDS experts deploying software in Cloud computing or\nsupercomputing.\n  We present a first proof-of-principle of this using NavP (Navigational\nProgramming) and fault-tolerant computing (FTC) in SDS, by employing program\nstate migration facilitated by Checkpoint-Restart (C/R). NavP provides a new\nnavigational view of computations in a distributed world for the application\nprogrammers. The tool of DHP (DMTCP Hop and Publish) we developed enables the\napplication programmers to navigate the computation among instances or nodes by\ninserting hop(destination) statements in their app code, and choose when to\npublish partial results at stages of their algorithms that they think\nworthwhile for future continuation. The result of using DHP is that a parallel\ndistributed SDS becomes easier to program and deploy, and this enables more\nefficient leveraging of the Amazon EC2 Spot market. This technical report\ndescribes a high-level design and an initial implementation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The extension of scalars functor along a finite ring homomorphism is a\nclassic example of a functor which preserves purity and pure injectivity. We\nconsider how this functor behaves when restricted to the class of Gorenstein\nflat modules over a right coherent rings, and give particular attention to the\nFrobenius category of Gorenstein flat and cotorsion modules by showing there is\nan induced triangulated functor on the stable categories. This enables a\ncomparison between pure injectivity for Gorenstein flat modules and pure\ninjectivity in the triangulated categories as well as an investigation in how\npurity for Gorenstein flat modules is transferred along the homomorphism.\nThroughout motivating examples from commutative algebra are considered,\nincluding over hypersurfaces where a pure injective analogue of Knoerrer\nperiodicity for Gorenstein flat modules is developed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report on controllable cavity modes through controlling the backscattering\nby two identical scatterers. Periodic changes of the backscattering coupling\nbetween two degenerate cavity modes are observed with the angle between two\nscatterers and elucidated by a theoretical model using two-mode approximation\nand numerical simulations. The periodically appearing single-peak cavity modes\nindicate mode degeneracy at diabolical points. Then interactions between single\nquantum dots and cavity modes are investigated. Enhanced emission of a quantum\ndot with a six-fold intensity increase is obtained in a microdisk at a\ndiabolical point. This method to control cavity modes allows large-scale\nintegration, high reproducibility and fexible design of the size, location,\nquantity and shape for scatterers, which can be applied for integrated photonic\nstructures with scatterer-modified light-matter interaction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Drell-Yan process is studied in the framework of TMD factorization in the\nSudakov region $s\\gg Q^2\\gg q_\\perp^2$ corresponding to recent LHC experiments\nwith $Q^2$ of order of mass of Z-boson and transverse momentum of DY pair\n$\\sim$ few tens GeV. The DY hadronic tensors are expressed in terms of quark\nand quark-gluon TMDs with ${1\\over Q^2}$ and ${1\\over N_c^2}$ accuracy. It is\ndemonstrated that in the leading order in $N_c$ the higher-twist\nquark-quark-gluon TMDs reduce to leading-twist TMDs due to QCD equation of\nmotion. The resulting hadronic tensors depend on two leading-twist TMDs: $f_1$\nresponsible for total DY cross section, and Boer-Mulders function $h_1^\\perp$.\nThe corresponding qualitative and semi-quantitative predictions seem to agree\nwith LHC data on five angular coefficients $A_0-A_4$ of DY pair production. The\nremaining three coefficients $A_5-A_7$ are determined by quark-quark-gluon TMDs\nmultiplied by extra ${1\\over N_c}$ so they appear to be relatively small in\naccordance with LHC results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Tsunami waves induced by landslides are a threat to human activities and\nsafety along coastal areas. In this paper, we characterize experimentally the\nwaves generated by the gravity-driven collapse of a dry granular column into\nwater. Three nonlinear wave regimes are identified depending on the Froude\nnumber $\\mathrm{Fr}_f$ based on the ratio of the velocity of the advancing\ngranular front and the velocity of linear gravity waves in shallow water:\ntransient bores for large $\\mathrm{Fr}_f$, solitary waves for intermediate\nvalues of $\\mathrm{Fr}_f$, and non-linear transition waves at small\n$\\mathrm{Fr}_f$. The wave amplitude relative to the water depth increases with\n$\\mathrm{Fr}_f$ in the three regimes but with different non-linear scalings,\nand the relative wavelength is an increasing or decreasing function of\n$\\mathrm{Fr}_f$. Two of these wave regimes are rationalized by considering that\nthe advancing granular front acts as a vertical piston pushing the water, while\nthe last one is found to be a transition from shallow to deep water conditions.\nThe present modeling contributes to a better understanding of the rich\nhydrodynamics of the generated waves, with coastal risk assessment as practical\napplications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We experimentally demonstrate time-resolved exciton propagation in a\nmonolayer semiconductor at cryogenic temperatures. Monitoring phonon-assisted\nrecombination of dark states, we find a highly unusual case of exciton\ndiffusion. While at 5 K the diffusivity is intrinsically limited by acoustic\nphonon scattering, we observe a pronounced decrease of the diffusion\ncoefficient with increasing temperature, far below the activation threshold of\nhigher-energy phonon modes. This behavior corresponds neither to well-known\nregimes of semiclassical free-particle transport nor to the thermally activated\nhopping in systems with strong localization. Its origin is discussed in the\nframework of both microscopic numerical and semi-phenomenological analytical\nmodels illustrating the observed characteristics of nonclassical propagation.\nChallenging the established description of mobile excitons in monolayer\nsemiconductors, these results open up avenues to study quantum transport\nphenomena for excitonic quasiparticles in atomically-thin van der Waals\nmaterials and their heterostructures.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the implementation of two-qubit gates when the physical systems\nused to realize the qubits possess additional quantum states in the accessible\nenergy range. We use optimal control theory to determine the maximum achievable\ngate speed for two-qubit gates in the qubit subspace of the many-level Hilbert\nspace, and we analyze the effect of the additional quantum states on the gate\nspeed. We identify two competing mechanisms. On one hand, higher energy levels\nare generally more strongly coupled to each other. Under suitable conditions,\nthis stronger coupling can be utilized to make two-qubit gates significantly\nfaster than the reference value based on simple qubits. On the other hand, a\nweak anharmonicity constrains the speed at which the system can be adequately\ncontrolled: according to the intuitive picture, faster operations require\nstronger control fields, which are more likely to excite higher levels in a\nweakly anharmonic system, which in turn leads to faster decoherence and\nuncontrolled leakage outside the qubit space. In order to account for this\nconstraint, we modify the pulse optimization algorithm to avoid pulses that\nlead to appreciable population of the higher levels. In this case we find that\nthe presence of the higher levels can lead to a significant reduction in the\nmaximum achievable gate speed. We also compare the optimal-control gate speeds\nwith those obtained using the cross-resonance/selective-darkening gate\nprotocol. We find that the latter, with some parameter optimization, can be\nused to achieve a relatively fast implementation of the CNOT gate. These\nresults can help the search for optimized gate implementations in realistic\nquantum computing architectures, such as those based on superconducting\ncircuits. They also provide guidelines for desirable conditions on\nanharmonicity that allow optimal utilization of the higher levels to achieve\nfast quantum gates.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Motion planning of an autonomous system with high-level specifications has\nwide applications. However, research of formal languages involving timed\ntemporal logic is still under investigation. Furthermore, many existing results\nrely on a key assumption that user-specified tasks are feasible in the given\nenvironment. Challenges arise when the operating environment is dynamic and\nunknown since the environment can be found prohibitive, leading to potentially\nconflicting tasks where pre-specified timed missions cannot be fully satisfied.\nSuch issues become even more challenging when considering time-bound\nrequirements. To address these challenges, this work proposes a control\nframework that considers hard constraints to enforce safety requirements and\nsoft constraints to enable task relaxation. The metric interval temporal logic\n(MITL) specifications are employed to deal with time-bound constraints. By\nconstructing a relaxed timed product automaton, an online motion planning\nstrategy is synthesized with a receding horizon controller to generate\npolicies, achieving multiple objectives in decreasing order of priority 1)\nformally guarantee the satisfaction of hard safety constraints; 2) mostly\nfulfill soft timed tasks; and 3) collect time-varying rewards as much as\npossible. Another novelty of the relaxed structure is to consider violations of\nboth time and tasks for infeasible cases. Simulation results are provided to\nvalidate the proposed approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, as the first part of the third step of our study on developing\ndata analysis procedures for using 3-dimensional information offered by\ndirectional direct Dark Matter detection experiments in the future, we present\nour double-Monte Carlo \"scattering-by-scattering\" simulation of the\n3-dimensional elastic WIMP-nucleus scattering process, which can provide 3-D\nvelocity information (the magnitude, the direction, and the incoming/scattering\ntime) of each incident halo WIMP as well as the recoil direction and the recoil\nenergy of the scattered target nucleus in different celestial coordinate\nsystems. For readers' reference, (animated) simulation plots with different\nWIMP masses and several frequently used target nuclei for all functionable\nunderground laboratories can be found and downloaded on our online\n(interactive) demonstration webpage (http://www.tir.tw/phys/hep/dm/amidas-2d/).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Conceptual graphs, which is a particular type of Knowledge Graphs, play an\nessential role in semantic search. Prior conceptual graph construction\napproaches typically extract high-frequent, coarse-grained, and time-invariant\nconcepts from formal texts. In real applications, however, it is necessary to\nextract less-frequent, fine-grained, and time-varying conceptual knowledge and\nbuild taxonomy in an evolving manner. In this paper, we introduce an approach\nto implementing and deploying the conceptual graph at Alibaba. Specifically, We\npropose a framework called AliCG which is capable of a) extracting fine-grained\nconcepts by a novel bootstrapping with alignment consensus approach, b) mining\nlong-tail concepts with a novel low-resource phrase mining approach, c)\nupdating the graph dynamically via a concept distribution estimation method\nbased on implicit and explicit user behaviors. We have deployed the framework\nat Alibaba UC Browser. Extensive offline evaluation as well as online A/B\ntesting demonstrate the efficacy of our approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Hermitian, pluriclosed metrics with vanishing Bismut-Ricci form give a\nnatural extension of Calabi-Yau metrics to the setting of complex, non-K\\\"ahler\nmanifolds, and arise independently in mathematical physics. We reinterpret this\ncondition in terms of the Hermitian-Einstein equation on an associated\nholomorphic Courant algebroid, and thus refer to solutions as Bismut\nHermitian-Einstein. This implies Mumford-Takemoto slope stability obstructions,\nand using these we exhibit infinitely many topologically distinct complex\nmanifolds in every dimension with vanishing first Chern class which do not\nadmit Bismut Hermitian-Einstein metrics. This reformulation also leads to a new\ndescription of pluriclosed flow in terms of Hermitian metrics on holomorphic\nCourant algebroids, implying new global existence results, in particular on all\ncomplex non-K\\\"ahler surfaces of nonnegative Kodaira dimension. On complex\nmanifolds which admit Bismut-flat metrics we show global existence and\nconvergence of pluriclosed flow to a Bismut-flat metric, which in turn gives a\nclassification of generalized K\\\"ahler structures on these spaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The zirconium isotopes with $A=$ 92$-$110 have one of the most complicated\nevolution of structure in the nuclear chart. In order to understand the\nstructural evolution of these isotopes, we carry a detailed calculation in a\ndefinite symmetry-based framework, the interacting boson model with\nconfiguration mixing (IBM-CM). We compare our calculation to a large range of\nexperimental data, such as energy levels, two neutron separation energies, $E2$\nand $E0$ transition rates, isotope shifts and magnetic moments. The structural\nevolution of the low lying spectra of these isotopes is explained using the\nnotion of intertwined quantum phase transitions (IQPTs), for which a QPT\ninvolving a crossing of two configurations (Type II) is accompanied by a QPT\ninvolving a shape evolution of each configuration separately (Type I). In our\nstudy, we find the occurrence of Type I QPT within the intruder configuration,\nchanging from weakly deformed to prolate deformed and finally to\n$\\gamma$-unstable, associated with the U(5), SU(3) and SO(6) dynamical symmetry\nlimits of the IBM, respectively. Alongside the Type I QPT, we also find the\noccurrence of Type II QPT between the normal and intruder configurations, where\nboth Types I and II have a critical-point near $A\\approx100$. The good\nagreement of our calculation with the vast empirical data along the chain of\nisotopes demonstrates the relevance of IQPTs to the zirconium isotopes, and can\nserve as a case study to set path for new investigations of IQPTs in other\nnuclei and other physical systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Learning models that generalize under different distribution shifts in\nmedical imaging has been a long-standing research challenge. There have been\nseveral proposals for efficient and robust visual representation learning among\nvision research practitioners, especially in the sensitive and critical\nbiomedical domain. In this paper, we propose an idea for out-of-distribution\ngeneralization of chest X-ray pathologies that uses a simple balanced batch\nsampling technique. We observed that balanced sampling between the multiple\ntraining datasets improves the performance over baseline models trained without\nbalancing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce two-scale loss functions for use in various gradient descent\nalgorithms applied to classification problems via deep neural networks. This\nnew method is generic in the sense that it can be applied to a wide range of\nmachine learning architectures, from deep neural networks to support vector\nmachines for example. These two-scale loss functions allow to focus the\ntraining onto objects in the training set which are not well classified. This\nleads to an increase in several measures of performance for\nappropriately-defined two-scale loss functions with respect to the more\nclassical cross-entropy when tested on traditional deep neural networks on the\nMNIST, CIFAR10, and CIFAR100 data-sets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work we shall study a possible pre-inflationary scenario for our\nUniverse and how this might be realized by $f(R)$ gravity. Specifically, we\nshall introduce a scenario in which the Universe in the pre-inflationary era\ncontracts until it reaches a minimum magnitude, and subsequently expands,\nslowly entering a slow-roll quasi-de Sitter inflationary era. This\npre-inflationary bounce avoids the cosmic singularity, and for the eras before\nand after the quasi-de Sitter inflationary stage, approximately satisfies the\nstring theory motivated scale factor duality $a(t)=a^{-1}(-t)$. We investigate\nwhich approximate forms of $f(R)$ can realize such a non-singular\npre-inflationary scenario, the quasi-de Sitter patch of which is described by\nan $R^2$ gravity, thus the exit from inflation is guaranteed. Furthermore,\nsince in string theory pre-Big Bang scenarios lead to an overall amplification\nof the gravitational wave energy spectrum, we examine in detail this\nperspective for the $f(R)$ gravity generating this pre-inflationary\nnon-singular bounce. As we show, in the $f(R)$ gravity case, the energy\nspectrum of the primordial gravitational waves background is also amplified,\nhowever the drawback is that the amplification is too small to be detected by\nfuture high frequency interferometers. Thus we conclude that, as in the case of\nsingle scalar field theories, $f(R)$ gravity cannot produce detectable\nstochastic gravitational waves and a synergistic theory of scalars and higher\norder curvature terms might be needed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Most available semantic parsing datasets, comprising of pairs of natural\nutterances and logical forms, were collected solely for the purpose of training\nand evaluation of natural language understanding systems. As a result, they do\nnot contain any of the richness and variety of natural-occurring utterances,\nwhere humans ask about data they need or are curious about. In this work, we\nrelease SEDE, a dataset with 12,023 pairs of utterances and SQL queries\ncollected from real usage on the Stack Exchange website. We show that these\npairs contain a variety of real-world challenges which were rarely reflected so\nfar in any other semantic parsing dataset, propose an evaluation metric based\non comparison of partial query clauses that is more suitable for real-world\nqueries, and conduct experiments with strong baselines, showing a large gap\nbetween the performance on SEDE compared to other common datasets.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Traffic congestion in big cities has been proven to be a difficult problem\nwith suboptimal effects in terms of driver delay and frustration, cost, and\nimpact on the environment. In principle, many transportation networks lack a\nunified framework, which will coordinate the traffic in such a manner, in order\nto suppress congestion and at the same time improve the travel time of the\nusers situated in it. The rapid advancements in information, communication, and\ncomputation technologies have given rise to more elaborate modeling frameworks,\naiming to act as the coordination unit necessary to counter the issue of\ncongestion in real-time conditions. Such actions might have an adverse effect\non the efficiency of the network, prospectively leading to greater waiting time\nintervals for each individual driver. We propose a macroscopic model equipped\nwith an underlying reservation feature, known as Route Reservation Architecture\n(RRA). Vehicles enter the network (mainstream-wise or from the on-ramps) as\nlong as this inflow does not incur a density that exceeds the network's\ncritical density. Those vehicles prospectively exceeding the critical density\nare stored as queues at the origin of the motorway stretch and within the\non-ramps. Once there is sufficient space, for those vehicles to be accommodated\nby the respective cell of the network, they are discharged from their queueing\ninstance at their respective origin, moving towards their assigned path freely.\nIn previous works, this architecture has been only applied in microscopic\nsimulations in the context of urban networks without the influence of source\nterms (on-ramps). When the critical density of the stretch is reached, the\nreservations are activated, instigating a waiting interval to the vehicles\nstored at queues, allowing vehicles only to enter the network at a later time\ninstant, such that the critical density is not crossed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Braneworld scenarios consider our observable universe as a brane embedded in\na five-dimensional bulk. In this work, we consider thick braneworld systems in\nthe recently proposed dynamically equivalent scalar-tensor representation of\n$f(R,T)$ gravity, where $R$ is the Ricci scalar and $T$ the trace of the\nstress-energy tensor. In the general $f\\left(R,T\\right)$ case we consider two\ndifferent models: a brane model without matter fields where the geometry is\nsupported solely by the gravitational fields, and a second model where matter\nis described by a scalar field with a potential. The particular cases for which\nthe function $f\\left(R,T\\right)$ is separable in the forms $F\\left(R\\right)+T$\nand $R+G\\left(T\\right)$, which give rise to scalar-tensor representations with\na single auxiliary scalar field, are studied separately. The stability of the\ngravitational sector is investigated and the models are shown to be stable\nagainst small perturbations of the metric. Furthermore, we show that in the\n$f\\left(R,T\\right)$ model in the presence of an extra matter field, the shape\nof the graviton zero-mode develops internal structure under appropriate choices\nof the parameters of the model.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We explore the magnetically-ordered ground state of the\nisovalently-substituted Mott-insulator Y$_{1-x}$La$_{x}$TiO$_{3}$ for $x$\n$\\leq$ 0.3 via single crystal growth, magnetometry, neutron diffraction, x-ray\nmagnetic circular dichroism (XMCD), muon spin rotation ($\\mu$SR) and\nsmall-angle neutron scattering (SANS). We find that the decrease in the\nmagnetic transition temperature on approaching the ferromagnetic (FM) -\nantiferromagnetic (AFM) phase boundary at the La concentration $x_c$ $\\approx$\n0.3 is accompanied by a strong suppression of both bulk and local ordered\nmagnetic moments, along with a volume-wise separation into magnetically-ordered\nand paramagnetic regions. The thermal phase transition does not show\nconventional second-order behavior, since neither a clear signature of dynamic\ncritical behavior nor a power-law divergence of the magnetic correlation length\nis found for the studied substitution range; this finding becomes increasingly\nobvious with substitution. Finally, from SANS and magnetometry measurements, we\ndiscern a crossover from easy-axis to easy-plane magneto-crystalline anisotropy\nwith increasing La substitution. These results indicate complex changes in\nmagnetic structure upon approaching the phase boundary.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Communication overhead is the key challenge for distributed training.\nGradient compression is a widely used approach to reduce communication traffic.\nWhen combining with parallel communication mechanism method like pipeline,\ngradient compression technique can greatly alleviate the impact of\ncommunication overhead. However, there exists two problems of gradient\ncompression technique to be solved. Firstly, gradient compression brings in\nextra computation cost, which will delay the next training iteration. Secondly,\ngradient compression usually leads to the decrease of convergence accuracy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Soft robotics is an emerging field that yields promising results for tasks\nthat require safe and robust interactions with the environment or with humans,\nsuch as grasping, manipulation, and human-robot interaction. Soft robots rely\non intrinsically compliant components and are difficult to equip with\ntraditional, rigid sensors which would interfere with their compliance. We\npropose a highly flexible tactile sensor that is low-cost and easy to\nmanufacture while measuring contact pressures independently from 14 taxels. The\nsensor is built from piezoresistive fabric for highly sensitive, continuous\nresponses and from a custom-designed flexible printed circuit board which\nprovides a high taxel density. From these taxels, location and intensity of\ncontact with the sensor can be inferred. In this paper, we explain the design\nand manufacturing of the proposed sensor, characterize its input-output\nrelation, evaluate its effects on compliance when equipped to the\nsilicone-based pneumatic actuators of the soft robotic RBO Hand 2, and\ndemonstrate that the sensor provides rich and useful feedback for\nlearning-based in-hand object recognition.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the area of beyond-planar graphs, i.e. graphs that can be drawn with some\nlocal restrictions on the edge crossings, the recognition problem is prominent\nnext to the density question for the different graph classes. For 1-planar\ngraphs, the recognition problem has been settled, namely it is NP-complete for\nthe general case, while optimal 1-planar graphs, i.e. those with maximum\ndensity, can be recognized in linear time. For 2-planar graphs, the picture is\nless complete. As expected, the recognition problem has been found to be\nNP-complete in general. In this paper, we consider the recognition of simple\noptimal 2-planar graphs. We exploit a combinatorial characterization of such\ngraphs and present a linear time algorithm for recognition and embedding.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we propose a new model called triple-path attentive recurrent\nnetwork (TPARN) for multichannel speech enhancement in the time domain. TPARN\nextends a single-channel dual-path network to a multichannel network by adding\na third path along the spatial dimension. First, TPARN processes speech signals\nfrom all channels independently using a dual-path attentive recurrent network\n(ARN), which is a recurrent neural network (RNN) augmented with self-attention.\nNext, an ARN is introduced along the spatial dimension for spatial context\naggregation. TPARN is designed as a multiple-input and multiple-output\narchitecture to enhance all input channels simultaneously. Experimental results\ndemonstrate the superiority of TPARN over existing state-of-the-art approaches.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A conversion matrix approach to solving network problems involving\ntime-varying circuit components is applied to the method of moments for\nelectromagnetic scattering analysis. Detailed formulations of this technique's\napplication to the scattering analysis of structures loaded with time-varying\ncircuit networks or constructed from general time-varying media are presented.\nThe computational cost of the method is discussed, along with an analysis of\ncompression techniques capable of significantly reducing computational cost for\npartially loaded systems. Several numerical examples demonstrate the\ncapabilities of the technique along with its validation against conventional\nmethods of modeling time-varying electromagnetic systems, such as finite\ndifference time domain and transient circuit co-simulation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A status updating system is considered in which multiple data sources\ngenerate packets to be delivered to a destination through a shared energy\nharvesting sensor. Only one source's data, when available, can be transmitted\nby the sensor at a time, subject to energy availability. Transmissions are\nprune to erasures, and each successful transmission constitutes a status update\nfor its corresponding source at the destination. The goal is to schedule source\ntransmissions such that the collective long-term average age-of-information\n(AoI) is minimized. AoI is defined as the time elapsed since the latest\nsuccessfully-received data has been generated at its source. To solve this\nproblem, the case with a single source is first considered, with a focus on\nthreshold waiting policies, in which the sensor attempts transmission only if\nthe time until both energy and data are available grows above a certain\nthreshold. The distribution of the AoI is fully characterized under such a\npolicy. This is then used to analyze the performance of the multiple sources\ncase under maximum-age-first scheduling, in which the sensor's resources are\ndedicated to the source with the maximum AoI at any given time. The achievable\ncollective long-term average AoI is derived in closed-form. Multiple numerical\nevaluations are demonstrated to show how the optimal threshold value behaves as\na function of the system parameters, and showcase the benefits of a\nthreshold-based waiting policy with intermittent energy and data arrivals.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Our lives can be seen as a complex weaving of activities; we switch from one\nactivity to another, to maximise our achievements or in reaction to demands\nplaced upon us. Observing a video of unscripted daily activities, we parse the\nvideo into its constituent activity threads through a process we call\nunweaving. To accomplish this, we introduce a video representation explicitly\ncapturing activity threads called a thread bank, along with a neural controller\ncapable of detecting goal changes and resuming of past activities, together\nforming UnweaveNet. We train and evaluate UnweaveNet on sequences from the\nunscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the\nefficacy of pretraining UnweaveNet in a self-supervised manner.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Nanoelectronic devices emulating neuro-synaptic functionalities through their\nintrinsic physics at low operating energies is imperative toward the\nrealization of brain-like neuromorphic computers. In this work, we leverage the\nnon-linear voltage dependent partial polarization switching of a ferroelectric\nfield effect transistor to mimic plasticity characteristics of biological\nsynapses. We provide experimental measurements of the synaptic characteristics\nfor a $28nm$ high-k metal gate technology based device and develop an\nexperimentally calibrated device model for large-scale system performance\nprediction. Decoupled read-write paths, ultra-low programming energies and the\npossibility of arranging such devices in a cross-point architecture demonstrate\nthe synaptic efficacy of the device. Our hardware-algorithm co-design analysis\nreveals that the intrinsic plasticity of the ferroelectric devices has\npotential to enable unsupervised local learning in edge devices with limited\ntraining data.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We realise the algebra $\\mathcal W$, the algebra $\\mathcal Z_0$ and the\nalgebras $\\mathcal Z_0\\otimes A$, where $A$ is a unital UHF algebra as\nFra\\\"iss\\'e limits of suitable classes of structures. In doing so, we show that\nsuch algebras are generic objects without the use of any classification result.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Differentially private noise mechanisms commonly use symmetric noise\ndistributions. This is attractive both for achieving the differential privacy\ndefinition, and for unbiased expectations in the noised answers. However, there\nare contexts in which a noisy answer only has utility if it is conservative,\nthat is, has known-signed error, which we call a padded answer. Seemingly, it\nis paradoxical to satisfy the DP definition with one-sided error, but we show\nhow it is possible to bury the paradox into approximate DP's delta parameter.\nWe develop a few mechanisms for one-sided padding mechanisms that always give\nconservative answers, but still achieve approximate differential privacy. We\nshow how these mechanisms can be applied in a few select areas including making\nthe cardinalities of set intersections and unions revealed in Private Set\nIntersection protocols differential private and enabling multiparty computation\nprotocols to compute on sparse data which has its exact sizes made differential\nprivate rather than performing a fully oblivious more expensive computation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Voice conversion for speaker anonymization is an emerging field in speech\nprocessing research. Many state-of-the-art approaches are based on the\nresynthesis of the phoneme posteriorgrams (PPG), the fundamental frequency (F0)\nof the input signal together with modified X-vectors. Our research focuses on\nthe role of F0 for speaker anonymization, which is an understudied area.\nUtilizing the VoicePrivacy Challenge 2020 framework and its datasets we\ndeveloped and evaluated eight low-complexity F0 modifications prior\nresynthesis. We found that modifying the F0 can improve speaker anonymization\nby as much as 8% with minor word-error rate degradation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The ever-growing cost of both training and inference for state-of-the-art\nneural networks has brought literature to look upon ways to cut off resources\nused with a minimal impact on accuracy. Using lower precision comes at the cost\nof negligible loss in accuracy. While training neural networks may require a\npowerful setup, deploying a network must be possible on low-power and\nlow-resource hardware architectures. Reconfigurable architectures have proven\nto be more powerful and flexible than GPUs when looking at a specific\napplication. This article aims to assess the impact of mixed-precision when\napplied to neural networks deployed on FPGAs. While several frameworks exist\nthat create tools to deploy neural networks using reduced-precision, few of\nthem assess the importance of quantization and the framework quality. FINN and\nBrevitas, two frameworks from Xilinx labs, are used to assess the impact of\nquantization on neural networks using 2 to 8 bit precisions and weights with\nseveral parallelization configurations. Equivalent accuracy can be obtained\nusing lower-precision representation and enough training. However, the\ncompressed network can be better parallelized allowing the deployed network\nthroughput to be 62 times faster. The benchmark set up in this work is\navailable in a public repository (https://github.com/QDucasse/nn benchmark).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Conventionally, in galaxy surveys, cosmological constraints on the growth and\nexpansion history of the universe have been obtained from the measurements of\nredshift-space distortions and baryon acoustic oscillations embedded in the\nlarge-scale galaxy density field. In this paper, we study how well one can\nimprove the cosmological constraints from the combination of the galaxy density\nfield with velocity and tidal fields, which are observed via the kinetic\nSunyaev-Zel'dovich (kSZ) and galaxy intrinsic alignment (IA) effects,\nrespectively. For illustration, we consider the deep galaxy survey by Subaru\nPrime Focus Spectrograph, whose survey footprint perfectly overlaps with the\nimaging survey of the Hyper Suprime-Cam and the CMB-S4 experiment. We find that\nadding the kSZ and IA effects significantly improves cosmological constraints,\nparticularly when we adopt the non-flat cold dark matter model which allows\nboth time variation of the dark energy equation-of-state and deviation of the\ngravity law from general relativity. Under this model, we achieve $31\\%$\nimprovement for the growth index $\\gamma$ and $>35\\%$ improvement for other\nparameters except for the curvature parameter, compared to the case of the\nconventional galaxy-clustering-only analysis. As another example, we also\nconsider the wide galaxy survey by the {\\it Euclid} satellite, in which shapes\nof galaxies are noisier but the survey volume is much larger. We demonstrate\nthat when the above model is adopted, the clustering analysis combined with kSZ\nand IA from the deep survey can achieve tighter cosmological constraints than\nthe clustering-only analysis from the wide survey.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The present work has as a first goal to extend the previous results in\n\\cite{CFL20} to weighted uncertainty principles with nontrivial radially\nsymmetric weights applied to curl-free vector fields. Part of these new\ninequalities generalize the family of Caffarelli-Kohn-Nirenberg (CKN)\ninequalities studied by Catrina and Costa in \\cite{CC} from scalar fields to\ncurl-free vector fields. We will apply a new representation of curl-free vector\nfields developed by Hamamoto in \\cite{HT21}. The newly obtained results are\nalso sharp and minimizers are completely described.\n  Secondly, we prove new sharp second order interpolation functional\ninequalities for scalar fields with radial weights generalizing the previous\nresults in \\cite{CFL20}. We apply new factorization methods being inspired by\nour recent work \\cite{CFL21}. The main novelty in this case is that we are able\nto find a new independent family of minimizers based on the solutions of\nKummer's differential equations.\n  We point out that the two types of weighted inequalities under consideration\n(first order inequalities for curl-free vector fields vs. second order\ninequalities for scalar fields) represent independent families of inequalities\nunless the weights are trivial.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We systematically investigate the ground-state properties of self-bound\ndroplets of quasi-two-dimensional binary Bose gases by using the Gaussian state\ntheory. We find that quantum droplets consists two macroscopic squeezed phases\nand a macroscopic coherent phase. We map out the phase diagram and determine\nall phase boundaries via both numerical and nearly analytical methods. In\nparticular, we find three easily accessible signatures for the quantum phases\nand the stablization mechanism of the self-bound droplets by precisely\nmeasuring their radial size. Our studies indicate that binary droplets\nrepresent an ideal platform for in-depth investigations of the quantum nature\nof the droplet state.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The automatic recognition of pathological speech, particularly from children\nwith any articulatory impairment, is a challenging task due to various reasons.\nThe lack of available domain specific data is one such obstacle that hinders\nits usage for different speech-based applications targeting pathological\nspeakers. In line with the challenge, in this work, we investigate a few data\naugmentation techniques to simulate training data for improving the children\nspeech recognition considering the case of cleft lip and palate (CLP) speech.\nThe augmentation techniques explored in this study, include vocal tract length\nperturbation (VTLP), reverberation, speaking rate, pitch modification, and\nspeech feature modification using cycle consistent adversarial networks\n(CycleGAN). Our study finds that the data augmentation methods significantly\nimprove the CLP speech recognition performance, which is more evident when we\nused feature modification using CycleGAN, VTLP and reverberation based methods.\nMore specifically, the results from this study show that our systems produce an\nimproved phone error rate compared to the systems without data augmentation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep Neural Networks (DNNs) have often supplied state-of-the-art results in\npattern recognition tasks. Despite their advances, however, the existence of\nadversarial examples have caught the attention of the community. Many existing\nworks have proposed methods for searching for adversarial examples within\nfixed-sized regions around training points. Our work complements and improves\nthese existing approaches by adapting the size of these regions based on the\nproblem complexity and data sampling density. This makes such approaches more\nappropriate for other types of data and may further improve adversarial\ntraining methods by increasing the region sizes without creating incorrect\nlabels.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A very short proof of Kneser's theorem via transversal is given.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the Rice-Mele (RM) model in the presence of an asymmetric\nhopping-induced non-Hermitian parameter, $\\gamma$. In particular, we examine\nthe effect of non-Hermiticity on the topological boundary modes and topological\npumping. For weak and moderate values of $\\gamma$, the inherent topological\nedge modes remain localized at the boundaries, whereas the bulk modes are\npumped to the boundary, leading to the typical non-Hermitian skin modes. Using\ngeneralized Brillouin zone (GBZ) scheme, we show that the\nnon-Hermiticity-induced skin modes can be distinguished from the topological\nboundary modes. Upon increasing $\\gamma$, the topological boundary mode\nlocalized at one edge is pumped to the other edge, leading to an unconventional\nstate pumping. This is in contrast to the standard topological pumping where\nadiabatic evolution of the parameters of the RM model leads to the pumping. We\nfurther show that the usual topological pumping due to the adiabatic and\nperiodic variation of the model parameters survives even with finite $\\gamma$.\nHowever, it depends upon the driving protocols and strength of the\nnon-Hermiticity. With increasing $\\gamma$, the adiabatic pumping is destroyed\nfirst and then re-emerges as an unconventional pumping which does not have any\nHermitian counterpart.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A vision transformer (ViT) is the dominant model in the computer vision\nfield. Despite numerous studies that mainly focus on dealing with inductive\nbias and complexity, there remains the problem of finding better transformer\nnetworks. For example, conventional transformer-based models usually use a\nprojection layer for each query (Q), key (K), and value (V) embedding before\nmulti-head self-attention. Insufficient consideration of semantic $Q, K$, and\n$V$ embedding may lead to a performance drop. In this paper, we propose three\ntypes of structures for $Q$, $K$, and $V$ embedding. The first structure\nutilizes two layers with ReLU, which is a non-linear embedding for $Q, K$, and\n$V$. The second involves sharing one of the non-linear layers to share\nknowledge among $Q, K$, and $V$. The third proposed structure shares all\nnon-linear layers with code parameters. The codes are trainable, and the values\ndetermine the embedding process to be performed among $Q$, $K$, and $V$. Hence,\nwe demonstrate the superior image classification performance of the proposed\napproaches in experiments compared to several state-of-the-art approaches. The\nproposed method achieved $71.4\\%$ with a few parameters (of $3.1M$) on the\nImageNet-1k dataset compared to that required by the original transformer model\nof XCiT-N12 ($69.9\\%$). Additionally, the method achieved $93.3\\%$ with only\n$2.9M$ parameters in transfer learning on average for the CIFAR-10, CIFAR-100,\nStanford Cars datasets, and STL-10 datasets, which is better than the accuracy\nof $92.2\\%$ obtained via the original XCiT-N12 model.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum many-body systems may defy thermalization even without disorder.\nIntriguingly, non-ergodicity may be caused by a fragmentation of the many-body\nHilbert-space into dynamically disconnected subspaces. The tilted\none-dimensional Fermi-Hubbard model was proposed as a platform to realize\nfragmented models perturbatively in the limit of large tilt. Here, we\ndemonstrate the validity of this effective description for the transient\ndynamics using ultracold fermions. The effective analytic model allows for a\ndetailed understanding of the emergent microscopic processes, which in our case\nexhibit a pronounced doublon-number dependence. We study this experimentally by\ntuning the doublon fraction in the initial state.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present LMT/AzTEC 1.1mm observations of $\\sim100$ luminous high-redshift\ndusty star-forming galaxy candidates from the $\\sim600\\,$sq.deg\n$Herschel$-ATLAS survey, selected on the basis of their SPIRE red far-infrared\ncolours and with $S_{500\\mu\\rm m}=35-80$ mJy. With an effective $\\theta_{\\rm\nFWHM}\\approx9.5\\,$ arcsec angular resolution, our observations reveal that at\nleast 9 per cent of the targets break into multiple systems with SNR $\\geq 4$\nmembers. The fraction of multiple systems increases to $\\sim23\\,$ per cent (or\nmore) if some non-detected targets are considered multiples, as suggested by\nthe data. Combining the new AzTEC and deblended $Herschel$ photometry we derive\nphotometric redshifts, IR luminosities, and star formation rates. While the\nmedian redshifts of the multiple and single systems are similar $(z_{\\rm\nmed}\\approx3.6)$, the redshift distribution of the latter is skewed towards\nhigher redshifts. Of the AzTEC sources $\\sim85\\,$ per cent lie at $z_{\\rm\nphot}>3$ while $\\sim33\\,$ per cent are at $z_{\\rm phot}>4$. This corresponds to\na lower limit on the space density of ultra-red sources at $4<z<6$ of\n$\\sim3\\times10^{-7}\\, \\textrm{Mpc}^{-3}$ with a contribution to the obscured\nstar-formation of $\\gtrsim 8\\times10^{-4}\\, \\textrm{M}_\\odot \\textrm{yr}^{-1}\n\\textrm{Mpc}^{-3}$. Some of the multiple systems have members with photometric\nredshifts consistent among them suggesting possible physical associations.\nGiven their angular separations, these systems are most likely galaxy\nover-densities and/or early-stage pre-coalescence mergers. Finally, we present\n3mm LMT/RSR spectroscopic redshifts of six red-$Herschel$ galaxies at $z_{\\rm\nspec}=3.85-6.03$, two of them (at $z \\sim 4.7$) representing new redshift\nconfirmations. Here we release the AzTEC and deblended $Herschel$ photometry as\nwell as catalogues of the most promising interacting systems and $z>4$\ngalaxies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Giant exoplanets on 10-100 au orbits have been directly imaged around young\nstars. The peak of the thermal emission from these warm young planets is in the\nnear-infrared (~1-5 microns), whereas mature, temperate exoplanets (i.e., those\nwithin their stars' habitable zones) radiate primarily in the mid-infrared\n(mid-IR: ~10 microns). If the background noise in the mid-IR can be mitigated,\nthen exoplanets with low masses--including rocky exoplanets--can potentially be\nimaged in very deep exposures. Here, we review the recent results of the\nBreakthrough Watch/New Earths in the Alpha Centauri Region (NEAR) program on\nthe Very Large Telescope (VLT) in Chile. NEAR pioneered a ground-based mid-IR\nobserving approach designed to push the capabilities for exoplanet imaging with\na specific focus on the closest stellar system, Alpha Centauri. NEAR combined\nseveral new optical technologies--including a mid-IR optimized coronagraph,\nadaptive optics system, and rapid chopping strategy to mitigate noise from the\ncentral star and thermal background within the habitable zone. We focus on the\nlessons of the VLT/NEAR campaign to improve future\ninstrumentation--specifically on strategies to improve noise mitigation through\nchopping. We also present the design and commissioning of the Large Binocular\nTelescope's Exploratory Survey for Super-Earths Orbiting Nearby Stars\n(LESSONS), an experiment in the Northern hemisphere that is building on what\nwas learned from NEAR to further push the sensitivity of mid-IR imaging.\nFinally, we briefly discuss some of the possibilities that mid-IR imaging will\nenable for exoplanet science.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The increasing accessibility of the internet facilitated social media usage\nand encouraged individuals to express their opinions liberally. Nevertheless,\nit also creates a place for content polluters to disseminate offensive posts or\ncontents. Most of such offensive posts are written in a cross-lingual manner\nand can easily evade the online surveillance systems. This paper presents an\nautomated system that can identify offensive text from multilingual code-mixed\ndata. In the task, datasets provided in three languages including Tamil,\nMalayalam and Kannada code-mixed with English where participants are asked to\nimplement separate models for each language. To accomplish the tasks, we\nemployed two machine learning techniques (LR, SVM), three deep learning (LSTM,\nLSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R)\nbased methods. Results show that XLM-R outperforms other techniques in Tamil\nand Malayalam languages while m-BERT achieves the highest score in the Kannada\nlanguage. The proposed models gained weighted $f_1$ score of $0.76$ (for\nTamil), $0.93$ (for Malayalam), and $0.71$ (for Kannada) with a rank of\n$3^{rd}$, $5^{th}$ and $4^{th}$ respectively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper investigates a multilevel inverter with a capability that produces\na wide voltage range with high quality. The selective harmonic elimination\n(SHE) method is considered for a single-phase 5-level cascaded H-bridge (CHB)\ninverter, in which the particle swarm optimization (PSO) algorithm solves the\nnonlinear equations. However, eliminating the low-order harmonics has been\nchallenging when a low range of output voltage is required. To surmount such\nchallenges and access to a wide range of output voltage, an adjustable dc-link\nis introduced that allows the inverter to increase the modulation index,\nresulting in a significant decrease in total harmonic distortion (THD). In this\npaper, to regulate the dc-link voltage amount, a 12-pulse rectifier is employed\nto allow the inverter to produce the output voltage requirements with less\ndistortion. To prove such claims, the PSO algorithm is modified to calculate\nthe optimal angles, as a result, the switching angles are applied in SIMULINK\nMATLAB to generate a 5-level output voltage.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modular graph functions (MGFs) are $\\mathrm{SL}(2,\\mathbb{Z})$-invariant\nfunctions on the Poincar\\'e upper half-plane associated with Feynman graphs of\na conformal scalar field on a torus. The low-energy expansion of genus-one\nsuperstring amplitudes involves suitably regularized integrals of MGFs over the\nfundamental domain for $\\mathrm{SL}(2,\\mathbb{Z})$. In earlier work, these\nintegrals were evaluated for all MGFs up to two loops and for higher loops up\nto weight six. These results led to the conjectured uniform transcendentality\nof the genus-one four-graviton amplitude in Type II superstring theory. In this\npaper, we explicitly evaluate the integrals of several infinite families of\nthree-loop MGFs and investigate their transcendental structure. Up to weight\nseven, the structure of the integral of each individual MGF is consistent with\nthe uniform transcendentality of string amplitudes. Starting at weight eight,\nthe transcendental weights obtained for the integrals of individual MGFs are no\nlonger consistent with the uniform transcendentality of string amplitudes.\nHowever, in all the cases we examine, the violations of uniform\ntranscendentality take on a special form given by the integrals of triple\nproducts of non-holomorphic Eisenstein series. If Type II superstring\namplitudes do exhibit uniform transcendentality, then the special combinations\nof MGFs which enter the amplitudes must be such that these integrals of triple\nproducts of Eisenstein series precisely cancel one another. Whether this indeed\nis the case poses a novel challenge to the conjectured uniform\ntranscendentality of genus-one string amplitudes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We create plots of algebraic integers in the complex plane, exploring the\neffect of sizing the integers according to various arithmetic invariants. We\nfocus on Galois theoretic invariants, in particular creating plots which\nemphasize algebraic integers whose Galois group is not the full symmetric\ngroup--these integers we call rigid. We then give some analysis of the\nresulting images, suggesting avenues for future research about the geometry of\nso-called rigid algebraic integers.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a model checking algorithm to test properties of systems that are\nexpressed in the multi-agent temporal logic ATL+. The specificities of this\nalgorithm are: it is on-the-fly, generating states only when they are needed,\nand it works by constructing a candidate formal proof in an inference system\ninspired by tableau proof systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Generating font glyphs of consistent style from one or a few reference\nglyphs, i.e., font completion, is an important task in topographical design. As\nthe problem is more well-defined than general image style transfer tasks, thus\nit has received interest from both vision and machine learning communities.\nExisting approaches address this problem as a direct image-to-image translation\ntask. In this work, we innovate to explore the generation of font glyphs as 2D\ngraphic objects with the graph as an intermediate representation, so that more\nintrinsic graphic properties of font styles can be captured. Specifically, we\nformulate a cross-modality cycled image-to-image model structure with a graph\nconstructor between an image encoder and an image renderer. The novel graph\nconstructor maps a glyph's latent code to its graph representation that matches\nexpert knowledge, which is trained to help the translation task. Our model\ngenerates improved results than both image-to-image baseline and previous\nstate-of-the-art methods for glyph completion. Furthermore, the graph\nrepresentation output by our model also provides an intuitive interface for\nusers to do local editing and manipulation. Our proposed cross-modality cycled\nrepresentation learning has the potential to be applied to other domains with\nprior knowledge from different data modalities. Our code is available at\nhttps://github.com/VITA-Group/Font_Completion_Graph.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The concept of super solution is a special type of generalized solutions with\ncertain degree of robustness and stability. In this paper we consider the\n$(1,1)$-super solutions of the model RB. Using the first moment method, we\nestablish a \"threshold\" such that as the constraint density crosses this value,\nthe expected number of $(1,1)$-super solutions goes from $0$ to infinity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a novel method that records a single compressive hologram in a\nshort time and extracts the depth of a scene from that hologram using a stereo\ndisparity technique. The method is verified with numerical simulations, but\nthere is no restriction on adapting this into an optical experiment. In the\nsimulations, a computer-generated hologram is first sampled with random binary\npatterns, and measurements are utilized in a recovery algorithm to form a\ncompressive hologram. The compressive hologram is then divided into two parts\n(two apertures), and these parts are separately reconstructed to form a stereo\nimage pair. The pair is eventually utilized in stereo disparity method for\ndepth map extraction. The depth maps of the compressive holograms with the\nsampling rates of 2, 25, and 50 percent are compared with the depth map\nextracted from the original hologram, on which compressed sensing is not\napplied. It is demonstrated that the depth profiles obtained from the\ncompressive holograms are in very good agreement with the depth profile\nobtained from the original hologram despite the data reduction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Current deep learning models for dynamics forecasting struggle with\ngeneralization. They can only forecast in a specific domain and fail when\napplied to systems with different parameters, external forces, or boundary\nconditions. We propose a model-based meta-learning method called DyAd which can\ngeneralize across heterogeneous domains by partitioning them into different\ntasks. DyAd has two parts: an encoder which infers the time-invariant hidden\nfeatures of the task with weak supervision, and a forecaster which learns the\nshared dynamics of the entire domain. The encoder adapts and controls the\nforecaster during inference using adaptive instance normalization and adaptive\npadding. Theoretically, we prove that the generalization error of such\nprocedure is related to the task relatedness in the source domain, as well as\nthe domain differences between source and target. Experimentally, we\ndemonstrate that our model outperforms state-of-the-art approaches on both\nturbulent flow and real-world ocean data forecasting tasks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we study the speed of extinction of continuous state branching\nprocesses in subcritical L\\'evy environments. More precisely, when the\nassociated L\\'evy process to the environment drifts to $-\\infty$ and, under a\nsuitable exponential martingale change of measure (Esscher transform), the\nenvironment either drifts to $-\\infty$ or oscillates. We extend recent results\nof Palau et al. (2016) and Li and Xu (2018), where the branching term is\nassociated to a spectrally positive stable L\\'evy process and complement the\nrecent article of Bansaye et al. (2021) where the critical case was studied.\nOur methodology combines a path analysis of the branching process together with\nits L\\'evy environment, fluctuation theory for L\\'evy processes and the\nasymptotic behaviour of exponential functionals of L\\'evy processes. As an\napplication of the aforementioned results, we characterise the process\nconditioned to survival also known as the $Q$-process.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Safe and smooth interacting with other vehicles is one of the ultimate goals\nof driving automation. However, recent reports of demonstrative deployments of\nautomated vehicles (AVs) indicate that AVs are still difficult to meet the\nexpectation of other interacting drivers, which leads to several AV accidents\ninvolving human-driven vehicles (HVs). This is most likely due to the lack of\nunderstanding about the dynamic interaction process, especially about the human\ndrivers. By investigating the causes of 4,300 video clips of traffic accidents,\nwe find that the limited dynamic visual field of drivers is one leading factor\nin inter-vehicle interaction accidents, especially in those involving trucks. A\ngame-theoretic decision algorithm considering social compatibility is proposed\nto handle the interaction with a human-driven truck at an unsignalized\nintersection. Starting from a probabilistic model for the visual field\ncharacteristics of truck drivers, social fitness and reciprocal altruism in the\ndecision are incorporated in the game payoff design. Human-in-the-loop\nexperiments are carried out, in which 24 subjects are invited to drive and\ninteract with AVs deployed with the proposed algorithm and two comparison\nalgorithms. Totally 207 cases of intersection interactions are obtained and\nanalyzed, which shows that the proposed decision-making algorithm can not only\nimprove both safety and time efficiency, but also make AV decisions more in\nline with the expectation of interacting human drivers. These findings can help\ninform the design of automated driving decision algorithms, to ensure that AVs\ncan be safely and efficiently integrated into the human-dominated traffic.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Terrestrial communication networks have experienced significant development\nin recent years by providing emerging services for ground users. However, one\ncritical challenge raised is to provide full coverage (especially in dense\nhigh-rise urban environments) for ground users due to scarce network resources\nand limited coverage. To meet this challenge, we propose a high altitude\nplatform (HAP)-reserved ground-air-space (GAS) transmission scheme, which\ncombines with the ground-to-space (G2S) transmission scheme to strengthen the\nterrestrial communication and save the transmission power. To integrate the two\ntransmission schemes, we propose a transmission control strategy. Wherein, the\nground user decides its transmission scheme, i.e., switches between the GAS\nlink transmission and the G2S link transmission with a probability. We then\nmaximize the overall throughput and derive the optimal probability that a\nground user adopts the GAS transmission scheme. Numerical results demonstrate\nthe superiority of the proposed transmission control strategy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Formulating cyber-security problems with attackers and defenders as a\npartially observable stochastic game has become a trend recently. Among them,\nthe one-sided two-player zero-sum partially observable stochastic game\n(OTZ-POSG) has emerged as a popular model because it allows players to compete\nfor multiple stages based on partial knowledge of the system. All existing work\non OTZ-POSG has focused on the simultaneous move scenario and assumed that one\nplayer's actions are private in the execution process. However, this assumption\nmay become questionable since one player's action may be detected by the\nopponent through deploying action detection strategies. Hence, in this paper,\nwe propose a turn-based OTZ-POSG with the assumption of public actions and\ninvestigate the existence and properties of a Stackelberg equilibrium for this\ngame. We first prove the existence of the Stackelberg equilibrium for the\none-stage case and show that the one-stage game can be converted into a\nlinear-fractional programming problem and therefore solved by linear\nprogramming. For multiple stages, the main challenge is the information leakage\nissue as the public run-time action reveals certain private information to the\nopponent and allows the opponent to achieve more rewards in the future. To deal\nwith this issue, we adopt the concept of $\\epsilon$-Stackelberg equilibrium and\nprove that this equilibrium can be achieved for finite-horizon OTZ-POSGs. We\npropose a space partition approach to solve the game iteratively and show that\nthe value function of the leader is piece-wise linear and the value function of\nthe follower is piece-wise constant for multiple stages. Finally, examples are\ngiven to illustrate the space partition approach and show that value functions\nare piece-wise linear and piece-wise constant.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Modeling of kinetic plasmas using electromagnetic particle in cell methods\n(EM-PIC) is a problem that is well worn, in that methods developed have been\nused extensively both understanding physics and exploiting them for device\ndesign. EM-PIC tools have largely relied on finite difference methods coupled\nwith particle representations of the distribution function. Refinements to\nensure consistency and charge conservation have largely been an ad-hoc efforts\nspecific to finite difference methods. Meanwhile, solution methods for field\nsolver have grown by leaps and bounds with significant performance metrics\ncompared to finite difference methods. Developing new EM-PIC computational\nschemes that leverage modern field solver technology means re-examining\nanalysis framework necessary for self-consistent EM-PIC solution. In this\npaper, we prescribe general rubrics for charge conservation, demonstrate how\nthese are satisfied in conventional finite difference PIC as well as finite\nelement PIC, and prescribe a novel charge conserving finite element PIC. Our\neffort leverages proper mappings on to de-Rham sequences and lays a groundwork\nfor understanding conditions that must be satisfied for consistency. Several\nnumerical results demonstrate the applicability of these rubrics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For a finite group $D$, we study categorical factorisation homology on\noriented surfaces equipped with principal $D$-bundles, which `integrates' a\n(linear) balanced braided category $\\mathcal{A}$ with $D$-action over those\nsurfaces. For surfaces with at least one boundary component, we identify the\nvalue of factorisation homology with the category of modules over an explicit\nalgebra in $\\mathcal{A}$, extending the work of Ben-Zvi, Brochier and Jordan to\nsurfaces with $D$-bundles. Furthermore, we show that the value of factorisation\nhomology on annuli, boundary conditions, and point defects can be described in\nterms of equivariant representation theory. Our main example comes from an\naction of Dynkin diagram automorphisms on representation categories of quantum\ngroups. We show that in this case factorisation homology gives rise to a\nquantisation of the moduli space of flat twisted bundles.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  After the discovery of the double-charm baryon $\\Xi_{cc}^{++}$ by LHCb, one\nof the most important topics is to search for the bottom-charm baryons which\ncontain a $b$ quark, a $c$ quark and a light quark. In this work, we study the\ntwo-body non-leptonic weak decays of a bottom-charm baryon into a spin-$1/2$\nbottomed baryon and a light pseudoscalar meson with the short-distance\ncontributions calculated under the factorization hypothesis and the\nlong-distance contributions considering the final-state-interaction effects.\nThe branching fractions of all fifty-seven decay channels are estimated. The\nresults indicate that $\\Xi_{bc}^+\\to\\Xi_b^0\\pi^+$,\n$\\Xi_{bc}^{0}\\to\\Xi_{b}^{-}\\pi^+$ and $\\Omega_{bc}^0\\to\\Omega_b^-\\pi^+$ decay\nmodes have relatively large decay rates and thus could be used to\nexperimentally search for the bottom-charm baryons. The topological diagrams\nand the SU(3) symmetry of bottom-charm baryon decays are discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the totally non-negative part of the complete flag variety and of\nits tropicalization. We start by showing that Lusztig's notion of non-negative\ncomplete flag variety coincides with the flags in the complete flag variety\nwhich have non-negative Pl\\\"ucker coordinates. This mirrors the\ncharacterization of the totally non-negative Grassmannian as those points in\nthe Grassmannian with all non-negative Pl\\\"ucker coordinates. We then study the\ntropical complete flag variety and complete flag Dressian, which are two\ntropical versions of the complete flag variety, capturing realizable and\nabstract flags of tropical linear spaces, respectively. The complete flag\nDressian properly contains the tropical complete flag variety. However, we show\nthat the totally non-negative parts of these spaces coincide.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  First-principles Hubbard-corrected approximate density-functional theory\n(DFT+U) is a low-cost, potentially high throughput method of simulating\nmaterials, but it has been hampered by empiricism and inconsistent band-gap\ncorrection in transition-metal oxides. DFT+U property prediction of\nnon-magnetic systems such as d0 and d10 transition-metal oxides is typically\nfaced with excessively large calculated Hubbard U values, and with difficulty\nin obtaining acceptable band-gaps and lattice volumes. Meanwhile, Hund's\nexchange coupling J is an important but often neglected component of DFT+U, and\nthe J parameter has proven challenging to directly calculate by means of linear\nresponse. In this work, we provide a revised formula for computing Hund's J\nusing established self-consistent field DFT+U codes. For non-magnetic systems,\nwe introduce a non-approximate technique for calculating U and J simultaneously\nin such codes, at no additional cost. Using unmodified Quantum ESPRESSO, we\nassess the resulting values using two different DFT+U functionals incorporating\nJ, namely the widely used DFT+(U-J) and the readily available DFT+U+J. We\nassess a test set comprising TiO2, ZrO2, HfO2, Cu2O and ZnO, and apply the\ncorrections both to metal and oxygen centered pseudoatomic subspaces. Starting\nfrom the PBE functional, we find that DFT+(U-J) is significantly out-performed\nin band-gap accuracy by DFT+U+J, the RMS band-gap error of which matches that\nof the hybrid functional HSE06. ZnO, a long-standing challenge case for DFT+U,\nis addressed by means of Zn 4s instead of Zn 3d correction, in which case the\nfirst-principles DFT+U+J band-gap error is half of that reported for HSE06.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Phase transitions are emergent phenomena where microscopic interactions drive\na disordered system into a collectively ordered phase. Near the boundary\nbetween two phases, the system can exhibit critical, scale-invariant behavior.\nHere, we report on a second-order phase transition accompanied by critical\nbehavior in a system of warm cesium spins driven by linearly-polarized light.\nThe ordered phase exhibits macroscopic magnetization when the interactions\nbetween the spins become dominant. We measure the phase diagram of the system\nand observe the collective behavior near the phase boundaries, including\npower-law dependence of the magnetization and divergence of the susceptibility.\nOut of equilibrium, we observe a critical slow-down of the spin response time\nby two orders of magnitude, exceeding five seconds near the phase boundary.\nThis work establishes a controlled platform for investigating equilibrium and\nnonequilibrium properties of magnetic phases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The booming vector manage system calls for feasible similarity hash function\nas a front-end to perform similarity analysis. In this paper, we make a\nsystematical survey on the existent well-known similarity hash functions to\ntease out the satisfied ones. We conclude that the similarity hash function\nMinHash and Nilsimsa can be directly marshaled into the pipeline of similarity\nanalysis using vector manage system. After that, we make a brief and empirical\ndiscussion on the performance, drawbacks of the these functions and highlight\nMinHash, the variant of SimHash and feature hashing are the best for vector\nmanagement system for large-scale similarity analysis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Blockchains are gaining momentum due to the interest of industries and people\nin \\emph{decentralized applications} (Dapps), particularly in those for trading\nassets through digital certificates secured on blockchain, called tokens. As a\nconsequence, providing a clear unambiguous description of any activities\ncarried out on blockchains has become crucial, and we feel the urgency to\nachieve that description at least for trading. This paper reports on how to\nleverage the \\emph{Ontology for Agents, Systems, and Integration of Services}\n(\"\\ONT{}\") as a general means for the semantic representation of smart\ncontracts stored on blockchain as software agents. Special attention is paid to\nnon-fungible tokens (NFTs), whose management through the ERC721 standard is\npresented as a case study.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the generic limit sets of one-dimensional cellular automata, which\nintuitively capture their asymptotic dynamics while discarding transient\nphenomena. As our main results, we characterize the automata whose generic\nlimit set is a singleton, and show that this class is $\\Sigma^0_2$-complete. We\nalso prove that given a CA whose generic limit set is guaranteed to be a\nsingleton, the sole configuration it contains cannot be algorithmically\ndetermined.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A novel formulation of the hyperspectral broadband phase retrieval is\ndeveloped for the scenario where both object and modulation phase masks are\nspectrally varying. The proposed algorithm is based on a complex domain version\nof the alternating direction method of multipliers (ADMM) and Spectral\nProximity Operators (SPO) derived for Gaussian and Poissonian observations.\nComputations for these operators are reduced to the solution of sets of cubic\n(for Gaussian) and quadratic (for Poissonian) algebraic equations. These\nproximity operators resolve two problems. Firstly, the complex domain spectral\ncomponents of signals are extracted from the total intensity observations\ncalculated as sums of the signal spectral intensities. In this way, the\nspectral analysis of the total intensities is achieved. Secondly, the noisy\nobservations are filtered, compromising noisy intensity observations and their\npredicted counterparts. The ability to resolve the hyperspectral broadband\nphase retrieval problem and to find the spectrum varying object are essentially\ndefined by the spectral properties of object and image formation operators. The\nsimulation tests demonstrate that the phase retrieval in this formulation can\nbe successfully resolved.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Cascading failures abound in complex systems and the BTW sandpile model\nprovides a theoretical underpinning for their analysis. Yet, it does not\naccount for the possibility of nodes having oscillatory dynamics such as in\npower grids and brain networks. Here we consider a network of Kuramoto\noscillators upon which the BTW model is unfolding, enabling us to study how the\nfeedback between the oscillatory and cascading dynamics can lead to new\nemergent behaviors. We assume that the more out-of-sync a node is with its\nneighbors the more vulnerable it is and lower its load-carrying capacity\naccordingly. And when a node topples and sheds load, its oscillatory phase is\nreset at random. This leads to novel cyclic behavior at an emergent, long\ntimescale. The system spends the bulk of its time in a synchronized state where\nload builds up with minimal cascades. Yet, eventually the system reaches a\ntipping point where a large cascade triggers a \"cascade of larger cascades,\"\nwhich can be classified as a Dragon King event. The system then undergoes a\nshort transient back to the synchronous, build-up phase. The coupling between\ncapacity and synchronization gives rise to endogenous cascade seeds in addition\nto the standard exogenous ones, and we show their respective roles. We\nestablish the phenomena from numerical studies and develop the accompanying\nmean-field theory to locate the tipping point, calculate the load in the\nsystem, determine the frequency of the long-time oscillations and find the\ndistribution of cascade sizes during the build-up phase.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Despite their remarkable performance on a wide range of visual tasks, machine\nlearning technologies often succumb to data distribution shifts. Consequently,\na range of recent work explores techniques for detecting these shifts.\nUnfortunately, current techniques offer no explanations about what triggers the\ndetection of shifts, thus limiting their utility to provide actionable\ninsights. In this work, we present Concept Bottleneck Shift Detection (CBSD): a\nnovel explainable shift detection method. CBSD provides explanations by\nidentifying and ranking the degree to which high-level human-understandable\nconcepts are affected by shifts. Using two case studies (dSprites and\n3dshapes), we demonstrate how CBSD can accurately detect underlying concepts\nthat are affected by shifts and achieve higher detection accuracy compared to\nstate-of-the-art shift detection methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Based on first-principles calculations, we have found a family of 2D\ntransition-metal (TM) chalcogenides MX5 (M = Zr, Hf and X = S, Se and Te) can\nhost quantum spin Hall (QSH) effect. The molecular dynamics simulation indicate\nthat they are all thermal-dynamically stable at room temperature, the largest\nband gap is 0.19 eV. We have investigated MX5's electronic properties and found\ntheir properties are very similar. The single-layer ZrX5 are all gapless\nsemimetals without consideration of spin-orbit coupling (SOC). The\nconsideration of SOC will result in insulating phases with band gaps of 0.05 eV\n(direct), 0.18 eV (direct) and 0.13 eV (indirect) for ZrS5, ZrSe5 to ZrTe5,\nrespectively. The evolution of Wannier charge centers and edge states confirm\nthey are all QSH insulators. The mechanisms for QSH effect in ZrX5 originate\nfrom the special nonsymmorphic space group features. In addition, the QSH state\nof ZrS5 survives at a large range of strain as long as the interchain coupling\nis not strong enough to reverse the band ordering. The single-layer ZrS5 will\noccur a topological insulator (TI)-to-semimetal (metal) or metal-to-semimetal\ntransition under certain strain. Monolayer MX5 expand the TI materials based on\nTM chalcogenides and may open up a new way to fabricate novel low power\nspintronic devices at room temperature.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a pose adaptive few-shot learning procedure and a two-stage data\ninterpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for\nsingle-image 3D reconstruction. While augmentations via interpolating\nfeature-label pairs are effective in classification tasks, they fall short in\nshape predictions potentially due to inconsistencies between interpolated\nproducts of two images and volumes when rendering viewpoints are unknown.\nPADMix targets this issue with two sets of mixup procedures performed\nsequentially. We first perform an input mixup which, combined with a pose\nadaptive learning procedure, is helpful in learning 2D feature extraction and\npose adaptive latent encoding. The stagewise training allows us to build upon\nthe pose invariant representations to perform a follow-up latent mixup under\none-to-one correspondences between features and ground-truth volumes. PADMix\nsignificantly outperforms previous literature on few-shot settings over the\nShapeNet dataset and sets new benchmarks on the more challenging real-world\nPix3D dataset.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Although genome-wide association studies (GWAS) on complex traits have\nachieved great successes, the current leading GWAS approaches simply perform to\ntest each genotype-phenotype association separately for each genetic variant.\nCuriously, the statistical properties for using these approaches is not known\nwhen a joint model for the whole genetic variants is considered. Here we\nadvance in GWAS in understanding the statistical properties of the \"population\nstructure correction\" (PSC) approach, a standard univariate approach in GWAS.\nWe further propose and analyse a correction to the PSC approach, termed as\n\"corrected population correction\" (CPC). Together with the theoretical results,\nnumerical simulations show that CPC is always comparable or better than PSC,\nwith a dramatic improvement in some special cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We analyze the impact of arbitrary dependence between the forward and\nbackward links in backscatter communication systems. Specifically, we quantify\nthe effect of positive and negative dependence between these fading links on\nchannel capacity, using Copula theory. The benefits of this approach are\nhighlighted over the classical framework of linear dependence based on\nPearson's correlation coefficient, which is also analyzed. Results show that\nfor a fixed transmit power budget, capacity grows with positive dependence as\nwell as with fading severity in the low signal-to-noise ratio (SNR) regime.\nConversely, fading dependence becomes immaterial in the high SNR regime.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a quantum-classical hybrid algorithm to encode a given arbitrarily\nquantum state $\\vert \\Psi \\rangle$ onto an optimal quantum circuit\n$\\hat{\\mathcal{C}}$ with a finite number of single- and two-qubit quantum\ngates. The proposed algorithm employs as an objective function the absolute\nvalue of fidelity $F = \\langle 0 \\vert \\hat{\\mathcal{C}}^{\\dagger} \\vert \\Psi\n\\rangle$, which is maximized iteratively to construct an optimal quantum\ncircuit $\\hat{\\mathcal{C}}$ with controlled accuracy. The key ingredient of the\nalgorithm is the sequential determination of a set of optimal two-qubit unitary\noperators one by one via the singular value decomposition of the fidelity\ntensor. Once the optimal unitary operators are determined, including the\nlocation of qubits on which each unitary operator acts, elementary quantum\ngates are assigned algebraically. With noiseless numerical simulations, we\ndemonstrate the algorithm to encode a ground state of quantum many-body\nsystems, including the spin-1/2 antiferromagnetic Heisenberg model and the\nspin-1/2 XY model. The results are also compared with the quantum circuit\nencoding of the same quantum state onto a quantum circuit in a given circuit\nstructure. Moreover, we demonstrate that the algorithm can also be applied to\nconstruct an optimal quantum circuit for classical data such as a classical\nimage that is represented as a quantum state by the amplitude encoding.\nFinally, we also experimentally demonstrate that a quantum circuit generated by\nthe AQCE algorithm can indeed represent the original quantum state reasonably\non a noisy real quantum device.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum computing is expected to play an important role in solving the\nproblem of huge computational costs in various applications by utilizing the\ncollective properties of quantum states, including superposition, interference,\nand entanglement, to perform computations. Quantum mechanical (QM) methods are\ncandidates for various applications and can provide accurate absolute energy\ncalculations in structure-based methods. QM methods are powerful tools for\ndescribing reaction pathways and their potential energy surfaces (PESs). In\nthis study, we applied quantum computing to describe the PES of the bimolecular\nnucleophilic substitution (SN2) reaction between chloromethane and chloride\nions. We performed noiseless and noise simulations using quantum algorithms and\ncompared the accuracy and noise effects of the ansatzes. In noiseless\nsimulations, the results from UCCSD and k-UpCCGSD are similar to those of full\nconfigurational interaction (FCI) with the same active space, which indicates\nthat quantum algorithms can describe the PES of the SN2 reaction. In noise\nsimulations, UCCSD is more susceptible to quantum noise than k-UpCCGSD.\nTherefore, k-UpCCGSD can serve as an alternative to UCCSD to reduce quantum\nnoisy effects in the noisy intermediate-scale quantum era, and k-UpCCGSD is\nsufficient to describe the PES of the SN2 reaction in this work. The results\nshowed the applicability of quantum computing to the SN2 reaction pathway and\nprovided valuable information for structure-based molecular simulations with\nquantum computing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Line congruences are $2$-dimensional families of lines in $3$-space. The\nsingularities that appear in generic line congruences are folds, cusps and\nswallowtails. In this paper we give a geometric description of these\nsingularities. The main tool used is the existence of an equiaffine pair\ndefining a generic line congruence.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Topological band insulators and (semi-) metals can arise out of atomic\ninsulators when the hopping strength between electrons increases. Such\ntopological phases are separated from the atomic insulator by a bulk gap\nclosing. In this work, we show that in many (magnetic) space groups, the\ncrystals with certain Wyckoff positions and orbitals being occupied must be\nsemimetal or metals in the atomic limit, e.g. the hopping strength between\nelectrons is infinite weak but not vanishing, which then are termed atomic\n(semi-)metals (ASMs). We derive a sufficient condition for realizing ASMs in\nspinless and spinful systems. Remarkably, we find that increasing the hopping\nstrength between electrons may transform an ASM into an insulator with both\nsymmetries and electron fillings of crystal are preserved. The induced\ninsulators inevitably are topologically non-trivial and at least are obstructed\natomic insulators (OAIs) that are labeled as trivial insulator in topological\nquantum chemistry website. Particularly, using silicon as an example, we show\nASM criterion can discover the OAIs missed by the recently proposed criterion\nof filling enforced OAI. Our work not only establishes an efficient way to\nidentify and design non-trivial insulators but also predicts that the group-IV\nelemental semiconductors are ideal candidate materials for OAI.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  An $s{\\operatorname{-}}t$ minimum cut in a graph corresponds to a minimum\nweight subset of edges whose removal disconnects vertices $s$ and $t$. Finding\nsuch a cut is a classic problem that is dual to that of finding a maximum flow\nfrom $s$ to $t$. In this work we describe a quantum algorithm for the minimum\n$s{\\operatorname{-}}t$ cut problem on undirected graphs. For an undirected\ngraph with $n$ vertices, $m$ edges, and integral edge weights bounded by $W$,\nthe algorithm computes with high probability the weight of a minimum\n$s{\\operatorname{-}}t$ cut in time $\\widetilde O(\\sqrt{m} n^{5/6} W^{1/3} +\nn^{5/3} W^{2/3})$, given adjacency list access to $G$. For simple graphs this\nbound is always $\\widetilde O(n^{11/6})$, even in the dense case when $m =\n\\Omega(n^2)$. In contrast, a randomized algorithm must make $\\Omega(m)$ queries\nto the adjacency list of a simple graph $G$ even to decide whether $s$ and $t$\nare connected.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Twisted layers of atomically thin two-dimensional materials realize a broad\nrange of novel quantum materials with engineered optical and transport\nphenomena arising from spin and valley degrees of freedom and strong electron\ncorrelations in hybridized interlayer bands. Here, we report experimental and\ntheoretical studies of WSe$_2$ homobilayers obtained in two stable\nconfigurations of 2H ($60^\\circ$ twist) and 3R ($0^\\circ$ twist) stackings by\ncontrolled chemical vapor synthesis of high-quality large-area crystals. Using\noptical absorption and photoluminescence spectroscopy at cryogenic\ntemperatures, we uncover marked differences in the optical characteristics of\n2H and 3R bilayer WSe$_2$ which we explain on the basis of beyond-DFT\ntheoretical calculations. Our results highlight the role of layer stacking for\nthe spectral multiplicity of momentum-direct intralayer exciton transitions in\nabsorption, and relate the multiplicity of phonon sidebands in the\nphotoluminescence to momentum-indirect excitons with different spin valley and\nlayer character. Our comprehensive study generalizes to other layered\nhomobilayer and heterobilayer semiconductor systems and highlights the role of\ncrystal symmetry and stacking for interlayer hybrid states.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a novel generic reputation bootstrapping framework for composite\nservices. Multiple reputation-related indicators are considered in a\nlayer-based framework to implicitly reflect the reputation of the component\nservices. The importance of an indicator on the future performance of a\ncomponent service is learned using a modified Random Forest algorithm. We\npropose a topology-aware Forest Deep Neural Network (fDNN) to find the\ncorrelations between the reputation of a composite service and reputation\nindicators of component services. The trained fDNN model predicts the\nreputation of a new composite service with the confidence value. Experimental\nresults with real-world dataset prove the efficiency of the proposed approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider dynamic stochastic economies with heterogeneous agents and\nintroduce the concept of uniformly self-justified equilibria (USJE) --\ntemporary equilibria for which forecasts are best uniform approximations to a\nselection of the equilibrium correspondence. In a USJE, individuals'\nforecasting functions for the next period's endogenous variables are assumed to\nlie in a compact, finite-dimensional set of functions, and the forecasts\nconstitute the best approximation within this set. We show that USJE always\nexist and develop a simple algorithm to compute them. Therefore, they are more\ntractable than rational expectations equilibria that do not always exist. As an\napplication, we discuss a stochastic overlapping generations exchange economy\nand provide numerical examples to illustrate the concept of USJE and the\ncomputational method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We revisit the connection between trajectories of accelerated mirrors and\nspacetime metrics. We present the general (1+1)D effective metric that can be\nobtained with a fibre-optical analogue through the Kerr effect. Then we\nintroduce a new connection between accelerated mirrors and the optical metric.\nIn particular, we connect them for two specific trajectories: The first one is\nthe black mirror that perfectly recreates the Schwarzchild spacetime. The\nsecond one is the Schwarzschild-Planck metric that is a regularized version of\nthe Schwarzschild case. The regularization depends on a length scale that has a\nclear physical interpretation in the fibre-optical analogue system. We study\nthe geometric properties and the Hawking radiation produced in these new\nanalogue metrics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A ubiquitous requirement in many practical reinforcement learning (RL)\napplications, including medical treatment, recommendation system, education and\nrobotics, is that the deployed policy that actually interacts with the\nenvironment cannot change frequently. Such an RL setting is called\nlow-switching-cost RL, i.e., achieving the highest reward while reducing the\nnumber of policy switches during training. Despite the recent trend of\ntheoretical studies aiming to design provably efficient RL algorithms with low\nswitching costs, none of the existing approaches have been thoroughly evaluated\nin popular RL testbeds. In this paper, we systematically studied a wide\ncollection of policy-switching approaches, including theoretically guided\ncriteria, policy-difference-based methods, and non-adaptive baselines. Through\nextensive experiments on a medical treatment environment, the Atari games, and\nrobotic control tasks, we present the first empirical benchmark for\nlow-switching-cost RL and report novel findings on how to decrease the\nswitching cost while maintain a similar sample efficiency to the case without\nthe low-switching-cost constraint. We hope this benchmark could serve as a\nstarting point for developing more practically effective low-switching-cost RL\nalgorithms. We release our code and complete results in\nhttps://sites.google.com/view/low-switching-cost-rl.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The constitutive characterization of the uniformity and homogeneity of binary\nelastic composites is presented in terms of a combination of the material\ngroupoids of the individual constituents. The incorporation of these two\ngroupoids within a single double groupoid is proposed as a viable geometric\ndifferential framework for a unified formulation of this and similar kinds of\nproblems in continuum mechanics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Cloud computing is a particular implementation of distributed computing. It\ninherited many properties of distributed computing such as scalability,\nreliability and distribution transparency. The transparency middle layer\nabstracts the underlying platform away from the end user. Virtualization\ntechnology is the foundation of Cloud computing. Virtual machine provides\nabstraction of the physical server resources and securely isolates different\nusers in multi-tenant environment. To the Cloud services consumer, all the\ncomputing power and resources are accessed through high speed internet access\nby client platforms. This eliminates the cost to build and maintain local data\ncenter. Resource pooling and rapid elasticity are the main characters of Cloud\ncomputing. The scalability of Cloud computing comes from resources which can\nspan multiple data centers and geographic regions. There is virtually no\nlimitation on the amount of resources available from Cloud. New processing and\nstorage resources can be added into the Cloud resource pool seamlessly.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the frame set of regular multivariate Gaussian Gabor frames\nusing methods from K\\\"ahler geometry such as H\\\"ormander's $\\dbar$-method, the\nOhsawa--Takegoshi extension theorem and a K\\\"ahler-variant of the symplectic\nembedding theorem of McDuff-Polterovich for ellipsoids. Our approach is based\non the well-known link between sets of interpolation for the Bargmann-Fock\nspace and the frame set of multivariate Gaussian Gabor frames. We state\nsufficient conditions in terms of a certain extremal type Seshadri constant of\nthe complex torus associated to a lattice to be a set of interpolation for the\nBargmann-Fock space, and give also a condition in terms of the generalized\nBuser-Sarnak invariant of the lattice. Our results on Gaussian Gabor frames are\nin terms of the Sehsadri constant and the generalized Buser-Sarnak invariant of\nthe associated symplectic dual lattice. The theory of H\\\"ormander estimates and\nthe Ohsawa--Takegoshi extension theorem allow us to give estimates for the\nframe bounds in terms of the Buser-Sarnack invariant and in the one-dimensional\ncase these bounds are sharp thanks to Faltings' work on Green functions in\nArakelov theory.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  ExpressInHost (https://gitlab.com/a.raguin/expressinhost) is a GTK/C++ based\nuser friendly graphical interface that allows tuning the codon sequence of an\nmRNA for recombinant protein expression in a host microorganism. Heterologous\ngene expression is widely implemented in biotechnology companies and academic\nresearch laboratories. However, expression of recombinant proteins can be\nchallenging. On the one hand, maximising translation speed is important,\nespecially in scalable production processes relevant to biotechnology\ncompanies, but on the other hand, solubility problems often arise as a\nconsequence, since translation \"pauses\" might be key to allow the nascent\npolypeptide chain to fold appropriately. To address this challenge, we have\ndeveloped a software that offers three distinct modes to tune codon sequences\nusing the genetic code redundancy. The tuning strategies implemented take into\naccount the specific tRNA resources of the host and that of the native\norganism. They balance rapid translation and native speed mimicking, which\nmight be important to allow proper protein folding, thereby avoiding protein\nsolubility problems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We describe a new approach for driving GeV-scale plasma accelerators with\nlong laser pulses. We show that the temporal phase of a long, high-energy\ndriving laser pulse can be modulated periodically by co-propagating it with\nlow-amplitude plasma wave driven by a short, low-energy seed pulse. Compression\nof the modulated driver by a dispersive optic generates a train of short pulses\nsuitable for resonantly driving a plasma accelerator. Modulation of the driver\noccures via well-controlled linear process, as confirmed by good agreement\nbetween particle-in-cell (PIC) simulations and an analytic model. PIC\nsimulations demonstrate that a 1.7 J, 1 ps driver and a 140 mJ, 40 fs seed\npulse can accelerate electrons to energies of 0.65 GeV in a plasma channel with\nan axial density of 2.5 x 10$^{17}$ cm$^{-3}$. This work opens a route to\nhigh-repetition-rate, GeV-scale plasma accelerators driven by thin-disk lasers,\nwhich can provide joule-scale, picosecond-duration laser pulses at\nmulti-kilohertz repetition rates and high wall-plug efficiencies.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  mmWave radars offer excellent depth resolution even at very long ranges owing\nto their high bandwidth. But their angular resolution is at least an\norder-of-magnitude worse than camera and lidar systems. Hence, mmWave radar is\nnot a capable 3-D imaging solution in isolation. We propose Metamoran, a system\nthat combines the complimentary strengths of radar and camera to obtain\naccurate, high resolution depth images over long ranges even in high clutter\nenvironments, all from a single fixed vantage point. Metamoran enables rich\nlong-range depth imaging with applications in security and surveillance,\nroadside safety infrastructure and wide-area mapping. Our approach leverages\nthe high angular resolution from cameras using computer vision techniques,\nincluding image segmentation and monocular depth estimation, to obtain object\nshape. Our core contribution is a method to convert this object shape into an\nRF I/Q equivalent, which we use in a novel radar processing pipeline to help\ndeclutter the scene and capture extremely weak reflections from objects at long\ndistances. We perform a detailed evaluation of Metamoran's depth imaging\ncapabilities in 400 diverse scenes. Our evaluation shows that Metamoran\nestimates the depth of static objects up to 90 m and moving objects up to 305 m\nand with a median error of 28 cm, an improvement of 13$\\times$ compared to a\nnaive radar+camera baseline and 23$\\times$ compared to monocular depth\nestimation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study interpolating sequences of $d$-tuples of matrices, by looking at the\ncommuting and the non-commuting case separately. In both cases, we will give a\ncharacterization of such sequences in terms of separation conditions on\nsuitable reproducing kernel Hilbert spaces, and we will give sufficient\nconditions stated in terms of separation via analytic functions. Examples of\nsuch interpolating sequences will also be given\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Unmanned aerial vehicles (UAVs) are becoming largely ubiquitous with an\nincreasing demand for aerial data. Accurate navigation and localization,\nrequired for precise data collection in many industrial applications, often\nrelies on RTK GNSS. These systems, able of centimeter-level accuracy, require a\nsetup and calibration process and are relatively expensive. This paper\naddresses the problem of accurate positioning and navigation of UAVs through\ncooperative localization. Inexpensive ultra-wideband (UWB) transceivers\ninstalled on both the UAV and a support ground robot enable centimeter-level\nrelative positioning. With fast deployment and wide setup flexibility, the\nproposed system is able to accommodate different environments and can also be\nutilized in GNSS-denied environments. Through extensive simulations and test\nfields, we evaluate the accuracy of the system and compare it to GNSS in urban\nenvironments where multipath transmission degrades accuracy. For completeness,\nwe include visual-inertial odometry in the experiments and compare the\nperformance with the UWB-based cooperative localization.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a novel approach to optimal transport between graphs from the\nperspective of stationary Markov chains. A weighted graph may be associated\nwith a stationary Markov chain by means of a random walk on the vertex set with\ntransition distributions depending on the edge weights of the graph. After\ndrawing this connection, we describe how optimal transport techniques for\nstationary Markov chains may be used in order to perform comparison and\nalignment of the graphs under study. In particular, we propose the graph\noptimal transition coupling problem, referred to as GraphOTC, in which the\nMarkov chains associated to two given graphs are optimally synchronized to\nminimize an expected cost. The joint synchronized chain yields an alignment of\nthe vertices and edges in the two graphs, and the expected cost of the\nsynchronized chain acts as a measure of distance or dissimilarity between the\ntwo graphs. We demonstrate that GraphOTC performs equal to or better than\nexisting state-of-the-art techniques in graph optimal transport for several\ntasks and datasets. Finally, we also describe a generalization of the GraphOTC\nproblem, called the FusedOTC problem, from which we recover the GraphOTC and OT\ncosts as special cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Following Cayley, MacMahon, and Sylvester, define a non-unitary partition to\nbe an integer partition with no part equal to one, and let $\\nu(n)$ denote the\nnumber of non-unitary partitions of size $n$. In a 2021 paper, the sixth author\nproved a formula to compute $p(n)$ by enumerating only non-unitary partitions\nof size $n$, and recorded a number of conjectures regarding the growth of\n$\\nu(n)$ as $n\\to \\infty$. Here we refine and prove some of these conjectures.\nFor example, we prove $p(n) \\sim \\nu(n)\\sqrt{n/\\zeta(2)}$ as $n\\to \\infty$, and\ngive Ramanujan-like congruences between $p(n)$ and $\\nu(n)$ such as\n$p(5n)\\equiv \\nu(5n)\\ (\\operatorname{mod} 5)$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Accurate downlink channel information is crucial to the beamforming design,\nbut it is difficult to obtain in practice. This paper investigates a deep\nlearning-based optimization approach of the downlink beamforming to maximize\nthe system sum rate, when only the uplink channel information is available. Our\nmain contribution is to propose a model-driven learning technique that exploits\nthe structure of the optimal downlink beamforming to design an effective hybrid\nlearning strategy with the aim to maximize the sum rate performance. This is\nachieved by jointly considering the learning performance of the downlink\nchannel, the power and the sum rate in the training stage. The proposed\napproach applies to generic cases in which the uplink channel information is\navailable, but its relation to the downlink channel is unknown and does not\nrequire an explicit downlink channel estimation. We further extend the\ndeveloped technique to massive multiple-input multiple-output scenarios and\nachieve a distributed learning strategy for multicell systems without an\ninter-cell signalling overhead. Simulation results verify that our proposed\nmethod provides the performance close to the state of the art numerical\nalgorithms with perfect downlink channel information and significantly\noutperforms existing data-driven methods in terms of the sum rate.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We report the observation of gravity-capillary waves on a torus of fluid. By\nmeans of an original technique, a stable torus is achieved by depositing water\non a superhydrophobic groove with a shallow wedge-shaped channel running along\nits perimeter. Using a spatio-temporal optical measurement, we report the full\ndispersion relation of azimuthal waves propagating along the inner and outer\ntorus borders, highlighting several branches modeled as varicose, sinuous and\nsloshing modes. Standing azimuthal waves are also studied leading to\npolygon-like patterns arising on the two torus borders with a number of sides\ndifferent when a tunable decoupling of the two interfaces occurs. The quantized\nnature of the dispersion relation is also evidenced.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Due to the sparse connectivity of superconducting quantum computers, qubit\ncommunication via SWAP gates accounts for the vast majority of overhead in\nquantum programs. We introduce a method for improving the speed and reliability\nof SWAPs at the level of the superconducting hardware's native gateset. Our\nmethod relies on four techniques: 1) SWAP Orientation, 2) Cross-Gate Pulse\nCancellation, 3) Commutation through Cross-Resonance, and 4) Cross-Resonance\nPolarity. Importantly, our Optimized SWAP is bootstrapped from the\npre-calibrated gates, and therefore incurs zero calibration overhead. We\nexperimentally evaluate our optimizations with Qiskit Pulse on IBM hardware.\nOur Optimized SWAP is 11% faster and 13% more reliable than the Standard SWAP.\nWe also experimentally validate our optimizations on application-level\nbenchmarks. Due to (a) the multiplicatively compounding gains from improved\nSWAPs and (b) the frequency of SWAPs, we observe typical improvements in\nsuccess probability of 10-40%. The Optimized SWAP is available through the\nSuperstaQ platform.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Realistic nuclear reaction cross-section models are an essential ingredient\nof reliable heavy-ion transport codes. Such codes are used for risk evaluation\nof manned space exploration missions as well as for ion-beam therapy dose\ncalculations and treatment planning. Therefore, in this study, a collection of\ntotal nuclear reaction cross-section data has been generated within a\nGSI-ESA-NASA collaboration. The database includes the experimentally measured\ntotal nucleus-nucleus reaction cross-sections. The Tripathi, Kox, Shen,\nKox-Shen, and Hybrid-Kurotama models are systematically compared with the\ncollected data. Details about the implementation of the models are given.\nLiterature gaps are pointed out and considerations are made about which models\nfit best the existing data for the most relevant systems to radiation\nprotection in space and heavy-ion therapy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce a new family of particle evolution samplers suitable for\nconstrained domains and non-Euclidean geometries. Stein Variational Mirror\nDescent and Mirrored Stein Variational Gradient Descent minimize the\nKullback-Leibler (KL) divergence to constrained target distributions by\nevolving particles in a dual space defined by a mirror map. Stein Variational\nNatural Gradient exploits non-Euclidean geometry to more efficiently minimize\nthe KL divergence to unconstrained targets. We derive these samplers from a new\nclass of mirrored Stein operators and adaptive kernels developed in this work.\nWe demonstrate that these new samplers yield accurate approximations to\ndistributions on the simplex, deliver valid confidence intervals in\npost-selection inference, and converge more rapidly than prior methods in\nlarge-scale unconstrained posterior inference. Finally, we establish the\nconvergence of our new procedures under verifiable conditions on the target\ndistribution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Generative Adversarial Networks (GAN) are a powerful methodology and can be\nused for unsupervised anomaly detection, where current techniques have\nlimitations such as the accurate detection of anomalies near the tail of a\ndistribution. GANs generally do not guarantee the existence of a probability\ndensity and are susceptible to mode collapse, while few GANs use likelihood to\nreduce mode collapse. In this paper, we create a GAN-based tail formation model\nfor anomaly detection, the Tail of distribution GAN (TailGAN), to generate\nsamples on the tail of the data distribution and detect anomalies near the\nsupport boundary. Using TailGAN, we leverage GANs for anomaly detection and use\nmaximum entropy regularization. Using GANs that learn the probability of the\nunderlying distribution has advantages in improving the anomaly detection\nmethodology by allowing us to devise a generator for boundary samples, and use\nthis model to characterize anomalies. TailGAN addresses supports with disjoint\ncomponents and achieves competitive performance on images. We evaluate TailGAN\nfor identifying Out-of-Distribution (OoD) data and its performance evaluated on\nMNIST, CIFAR-10, Baggage X-Ray, and OoD data shows competitiveness compared to\nmethods from the literature.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Aim: To study the multimessenger nature of the signal that can result from\nthe phase transition of a neutron star to a quark star and their corresponding\nastrophysical observations. Methods: The phase transition process is initiated\nby the abrupt pressure and density changes at the star center, giving rise to a\nshock which deconfines matter followed by a weak front converting excess down\nto strange quarks to attain absolute stability. This process's effects are\ninvestigated by understanding how the energy escapes from the star in the form\nof neutrino-antineutrino annihilation. For such annihilation process, the\ncorresponding energy deposition rate is calculated. Structural changes due to\nthe energy loss have been investigated in the likes of misalignment angle\nevolution of the star and its astrophysical observation through gravitational\nwaves. Results: The energy and time signature for the neutrino-antineutrino\nannihilation is compared with the observed isotropic energy for a short\ngamma-ray burst. The misalignment angle evolves to align the star's tilt axis,\nwhich can lead to the sudden increase or decrease of radio intensity from the\npulsar. The corresponding gravitational wave emission, both continuous and\nburst, all lead towards multimessenger signals coming from the phase\ntransition.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study orbit codes in the field extension ${\\mathbb F}_{q^n}$. First we\nshow that the automorphism group of a cyclic orbit code is contained in the\nnormalizer of the Singer subgroup if the orbit is generated by a subspace that\nis not contained in a proper subfield of ${\\mathbb F}_{q^n}$. We then\ngeneralize to orbits under the normalizer of the Singer subgroup. In that\nsituation some exceptional cases arise and some open cases remain. Finally we\ncharacterize linear isometries between such codes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Fair division with unequal shares is an intensively studied recourse\nallocation problem. For $ i\\in [n] $, let $ \\mu_i $ be an atomless probability\nmeasure on the measurable space $(C,\\mathcal{S}) $ and let $ t_i $ be positive\nnumbers (entitlements) with $ \\sum_{i=1}^{n}t_i=1 $. A fair division is a\npartition of $ C $ into sets $ S_i\\in \\mathcal{S} $ with $ \\mu_i(S_i)\\geq t_i$\nfor every $ i\\in [n] $.\n  We introduce new algorithms to solve the fair division problem with\nirrational entitlements. They are based on the classical Last diminisher\ntechnique and we believe that they are simpler than the known methods. Then we\nshow that a fair division always exists even for infinitely many players.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  De-embedding antennas from the channel using Spherical Wave Functions (SWF)\nis a useful method to reduce the numerical effort in the simulation of wearable\nantennas. In this paper an analytical solution to the De-embedding problem is\npresented in form of surface integrals. This new integral solution is helpful\non a theoretical level to derive insights and is also well suited for\nimplementation in Finite Difference Time Domain (FDTD) numerical software. The\nspherical wave function coefficients are calculated directly from near-field\nvalues. Furthermore, the presence of a near-field scatterer in the de-embedding\nproblem is discussed on a theoretical level based on the Huygens Equivalence\nTheorem. This makes it possible to exploit the degrees of freedom in such a way\nthat it is sufficient to only use out-going spherical wave functions and still\nobtain correct results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Solution robustness focuses on structural similarities between the nominal\nsolution and the scenario solutions. Most other robust optimization approaches\nfocus on the quality robustness and only evaluate the relevance of their\nsolutions through the objective function value. However, it can be more\nimportant to optimize the solution robustness and, once the uncertainty is\nrevealed, find an alternative scenario solution $x^s$ which is as similar as\npossible to the nominal solution $x^{nom}$. This for example occurs when the\nrobust solution is implemented on a regular basis or when the uncertainty is\nrevealed late. We call this distance between $x^{nom}$ and $x^s$ the solution\ncost. We consider the proactive problem which minimizes the average solution\ncost over a discrete set of scenarios while ensuring the optimality of the\nnominal objective of $x^{nom}$. We show for two different solution distances\n$d_{val}$ and $d_{struct}$ that the proactive problem is NP-hard for both the\ninteger min-cost flow problem with uncertain arc demands and for the integer\nmax-flow problem with uncertain arc capacities. For these two problems, we\nprove that once the uncertainty is revealed, even identifying a reactive\nsolution $x^r$ with a minimal distance to a given solution $x^{nom}$ is NP-hard\nfor $d_{struct}$, and that it is polynomial for $d_{val}$. We highlight the\nbenefits of solution robustness in a case study on a railroad planning problem.\nFirst, we compare our proactive approach to the anchored and the $k$-distance\napproaches. Then, we show the efficiency of the proactive solution over\nreactive solutions. Finally, we illustrate the solution cost reduction when\nrelaxing the optimality constraint on the nominal objective of the proactive\nsolution $x^{nom}$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we study a simple extension of the total weighted flowtime\nminimization problem for single and identical parallel machines. While the\nstandard problem simply defines a set of jobs with their processing times and\nweights and assumes that all jobs have release date 0 and have no deadline, we\nassume that the release date of each job is a decision variable that is only\nconstrained by a single global latest arrival deadline. To our knowledge, this\nsimple yet practically highly relevant extension has never been studied. Our\nmain contribution is that we show the NP- completeness of the problem even for\nthe single machine case and provide an exhaustive empirical study of different\ntypical approaches including genetic algorithms, tree search, and constraint\nprogramming.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study in detail a recently proposed mechanism for producing non-thermal\ndark photon dark matter at the end of inflation in the mass range $\\mu\\,{\\rm\neV} \\lesssim m \\lesssim 10\\,{\\rm TeV}$. A tachyonic instability induced by a\nrolling inflaton leads to the coherent production of dark (abelian) gauge\nbosons with a peak in the power spectrum corresponding to the Hubble scale at\nthe end of inflation. As the Universe expands after inflation the dark photons\nredshift and, at some point in their cosmic evolution, they obtain a mass. We\nfocus in particular on the case where the dark photons are relativistic at the\ntime their mass is generated and examine the associated cosmic evolution to\ncompute the relic abundance today. We also examine the late time power spectrum\ndemonstrating explicitly that it preserves the peak generated at the end of\ninflation. We show that the peak corresponds to small physical scales today,\n$\\ell_{\\rm today} \\sim {\\rm cm} - 100\\,{\\rm km}$, with large density\nfluctuations at $\\ell_{\\rm today}$ leading to a clumpy nature for the dark\nphoton dark matter. We also discuss potential phenomenology and future\ndirections, briefly commenting on the non-relativistic case.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present an efficient approach for doing approximate Bayesian inference\nwhen only a limited number of noisy likelihood evaluations can be obtained due\nto computational constraints, which is becoming increasingly common for\napplications of complex models. Our main methodological innovation is to model\nthe log-likelihood function using a Gaussian process (GP) in a local fashion\nand apply this model to emulate the progression that an exact\nMetropolis-Hastings (MH) algorithm would take if it was applicable. New\nlog-likelihood evaluation locations are selected using sequential experimental\ndesign strategies such that each MH accept/reject decision is done within a\npre-specified error tolerance. The resulting approach is conceptually simple\nand sample-efficient as it takes full advantage of the GP model. It is also\nmore robust to violations of GP modelling assumptions and better suited for the\ntypical situation where the posterior is substantially more concentrated than\nthe prior, compared with various existing inference methods based on global GP\nsurrogate modelling. We discuss the probabilistic interpretations and central\ntheoretical aspects of our approach, and we then demonstrate the benefits of\nthe resulting algorithm in the context of likelihood-free inference for\nsimulator-based statistical models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider topologically twisted $\\mathcal{N}=2$, $SU(2)$ gauge theory with\na massive adjoint hypermultiplet on a smooth, compact four-manifold $X$. A\nconsistent formulation requires coupling the theory to a ${\\rm Spin}^c$\nstructure, which is necessarily non-trivial if $X$ is non-spin. We derive\nexplicit formulae for the topological correlation functions when $b_2^+\\geq 1$.\nWe demonstrate that, when the ${\\rm Spin}^c$ structure is canonically\ndetermined by an almost complex structure and the mass is taken to zero, the\npath integral reproduces known results for the path integral of the\n$\\mathcal{N}=4$ gauge theory with Vafa-Witten twist. On the other hand, we\nreproduce results from Donaldson-Witten theory after taking a suitable infinite\nmass limit. The topological correlators are functions of the UV coupling\nconstant $\\tau_{\\rm uv}$ and we confirm that they obey the expected $S$-duality\ntransformation laws. The holomorphic part of the partition function is a\ngenerating function for the Euler numbers of the matter (or obstruction) bundle\nover the instanton moduli space. For $b_2^+=1$, we derive a non-holomorphic\ncontribution to the path integral, such that the partition function and\ncorrelation functions are mock modular forms rather than modular forms. We\ncomment on the generalization of this work to the large class of\n$\\mathcal{N}=2$ theories of class $S$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Scene text recognition (STR) has been widely studied in academia and\nindustry. Training a text recognition model often requires a large amount of\nlabeled data, but data labeling can be difficult, expensive, or time-consuming,\nespecially for Traditional Chinese text recognition. To the best of our\nknowledge, public datasets for Traditional Chinese text recognition are\nlacking. This paper presents a framework for a Traditional Chinese synthetic\ndata engine which aims to improve text recognition model performance. We\ngenerated over 20 million synthetic data and collected over 7,000 manually\nlabeled data TC-STR 7k-word as the benchmark. Experimental results show that a\ntext recognition model can achieve much better accuracy either by training from\nscratch with our generated synthetic data or by further fine-tuning with TC-STR\n7k-word.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A shape invariant nonseparable and nondiagonalizable two-dimensional model\nwith anharmonic complex interaction, first studied by Cannata, Ioffe, and\nNishnianidze, is re-examined with the purpose of providing an algebraic\nconstruction of the associated functions to the excited-state wavefunctions,\nneeded to complete the basis. The two operators $A^+$ and $A^-$, coming from\nthe shape invariant supersymmetric approach, where $A^+$ acts as a raising\noperator while $A^-$ annihilates all wavefunctions, are completed by\nintroducing a novel pair of operators $B^+$ and $B^-$, where $B^-$ acts as the\nmissing lowering operator. It is then shown that building the associated\nfunctions as polynomials in $A^+$ and $B^+$ acting on the ground state provides\na much more efficient approach than that used in the original paper. In\nparticular, we have been able to extend the previous results obtained for the\nfirst two excited states of the quartic anharmonic oscillator either by\nconsidering the next three excited states or by adding a cubic or a sextic term\nto the Hamiltonian.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that low-lying excitations of a 2D BCS superconductor are\nsignificantly altered when coupled to an externally driven cavity, which\ninduces controllable long-range attractive interactions between the electrons.\nWe find that they combine non-linearly with intrinsic local interactions to\nincrease the Bogoliubov quasiparticle excitation energies, thus enlarging the\nsuperconducting gap. The long-range nature of the driven-cavity-induced\nattraction qualitatively changes the collective excitations of the\nsuperconductor. Specifically, they lead to the appearance of additional\ncollective excitations of the excitonic modes. Furthermore, the Higgs mode is\npushed into the gap and now lies below the Bogoliubov quasiparticle continuum\nsuch that it cannot decay into quasiparticles. This way, the Higgs mode's\nlifetime is greatly enhanced.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Phylogenetic networks can model more complicated evolutionary phenomena that\ntrees fail to capture such as horizontal gene transfer and hybridization. The\nsame Markov models that are used to model evolution on trees can also be\nextended to networks and similar questions, such as the identifiability of the\nnetwork parameter or the invariants of the model, can be asked. In this paper\nwe focus on finding the invariants of the Cavendar-Farris-Neyman (CFN) model on\nlevel-1 phylogenetic networks. We do this by reducing the problem to finding\ninvariants of sunlet networks, which are level-1 networks consisting of a\nsingle cycle with leaves at each vertex. We then determine all quadratic\ninvariants in the sunlet network ideal which we conjecture generate the full\nideal.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We use combined South Pole Telescope (SPT)+Planck temperature maps to analyze\nthe circumgalactic medium (CGM) encompassing 138,235 massive, quiescent 0.5\n$\\leq$ z $\\leq$ 1.5 galaxies selected from data from the Dark Energy Survey\n(DES) and Wide-Field Infrared Survey Explorer (WISE). Images centered on these\ngalaxies were cut from the 1.85 arcmin resolution maps with frequency bands at\n95, 150, and 220 GHz. The images were stacked, filtered, and fit with a\ngray-body dust model to isolate the thermal Sunyaev-Zel'dovich (tSZ) signal,\nwhich is proportional to the total energy contained in the CGM of the galaxies.\nWe separate these $M_{\\star} = 10^{10.9} M_\\odot$ - $10^{12} M_\\odot$ galaxies\ninto 0.1 dex stellar mass bins, detecting tSZ per bin up to $5.6\\sigma$ and a\ntotal signal-to-noise ratio of $10.1\\sigma$. We also detect dust with an\noverall signal-to-noise ratio of $9.8\\sigma$, which overwhelms the tSZ at\n150GHz more than in other lower-redshift studies. We correct for the $0.16$ dex\nuncertainty in the stellar mass measurements by parameter fitting for an\nunconvolved power-law energy-mass relation, $E_{\\rm therm} = E_{\\rm therm,peak}\n\\left(M_\\star/M_{\\star,{\\rm peak}} \\right)^\\alpha$, with the peak stellar mass\ndistribution of our selected galaxies defined as $M_{\\star,{\\rm peak}}= 2.3\n\\times 10^{11} M_\\odot$. This yields an $E_{\\rm therm,peak}=\n5.98_{-1.00}^{+1.02} \\times 10^{60}$ erg and $\\alpha=3.77_{-0.74}^{+0.60}$.\nThese are consistent with $z \\approx 0$ observations and within the limits of\nmoderate models of active galactic nuclei (AGN) feedback. We also compute the\nradial profile of our full sample, which is similar to that recently measured\nat lower-redshift by Schaan et al. (2021).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The introduction of the Basic Linear Algebra Subroutine (BLAS) in the 1970s\npaved the way for different libraries to solve the same problem with an\nimproved approach and hardware. The new BLAS implementation led to\nHigh-Performance Computing (HPC) innovation. All the love went to the level 3\nBLAS due to its humongous application in different fields, not bounded by\ncomputer science. However, level 1 and level 2 got neglected; we tried to solve\nthe problem by introducing the new algorithm for the Vector-Vector dot product,\nVector-Vector outer product and Matrix-Vector product, which improves the\nperformance of these operations in a significant way. We are not introducing\nany library but algorithms, which improves upon the current state of art\nalgorithms. Also, we rely on the FMA instruction, OpenMP, and the compiler to\noptimize the code rather than implementing the algorithm in assembly.\nTherefore, our current implementation is machine oblivious and depends on the\ncompilers ability to optimize the code.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In situations where explanations of black-box models may be useful, the\nfairness of the black-box is also often a relevant concern. However, the link\nbetween the fairness of the black-box model and the behavior of explanations\nfor the black-box is unclear. We focus on explanations applied to tabular\ndatasets, suggesting that explanations do not necessarily preserve the fairness\nproperties of the black-box algorithm. In other words, explanation algorithms\ncan ignore or obscure critical relevant properties, creating incorrect or\nmisleading explanations. More broadly, we propose future research directions\nfor evaluating and generating explanations such that they are informative and\nrelevant from a fairness perspective.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Embedding Artificial Intelligence onto low-power devices is a challenging\ntask that has been partly overcome with recent advances in machine learning and\nhardware design. Presently, deep neural networks can be deployed on embedded\ntargets to perform different tasks such as speech recognition,object detection\nor Human Activity Recognition. However, there is still room for optimization of\ndeep neural networks onto embedded devices. These optimizations mainly address\npower consumption,memory and real-time constraints, but also an easier\ndeployment at the edge. Moreover, there is still a need for a better\nunderstanding of what can be achieved for different use cases. This work\nfocuses on quantization and deployment of deep neural networks onto low-power\n32-bit microcontrollers. The quantization methods, relevant in the context of\nan embedded execution onto a microcontroller, are first outlined. Then, a new\nframework for end-to-end deep neural networks training, quantization and\ndeployment is presented. This framework, called MicroAI, is designed as an\nalternative to existing inference engines (TensorFlow Lite for Microcontrollers\nand STM32CubeAI). Our framework can indeed be easily adjusted and/or extended\nfor specific use cases. Execution using single precision 32-bit floating-point\nas well as fixed-point on 8- and 16-bit integers are supported. The proposed\nquantization method is evaluated with three different datasets (UCI-HAR, Spoken\nMNIST and GTSRB). Finally, a comparison study between MicroAI and both existing\nembedded inference engines is provided in terms of memory and power efficiency.\nOn-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq\nApollo3 and STM32L452RE).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Secured Overnight Funding Rate (SOFR) is becoming the main Risk-Free Rate\nbenchmark in US dollars, thus interest rate term structure models need to be\nupdated to reflect the key features exhibited by the dynamics of SOFR and the\nforward rates implied by SOFR futures. Historically, interest rate term\nstructure modelling has been based on rates of substantially longer time to\nmaturity than overnight, but with SOFR the overnight rate now is the primary\nmarket observable. This means that the empirical idiosyncrasies of the\novernight rate cannot be ignored when constructing interest rate models in a\nSOFR-based world.\n  As a rate reflecting transactions in the Treasury overnight repurchase\nmarket, the dynamics of SOFR are closely linked to the dynamics of the\nEffective Federal Funds Rate (EFFR), which is the interest rate most directly\nimpacted by US monetary policy target rate decisions. Therefore, these rates\nfeature jumps at known times (Federal Open Market Committee meeting dates), and\nmarket expectations of these jumps are reflected in prices for futures written\non these rates. On the other hand, forward rates implied by Fed Funds and SOFR\nfutures continue to evolve diffusively. The model presented in this paper\nreflects the key empirical features of SOFR dynamics and is calibrated to\nfutures prices. In particular, the model reconciles diffusive forward rate\ndynamics with piecewise constant paths of the target short rate.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper describes a stereo image-based visual servoing system for\ntrajectory tracking by a non-holonomic robot without externally derived pose\ninformation nor a known visual map of the environment. It is called trajectory\nservoing. The critical component is a feature-based, indirect Simultaneous\nLocalization And Mapping (SLAM) method to provide a pool of available features\nwith estimated depth, so that they may be propagated forward in time to\ngenerate image feature trajectories for visual servoing. Short and long\ndistance experiments show the benefits of trajectory servoing for navigating\nunknown areas without absolute positioning. Empirically, trajectory servoing\nhas better trajectory tracking performance than pose-based feedback when both\nrely on the same underlying SLAM system.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Evolution of the reduced density matrix for a subsystem is studied to\ndetermine deviations from its Markov character for a system consisting of a\nclosed chain of $N$ oscillators with one of them serving as a subsystem. The\ndependence on $N$ and on the coupling of the two subsystems is investigated\nnumerically. The found deviations strongly depend on $N$ and the coupling. In\nthe most beneficial case with $N-1=100$ and the coupling randomized in its\nstructure the deviations fall with the evolution time up 3\\%. In other cases\nthey remain to be of the order 30\\% or even more.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The concept of minimum length, widely accepted as a low-energy effect of\nquantum gravity, manifests itself in quantum mechanics through generalized\nuncertainty principles. Curved momentum space, on the other hand, is at the\nheart of similar applications such as doubly special relativity. We introduce a\nduality between theories yielding generalized uncertainty principles and\nquantum mechanics on nontrivial momentum space. In particular, we find\ncanonically conjugate variables which map the former into the latter. In that\nvein, we explicitly derive the vielbein corresponding to a generic generalized\nuncertainty principle in $d$ dimensions. Assuming the predominantly used\nquadratic form of the modification, the curvature tensor in momentum space is\nproportional to the noncommutativity of the coordinates in the modified\nHeisenberg algebra. Yet, the metric is non-Euclidean even in the flat case\ncorresponding to commutative space, because the resulting momentum basis is\nnoncanonical. These insides are used to constrain the curvature and the\ndeviation from the canonical basis.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  There has been much interest in novel models of dark matter that exhibit\ninteresting behavior on galactic scales. A primary motivation is the observed\nBaryonic Tully-Fisher Relation in which the mass of galaxies increases as the\nquartic power of rotation speed. This scaling is not obviously accounted for by\nstandard cold dark matter. This has prompted the development of dark matter\nmodels that exhibit some form of so-called MONDian phenomenology to account for\nthis galactic scaling, while also recovering the success of cold dark matter on\nlarge scales. A beautiful example of this are the so-called superfluid dark\nmatter models, in which a complex bosonic field undergoes spontaneous symmetry\nbreaking on galactic scales, entering a superfluid phase with a 3/2 kinetic\nscaling in the low energy effective theory, that mediates a long-ranged MONDian\nforce. In this work we examine the causality and locality properties of these\nand other related models. We show that the Lorentz invariant completions of the\nsuperfluid models exhibit high energy perturbations that violate global\nhyperbolicity of the equations of motion in the MOND regime and can be\nsuperluminal in other parts of phase space. We also examine a range of\nalternate models, finding that they also exhibit forms of non-locality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Not all video frames are equally informative for recognizing an action. It is\ncomputationally infeasible to train deep networks on all video frames when\nactions develop over hundreds of frames. A common heuristic is uniformly\nsampling a small number of video frames and using these to recognize the\naction. Instead, here we propose full video action recognition and consider all\nvideo frames. To make this computational tractable, we first cluster all frame\nactivations along the temporal dimension based on their similarity with respect\nto the classification task, and then temporally aggregate the frames in the\nclusters into a smaller number of representations. Our method is end-to-end\ntrainable and computationally efficient as it relies on temporally localized\nclustering in combination with fast Hamming distances in feature space. We\nevaluate on UCF101, HMDB51, Breakfast, and Something-Something V1 and V2, where\nwe compare favorably to existing heuristic frame sampling methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Sampled environment transitions are a critical input to deep reinforcement\nlearning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and\neasy generation of large amounts of samples such that perceived progress in DRL\ndoes not necessarily correspond to improved sample efficiency. As simulating\nreal world processes is often prohibitively hard and collecting real world\nexperience is costly, sample efficiency is an important indicator for\neconomically relevant applications of DRL. We investigate progress in sample\nefficiency on Atari games and continuous control tasks by comparing the number\nof samples that a variety of algorithms need to reach a given performance level\naccording to training curves in the corresponding publications. We find\nexponential progress in sample efficiency with estimated doubling times of\naround 10 to 18 months on Atari, 5 to 24 months on state-based continuous\ncontrol and of around 4 to 9 months on pixel-based continuous control depending\non the specific task and performance level.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The development of alternative platforms for computing has been a\nlongstanding goal for physics, and represents a particularly pressing concern\nas conventional transistors approach the limit of miniaturization. A potential\nalternatice paradigm is that of reservoir computing, which leverages unknown,\nbut highly non-linear transformations of input-data to perform computations.\nThis has the advantage that many physical systems exhibit precisely the type of\nnon-linear input-output relationships necessary for them to function as\nreservoirs. Consequently, the quantum effects which obstruct the further\ndevelopment of silicon electronics become an advantage for a reservoir\ncomputer. Here we demonstrate that even the most basic constituents of matter -\natoms - can act as a reservoir for optical computers, thanks to the phenomenon\nof High Harmonic Generation (HHG). A prototype single-atom computer for\nclassification problems is proposed, where parameters of the classification\nmodel are mapped to optical elements. We numerically demonstrate that this\n`all-optical' computer can successfully classify data with an accuracy that is\nstrongly dependent on dynamical non-linearities. This may pave the way for the\ndevelopment of petahertz information processing platforms.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the study of algebraic curves and their moduli spaces, it is important to\ndetermine the $a$-numbers of curves over a field of positive characteristic. It\nis known that non-hyperelliptic curves of genus $3$ are classified by the\nstructures of their automorphism groups as finite groups. In this paper, we\ndetermine the $a$-numbers of non-hyperelliptic curves of genus $3$ with cyclic\nautomorphism group of order $6$ or $9$. Moreover, we also find the exact number\nof the isomorphism classes of such curves attaining the possible maximal\n$a$-number.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The recent discoveries of many metal superhydrides provide a new route to\nroom-temperature superconductors. However, their stability and structure trends\nand the large chemical driving force needed to dissociate H2 molecules and form\nH covalent network cannot be explained by direct metal-hydrogen bonds and\nvolume effect. Here, we demonstrate that the understanding of superhydrides\nformation needs a perspective beyond traditional chemical bond theory. Using\nhigh-throughput calculations, we show that, after removing H atoms, the\nremaining metal lattices exhibit large electron localization at the\ninterstitial regions, which matches excellently to the H lattice like a\ntemplate. Furthermore, H lattices consist of 3D aromatic building units that\nare greatly stabilized by chemical templates of metals close to s-d border. The\nchemical template theory can naturally explain the stability and structure\ntrends of superhydrides and help to predict new materials such as two-metal\nsuperhydrides.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove that in a large class of Banach spaces of analytic functions in the\nunit disc $\\mathbb{D}$ an (unbounded) operator $Af=G\\cdot f'+g\\cdot f$ with\n$G,\\, g$ analytic in $\\mathbb{D}$ generates a $C_0$-semigroup of weighted\ncomposition operators if and only if it generates a $C_0$-semigroup. Particular\ninstances of such spaces are the classical Hardy spaces. Our result generalizes\nprevious results in this context and it is related to cocycles of flows of\nanalytic functions on Banach spaces. Likewise, for a large class of\nnon-separable Banach spaces $X $ of analytic functions in $\\mathbb{D}$\ncontained in the Bloch space, we prove that no non-trivial holomorphic flow\ninduces a $C_0$-semigroup of weighted composition operators on $X$. This\ngeneralizes previous results regarding $C_0$-semigroup of (unweighted)\ncomposition operators.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this article we introduce the new modulus\n$\\triangle'_{X,\\phi}(\\varepsilon)$, for which we prove that in the general case\nis different from the classical modulus of noncompact convexity. The main\nresult of the paper is showing the continuity of the modulus of noncompact\nconvexity for arbitrary minimalizable (strictly minimalizable) measure of\nnoncompactness on arbitrary metric space.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the effect of a thermal bath on quantum correlations induced by\nthe gravitational interaction between two massive cat states. Entanglement\n(measured by the concurrence) and quantum coherence (measured by the\n$l_1$-norm) are analyzed from the thermal quantum density operator. We\ninvestigate and discuss the behavior of these quantities under temperature\nvariations in different regimes, including some that are expected to be\nexperimentally feasible in the future.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this work, we examine sampling problems with non-smooth potentials. We\npropose a novel Markov chain Monte Carlo algorithm for sampling from non-smooth\npotentials. We provide a non-asymptotical analysis of our algorithm and\nestablish a polynomial-time complexity $\\tilde {\\cal O}(d\\varepsilon^{-1})$ to\nobtain $\\varepsilon$ total variation distance to the target density, better\nthan most existing results under the same assumptions. Our method is based on\nthe proximal bundle method and an alternating sampling framework. This\nframework requires the so-called restricted Gaussian oracle, which can be\nviewed as a sampling counterpart of the proximal mapping in convex\noptimization. One key contribution of this work is a fast algorithm that\nrealizes the restricted Gaussian oracle for any convex non-smooth potential\nwith bounded Lipschitz constant.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The nearby Seyfert type galaxy NGC 1275 contains a bright radio nucleus at\nits center, revealed through high-spatial resolution imaging to be the source\nof the jets emanating from the galaxy. Coincident with the emergence of a new\ncomponent C3 in the nucleus since 2005, flux densities from NGC 1275, at least\nat radio, millimeter (mm), and gamma-ray frequencies, had been increasing up\nthrough 2017 and leveled off afterwards. We analyze the long-term light curves\nof the nucleus that span the rising trend to 2015 July, and find a pair of\napproximately year-long quasi-periodic oscillations, with periods of $P_l\\simeq\n345$\\,d and $P_h\\simeq 386$\\,d respectively, in emission at 1.3-mm wavelength.\nWe discuss the case that there would be a long precession period $P_{\\rm\nprec}\\simeq 9$\\,yr, causing the appearance of $P_h$ that is slightly higher\nthan $P_l$. The accretion disk around the central supermassive black hole\n(SMBH) would be precessing at $P_{\\rm prec}$, induced by either the\nLense-Thirring effect or the existence of a companion SMBH. In the two\nscenarios, $P_l$ would be the jet wobbling timescale or the SMBH binary period\nrespectively. The finding, which could be verified through high-spatial\nresolution mm imaging, would not only identify the nature of the jet variation\nbut also help reveal the full features of the galaxy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A deep learning based method with the convolutional neural network (CNN)\nalgorithm for determining the impact parameters is developed using the\nconstrained molecular dynamics model simulations, focusing on the heavy-ion\ncollisions at the low-intermediate incident energies from several ten to one\nhundred MeV/nucleon in which the emissions of heavy fragments with the charge\nnumbers larger than 3 become crucial. To make the CNN applicable in the task of\nthe impact parameter determination at the present energy range, specific\nimprovements are made in the input selection, the CNN construction and the CNN\ntraining. It is demonstrated from the comparisons of the deep CNN method and\nthe conventional methods with the impact parameter-sensitive observables, that\nthe deep CNN method shows better performance for determining the impact\nparameters, especially leading to the capability of providing better\nrecognition of the central collision events. With a proper consideration of the\nexperimental filter effect in both training and testing processes to keep\nconsistency with the actual experiments, the good performance of the deep CNN\nmethod holds, and shows significantly better in terms of predicting the impact\nparameters and recognizing the central collision events, compared to that of\nthe conventional methods, demonstrating the superiority of the present deep CNN\nmethod. The deep CNN method with the consideration of the filter effect is\napplied in the deduction of nuclear stopping power. Higher accuracy for the\nstopping power deduction is achieved benefitting from the better impact\nparameter determination using the deep CNN method, compared to using the the\nconventional methods. This result reveals the importance to select a reliable\nimpact parameter determination method in the experimental deduction of the\nnuclear stopping power as well as other observables.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We studied the injection-locking properties of a resonant-tunneling-diode\nterahertz oscillator in the small-signal injection regime with a\nfrequency-stabilized continuous THz wave. The linewidth of the emission\nspectrum dramatically decreased to less than 120 mHz (HWHM) from 4.4 MHz in the\nfree running state as a result of the injection locking. We experimentally\ndetermined the amplitude of injection voltage at the antenna caused by the\ninjected THz wave. The locking range was proportional to the injection\namplitude and consistent with Adler's model. As increasing the injection\namplitude, we observed decrease of the noise component in the power spectrum,\nwhich manifests the free-running state, and alternative increase of the\ninjection-locked component. The noise component and the injection-locked\ncomponent had the same power at the threshold injection amplitude as small as\n$5\\times10^{-4}$ of the oscillation amplitude. This threshold behavior can be\nqualitatively explained by Maffezzoni's model of noise reduction in general\nlimit-cycle oscillators.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Sombor index of a graph $G$ was recently introduced by Gutman from the\ngeometric point of view, defined as $SO(G)=\\sum_{uv\\in\nE(G)}\\sqrt{d(u)^2+d(v)^2}$, where $d(u)$ is the degree of a vertex $u$. For two\nreal numbers $\\alpha$ and $\\beta$, the $\\alpha$-Sombor index and general Sombor\nindex of $G$ are two generalized forms of the Sombor index defined as\n$SO_\\alpha(G)=\\sum_{uv\\in E(G)}(d(u)^{\\alpha}+d(v)^{\\alpha})^{1/\\alpha}$ and\n$SO_\\alpha(G;\\beta)=\\sum_{uv\\in E(G)}(d(u)^{\\alpha}+d(v)^{\\alpha})^{\\beta}$,\nrespectively. A $k$-polygonal cactus is a connected graph in which every block\nis a cycle of length $k$. In this paper, we establish a lower bound on\n$\\alpha$-Sombor index for $k$-polygonal cacti and show that the bound is\nattained only by chemical $k$-polygonal cacti. The extremal $k$-polygonal cacti\nfor $SO_\\alpha(G;\\beta)$ with some particular $\\alpha$ and $\\beta$ are also\nconsidered.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Machine learning algorithms have achieved remarkable results and are widely\napplied in a variety of domains. These algorithms often rely on sensitive and\nprivate data such as medical and financial records. Therefore, it is vital to\ndraw further attention regarding privacy threats and corresponding defensive\ntechniques applied to machine learning models. In this paper, we present\nTenSEAL, an open-source library for Privacy-Preserving Machine Learning using\nHomomorphic Encryption that can be easily integrated within popular machine\nlearning frameworks. We benchmark our implementation using MNIST and show that\nan encrypted convolutional neural network can be evaluated in less than a\nsecond, using less than half a megabyte of communication.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a generic multistage stochastic model for the Alternating Current\nOptimal Power Flow (AC OPF) problem for radial distribution networks, to\naccount for the random electricity production of renewable energy sources and\ndynamic constraints of storage systems. We consider single-phase radial\nnetworks. Radial three-phase balanced networks (medium-voltage distribution\nnetworks typically have this structure) reduce to the former case. This induces\na large scale optimization problem, which, given the non-convex nature of the\nAC OPF, is generally challenging to solve to global optimality. We derive a\npriori conditions guaranteeing a vanishing relaxation gap for the multi-stage\nAC OPF problem, which can thus be solved using convex optimization algorithms.\nWe also give an a posteriori upper bound on the relaxation gap. In particular,\nwe show that a null or low relaxation gap may be expected for applications with\nlight reverse power flows or if sufficient storage capacities with low cost are\navailable. Then, we discuss the validity of our results when incorporating\nvoltage regulation devices. Finally, we illustrate our results on problems of\nplanning of a realistic distribution feeder with distributed solar production\nand storage systems. Scenario trees for solar production are constructed from a\nstochastic model, by a quantile-based algorithm.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Consider a finite undirected unweighted graph G and add a new node to it\narbitrarily connecting it to pre-existing nodes. We study the behavior of the\nPerron eigenvalue of the non-backtracking matrix of G before and after such a\nnode addition. We prove an interlacing-type result for said eigenvalue, namely,\nthe Perron eigenvalue never decreases after node addition. Furthermore, our\nmethods lead to bounds for the difference between the eigenvalue before and\nafter node addition. These are the first known bounds that have been\nestablished in full rigor. Our results depend on the assumption of\ndiagonalizability of the non-backtracking matrix. Practical experience says\nthat this assumption is fairly mild in many families of graphs, though\nnecessary and sufficient conditions for it remain an open question.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We use the Milky Way's nuclear star cluster (NSC) to test the existence of a\ndark matter 'soliton core', as predicted in ultra-light dark matter (ULDM)\nmodels. Since the soliton core size is proportional to mDM^{-1}, while the core\ndensity grows as mDM^{2}, the NSC (dominant stellar component within about 3\npc) is sensitive to a specific window in the dark matter particle mass, mDM. We\napply a spherical isotropic Jeans model to fit the NSC line-of-sight velocity\ndispersion data, assuming priors on the precisely measured Milky Way's\nsupermassive black hole (SMBH) mass and the well-measured NSC density profile.\nWe find that the current observational data reject the existence of a soliton\ncore for a single ULDM particle with mass in the range 10^{-20.4} < mDM <\n10^{-18.5} eV, assuming that the soliton core structure is not affected by the\nMilky Way's SMBH. We test our methodology on mock data, confirming that we are\nsensitive to the same range in ULDM mass as for the real data. Dynamical\nmodelling of a larger region of the Galactic centre, including the nuclear\nstellar disc, promises tighter constraints over a broader range of mDM. We will\nconsider this in future work.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a new technique to obtain outer-bounds on the capacity region of\nnetworks with ultra low-rate feedback. We establish a connection between the\nachievable rates in the forward channel and the minimum distortion that can be\nattained over the feedback channel.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  New detector approaches in Positron Emission Tomography imaging will play an\nimportant role in reducing costs, lowering administered radiation doses, and\nimproving overall performance. PETALO employs liquid xenon as the active\nscintillating medium and UV-sensitive silicon photomultipliers for\nscintillation readout. The scintillation time in liquid xenon is fast enough to\nregister time-of-flight information for each detected coincidence, and\nsufficient scintillation is produced with low enough fluctuations to obtain\ngood energy resolution. The present simulation study examines a full-body-sized\nPETALO detector and evaluates its potential performance in PET image\nreconstruction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Controllable music generation with deep generative models has become\nincreasingly reliant on disentanglement learning techniques. However, current\ndisentanglement metrics, such as mutual information gap (MIG), are often\ninadequate and misleading when used for evaluating latent representations in\nthe presence of interdependent semantic attributes often encountered in\nreal-world music datasets. In this work, we propose a dependency-aware\ninformation metric as a drop-in replacement for MIG that accounts for the\ninherent relationship between semantic attributes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Gender disparity in science is one of the most focused debating points among\nauthorities and the scientific community. Over the last few decades, numerous\ninitiatives have endeavored to accelerate gender equity in academia and\nresearch society. However, despite the ongoing efforts, gaps persist across the\nworld, and more measures need to be taken. Using social network analysis,\nnatural language processing, and machine learning, in this study, we\ncomprehensively analyzed gender-specific patterns in the highly\ninterdisciplinary and evolving field of artificial intelligence for the period\nof 2000-2019. Our findings suggest an overall increasing rate of mixed-gender\ncollaborations. From the observed gender-specific collaborative patterns, the\nexistence of disciplinary homophily at both dyadic and team levels is\nconfirmed. However, a higher preference was observed for female researchers to\nform homophilous collaborative links. Our core-periphery analysis indicated a\nsignificant positive association between having diverse collaboration and\nscientific performance and experience. We found evidence in support of\nexpecting the rise of new female superstar researchers in the artificial\nintelligence field.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Adding to the literature on the data-driven detection of bid-rigging cartels,\nwe propose a novel approach based on deep learning (a subfield of artificial\nintelligence) that flags cartel participants based on their pairwise bidding\ninteractions with other firms. More concisely, we combine a so-called\nconvolutional neural network for image recognition with graphs that in a\npairwise manner plot the normalized bid values of some reference firm against\nthe normalized bids of any other firms participating in the same tenders as the\nreference firm. Based on Japanese and Swiss procurement data, we construct such\ngraphs for both collusive and competitive episodes (i.e when a bid-rigging\ncartel is or is not active) and use a subset of graphs to train the neural\nnetwork such that it learns distinguishing collusive from competitive bidding\npatterns. We use the remaining graphs to test the neural network's\nout-of-sample performance in correctly classifying collusive and competitive\nbidding interactions. We obtain a very decent average accuracy of around 90% or\nslightly higher when either applying the method within Japanese, Swiss, or\nmixed data (in which Swiss and Japanese graphs are pooled). When using data\nfrom one country for training to test the trained model's performance in the\nother country (i.e. transnationally), predictive performance decreases (likely\ndue to institutional differences in procurement procedures across countries),\nbut often remains satisfactorily high. All in all, the generally quite high\naccuracy of the convolutional neural network despite being trained in a rather\nsmall sample of a few 100 graphs points to a large potential of deep learning\napproaches for flagging and fighting bid-rigging cartels.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The amount of multimedia content shared everyday, combined with the level of\nrealism reached by recent fake-generating technologies, threatens to impair the\ntrustworthiness of online information sources. The process of uploading and\nsharing data tends to hinder standard media forensic analyses, since multiple\nre-sharing steps progressively hide the traces of past manipulations. At the\nsame time though, new traces are introduced by the platforms themselves,\nenabling the reconstruction of the sharing history of digital objects, with\npossible applications in information flow monitoring and source identification.\nIn this work, we propose a supervised framework for the reconstruction of image\nsharing chains on social media platforms. The system is structured as a cascade\nof backtracking blocks, each of them tracing back one step of the sharing chain\nat a time. Blocks are designed as ensembles of classifiers trained to analyse\nthe input image independently from one another by leveraging different feature\nrepresentations that describe both content and container of the media object.\nIndividual decisions are then properly combined by a late fusion strategy.\nResults highlight the advantages of employing multiple clues, which allow\naccurately tracing back up to three steps along the sharing chain.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The theory of deep learning focuses almost exclusively on supervised\nlearning, non-convex optimization using stochastic gradient descent, and\noverparametrized neural networks. It is common belief that the optimizer\ndynamics, network architecture, initialization procedure, and other factors tie\ntogether and are all components of its success. This presents theoretical\nchallenges for analyzing state-based and/or online deep learning.\n  Motivated by applications in control, we give a general black-box reduction\nfrom deep learning to online convex optimization. This allows us to decouple\noptimization, regret, expressiveness, and derive agnostic online learning\nguarantees for fully-connected deep neural networks with ReLU activations. We\nquantify convergence and regret guarantees for any range of parameters and\nallow any optimization procedure, such as adaptive gradient methods and second\norder methods. As an application, we derive provable algorithms for deep\ncontrol in the online episodic setting.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Context. Cosmic rays (CRs) play an important role in the chemistry and\ndynamics of the interstellar medium. In dense environments, they represent the\nmain ionising agent, driving the rich chemistry of molecular ions and\ndetermining the ionisation fraction, which regulates the degree of coupling\nbetween the gas and magnetic fields. Estimates of the CR ionisation rate\n($\\zeta_2$) span several orders of magnitude, depending on the targeted sources\nand on the used method.\n  Aims. Recent theoretical models have characterised the CR attenuation with\nincreasing density. We aim to test these models for the attenuation of CRs in\nthe low-mass pre-stellar core L1544.\n  Methods. We use a state-of-the-art gas-grain chemical model, which accepts\nthe CR ionisation rate profile as input, to predict the abundance profiles of\nfour ions: $\\rm N_2H^+$, $\\rm N_2D^+$, $\\rm HC^{18}O^+$, and $\\rm DCO^+$.\nNon-LTE radiative transfer is performed to produce synthetic spectra based on\nthe derived abundances. These are compared with observations obtained with the\nInstitut de Radioastronomie Millim\\'etrique (IRAM) 30m telescope.\n  Results. Our results indicate that a model with $\\zeta_2 > 10^{-16} \\rm \\,\ns^{-1}$ is excluded by the observations. Also the model with the standard\n$\\zeta_2 = 1.3 \\times 10^{-17} \\rm \\, s^{-1}$ produces a worse agreement with\nrespect to the attenuation model based on Voyager observations, which has an\naverage $\\zeta_2 = 3 \\times 10^{-17} \\rm \\, s^{-1}$ at the column densities\ntypical of L1544. The single-dish data, however, are not sensitive to the\nattenuation of the CR profile, which changes only by a factor of two in the\nrange of column densities spanned by the core model. Interferometric\nobservations at higher spatial resolution, combined with observations of\ntransitions with lower critical density are needed to observe a decrease of\n$\\zeta_2$ with density.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Dark matter particles with Planck-scale mass ($\\simeq10^{19}\\text{GeV}/c^2$)\narise in well-motivated theories and could be produced by several cosmological\nmechanisms. Using a blind analysis of data collected over a 813 d live time\nwith DEAP-3600, a 3.3 t single-phase liquid argon-based dark matter experiment\nat SNOLAB, a search for supermassive dark matter was performed, looking for\nmultiple-scatter signals. No candidate signal events were observed, leading to\nthe first direct detection constraints on Planck-scale mass dark matter.\nLeading limits constrain dark matter masses between $8.3\\times10^{6}$ and\n$1.2\\times10^{19} \\text{GeV}/c^2$, and cross sections for scattering on\n$^{40}$Ar between $1.0\\times10^{-23}$ and $2.4\\times10^{-18} \\text{cm}^2$.\nThese are used to constrain two composite dark matter models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Growth-fragmentation processes describe the evolution of systems in which\ncells grow slowly and fragment suddenly. Despite originating as a way to\ndescribe biological phenomena, they have recently been found to describe the\nlengths of certain curves in statistical physics models. In this note, we\ndescribe a new growth-fragmentation process connected to random planar maps\nwith faces of large degree, having as a key ingredient the ricocheted stable\nprocess recently discovered by Budd. The process has applications to the\nexcursions of planar Brownian motion and Liouville quantum gravity.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the problem of assigning or allocating resources to a set of\njobs. We consider the case when the resources are fungible, that is, the job\ncan be done with any mix of the resources, but with different efficiencies. In\nour formulation we maximize a total utility subject to a given limit on the\nresource usage, which is a convex optimization problem and so is tractable. In\nthis paper we develop a custom, parallelizable algorithm for solving the\nresource allocation problem that scales to large problems, with millions of\njobs. Our algorithm is based on the dual problem, in which the dual variables\nassociated with the resource usage limit can be interpreted as resource prices.\nOur method updates the resource prices in each iteration, ultimately\ndiscovering the optimal resource prices, from which an optimal allocation is\nobtained. We provide an open-source implementation of our method, which can\nsolve problems with millions of jobs in a few seconds on CPU, and under a\nsecond on a GPU; our software can solve smaller problems in milliseconds. On\nlarge problems, our implementation is up to three orders of magnitude faster\nthan a commerical solver for convex optimization.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Offline reinforcement learning restricts the learning process to rely only on\nlogged-data without access to an environment. While this enables real-world\napplications, it also poses unique challenges. One important challenge is\ndealing with errors caused by the overestimation of values for state-action\npairs not well-covered by the training data. Due to bootstrapping, these errors\nget amplified during training and can lead to divergence, thereby crippling\nlearning. To overcome this challenge, we introduce Regularized Behavior Value\nEstimation (R-BVE). Unlike most approaches, which use policy improvement during\ntraining, R-BVE estimates the value of the behavior policy during training and\nonly performs policy improvement at deployment time. Further, R-BVE uses a\nranking regularisation term that favours actions in the dataset that lead to\nsuccessful outcomes. We provide ample empirical evidence of R-BVE's\neffectiveness, including state-of-the-art performance on the RL Unplugged ATARI\ndataset. We also test R-BVE on new datasets, from bsuite and a challenging\nDeepMind Lab task, and show that R-BVE outperforms other state-of-the-art\ndiscrete control offline RL methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Multi-source domain adaptation aims at leveraging the knowledge from multiple\ntasks for predicting a related target domain. Hence, a crucial aspect is to\nproperly combine different sources based on their relations. In this paper, we\nanalyzed the problem for aggregating source domains with different label\ndistributions, where most recent source selection approaches fail. Our proposed\nalgorithm differs from previous approaches in two key ways: the model\naggregates multiple sources mainly through the similarity of semantic\nconditional distribution rather than marginal distribution; the model proposes\na \\emph{unified} framework to select relevant sources for three popular\nscenarios, i.e., domain adaptation with limited label on target domain,\nunsupervised domain adaptation and label partial unsupervised domain adaption.\nWe evaluate the proposed method through extensive experiments. The empirical\nresults significantly outperform the baselines.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $(X\\ni x,B)$ be an lc surface germ. If $X\\ni x$ is klt, we show that\nthere exists a divisor computing the minimal log discrepancy of $(X\\ni x,B)$\nthat is a Koll\\'ar component of $X\\ni x$. If $B\\not=0$ or $X\\ni x$ is not Du\nVal, we show that any divisor computing the minimal log discrepancy of $(X\\ni\nx,B)$ is a potential lc place of $X\\ni x$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Quantum error correction requires the detection of errors by reliable\nmeasurements of suitable multi-qubit correlation operators. Here, we\nexperimentally demonstrate a fault-tolerant weight-4 parity check measurement\nscheme. An additional 'flag' qubit serves to detect errors occurring throughout\nthe parity measurement, which would otherwise proliferate into uncorrectable\nweight-2 errors on the qubit register. We achieve a flag-conditioned parity\nmeasurement single-shot fidelity of 93.2(2)\\%. Deliberately injecting bit and\nphase-flip errors, we show that the fault-tolerant protocol is capable of\nreliably intercepting such faults. For holistic benchmarking of the parity\nmeasurement scheme, we use entanglement witnessing to show that the implemented\ncircuit generates genuine six-qubit multi-partite entanglement. The\nfault-tolerant parity measurement scheme is an essential building block in a\nbroad class of stabilizer quantum error correction protocols, including\ntopological color codes. Our hardware platform is based on atomic ions stored\nin a segmented microchip ion trap. The qubit register is dynamically\nreconfigured via shuttling operations, enabling effective full connectivity\nwithout operational cross-talk, which provides key capabilities for scalable\nfault-tolerant quantum computing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep learning-based fluence map prediction(DL-FMP) method has been reported\nin the literature, which generated fluence maps for desired dose by deep neural\nnetwork(DNN)-based inverse mapping. We hypothesized that DL-FMP is similar to\ngeneral fluence map optimization(FMO) because it's theoretically based on a\ngeneral inverse mapping. We designed four experiments to validate the\ngeneralizability of DL-FMP to other types of plans apart from the training\ndata, which contained only clinical head and neck(HN) full-arc VMAT plans. The\nfirst three experiments quantified the generalizability of DL-FMP to multiple\nanatomical sites, different delivery modalities, and various degree of\nmodulation(DOM), respectively. The fourth experiment explored the\ngeneralizability and stability to infeasible dose inputs. Results of the first\nexperiment manifested that DL-FMP can generalize to lung, liver, esophagus and\nprostate, with gamma passing rates (GPR) higher than 95%(2%/2mm). The second\nexperiment showed that DL-FMP can generalize to partial-arc plans and predict\npartial-arc fluences. GPR(3mm/3%) ranged from 96% to 99%. DL-FMP cannot\ngenerate fluence maps in discrete beam angles for IMRT input. But the predicted\ndose still agreed with ground truth dose with 93% GPR(5%/5mm). The third\nexperiment demonstrated that DL-FMP can generalize to various DOMs, with\nGPRs(3%/3mm) ranged in 94%-98%. Moreover, the DOM of predicted fluence maps\ncorrelated to the optimality of the input dose accordingly. The fourth\nexperiment exemplified that DL-FMP can make stable predictions for infeasible\ndose input. In conclusion, we validated that DL-FMP can generalize to plans for\nmultiple anatomical sites, plans of different delivery modalities and plans\nwith various DOM. It can also make stable prediction for infeasible input.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper presents Strichartz estimates for the linearized 1D periodic\nDysthe equation on the torus, namely estimate of the $L^6_{x,t}(\\mathbb{T}^2)$\nnorm of the solution in terms of the initial data, and estimate of the\n$L^4_{x,t}(\\mathbb{T}^2)$ norm in terms of the Bourgain space norm. The paper\nalso presents other results such as bilinear and trilinear estimates pertaining\nto local well-posedness of the 1-dimensional periodic Dysthe equation in a\nsuitable Bourgain space, and ill-posedness results in Sobolev spaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Image caption generation is one of the most challenging problems at the\nintersection of vision and language domains. In this work, we propose a\nrealistic captioning task where the input scenes may incorporate visual objects\nwith no corresponding visual or textual training examples. For this problem, we\npropose a detection-driven approach that consists of a single-stage generalized\nzero-shot detection model to recognize and localize instances of both seen and\nunseen classes, and a template-based captioning model that transforms\ndetections into sentences. To improve the generalized zero-shot detection\nmodel, which provides essential information for captioning, we define effective\nclass representations in terms of class-to-class semantic similarities, and\nleverage their special structure to construct an effective unseen/seen class\nconfidence score calibration mechanism. We also propose a novel evaluation\nmetric that provides additional insights for the captioning outputs by\nseparately measuring the visual and non-visual contents of generated sentences.\nOur experiments highlight the importance of studying captioning in the proposed\nzero-shot setting, and verify the effectiveness of the proposed\ndetection-driven zero-shot captioning approach.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the simple random walk the steps are independent, viz., the walker has no\nmemory. In contrast, in the Elephant random walk(ERW), which was introduced by\nSchuetz and Trimper in 2004, the next step always depends on the whole path so\nfar. Various authors have studied further properties of the ERW. In an earlier\npaper we studied the case when the Elephant remembers only a finite part of the\nfirst or last steps. In both cases there was no separation into two different\nregimes as in the classical ERW. We also posed the question about what happens\nif she remembers a gradually increasing past. This paper will give some answers\nto that question. We also discuss related questions for ERW:s with delays.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Many problems in science and engineering require an efficient numerical\napproximation of integrals or solutions to differential equations. For systems\nwith rapidly changing dynamics, an equidistant discretization is often\ninadvisable as it either results in prohibitively large errors or computational\neffort. To this end, adaptive schemes, such as solvers based on Runge--Kutta\npairs, have been developed which adapt the step size based on local error\nestimations at each step. While the classical schemes apply very generally and\nare highly efficient on regular systems, they can behave sub-optimal when an\ninefficient step rejection mechanism is triggered by structurally complex\nsystems such as chaotic systems. To overcome these issues, we propose a method\nto tailor numerical schemes to the problem class at hand. This is achieved by\ncombining simple, classical quadrature rules or ODE solvers with data-driven\ntime-stepping controllers. Compared with learning solution operators to ODEs\ndirectly, it generalises better to unseen initial data as our approach employs\nclassical numerical schemes as base methods. At the same time it can make use\nof identified structures of a problem class and, therefore, outperforms\nstate-of-the-art adaptive schemes. Several examples demonstrate superior\nefficiency. Source code is available at\nhttps://github.com/lueckem/quadrature-ML.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The fundamental aim of the healthcare sector is to incorporate different\ntechnologies to observe and keep a track of the various clinical parameters of\nthe patients in day to day life. Distant patient observation applications are\nbecoming popular as economical healthcare services are facilitated by these\napps. The process of data management gathered through these applications also\nrequire due attention. Although cloud facilitated healthcare applications cater\na variety of solutions to store patients record and deliver the required data\nas per need of all the stakeholders but are affected by security issues, more\nresponse time and affecting the continues availability of the system. To\novercome these challenges, an intelligent IoT based distributed framework to\ndeploy remote healthcare services is proposed in this chapter. In the proposed\nmodel, various entities of the system are interconnected using IoT and\nDistributed Database Management Systems is used to cater secure and fast data\navailability to the patients and health care workers. The concept of Blockchain\nis used to ensure the security of the patient medical records. The proposed\nmodel will comprise of intelligent analysis of the clinical records fetched\nfrom Distributed Database Management Systems secured with Blockchain. Proposed\nmodel is tested with true clinical data and results are discussed in detail.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The disc instability model accounts well for most of the observed properties\nof dwarf novae and soft X-ray transients, but the rebrightenings, reflares, and\nechoes occurring at the end of outbursts or shortly after in WZ Sge stars or\nsoft X-ray transients have not yet been convincingly explained by any model. We\ndetermine the additional ingredients that must be added to the DIM to account\nfor the observed rebrightenings. We analyse in detail a recently discovered\nsystem, TCP J21040470+4631129, which has shown very peculiar rebrightenings,\nmodel its light curve using our numerical code including mass transfer\nvariations from the secondary, inner-disc truncation, disc irradiation by a hot\nwhite dwarf and, in some cases, the mass-transfer stream over(under)flow. We\nshow that the luminosity in quiescence is dominated by a hot white dwarf that\ncools down on time scales of months. The mass transfer rate from the secondary\nhas to increase by several orders of magnitudes during the initial\nsuperoutburst for a reason that remains elusive, slowly returning to its\nsecular average, causing the observed succession of outbursts with increasing\nquiescence durations, until the disc can be steady, cold, and neutral; its\ninner parts being truncated either by the white dwarf magnetic field or by\nevaporation. The very short, quiescence phases between reflares are reproduced\nwhen the mass-transfer stream overflows the disc. Using similar additions to\nthe DIM, we have also produced light curves close to those observed in two WZ\nSge stars, the prototype and EG Cnc. Our model successfully explains the\nreflares observed in WZ Sge systems. It requires, however, the inner disc\ntruncation in dwarf novae to be due not (only) to the white dwarf magnetic\nfield but, as in X-ray binaries, rather to evaporation of the inner disc. A\nsimilar model could also explain reflares observed in soft X-ray transients.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Based on observations with the IRAM 30-m and Yebes 40-m telescopes, we report\nevidence of the detection of Milky Way-like, low-excitation molecular gas, up\nto the transition CO($J=5-4$), in a distant, dusty star-forming galaxy at\n$z_{CO}=1.60454$. WISE J122651.0+214958.8 (alias SDSSJ1226, the Cosmic\nSeahorse), is strongly lensed by a foreground galaxy cluster at $z=0.44$ with a\nsource magnification of $\\mu=9.5\\pm0.7$. This galaxy was selected by\ncross-correlating near-to-mid infrared colours within the full-sky AllWISE\nsurvey, originally aiming to discover rare analogs of the archetypical strongly\nlensed submillimeter galaxy SMM J2135-0102, the Cosmic Eyelash. We derive an\napparent (i.e. not corrected for lensing magnification) rest-frame 8-1000\n$\\mu$m infrared luminosity of $\\mu L_\\mathrm{IR}=1.66^{+0.04}_{-0.04}\\times\n10^{13}$ L$_\\odot$ and apparent star-formation rate\n$\\mu\\mathrm{SFR}_\\mathrm{IR}=2960\\pm70$ M$_\\odot$ yr$^{-1}$. SDSSJ1226 is\nultra-bright at $S_{350\\mu m}\\simeq170$ mJy and shows similarly bright low-$J$\nCO line intensities as SMM J2135-0102, however, with exceptionally small\nCO($J=5-4$) intensity. We consider different scenarios to reconcile our\nobservations with typical findings of high-$z$ starbursts, and speculate about\nthe presence of a previously unseen star-formation mechanism in cosmic noon\nsubmillimeter galaxies. In conclusion, the remarkable low line luminosity ratio\n$r_{5,2}=0.11\\pm0.02$ is best explained by an extended, main-sequence\nstar-formation mode -- representing a missing link between starbursts to\nlow-luminosity systems during the epoch of peak star-formation history.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Opponent modeling is essential to exploit sub-optimal opponents in strategic\ninteractions. Most previous works focus on building explicit models to directly\npredict the opponents' styles or strategies, which require a large amount of\ndata to train the model and lack adaptability to unknown opponents. In this\nwork, we propose a novel Learning to Exploit (L2E) framework for implicit\nopponent modeling. L2E acquires the ability to exploit opponents by a few\ninteractions with different opponents during training, thus can adapt to new\nopponents with unknown styles during testing quickly. We propose a novel\nopponent strategy generation algorithm that produces effective opponents for\ntraining automatically. We evaluate L2E on two poker games and one grid soccer\ngame, which are the commonly used benchmarks for opponent modeling.\nComprehensive experimental results indicate that L2E quickly adapts to diverse\nstyles of unknown opponents.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The analysis of single particle trajectories plays an important role in\nelucidating dynamics within complex environments such as those found in living\ncells. However, the characterization of intracellular particle motion is often\nconfounded by confinement of the particles within non-trivial subcellular\ngeometries. Here, we focus specifically on the case of particles undergoing\nBrownian motion within a tubular network, as found in some cellular organelles.\nAn unraveling algorithm is developed to uncouple particle motion from the\nconfining network structure, allowing for an accurate extraction of the\ndiffusion coefficient, as well as differentiating between Brownian and\nfractional Brownian dynamics. We validate the algorithm with simulated\ntrajectories and then highlight its application to an example system: analyzing\nthe motion of membrane proteins confined in the tubules of the peripheral\nendoplasmic reticulum in mammalian cells. We show that these proteins undergo\ndiffusive motion with a well-characterized diffusivity. Our algorithm provides\na generally applicable approach for disentangling geometric morphology and\nparticle dynamics in networked architectures.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Gravitational waves emitted by merging black holes between $\\sim\n10^4-10^7~M_\\odot$ will be detectable by the Laser Interferometer Space Antenna\n(LISA) with signal-to-noise ratios of several hundred out to redshift 20.\nSupermassive black hole ($10^7$~M$_{\\odot}$ - $10^{10}$~M$_{\\odot}$) binary\nformation, coalescence and merger within massive galaxies is well-studied.\nHowever, low-to-intermediate mass black holes (IMBHs) are hosted by low-mass\nand dwarf galaxies; it is not trivial to extrapolate black hole merger\ntimescales to this IMBH binary regime, due to the starkly different host galaxy\nstructure, kinematics, and morphology compared to massive galaxy hosts. We\nperform ultra-high resolution $N$-body simulations to study IMBH dynamics in\nnucleated dwarf galaxies whose structural parameters are obtained from\nobservations of nearby dwarf galaxies. Starting from 50 parsecs, an IMBH\nquickly forms a binary. Thereafter, the binary orbit shrinks rapidly due to the\nhigh central stellar densities furnished by nuclear star clusters (NSCs). We\nfind high eccentricities ($e \\sim 0.4-0.99$) in our suite of IMBH binaries, and\nresidual eccentricity may persist to the LISA regime. IMBH merger times are\ntypically a few hundred million years, with a few exceptionally short merger\ntimes for high eccentricities. We find that IMBH-stellar encounters originate\npredominantly from NSCs, if the NSC-to-IMBH binary mass ratio is greater than\n10; otherwise, bulge stars contribute significantly. As the IMBH binary ejects\nstars, however, the NSCs is disrupted. We conclude that comparable-mass IMBHs\nmerge very efficiently in nucleated dwarf galaxies, making them promising LISA\nsources, as well as a channel for IMBH growth.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Android system manages access to sensitive APIs by permission\nenforcement. An application (app) must declare proper permissions before\ninvoking specific Android APIs. However, there is no official documentation\nproviding the complete list of permission-protected APIs and the corresponding\npermissions to date. Researchers have spent significant efforts extracting such\nAPI protection mapping from the Android API framework, which leverages static\ncode analysis to determine if specific permissions are required before\naccessing an API. Nevertheless, none of them has attempted to analyze the\nprotection mapping in the native library (i.e., code written in C and C++), an\nessential component of the Android framework that handles communication with\nthe lower-level hardware, such as cameras and sensors. While the protection\nmapping can be utilized to detect various security vulnerabilities in Android\napps, such as permission over-privilege and component hijacking, imprecise\nmapping will lead to false results in detecting such security vulnerabilities.\nTo fill this gap, we develop a prototype system, named NatiDroid, to facilitate\nthe cross-language static analysis to benchmark against two state-of-the-art\ntools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000\nAndroid apps, including system apps from custom Android ROMs and third-party\napps from the Google Play. Our NatiDroid can identify up to 464 new\nAPI-permission mappings, in contrast to the worst-case results derived from\nboth Axplorer and Arcade, where approximately 71% apps have at least one false\npositive in permission over-privilege and up to 3.6% apps have at least one\nfalse negative in component hijacking. Additionally, we identify that 24\ncomponents with at least one Native-triggered component hijacking vulnerability\nare misidentified by two benchmarks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Low-temperature heat capacities (Cp) of nanostructured rock salt (rs-ZnO) and\nwurtzite (w-ZnO) polymorphs of zinc oxide were measured in the 2-315 K\ntemperature range. No significant influence of nanostructuring on Cp of w-ZnO\nhas been observed. The measured Cp of rock salt ZnO is lower than that of\nwurtzite ZnO below 100 K and is higher above this temperature. Using available\nthermodynamic data, we established that the equilibrium pressure between\nnanocrystalline w-ZnO and rs-ZnO is close to 4.6 GPa at 300 K (half as much as\nthe onset pressure of direct phase transformation) and slightly changes with\ntemperature up to 1000 K.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The atomistic mechanisms occurring during the processes of aging and\nrejuvenation in glassy materials involve very small structural rearrangements\nthat are extremely difficult to capture experimentally. Here we use in-situ\nX-ray diffraction to investigate the structural rearrangements during annealing\nfrom 77 K up to the crystallization temperature in Cu44Zr44Al8Hf2Co2 bulk\nmetallic glass rejuvenated by high pressure torsion performed at cryogenic\ntemperatures and at room temperature. Using a measure of the configurational\nentropy calculated from the X-ray pair correlation function, the structural\nfootprint of the deformation-induced rejuvenation in bulk metallic glass is\nrevealed. With synchrotron radiation, temperature and time resolutions\ncomparable to calorimetric experiments are possible. This opens hitherto\nunavailable experimental possibilities allowing to unambiguously correlate\nchanges in atomic configuration and structure to calorimetrically observed\nsignals and can attribute those to changes of the dynamic and vibrational\nrelaxations ({\\alpha}-, {\\beta}- and {\\gamma}-transition) in glassy materials.\nThe results suggest that the structural footprint of the {\\beta}-transition is\nrelated to entropic relaxation with characteristics of a first-order\ntransition. Dynamic mechanical analysis data shows that in the range of the\n{\\beta}-transition, non-reversible structural rearrangements are preferentially\nactivated. The low-temperature {\\gamma}-transition is mostly triggering\nreversible deformations and shows a change of slope in the entropic footprint\nsuggesting second-order characteristics.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A broadcast on a connected graph $G=(V,E)$ is a function $f:V\\rightarrow\n\\{0,1,\\dots,\\operatorname{diam}(G)\\}$ such that $f(v)\\leq e(v)$ (the\neccentricity of $v$) for all $v\\in V$ if $|V|\\geq2$, and $f(v)=1$ if $V=\\{v\\}$.\nThe cost of $f$ is $\\sigma(f)=\\sum_{v\\in V}f(v)$. Let $V_{f}% ^{+}$ denote the\nset of vertices $v$ such that $f(v)$ is positive. A vertex $u$ hears $f$ from\n$v\\in V_{f}^{+}$ if the distance $d(u,v)\\leq f(v)$. When $f$ is a broadcast\nsuch that every vertex $x$ that hears $f$ from more than one vertex in\n$V_{f}^{+}$ also satisfies $d(x,u)\\geq f(u)$ for all $u\\in V_{f}^{+}$, we say\nthat the broadcast only overlaps in boundaries. A broadcast $f$ is boundary\nindependent if it overlaps only in boundaries. Denote by\n$i_{\\operatorname{bn}}(G)$ the minimum cost of a maximal boundary independent\nbroadcast.\n  We obtain a characterization of maximal boundary independent broadcasts, show\nthat $i_{\\operatorname{bn}}(T^{\\prime})\\leq i_{\\operatorname{bn}}(T)$ for any\nsubtree $T^{\\prime}$ of a tree $T$, and determine an upper bound for\n$i_{\\operatorname{bn}}(T)$ in terms of the broadcast domination number of $T$.\nWe show that this bound is sharp for an infinite class of trees.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A privacy-preserving Context-Aware Publish-Subscribe System (CA-PSS) enables\nan intermediary (broker) to match the content from a publisher and the\nsubscription by a subscriber based on the current context while preserving\nconfidentiality of the subscriptions and notifications. While a\nprivacy-preserving Ride-Hailing Service (RHS) enables an intermediary (service\nprovider) to match a ride request with a taxi driver in a privacy-friendly\nmanner. In this work, we attack a privacy-preserving CA-PSS proposed by Nabeel\net al. (2013), where we show that any entity in the system including the broker\ncan learn the confidential subscriptions of the subscribers. We also attack a\nprivacy-preserving RHS called lpRide proposed by Yu et al. (2019), where we\nshow that any rider/driver can efficiently recover the secret keys of all other\nriders and drivers. Also, we show that any rider/driver will be able to learn\nthe location of any rider. The attacks are based on our cryptanalysis of the\nmodified Paillier cryptosystem proposed by Nabeel et al. that forms a building\nblock for both the above protocols.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We generalize the recently proposed $\\mathcal{P T}$-symmetric axion haloscope\nto a larger array with more $\\mathcal{P T}$-symmetric structures. By broadening\nthe response bandwidth of the signal without increasing the readout noise, the\noptimized scan rate of the axion haloscope is significantly enhanced, as well\nas is the signal power. Furthermore, we show that the robustness of the\ndetector towards the variations of the array coupling is the strongest when a\nbinary tree structure is introduced which contains a largely enhanced\n$\\mathcal{P T}$ symmetry. The multiple allowed probing sensors can further\nincrease the scan rate by a factor of the sensors' number due to the\ncorrelation of the signals. This type of array can strongly boost the search\nfor an axion compared to single-mode resonant detection. The enhancement to the\nscan rate becomes the most manifest when applied to the proposed detection\nusing a superconducting radio-frequency cavity with an ac magnetic field where\nmost of the parameter space of the QCD axion above kHz can be probed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we generalize the notion of the Taylor spectrum to modules over\nan arbitrary Lie algebra and study it for finite-dimensional modules. We show\nthat the spectrum can be described as the set of simple submodules in case of\nnilpotent and semisimple Lie algebras. We also show that this result does not\nhold for solvable Lie algebras and obtain a precise description of the spectrum\nin case of Borel subalgebras of semisimple Lie algebras.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We are concerned with the existence of periodic travelling-wave solutions for\nthe generalized surface quasi-geostrophic (gSQG) equation(including\nincompressible Euler equation), known as von K\\'arm\\'an vortex street. These\nsolutions are of $C^1$ type, and are obtained by studying a semilinear problem\non an infinite strip whose width equals to the period. By a variational\ncharacterization of solutions, we also show the relationship between vortex\nsize, travelling speed and street structure. In particular, the vortices with\npositive and negative intensity have equal or unequal scaling size in our\nconstruction, which constitutes the regularization for K\\'arm\\'an point vortex\nstreet.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we consider the problem of wireless federated learning based\non sign stochastic gradient descent (signSGD) algorithm via a multiple access\nchannel. When sending locally computed gradient's sign information, each mobile\ndevice requires to apply precoding to circumvent wireless fading effects. In\npractice, however, acquiring perfect knowledge of channel state information\n(CSI) at all mobile devices is infeasible. In this paper, we present a simple\nyet effective precoding method with limited channel knowledge, called\nsign-alignment precoding. The idea of sign-alignment precoding is to protect\nsign-flipping errors from wireless fadings. Under the Gaussian prior assumption\non the local gradients, we also derive the mean squared error (MSE)-optimal\naggregation function called Bayesian over-the-air computation (BayAirComp). Our\nkey finding is that one-bit precoding with BayAirComp aggregation can provide a\nbetter learning performance than the existing precoding method even using\nperfect CSI with AirComp aggregation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Graph embeddings are low dimensional representations of nodes, edges or whole\ngraphs. Such representations allow for data in a network format to be used\nalong with machine learning models for a variety of tasks (e.g., node\nclassification), where using a similarity matrix would be impractical. In\nrecent years, many methods for graph embedding generation have been created\nbased on the idea of random walks. We propose MultiWalk, a framework that uses\nan ensemble of these methods to generate the embeddings. Our experiments show\nthat the proposed framework, using an ensemble composed of two state-of-the-art\nmethods, can generate embeddings that perform better in classification tasks\nthan each method in isolation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The relaxation dynamics of thermal capillary waves for nanoscale liquid films\non anisotropic-slip substrates are investigated, using both molecular dynamics\n(MD) simulations and a Langevin model. The anisotropy of slip on substrates is\nachieved using a specific lattice plane of a face-centred cubic lattice. This\nsurface's anisotropy breaks the simple scalar proportionality between slip\nvelocity and wall shear stress and requires the introduction of a\nslip-coefficient tensor. The Langevin equation can describe both the growth of\ncapillary wave spectra and the relaxation of capillary wave correlations, with\nthe former providing a time scale for the surface to reach thermal equilibrium.\nTemporal correlations of interfacial Fourier modes, measured at thermal\nequilibrium in MD, demonstrate that (i) larger slip lengths lead to a faster\ndecay in wave correlations, and (ii) unlike on isotropic-slip substrates, the\ntime correlations of waves on anisotropic-slip substrates are wave-direction\ndependent. These findings emerge naturally from the proposed Langevin equation,\nwhich becomes wave-direction dependent, agrees well with MD results, and allows\nto produce experimentally-verifiable predictions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We prove existence results for a stationary Schr{\\\"o}dinger equation with\nperiodic magnetic potential satisfying a local integrability condition on the\nwhole space using a critical value function.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The total variation diminishing (TVD) property is an important tool for\nensuring nonlinear stability and convergence of numerical solutions of\none-dimensional scalar conservation laws. However, it proved to be challenging\nto extend this approach to two-dimensional problems. Using the anisotropic\ndefinition for discrete total variation (TV), it was shown in \\cite{Goodman}\nthat TVD solutions of two-dimensional hyperbolic equations are at most first\norder accurate. We propose to use an alternative definition resulting from a\nfull discretization of the semi-discrete Raviart-Thomas TV. We demonstrate\nnumerically using the second order discontinuous Galerkin method that limited\nsolutions of two-dimensional hyperbolic equations are TVD in means when total\nvariation is computed using the new definition.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we present our approaches for the FinSim-3 Shared Task 2021:\nLearning Semantic Similarities for the Financial Domain. The aim of this shared\ntask is to correctly classify a list of given terms from the financial domain\ninto the most relevant hypernym (or top-level) concept in an external ontology.\nFor our system submission, we evaluate two methods: a Sentence-RoBERTa\n(SRoBERTa) embeddings model pre-trained on a custom corpus, and a dual\nword-sentence embeddings model that builds on the first method by improving the\nproposed baseline word embeddings construction using the FastText model to\nboost the classification performance. Our system ranks 2nd overall on both\nmetrics, scoring 0.917 on Average Accuracy and 1.141 on Mean Rank.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose a method for building a squeezed vacuum state laser with zero\ndiffusion, which results from the introduction of the reservoir engineering\ntechnique into the laser theory. As well as the reservoir engineering, our\nsqueezed vacuum laser demands the construction of an effective atom-field\ninteraction. And by building an isomorphism between the cavity field operators\nin the effective and the Jaynes-Cummings Hamiltonians, we derive the equations\nof our effective laser directly from the conventional laser theory. Our method,\nwhich is less susceptible to errors than reservoir engineering, can be extended\nfor the construction of other nonclassical state lasers, and our squeezed\nvacuum laser can contribute to the newly emerging field of gravitational\ninterferometry.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Delay differential equations form the underpinning of many complex dynamical\nsystems. The forward problem of solving random differential equations with\ndelay has received increasing attention in recent years. Motivated by the\nchallenge to predict the COVID-19 caseload trajectories for individual states\nin the U.S., we target here the inverse problem. Given a sample of observed\nrandom trajectories obeying an unknown random differential equation model with\ndelay, we use a functional data analysis framework to learn the model\nparameters that govern the underlying dynamics from the data. We show existence\nand uniqueness of the analytical solutions of the population delay random\ndifferential equation model when one has discrete time delays in the functional\nconcurrent regression model and also for a second scenario where one has a\ndelay continuum or distributed delay. The latter involves a functional linear\nregression model with history index. The derivative of the process of interest\nis modeled using the process itself as predictor and also other functional\npredictors with predictor-specific delayed impacts. This dynamics learning\napproach is shown to be well suited to model the growth rate of COVID-19 for\nthe states that are part of the U.S., by pooling information from the\nindividual states, using the case process and concurrently observed economic\nand mobility data as predictors.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The hidden symmetry of the asymmetric quantum Rabi model (AQRM) with a\nhalf-integral bias (ibQRM$_{\\ell}$) was uncovered in recent studies by the\nexplicit construction of operators $J_\\ell$ commuting with the Hamiltonian. The\nexistence of such symmetry has been widely believed to cause the degeneration\nof the spectrum, that is, the crossings on the energy curves. In this paper we\npropose a conjectural relation between the symmetry and degeneracy for the\nibQRM$_{\\ell}$ given explicitly in terms of two polynomials appearing\nindependently in the respective investigations. Concretely, one of the\npolynomials appears as the quotient of the constraint polynomials that assure\nthe existence of degenerate solutions while the other determines a quadratic\nrelation (in general, it defines a curve of hyperelliptic type) between the\nibQRM$_{\\ell}$ Hamiltonian and its basic commuting operator $J_\\ell$. Following\nthis conjecture, we derive several interesting structural insights of the whole\nspectrum. For instance, the energy curves are naturally shown to lie on a\nsurface determined by the family of hyperelliptic curves by considering the\ncoupling constant as a variable. This geometric picture contains the\ngeneralization of the parity decomposition of the symmetric quantum Rabi model.\nMoreover, it allows us to describe a remarkable approximation of the first\n$\\ell$ energy curves by the zero-section of the corresponding hyperelliptic\ncurve. These investigations naturally lead to a geometric picture of the\n(hyper-)elliptic surfaces given by the Kodaira-N\\'eron type model for a family\nof curves over the projective line in connection with the energy curves, which\nmay be expected to provide a complex analytic proof of the conjecture.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Gene regulatory network inference is crucial for understanding the complex\nmolecular interactions in various genetic and environmental conditions. The\nrapid development of single-cell RNA sequencing (scRNA-seq) technologies\nunprecedentedly enables gene regulatory networks inference at the single cell\nresolution. However, traditional graphical models for continuous data, such as\nGaussian graphical models, are inappropriate for network inference of\nscRNA-seq's count data. Here, we model the scRNA-seq data using the\nmultivariate Poisson log-normal (PLN) distribution and represent the precision\nmatrix of the latent normal distribution as the regulatory network. We propose\nto first estimate the latent covariance matrix using a moment estimator and\nthen estimate the precision matrix by minimizing the lasso-penalized D-trace\nloss function. We establish the convergence rate of the covariance matrix\nestimator and further establish the convergence rates and the sign consistency\nof the proposed PLNet estimator of the precision matrix in the high dimensional\nsetting. The performance of PLNet is evaluated and compared with available\nmethods using simulation and gene regulatory network analysis of scRNA-seq\ndata.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  COMPASS is a longitudinal, prospective cohort study collecting data annually\nfrom students attending high school in jurisdictions across Canada. We aimed to\ndiscover significant frequent/rare associations of behavioral factors among\nCanadian adolescents related to cannabis use. We use a subset of COMPASS\ndataset which contains 18,761 records of students in grades 9 to 12 with 31\nselected features (attributes) involving various characteristics, from living\nhabits to academic performance. We then used the Pattern Discovery and\nDisentanglement (PDD) algorithm that we have developed to detect strong and\nrare (yet statistically significant) associations from the dataset. PDD used\nthe criteria derived from disentangled statistical spaces (known as\nRe-projected Adjusted-Standardized Residual Vector Spaces, notated as RARV). It\noutperformed methods using other criteria (i.e. support and confidence) popular\nas reported in the literature. Association results showed that PDD can\ndiscover: i) a smaller set of succinct significant associations in clusters;\nii) frequent and rare, yet significant, patterns supported by population health\nrelevant study; iii) patterns from a dataset with extremely imbalanced groups\n(majority class: minority class = 88.3%: 11.7%).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We introduce a new CT image reconstruction algorithm that is less affected by\nvarious artifacts. The new reconstruction algorithm is a method of minimizing\nthe difference between synchrotron X-ray tomography data and sinograms\ngenerated using Radon transform of CT images. The CT image is iteratively\nupdated to reduce the difference from the sinogram of the data. This method can\nobtain clean CT images from the projection data, which can create ring\nartifacts or metal artifacts. Also, even if the sample size is larger than the\nCCD and/or the projection image does not satisfy the Beer-Lambert law, a clean\nCT image can be reconstructed. Our new reconstruction algorithm can also be\napplied to fan beam CT or cone beam CT\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Efficient, reliable and reproducible target volume delineation is a key step\nin the effective planning of breast radiotherapy. However, post-operative\nbreast target delineation is challenging as the contrast between the tumor bed\nvolume (TBV) and normal breast tissue is relatively low in CT images. In this\nstudy, we propose to mimic the marker-guidance procedure in manual target\ndelineation. We developed a saliency-based deep learning segmentation (SDL-Seg)\nalgorithm for accurate TBV segmentation in post-operative breast irradiation.\nThe SDL-Seg algorithm incorporates saliency information in the form of markers'\nlocation cues into a U-Net model. The design forces the model to encode the\nlocation-related features, which underscores regions with high saliency levels\nand suppresses low saliency regions. The saliency maps were generated by\nidentifying markers on CT images. Markers' locations were then converted to\nprobability maps using a distance-transformation coupled with a Gaussian\nfilter. Subsequently, the CT images and the corresponding saliency maps formed\na multi-channel input for the SDL-Seg network. Our in-house dataset was\ncomprised of 145 prone CT images from 29 post-operative breast cancer patients,\nwho received 5-fraction partial breast irradiation (PBI) regimen on GammaPod.\nThe performance of the proposed method was compared against basic U-Net. Our\nmodel achieved mean (standard deviation) of 76.4 %, 6.76 mm, and 1.9 mm for\nDSC, HD95, and ASD respectively on the test set with computation time of below\n11 seconds per one CT volume. SDL-Seg showed superior performance relative to\nbasic U-Net for all the evaluation metrics while preserving low computation\ncost. The findings demonstrate that SDL-Seg is a promising approach for\nimproving the efficiency and accuracy of the on-line treatment planning\nprocedure of PBI, such as GammaPod based PBI.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this study, we propose an extended Watts model to examine the effect of\ninitiators on information cascades. The extended Watts model assumes that nodes\nwith connections to initiators have low adoption thresholds than other nodes,\ndue to the significant influence of initiators. We develop a tree approximation\nto describe the active node fraction for the extended Watts model in random\nnetworks and derive the cascade condition for a global cascade to occur with a\nsmall fraction of initiators. By analyzing the active node fraction and the\ncascade window of the extended Watts model on the Erdos-Renyi random graph, we\nfind that increasing the influence of initiators facilitates the possibility of\nglobal cascades, i.e., how many nodes eventually become active is significantly\naffected by the fraction of initiators and the threshold of nodes directly\nconnected to initiators, which determine cascade dynamics at early stages.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $R=K[G]$ be a group ring of a group $G$ over a field $K$. The Ore\ncondition says that for any $a,b\\in R$ there exist $u,v\\in R$ such that\n$au=bv$, where $u\\ne0$ or $v\\ne0$. It always holds whenever $G$ is amenable.\nRecently it was shown that for R.\\,Thompson's group $F$ the converse is also\ntrue. So the famous amenability problem for $F$ is equivalent to the question\non the Ore condition for the group ring of the same group.\n  It is easy to see that the problem on the Ore condition for $K[F]$ is\nequivalent to the same property for the monoid ring $K[M]$, where $M$ is the\nmonoid of positive elements of $F$. In this paper we reduce the problem to the\ncase when $a$, $b$ are homogeneous elements of the same degree in the monoid\nring. We study the case of degree $1$ and find solutions of the Ore equation.\nFor the case of degree $2$, we study the case of linear combinations of\nmonomials from $S=\\{x_0^2,x_0x_1,x_0x_2,x_1^2,x_1x_2\\}$. This set is not\ndoubling, that is, there are nonempty finite subsets $X\\subset M\\subset F$ such\nthat $|SX| < 2|X|$. As a consequence, the Ore condition holds for linear\ncombinations of these monomials. We give an estimate for the degree of $u$, $v$\nin the above equation.\n  The case of monomials of higher degree is open as well as the case of degree\n$2$ for monomials on $x_0,x_1,...,x_m$, where $m\\ge3$. Recall that negative\nanswer to any of these questions will immediately imply non-amenability of $F$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Feed-forward loops (FFLs) are among the most ubiquitously found motifs of\nreaction networks in nature. However, little is known about their stochastic\nbehavior and the variety of network phenotypes they can exhibit. In this study,\nwe provide full characterizations of the properties of stochastic multimodality\nof FFLs, and how switching between different network phenotypes are controlled.\nWe have computed the exact steady state probability landscapes of all eight\ntypes of coherent and incoherent FFLs using the finite-butter ACME algorithm,\nand quantified the exact topological features of their high-dimensional\nprobability landscapes using persistent homology. Through analysis of the\ndegree of multimodality for each of a set of 10,812 probability landscapes,\nwhere each landscape resides over 10^5-10^6 microstates, we have constructed\ncomprehensive phase diagrams of all relevant behavior of FFL multimodality over\nbroad ranges of input and regulation intensities, as well as different regimes\nof promoter binding dynamics. Our results show that with slow binding and\nunbinding dynamics of transcription factor to promoter, FFLs exhibit strong\nstochastic behavior that is very different from what would be inferred from\ndeterministic models. In addition, input intensity play major roles in the\nphenotypes of FFLs: At weak input intensity, FFL exhibit monomodality, but\nstrong input intensity may result in up to 6 stable phenotypes. Furthermore, we\nfound that gene duplication can enlarge stable regions of specific\nmultimodalities and enrich the phenotypic diversity of FFL networks, providing\nmeans for cells towards better adaptation to changing environment. Our results\nare directly applicable to analysis of behavior of FFLs in biological processes\nsuch as stem cell differentiation and for design of synthetic networks when\ncertain phenotypic behavior is desired.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Autoregressive models are a class of exact inference approaches with highly\nflexible functional forms, yielding state-of-the-art density estimates for\nnatural images. Yet, the sequential ordering on the dimensions makes these\nmodels computationally expensive and limits their applicability to\nlow-resolution imagery. In this work, we propose Pixel-Pyramids, a\nblock-autoregressive approach employing a lossless pyramid decomposition with\nscale-specific representations to encode the joint distribution of image\npixels. Crucially, it affords a sparser dependency structure compared to fully\nautoregressive approaches. Our PixelPyramids yield state-of-the-art results for\ndensity estimation on various image datasets, especially for high-resolution\ndata. For CelebA-HQ 1024 x 1024, we observe that the density estimates (in\nterms of bits/dim) are improved to ~44% of the baseline despite sampling speeds\nsuperior even to easily parallelizable flow-based models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The relation between the spectra of operator pencils with unbounded\ncoefficients and of associated linear relations is investigated. It turns out\nthat various types of spectrum coincide and the same is true for the Weyr\ncharacteristics. This characteristic describes how many independent Jordan\nchains up to a certain length exist. Furthermore, the change of this\ncharacteristic subject to one-dimensional perturbations is investigated.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A discontinuous Galerkin pressure correction numerical method for solving the\nincompressible Navier-Stokes equations is formulated and analyzed. We prove\nunconditional stability of the propose scheme. Convergence of the discrete\nvelocity is established by deriving a priori error estimates. Numerical results\nverify the convergence rates.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To reveal the importance of temporal precision in ground truth audio event\nlabels, we collected precise (~0.1 sec resolution) \"strong\" labels for a\nportion of the AudioSet dataset. We devised a temporally strong evaluation set\n(including explicit negatives of varying difficulty) and a small strong-labeled\ntraining subset of 67k clips (compared to the original dataset's 1.8M clips\nlabeled at 10 sec resolution). We show that fine-tuning with a mix of weak and\nstrongly labeled data can substantially improve classifier performance, even\nwhen evaluated using only the original weak labels. For a ResNet50\narchitecture, d' on the strong evaluation data including explicit negatives\nimproves from 1.13 to 1.41. The new labels are available as an update to\nAudioSet.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Finding the set of the n items most dissimilar from each other out of a\nlarger population becomes increasingly difficult and computationally expensive\nas either n or the population size grows large. Finding the set of the n most\ndissimilar items is different than simply sorting an array of numbers because\nthere exists a pairwise relationship between each item and all other items in\nthe population. For instance, if you have a set of the most dissimilar n=4\nitems, one or more of the items from n=4 might not be in the set n=5. An exact\nsolution would have to search all possible combinations of size n in the\npopulation, exhaustively. We present an open-source software called similarity\ndownselection (SDS), written in Python and freely available on GitHub. SDS\nimplements a heuristic algorithm for quickly finding the approximate set(s) of\nthe n most dissimilar items. We benchmark SDS against a Monte Carlo method,\nwhich attempts to find the exact solution through repeated random sampling. We\nshow that for SDS to find the set of n most dissimilar conformers, our method\nis not only orders of magnitude faster, but is also more accurate than running\nthe Monte Carlo for 1,000,000 iterations, each searching for set sizes n=3-7\nout of a population of 50,000. We also benchmark SDS against the exact solution\nfor example small populations, showing SDS produces a solution close to the\nexact solution in these instances.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Image-to-image (I2I) translation methods based on generative adversarial\nnetworks (GANs) typically suffer from overfitting when limited training data is\navailable. In this work, we propose a data augmentation method (ReMix) to\ntackle this issue. We interpolate training samples at the feature level and\npropose a novel content loss based on the perceptual relations among samples.\nThe generator learns to translate the in-between samples rather than memorizing\nthe training set, and thereby forces the discriminator to generalize. The\nproposed approach effectively reduces the ambiguity of generation and renders\ncontent-preserving results. The ReMix method can be easily incorporated into\nexisting GAN models with minor modifications. Experimental results on numerous\ntasks demonstrate that GAN models equipped with the ReMix method achieve\nsignificant improvements.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Many results in mass partitions are proved by lifting $\\mathbb{R}^d$ to a\nhigher-dimensional space and dividing the higher-dimensional space into pieces.\nWe extend such methods to use lifting arguments to polyhedral surfaces. Among\nother results, we prove the existence of equipartitions of $d+1$ measures in\n$\\mathbb{R}^d$ by parallel hyperplanes and of $d+2$ measures in $\\mathbb{R}^d$\nby concentric spheres. For measures whose supports are sufficiently well\nseparated, we prove results where one can cut a fixed (possibly different)\nfraction of each measure either by parallel hyperplanes, concentric spheres,\nconvex polyhedral surfaces of few facets, or convex polytopes with few\nvertices.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The paper deals with studying a connection of the Littlewood--Offord problem\nwith estimating the concentration functions of some symmetric infinitely\ndivisible distributions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We address the problem of synthesizing distorting mechanisms that maximize\nprivacy of stochastic dynamical systems. Information about the system state is\nobtained through sensor measurements. This data is transmitted to a remote\nstation through an unsecured/public communication network. We aim to keep part\nof the system state private (a private output); however, because the network is\nunsecured, adversaries might access sensor data and input signals, which can be\nused to estimate private outputs. To prevent an accurate estimation, we pass\nsensor data and input signals through a distorting (privacy-preserving)\nmechanism before transmission, and send the distorted data to the trusted user.\nThese mechanisms consist of a coordinate transformation and additive dependent\nGaussian vectors. We formulate the synthesis of the distorting mechanisms as a\nconvex program, where we minimize the mutual information (our privacy metric)\nbetween an arbitrarily large sequence of private outputs and the disclosed\ndistorted data for desired distortion levels -- how different actual and\ndistorted data are allowed to be.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Unsupervised domain adaptation leverages rich information from a labeled\nsource domain to model an unlabeled target domain. Existing methods attempt to\nalign the cross-domain distributions. However, the statistical representations\nof the alignment of the two domains are not well addressed. In this paper, we\npropose deep least squares alignment (DLSA) to estimate the distribution of the\ntwo domains in a latent space by parameterizing a linear model. We further\ndevelop marginal and conditional adaptation loss to reduce the domain\ndiscrepancy by minimizing the angle between fitting lines and intercept\ndifferences and further learning domain invariant features. Extensive\nexperiments demonstrate that the proposed DLSA model is effective in aligning\ndomain distributions and outperforms state-of-the-art methods.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Grain boundary-based mechanisms are known to control the plastic deformation\nand failure of nanocrystalline metals, with manipulation of the boundary\nstructure a promising path for tuning this response. In this study, the role of\ninterfacial structural disorder on plasticity and failure of nanocrystalline\nCu-Zr alloys is investigated with in situ scanning electron microscopy tensile\ndeformation experiments. Two model materials are created, one with only the\ntypical ordered grain boundaries and another with amorphous intergranular films\ninterspersed into the boundary network, while the microstructures are otherwise\nidentical. Hence, the importance of complexion type on plasticity and failure\nis isolated by only varying complexion structure. The tensile experiments show\nthat failure of the samples containing amorphous films is significantly\nretarded, as evidenced by an increase in the cross-sectional area reduction, a\ndecrease in the occurrence of shear-dominated failure, a decrease in strain\nlocalization, and fracture surfaces with more elongated dimple features. As a\nwhole, this study provides direct evidence that structural disorder at the\ngrain boundaries can be beneficial for improving the ductility of\nnanocrystalline metals.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper studies worst-case robust optimal tracking using noisy\ninput-output data. We utilize behavioral system theory to represent system\ntrajectories, while avoiding explicit system identification. We assume that the\nrecent output data used in the data-dependent representation are noisy and we\nprovide a non-conservative design procedure for robust control based on\noptimization with a linear cost and LMI constraints. Our methods rely on the\nparameterization of noise sequences compatible with the data-dependent system\nrepresentation and on a suitable reformulation of the performance\nspecification, which further enable the application of the S-lemma to derive an\nLMI optimization problem. The performance of the new controller is discussed\nthrough simulations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Predictive machine learning models often lack interpretability, resulting in\nlow trust from model end users despite having high predictive performance.\nWhile many model interpretation approaches return top important features to\nhelp interpret model predictions, these top features may not be well-organized\nor intuitive to end users, which limits model adoption rates. In this paper, we\npropose CrystalCandle, a user-facing model explainer that creates\nuser-digestible interpretations and insights reflecting the rationale behind\nmodel predictions. CrystalCandle builds an end-to-end pipeline from machine\nlearning platforms to end user platforms, and provides users with an interface\nfor implementing model interpretation approaches and for customizing narrative\ninsights. CrystalCandle is a platform consisting of four components: Model\nImporter, Model Interpreter, Narrative Generator, and Narrative Exporter. We\ndescribe these components, and then demonstrate the effectiveness of\nCrystalCandle through use cases at LinkedIn. Quantitative performance analyses\nindicate that CrystalCandle's narrative insights lead to lifts in adoption\nrates of predictive model recommendations, as well as to increases in\ndownstream key metrics such as revenue when compared to previous approaches,\nwhile qualitative analyses indicate positive feedback from end users.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Deep learning models are vulnerable to adversarial examples, which can fool a\ntarget classifier by imposing imperceptible perturbations onto natural\nexamples. In this work, we consider the practical and challenging\ndecision-based black-box adversarial setting, where the attacker can only\nacquire the final classification labels by querying the target model without\naccess to the model's details. Under this setting, existing works often rely on\nheuristics and exhibit unsatisfactory performance. To better understand the\nrationality of these heuristics and the limitations of existing methods, we\npropose to automatically discover decision-based adversarial attack algorithms.\nIn our approach, we construct a search space using basic mathematical\noperations as building blocks and develop a random search algorithm to\nefficiently explore this space by incorporating several pruning techniques and\nintuitive priors inspired by program synthesis works. Although we use a small\nand fast model to efficiently evaluate attack algorithms during the search,\nextensive experiments demonstrate that the discovered algorithms are simple yet\nquery-efficient when transferred to larger normal and defensive models on the\nCIFAR-10 and ImageNet datasets. They achieve comparable or better performance\nthan the state-of-the-art decision-based attack methods consistently.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Cloud-based software has many advantages. When services are divided into many\nindependent components, they are easier to update. Also, during peak demand, it\nis easier to scale cloud services (just hire more CPUs). Hence, many\norganizations are partitioning their monolithic enterprise applications into\ncloud-based microservices.\n  Recently there has been much work using machine learning to simplify this\npartitioning task. Despite much research, no single partitioning method can be\nrecommended as generally useful. More specifically, those prior solutions are\n\"brittle\"; i.e. if they work well for one kind of goal in one dataset, then\nthey can be sub-optimal if applied to many datasets and multiple goals.\n  In order to find a generally useful partitioning method, we propose DEEPLY.\nThis new algorithm extends the CO-GCN deep learning partition generator with\n(a) a novel loss function and (b) some hyper-parameter optimization. As shown\nby our experiments, DEEPLY generally outperforms prior work (including CO-GCN,\nand others) across multiple datasets and goals. To the best of our knowledge,\nthis is the first report in SE of such stable hyper-parameter optimization.\n  To aid reuse of this work, DEEPLY is available on-line at\nhttps://bit.ly/2WhfFlB.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Let $G$ be a finite insoluble group with soluble radical $R(G)$. In this\npaper we investigate the soluble graph of $G$, which is a natural\ngeneralisation of the widely studied commuting graph. Here the vertices are the\nelements in $G \\setminus R(G)$, with $x$ adjacent to $y$ if they generate a\nsoluble subgroup of $G$. Our main result states that this graph is always\nconnected and its diameter, denoted $\\delta_{\\mathcal{S}}(G)$, is at most $5$.\nMore precisely, we show that $\\delta_{\\mathcal{S}}(G) \\leqslant 3$ if $G$ is\nnot almost simple and we obtain stronger bounds for various families of almost\nsimple groups. For example, we will show that $\\delta_{\\mathcal{S}}(S_n) = 3$\nfor all $n \\geqslant 6$. We also establish the existence of simple groups with\n$\\delta_{\\mathcal{S}}(G) \\geqslant 4$. For instance, we prove that\n$\\delta_{\\mathcal{S}}(A_{2p+1}) \\geqslant 4$ for every Sophie Germain prime $p\n\\geqslant 5$, which demonstrates that our general upper bound of $5$ is close\nto best possible. We conclude by briefly discussing some variations of the\nsoluble graph construction and we present several open problems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We apply high-throughput DFT calculations and symbolic regression to hybrid\ninorganic/organic interfaces with the intent to extract physically meaningful\ncorrelations between the adsorption-induced work function modifications and the\nproperties of the constituents. We separately investigate two cases:\nHypothetical, free standing self-assembled monolayers with a large intrinsic\ndipole moment, and metal-organic interfaces with a large charge-transfer\ninduced dipole. For the former we find - without notable prior assumptions -\nthe Topping model, as expected from literature. For the latter, highly accurate\ncorrelations are found, which are, however, clearly unphysical.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To enable safe and efficient use of multi-robot systems in everyday life, a\nrobust and fast method for coordinating their actions must be developed. In\nthis paper, we present a distributed task allocation and scheduling algorithm\nfor missions where the tasks of different robots are tightly coupled with\ntemporal and precedence constraints. The approach is based on representing the\nproblem as a variant of the vehicle routing problem, and the solution is found\nusing a distributed metaheuristic algorithm based on evolutionary computation\n(CBM-pop). Such an approach allows a fast and near-optimal allocation and can\ntherefore be used for online replanning in case of task changes. Simulation\nresults show that the approach has better computational speed and scalability\nwithout loss of optimality compared to the state-of-the-art distributed\nmethods. An application of the planning procedure to a practical use case of a\ngreenhouse maintained by a multi-robot system is given.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Convolutional Neural Networks (CNNs), one of the most representative\nalgorithms of deep learning, are widely used in various artificial intelligence\napplications. Convolution operations often take most of the computational\noverhead of CNNs. The FFT-based algorithm can improve the efficiency of\nconvolution by reducing its algorithm complexity, there are a lot of works\nabout the high-performance implementation of FFT-based convolution on many-core\nCPUs. However, there is no optimization for the non-uniform memory access\n(NUMA) characteristics in many-core CPUs. In this paper, we present a\nNUMA-aware FFT-based convolution implementation on ARMv8 many-core CPUs with\nNUMA architectures. The implementation can reduce a number of remote memory\naccess through the data reordering of FFT transformations and the three-level\nparallelization of the complex matrix multiplication. The experiment results on\na ARMv8 many-core CPU with NUMA architectures demonstrate that our NUMA-aware\nimplementation has much better performance than the state-of-the-art work in\nmost cases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A formalism is discussed that allows for a straightforward treatment of the\nrelativistic three-body problem while keeping the correct analytic structure.\nIn particular it is demonstrated that sacrificing covariance for analyticity\ncan be justified by the hierarchy of different contributions in the spirit of\nan effective field theory. For definiteness the formalism is applied to the\n$KK\\bar K$ system allowing for the emergence of the $a_0(980)$ and the\n$f_0(980)$ as hadronic molecules. For simplicity all inelastic channels are\nswitched off.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Over the last few years, contextualized pre-trained transformer models such\nas BERT have provided substantial improvements on information retrieval tasks.\nRecent approaches based on pre-trained transformer models such as BERT,\nfine-tune dense low-dimensional contextualized representations of queries and\ndocuments in embedding space. While these dense retrievers enjoy substantial\nretrieval effectiveness improvements compared to sparse retrievers, they are\ncomputationally intensive, requiring substantial GPU resources, and dense\nretrievers are known to be more expensive from both time and resource\nperspectives. In addition, sparse retrievers have been shown to retrieve\ncomplementary information with respect to dense retrievers, leading to\nproposals for hybrid retrievers. These hybrid retrievers leverage low-cost,\nexact-matching based sparse retrievers along with dense retrievers to bridge\nthe semantic gaps between query and documents. In this work, we address this\ntrade-off between the cost and utility of sparse vs dense retrievers by\nproposing a classifier to select a suitable retrieval strategy (i.e., sparse\nvs. dense vs. hybrid) for individual queries. Leveraging sparse retrievers for\nqueries which can be answered with sparse retrievers decreases the number of\ncalls to GPUs. Consequently, while utility is maintained, query latency\ndecreases. Although we use less computational resources and spend less time, we\nstill achieve improved performance. Our classifier can select between sparse\nand dense retrieval strategies based on the query alone. We conduct experiments\non the MS MARCO passage dataset demonstrating an improved range of\nefficiency/effectiveness trade-offs between purely sparse, purely dense or\nhybrid retrieval strategies, allowing an appropriate strategy to be selected\nbased on a target latency and resource budget.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We show that the vacuum-subtracted maximal volume, the proposed holographic\ndual to complexity of formation, can be negative when contributions from\ncompact directions are included. We construct explicit solutions with\narbitrarily negative complexity of formation in asymptotically AdS$_{4}\\times\nS^{7}$ SUGRA. These examples rely critically on the compact directions,\nspecifically the fact that the full eleven-dimensional spacetime is not\nasymptotically AdS$_{11}$. While there is some ambiguity in the extension of\nthe holographic complexity proposal to the compact directions, we show that the\ntwo natural candidates can both have arbitrarily negative complexity of\nformation in SUGRA solutions. We further find examples in which complexity can\neven decrease at late times, including cases of both single-sided geometries\nand two-sided wormholes. In particular, we construct a cosmological wormhole\nwith simultaneously negative and decreasing complexity of formation (as\ncomputed by volume) at late times. We find a distinguished role for relevant\nprimaries in these constructions and comment on possible interpretations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Asymptotic mean value properties, their converse and some other related\nresults are considered for solutions to the $m$-dimensional Helmholtz equation\n(metaharmonic functions) and solutions to its modified counterpart (panharmonic\nfunctions). Some of these properties have no analogues for harmonic functions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\nof well-exposed areas must be expanded and any missing information due to\nover/under-exposure must be recovered (hallucinated). The majority of methods\nfocus on the former and are relatively successful, while most attempts on the\nlatter are not of sufficient quality, even ones based on Convolutional Neural\nNetworks (CNNs). A major factor for the reduced inpainting quality in some\nworks is the choice of loss function. Work based on Generative Adversarial\nNetworks (GANs) shows promising results for image synthesis and LDR inpainting,\nsuggesting that GAN losses can improve inverse tone mapping results. This work\npresents a GAN-based method that hallucinates missing information from badly\nexposed areas in LDR images and compares its efficacy with alternative\nvariations. The proposed method is quantitatively competitive with\nstate-of-the-art inverse tone mapping methods, providing good dynamic range\nexpansion for well-exposed areas and plausible hallucinations for saturated and\nunder-exposed areas. A density-based normalisation method, targeted for HDR\ncontent, is also proposed, as well as an HDR data augmentation method targeted\nfor HDR hallucination.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the magnetic symmetry of the topological antiferromagnetic\nmaterial Mn$_3$Ge by using torque measurements. Below the Neel temperature,\ndetailed angle-dependent torque measurements were performed on Mn$_3$Ge single\ncrystals in directions parallel and perpendicular to the Kagome basal plane.\nThe out-of plane torque data exhibit $\\pm\\sin\\theta$ and $\\sin2\\theta$\nbehaviors, of which the former results from the spontaneous ferromagnetism\nwithin the basal plane and the latter from the in- and out-of-plane\nsusceptibility anisotropy. The reversible component of the in-plane torque\nexhibits $\\sin6\\varphi$ behavior, revealing the six-fold symmetry of the\nin-plane magnetic free energy. Moreover, we find that the free energy minima\nare pinned to the direction of spontaneous ferromagnetism, which correspond to\nthe maxima of the irreversible component of the in-plane torque. We provide an\neffective spin model to describe the in-plane magnetic anisotropy. Our results\ndemonstrate that the ground state of Mn$_3$Ge is described by the coexistence\nof a strong six-fold antichiral order and a weak ferromagnetic order induced by\nsecond-order spin anisotropy.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper provides three nearly-optimal algorithms for scheduling $t$ jobs\nin the $\\mathsf{CLIQUE}$ model. First, we present a deterministic scheduling\nalgorithm that runs in $O(\\mathsf{GlobalCongestion} + \\mathsf{dilation})$\nrounds for jobs that are sufficiently efficient in terms of their memory. The\n$\\mathsf{dilation}$ is the maximum round complexity of any of the given jobs,\nand the $\\mathsf{GlobalCongestion}$ is the total number of messages in all jobs\ndivided by the per-round bandwidth of $n^2$ of the $\\mathsf{CLIQUE}$ model.\nBoth are inherent lower bounds for any scheduling algorithm.\n  Then, we present a randomized scheduling algorithm which runs $t$ jobs in\n$O(\\mathsf{GlobalCongestion} + \\mathsf{dilation}\\cdot\\log{n}+t)$ rounds and\nonly requires that inputs and outputs do not exceed $O(n\\log n)$ bits per node,\nwhich is met by, e.g., almost all graph problems. Lastly, we adjust the\n\\emph{random-delay-based} scheduling algorithm [Ghaffari, PODC'15] from the\n$\\mathsf{CLIQUE}$ model and obtain an algorithm that schedules any $t$ jobs in\n$O(t / n + \\mathsf{LocalCongestion} + \\mathsf{dilation}\\cdot\\log{n})$ rounds,\nwhere the $\\mathsf{LocalCongestion}$ relates to the congestion at a single node\nof the $\\mathsf{CLIQUE}$. We compare this algorithm to the previous approaches\nand show their benefit.\n  We schedule the set of jobs on-the-fly, without a priori knowledge of its\nparameters or the communication patterns of the jobs. In light of the inherent\nlower bounds, all of our algorithms are nearly-optimal.\n  We exemplify the power of our algorithms by analyzing the message complexity\nof the state-of-the-art MIS protocol [Ghaffari, Gouleakis, Konrad, Mitrovic and\nRubinfeld, PODC'18], and we show that we can solve $t$ instances of MIS in $O(t\n+ \\log\\log\\Delta\\log{n})$ rounds, that is, in $O(1)$ amortized time, for $t\\geq\n\\log\\log\\Delta\\log{n}$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The widespread diffusion of mobile phones is triggering an exponential growth\nof mobile data traffic that is likely to cause, in the near future,\nconsiderable traffic overload issues even in last-generation cellular networks.\nOffloading part of the traffic to other networks is considered a very promising\napproach and, in particular, in this paper, we consider offloading through\nopportunistic networks of users' devices. However, the performance of this\nsolution strongly depends on the pattern of encounters between mobile nodes,\nwhich should therefore be taken into account when designing offloading control\nalgorithms. In this paper, we propose an adaptive offloading solution based on\nthe Reinforcement Learning framework and we evaluate and compare the\nperformance of two well-known learning algorithms: Actor-Critic and Q-Learning.\nMore precisely, in our solution the controller of the dissemination process,\nonce trained, is able to select a proper number of content replicas to be\ninjected into the opportunistic network to guarantee the timely delivery of\ncontents to all interested users. We show that our system based on\nReinforcement Learning is able to automatically learn a very efficient strategy\nto reduce the traffic on the cellular network, without relying on any\nadditional context information about the opportunistic network. Our solution\nachieves a higher level of offloading with respect to other state-of-the-art\napproaches, in a range of different mobility settings. Moreover, we show that a\nmore refined learning solution, based on the Actor-Critic algorithm, is\nsignificantly more efficient than a simpler solution based on Q-learning.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the context of flow visualization a triple decomposition of the velocity\ngradient into irrotational straining flow, shear flow and rigid body rotational\nflow was proposed by Kolar in 2007 [V. Kolar, International journal of heat and\nfluid flow, 28, 638, (2007)], which has recently received renewed interest. The\ntriple decomposition opens for a refined energy stability analysis of the\nNavier-Stokes equations, with implications for the mathematical analysis of the\nstructure, computability and regularity of turbulent flow. We here perform an\nenergy stability analysis of turbulent incompressible flow, which suggests a\nscenario where at macroscopic scales any exponentially unstable irrotational\nstraining flow structures rapidly evolve towards linearly unstable shear flow\nand stable rigid body rotational flow. This scenario does not rule out\nirrotational straining flow close to the Kolmogorov microscales, since there\nviscous dissipation stabilizes the unstable flow structures. In contrast to\nworst case energy stability estimates, this refined stability analysis reflects\nthe existence of stable flow structures in turbulence over extended time.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We have studied the effect of nitriding on the humidity sensing properties of\nhydrogenated amorphous carbon (a-C:H) films. The films were prepared in two\nstages combining the techniques of physical deposition in vapor phase\nevaporation (PAPVD) and plasma pulsed nitriding. By deconvolution of the Raman\nspectrum we identified two peaks corresponding to the D and G modes\ncharacteristic of a-C:H. After the N$_2$-H$_2$ plasma treating, the peaks\nnarrowed and shifted to the right, which we associated with the incorporation\nof N into the structure. We compared the sensitivity to the relative humidity\n(RH) of the films before and after the N$_2$-H$_2$ plasma treatment. The\nnitriding improved the humidity sensitivity measured as the low frequency\nresistance.\n  By impedance spectroscopy we studied the frequency dependence of the AC\nconductivity $\\sigma$ at different RH conditions. Before nitriding\n$\\sigma(\\omega)\\sim A \\omega^s$, it seemed to have the universal behaviour seen\nin other amorphous systems. The humidity changed the overall scale $A$. After\nnitriding, the exponent $s$ increased, and became RH dependent. We associated\nthis behaviour to the change of the interaction mechanism between the water\nmolecule and the substrate when the samples were nitriding.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Internet-of-Things (IoT) systems have spread among different application\ndomains, from home automation to industrial manufacturing processes. The rushed\ndevelopment by competing vendors to meet the market demand of IoT solutions,\nthe lack of interoperability standards, and the overall lack of a defined set\nof best practices have resulted in a highly complex, heterogeneous, and\nfrangible ecosystem. Several works have been pushing towards visual programming\nsolutions to abstract the underlying complexity and help humans reason about\nit. As these solutions begin to meet widespread adoption, their building blocks\nusually do not consider reliability issues. Node-RED, being one of the most\npopular tools, also lacks such mechanisms, either built-in or via extensions.\nIn this work we present SHEN (Self-Healing Extensions for Node-RED) which\nprovides 17 nodes that collectively enable the implementation of self-healing\nstrategies within this visual framework. We proceed to demonstrate the\nfeasibility and effectiveness of the approach using real devices and fault\ninjection techniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study controllable text summarization which allows users to gain control\non a particular attribute (e.g., length limit) of the generated summaries. In\nthis work, we propose a novel training framework based on Constrained Markov\nDecision Process (CMDP), which conveniently includes a reward function along\nwith a set of constraints, to facilitate better summarization control. The\nreward function encourages the generation to resemble the human-written\nreference, while the constraints are used to explicitly prevent the generated\nsummaries from violating user-imposed requirements. Our framework can be\napplied to control important attributes of summarization, including length,\ncovered entities, and abstractiveness, as we devise specific constraints for\neach of these aspects. Extensive experiments on popular benchmarks show that\nour CMDP framework helps generate informative summaries while complying with a\ngiven attribute's requirement.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The quantum levels population behavior of the two coupled flux qubits\ndepending on the external driving field characteristics is studied. The\nexplicit expressions for the multiphoton transition probabilities at an\narbitrary control field amplitude is obtained for the case of small tunnel\nsplitting energies. We describe the controllable features of their formation\nand thereby creating or destroying entanglement by system bias tuning on the\ndirect inter-level transition and during the transition through intermediate\nstates. We found a feature of the qubits population inverting that ends in the\nindependence of the resonances positions from the qubits coupling strength.\nUsing Floquet--Markov equation we numerically demonstrate, that the positions\nof multiphoton resonances are stable to dissipative processes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  As a platform for optoelectronic devices based on exciton dynamics, monolayer\ntransition metal dichalcogenides (TMDCs) are often placed near metal interfaces\nor inside planar cavities. While the radiative properties of point dipoles at\nmetal interfaces has been studied extensively, those of excitons, which are\ndelocalized and exhibit a temperature-dependent momentum distribution, lack a\nthorough treatment. Here, we analyze the emission properties of excitons in\nTMDCs near planar metal interfaces and explore their dependence on exciton\ncenter-of-mass momentum, transition dipole orientation, and temperature.\nDefining a characteristic energy scale $k_B T_c = (\\hbar k)^2/2m$~($k$ being\nthe radiative wavevector and $m$ the exciton mass), we find that at\ntemperatures $T\\gg T_c$ and low densities where the momentum distribution can\nbe characterized by Maxwell-Boltzmann statistics, the modified emission\nrates~(normalized to free space) behave similarly to point dipoles at\ntemperatures $T\\gg T_c$. This similarity in behavior arises due to the broad\nnature of wavevector components making up the exciton and point dipole\nemission. On the other hand, the narrow momentum distribution of excitons for\n$T<T_c$ can result in significantly different emission behavior as compared to\npoint dipoles. These differences can be further amplified by considering\nexcitons with a Bose Einstein distribution at high phase space densities. We\nfind suppression or enhancement of emission relative to the point dipole case\nby several orders of magnitude. These insights can help optimize the\nperformance of optoelectronic devices that incorporate 2D semiconductors near\nmetal electrodes and can inform future studies of exciton radiative dynamics at\nlow temperatures. Additionally, these studies show that nanoscale optical\ncavities are a viable pathway to generating long-lifetime exciton states in\nTMDCs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Experimental High-Energy Physics (HEP), especially the Large Hadron Collider\n(LHC) programme at the European Organization for Nuclear Research (CERN), is\none of the most computationally intensive activities in the world. This demand\nis set to increase significantly with the upcoming High-Luminosity LHC\n(HL-LHC), and even more in future machines, such as the Future Circular\nCollider (FCC). As a consequence, event reconstruction, and in particular jet\nclustering, is bound to become an even more daunting problem, thus challenging\npresent day computing resources. In this work, we present the first digital\nquantum algorithm to tackle jet clustering, opening the way for digital quantum\nprocessors to address this challenging problem. Furthermore, we show that, at\npresent and future collider energies, our algorithm has comparable, yet\ngenerally lower complexity relative to the classical state-of-the-art $k_t$\nclustering algorithm.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Best match graphs (BMGs) are vertex-colored digraphs that naturally arise in\nmathematical phylogenetics to formalize the notion of evolutionary closest\ngenes w.r.t. an a priori unknown phylogenetic tree. BMGs are explained by\nunique least resolved trees. We prove that the property of a rooted,\nleaf-colored tree to be least resolved for some BMG is preserved by the\ncontraction of inner edges. For the special case of two-colored BMGs, this\nleads to a characterization of the least resolved trees (LRTs) of\nbinary-explainable trees and a simple, polynomial-time algorithm for the\nminimum cardinality completion of the arc set of a BMG to reach a BMG that can\nbe explained by a binary tree.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study a class of deterministic finite-horizon two-player nonzero-sum\ndifferential games where players are endowed with different kinds of controls.\nWe assume that Player 1 uses piecewise-continuous controls, while Player 2 uses\nimpulse controls. For this class of games, we seek to derive conditions for the\nexistence of feedback Nash equilibrium strategies for the players. More\nspecifically, we provide a verification theorem for identifying such\nequilibrium strategies, using the Hamilton-Jacobi-Bellman (HJB) equations for\nPlayer 1 and the quasi-variational inequalities (QVIs) for Player 2. Further,\nwe show that the equilibrium number of interventions by Player 2 is upper\nbounded. Furthermore, we specialize the obtained results to a scalar two-player\nlinear-quadratic differential game. In this game, Player 1's objective is to\ndrive the state variable towards a specific target value, and Player 2 has a\nsimilar objective with a different target value. We provide, for the first\ntime, an analytical characterization of the feedback Nash equilibrium in a\nlinear-quadratic differential game with impulse control. We illustrate our\nresults using numerical experiments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Why can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidence indicating that the adaptations of PLMs to\nvarious few-shot tasks can be reparameterized as optimizing only a few free\nparameters in a unified low-dimensional intrinsic task subspace, which may help\nus understand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. To find such a subspace and examine its universality, we propose an\nanalysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort\nto the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen data or tasks by only tuning parameters in\nthis subspace. In the experiments, we study diverse few-shot NLP tasks and\nsurprisingly find that in a 250-dimensional subspace found with 100 tasks, by\nonly tuning 250 free parameters, we can recover 97% and 83% of the full prompt\ntuning performance for 100 seen tasks (using different training data) and 20\nunseen tasks, respectively, showing great generalization ability of the found\nintrinsic task subspace. Besides being an analysis tool, IPT could further\nbring practical benefits, such as improving the prompt tuning stability.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Notoriously, the two main problems of the standard $\\Lambda$CDM model of\ncosmology are the cosmological constant $\\Lambda$ and the cold dark matter,\nCDM. This essay shows that both the $\\Lambda$ and the CDM arise as integration\nconstants in a careful derivation of Einstein's equations from first principles\nin a Lorentz gauge theory. The dark sector of the universe might only reflect\nthe geometry of a spontaneous symmetry breaking that is necessary for the\nexistence of a spacetime and an observer therein.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The spectral signatures of magnetic skyrmions under microwave field\nexcitation are of fundamental interest and can be an asset for high frequency\napplications. These topological solitons can be tailored in multilayered thin\nfilms, but the experimental observation of their spin wave dynamics remains\nelusive, in particular due to large damping. Here, we study Pt/FeCoB/AlO$_x$\nmultilayers hosting dense and robust skyrmion lattices at room temperature with\nGilbert damping of $\\sim 0.02$. We use magnetic force microscopy to\ncharacterise their static magnetic phases and broadband ferromagnetic resonance\nto probe their high frequency response. Micromagnetic simulations reproduce the\nexperiments with accuracy and allow us to identify distinct resonant modes\ndetected in the skyrmion lattice phase. Low ($<$ 2 GHz) and intermediate\nfrequency ($2-8$ GHz) modes involve excitations localised to skyrmion edges in\nconjunction with precession of the uniform background magnetisation, while a\nhigh frequency ($>$ 12 GHz) mode corresponds to in-phase skyrmion core\nprecession emitting spin waves into uniform background with wavelengths in the\n50--80 nm range commensurate with the lattice structure. These findings could\nbe instrumental in the investigation of room temperature wave scattering and\nthe implementation of novel microwave processing schemes in reconfigurable\narrays of solitons.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  There is a resurging interest in automation because of rapid progress of\nmachine learning and AI. In our perspective, innovation is not an exemption\nfrom their expansion. This situation gives us an opportunity to reflect on a\ndirection of future innovation studies. In this conceptual paper, we propose a\nframework of innovation process by exploiting the concept of unit process.\nDeploying it in the context of automation, we indicate the important aspects of\ninnovation process, i.e. human, organizational, and social factors. We also\nhighlight the cognitive and interactive underpinnings at micro- and\nmacro-levels of the process. We propose to embrace all those factors in what we\ncall Innovation-Automation-Strategy cycle (IAS). Implications of IAS for future\nresearch are also put forward.\n  Keywords: innovation, automation of innovation, unit process,\ninnovation-automation-strategy cycle\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In many applications, a large number of features are collected with the goal\nto identify a few important ones. Sometimes, these features lie in a metric\nspace with a known distance matrix, which partially reflects their\nco-importance pattern. Proper use of the distance matrix will boost the power\nof identifying important features. Hence, we develop a new multiple testing\nframework named the Distance Assisted Recursive Testing (DART). DART has two\nstages. In stage 1, we transform the distance matrix into an aggregation tree,\nwhere each node represents a set of features. In stage 2, based on the\naggregation tree, we set up dynamic node hypotheses and perform multiple\ntesting on the tree. All rejections are mapped back to the features. Under mild\nassumptions, the false discovery proportion of DART converges to the desired\nlevel in high probability converging to one. We illustrate by theory and\nsimulations that DART has superior performance under various models compared to\nthe existing methods. We applied DART to a clinical trial in the allogeneic\nstem cell transplantation study to identify the gut microbiota whose abundance\nwill be impacted by the after-transplant care.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A subalgebra S of a Leibniz algebra L is called self-idealizing in L if it\ncoincides with its idealizer IL(S). In this paper we study the structure of\nLeibniz algebras, whose subalgebras are either ideals or self-idealizing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Motivated by a kind of Penrose correspondence, we investigate the space of\nhyperplane sections of Segre quartic surfaces which have an ordinary cusp. We\nshow that the space of such hyperplane sections is empty for two kinds of Segre\nsurfaces, and it is a connected surface for all other kinds of Segre surfaces.\nWe also show that when it is non-empty, the closure of the space is either\nbirational to the surface itself or birational to a double covering of the\nsurface, whose branch divisor consists of some specific lines on the surface.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present 77 K isotherms of krypton adsorption on bundles of closed\nhighly-pure HiPco single-walled carbon nanotubes (SWCNTs). Two volumetric\nadsorption protocols were used, one with an increasing Kr dose per injection\n(IAD), one with a constant dose (CAD). Detailed microstructural examination\nshowed that the SWCNTs combine into small bundles (of 25-30 SWCNTs) which are\nheterogeneous in diameter with a consequential range of interstitial channel\n(IC) shapes and sizes. The IC-sites are the subnanoscaled pores with\nalternating enlargements and constrictions along the tube axes. This results in\nadsorption dosing (AD) dependent characteristics of the low-pressure region of\nthe isotherm. In the IAD protocol the switch-back behavior of the isotherm\nstemmed from metastable adsorption. Using the CAD protocol, different branches\nare observed. Well-pronounced substeps were established which we interpret as\ncorresponding to the formation of various phases of confined Kr with different\natoms arrangement. The height of a given substep obtained in different\nmeasurements depends on the AD value which can strongly influence the\npopulation of the site. Some substeps existing only for certain values of AD\nsuggests the existence of a certain selectivity or of a preferential phase\nformation according to this value.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We benchmark a subseasonal forecasting toolkit of simple learned models that\noutperform both operational practice and state-of-the-art machine learning and\ndeep learning methods. These models, introduced by Mouatadid et al. (2022),\ninclude (a) Climatology++, an adaptive alternative to climatology that, for\nprecipitation, is 9% more accurate and 250% more skillful than the United\nStates operational Climate Forecasting System (CFSv2); (b) CFSv2++, a learned\nCFSv2 correction that improves temperature and precipitation accuracy by 7-8%\nand skill by 50-275%; and (c) Persistence++, an augmented persistence model\nthat combines CFSv2 forecasts with lagged measurements to improve temperature\nand precipitation accuracy by 6-9% and skill by 40-130%. Across the contiguous\nU.S., the Climatology++, CFSv2++, and Persistence++ toolkit consistently\noutperforms standard meteorological baselines, state-of-the-art machine and\ndeep learning methods, and the European Centre for Medium-Range Weather\nForecasts ensemble.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We generate constrained realizations (CRs) of the density and peculiar\nvelocity fields within $200 \\; h^{-1} \\, \\mathrm{Mpc}$ from the final release\nof the Two-Micron All-Sky Redshift Survey (2MRS) $-$ the densest all-sky\nredshift survey to date. The CRs are generated by combining a Wiener filter\nestimator in spherical Fourier-Bessel space with random realizations of\nlog-normally distributed density fields and Poisson-sampled galaxy positions.\nThe algorithm is tested and calibrated on a set of semi-analytic mock catalogs\nmimicking the environment of the Local Group (LG), to rigorously account for\nthe statistical and systematic errors of the reconstruction method. By\ncomparing our peculiar velocity CRs with the observed velocities from the\nCosmicflows-3 catalog, we constrain the normalized linear growth rate to $f\n\\sigma_8^\\mathrm{lin} = 0.367 \\pm 0.060$, which is consistent at the $1.1\n\\sigma$ level with the latest Planck results as well as other direct probes.\nSimultaneously, we estimate a bulk flow contribution from sources beyond the\n2MRS reconstruction volume of $B^\\mathrm{ext} = 199 \\pm 68 \\; \\mathrm{km} \\,\n\\mathrm{s}^{-1}$ towards $l = 299 \\pm 18^\\circ$, $b = 8 \\pm 19^\\circ$. The\ntotal reconstructed velocity field at the position of the LG, smoothed with a\n$1 \\; h^{-1} \\, \\mathrm{Mpc}$ Gaussian, is $685 \\pm 75 \\; \\mathrm{km} \\,\n\\mathrm{s}^{-1}$ towards $l = 270.6 \\pm 6.6^\\circ$, $b = 35.5 \\pm 7.2^\\circ$,\nin good agreement with the observed CMB dipole. The total reconstructed bulk\nflow within different radii is compatible with other measurements. Within a $50\n\\; h^{-1} \\, \\mathrm{Mpc}$ Gaussian window we find a bulk flow of $274 \\pm 50\n\\; \\mathrm{km} \\, \\mathrm{s}^{-1}$ towards $l = 287 \\pm 9^\\circ$, $b = 11 \\pm\n10^\\circ$. The code used to generate the CRs and obtain these results, dubbed\nCORAS, is made publicly available.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Accurate indoor localization is a crucial enabling technology for many\nrobotics applications, from warehouse management to monitoring tasks.\nUltra-wideband (UWB) time difference of arrival (TDOA)-based localization is a\npromising lightweight, low-cost solution that can scale to a large number of\ndevices -- making it especially suited for resource-constrained multi-robot\napplications. However, the localization accuracy of standard, commercially\navailable UWB radios is often insufficient due to significant measurement bias\nand outliers. In this letter, we address these issues by proposing a robust UWB\nTDOA localization framework comprising of (i) learning-based bias correction\nand (ii) M-estimation-based robust filtering to handle outliers. The key\nproperties of our approach are that (i) the learned biases generalize to\ndifferent UWB anchor setups and (ii) the approach is computationally efficient\nenough to run on resource-constrained hardware. We demonstrate our approach on\na Crazyflie nano-quadcopter. Experimental results show that the proposed\nlocalization framework, relying only on the onboard IMU and UWB, provides an\naverage of 42.08 percent localization error reduction (in three different\nanchor setups) compared to the baseline approach without bias compensation. {We\nalso show autonomous trajectory tracking on a quadcopter using our UWB TDOA\nlocalization approach.}\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  3D human pose estimation from monocular images is a highly ill-posed problem\ndue to depth ambiguities and occlusions. Nonetheless, most existing works\nignore these ambiguities and only estimate a single solution. In contrast, we\ngenerate a diverse set of hypotheses that represents the full posterior\ndistribution of feasible 3D poses. To this end, we propose a normalizing flow\nbased method that exploits the deterministic 3D-to-2D mapping to solve the\nambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and\nocclusions are effectively modeled by incorporating uncertainty information of\nthe 2D detector as condition. Further keys to success are a learned 3D pose\nprior and a generalization of the best-of-M loss. We evaluate our approach on\nthe two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all\ncomparable methods in most metrics. The implementation is available on GitHub.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Retrieving keywords (bidwords) with the same intent as query, referred to as\nclose variant keywords, is of prime importance for effective targeted search\nadvertising. For head and torso search queries, sponsored search engines use a\nhuge repository of same intent queries and keywords, mined ahead of time.\nOnline, this repository is used to rewrite the query and then lookup the\nrewrite in a repository of bid keywords contributing to significant revenue.\nRecently generative retrieval models have been shown to be effective at the\ntask of generating such query rewrites. We observe two main limitations of such\ngenerative models. First, rewrites generated by these models exhibit low\nlexical diversity, and hence the rewrites fail to retrieve relevant keywords\nthat have diverse linguistic variations. Second, there is a misalignment\nbetween the training objective - the likelihood of training data, v/s what we\ndesire - improved quality and coverage of rewrites. In this work, we introduce\nCLOVER, a framework to generate both high-quality and diverse rewrites by\noptimizing for human assessment of rewrite quality using our diversity-driven\nreinforcement learning algorithm. We use an evaluation model, trained to\npredict human judgments, as the reward function to finetune the generation\npolicy. We empirically show the effectiveness of our proposed approach through\noffline experiments on search queries across geographies spanning three major\nlanguages. We also perform online A/B experiments on Bing, a large commercial\nsearch engine, which shows (i) better user engagement with an average increase\nin clicks by 12.83% accompanied with an average defect reduction by 13.97%, and\n(ii) improved revenue by 21.29%.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we design and analyze third order positivity-preserving\ndiscontinuous Galerkin (DG) schemes for solving the time-dependent system of\nPoisson--Nernst--Planck (PNP) equations, which has found much use in diverse\napplications. Our DG method with Euler forward time discretization is shown to\npreserve the positivity of cell averages at all time steps. The positivity of\nnumerical solutions is then restored by a scaling limiter in reference to\npositive weighted cell averages. The method is also shown to preserve steady\nstates. Numerical examples are presented to demonstrate the third order\naccuracy and illustrate the positivity-preserving property in both one and two\ndimensions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  By directly inverting several neutron star observables in the\nthree-dimensional parameter space for the Equation of State of super-dense\nneutron-rich nuclear matter, we show that the lower radius limit for PSR\nJ0740+6620 of mass $2.08\\pm 0.07~M_{\\odot}$ from Neutron Star Interior\nComposition Explorer (NICER)'s very recent observation sets a much tighter\nlower boundary than previously known for nuclear symmetry energy in the density\nrange of $(1.0\\sim 3.0)$ times the saturation density $\\rho_0$ of nuclear\nmatter. The super-soft symmetry energy leading to the formation of proton\npolarons in this density region of neutron stars is clearly disfavoured by the\nfirst radius measurement for the most massive neutron star observed reliably so\nfar.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The development of neuromorphic hardware and modeling of biological neural\nnetworks requires algorithms with local learning rules. Artificial neural\nnetworks using local learning rules to perform principal subspace analysis\n(PSA) and clustering have recently been derived from principled objective\nfunctions. However, no biologically plausible networks exist for minor subspace\nanalysis (MSA), a fundamental signal processing task. MSA extracts the\nlowest-variance subspace of the input signal covariance matrix. Here, we\nintroduce a novel similarity matching objective for extracting the minor\nsubspace, Minor Subspace Similarity Matching (MSSM). Moreover, we derive an\nadaptive MSSM algorithm that naturally maps onto a novel neural network with\nlocal learning rules and gives numerical results showing that our method\nconverges at a competitive rate.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present deep imaging of Sirius B, the closest and brightest white dwarf,\nto constrain post-main-sequence planetary evolution in the Sirius system. We\nuse Keck/NIRC2 in L'-band (3.776 $\\mu$m) across three epochs in 2020 using the\ntechnique of angular differential imaging. Our observations are speckle-limited\nout to 1 AU and background-limited beyond. The 5$\\sigma$ detection limits from\nour best performing epoch are 17 to 20.4 L' absolute magnitude. We consider\nmultiple planetary formation pathways in the context of Sirius B's evolution to\nderive mass sensitivity limits, and achieve sub-Jupiter sensitivities at sub-AU\nseparations, reaching 1.6 $\\mathrm{M_J}$ to 2.4 $\\mathrm{M_J}$ at 0.5 AU down\nto a sensitivity of 0.7 $\\mathrm{M_J}$ to 1.2 $\\mathrm{M_J}$ at >1 AU.\nConsistent with previous results, we do not detect any companions around Sirius\nB. Our strong detection limits demonstrate the potential of using high-contrast\nimaging to characterize nearby white dwarfs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider the classical Ramsey-Cass-Koopmans capital accumulation model,\nand present three examples in which the Hamilton-Jacobi-Bellman (HJB) equation\nis neither necessary nor sufficient for a function to be the value function.\nNext, we present assumptions under which the HJB equation becomes a necessary\nand sufficient condition for a function to be the value function, and using\nthis result, we propose a new method for solving the original problem using the\nsolution to the HJB equation. Our assumptions are so mild that many\nmacroeconomic growth models satisfy them. Therefore, our results ensure that\nthe solution to the HJB equation is rigorously the value function in many\nmacroeconomic models, and present a new solving method for these models.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The inside of the electrical double layer at perovskite oxide\nheterointerfaces is examined. Here, we report the local polarization and\nvalence distribution in LaNiO$_3$/LaMnO$_3$ and LaMnO$_3$/LaNiO$_3$ bilayers on\na SrTiO$_3$ (001) substrate. Simultaneous measurements of two aspects of the\nstructure are realized by using Bayesian inference based on resonant- and\nnonresonant-surface X-ray diffraction data. The results show that the average\nMn valences are Mn$^{3.12+}$ and Mn$^{3.19+}$ for the two samples. The\nintensity of their local electric field is $\\sim$1~GV/m and the direction of\nthe field points from LaMnO$_3$ to LaNiO$_3$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We perform a detailed analysis of the asymptotic behavior of a multifield\ncosmological model with phantom terms. Specifically, we consider the\nChiral-Phantom model consisting of two scalar fields with a mixed kinetic term,\nwhile one scalar field has negative kinetic energy, that is, it has phantom\nproperties. We show that the Hubble function can change sign, and we study the\nglobal evolution of the field equation in the finite and infinity regions with\nthe use of Poincar\\'{e} variables. We find that in the Chiral-Phantom model the\nlimit of the quintessence scalar field is recovered while the cosmological\nevolution differs from the standard hyperbolic theory. Finally, the linear\ncosmological perturbations are studied.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Limited-valency colloidal particles can self-assemble into polymeric\nstructures analogous to molecules. While their structural equilibrium\nproperties have attracted wide attention, insight into their dynamics has\nproven challenging. Here, we investigate the polymerization dynamics of\nsemiflexible polymers in two dimensions (2D) by direct observation of\nassembling divalent particles, bonded by critical Casimir forces. The\nreversible critical Casimir force creates living polymerization conditions with\ntunable chain dissociation, association and bending rigidity. We find that\nunlike dilute polymers that show exponential size distributions in excellent\nagreement with Flory theory, concentrated samples exhibit arrest of rotational\nand translational diffusion due to a continuous isotropic-to-nematic transition\nin 2D, slowing down the growth kinetics. These effects are circumvented by\naddition of higher-valency particles, cross-linking the polymers into networks.\nOur results connecting polymer flexibility, polymer interactions and the\npeculiar isotropic-nematic transition in 2D offer insight into polymerization\nprocesses of synthetic two-dimensional polymers, and biopolymers at membranes\nand interfaces.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider massive Dirac equations on asymptotically static spacetimes with\na Cauchy surface of bounded geometry. We prove that the associated quantized\nDirac field admits in and out states, which are asymptotic vacuum states when\nsome time coordinate tends to $\\mp\\infty$. We also show that the in /out states\nare Hadamard states.(new version with some typos corrected).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A review of positive energy theorems for asymptotically hyperbolic manifolds\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper presents a sensor-level mapless collision avoidance algorithm for\nuse in mobile robots that map raw sensor data to linear and angular velocities\nand navigate in an unknown environment without a map. An efficient training\nstrategy is proposed to allow a robot to learn from both human experience data\nand self-exploratory data. A game format simulation framework is designed to\nallow the human player to tele-operate the mobile robot to a goal and human\naction is also scored using the reward function. Both human player data and\nself-playing data are sampled using prioritized experience replay algorithm.\nThe proposed algorithm and training strategy have been evaluated in two\ndifferent experimental configurations: \\textit{Environment 1}, a simulated\ncluttered environment, and \\textit{Environment 2}, a simulated corridor\nenvironment, to investigate the performance. It was demonstrated that the\nproposed method achieved the same level of reward using only 16\\% of the\ntraining steps required by the standard Deep Deterministic Policy Gradient\n(DDPG) method in Environment 1 and 20\\% of that in Environment 2. In the\nevaluation of 20 random missions, the proposed method achieved no collision in\nless than 2~h and 2.5~h of training time in the two Gazebo environments\nrespectively. The method also generated smoother trajectories than DDPG. The\nproposed method has also been implemented on a real robot in the real-world\nenvironment for performance evaluation. We can confirm that the trained model\nwith the simulation software can be directly applied into the real-world\nscenario without further fine-tuning, further demonstrating its higher\nrobustness than DDPG. The video and code are available:\nhttps://youtu.be/BmwxevgsdGc\nhttps://github.com/hanlinniu/turtlebot3_ddpg_collision_avoidance\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Entropy regularization is an efficient technique for encouraging exploration\nand preventing a premature convergence of (vanilla) policy gradient methods in\nreinforcement learning (RL). However, the theoretical understanding of entropy\nregularized RL algorithms has been limited. In this paper, we revisit the\nclassical entropy regularized policy gradient methods with the soft-max policy\nparametrization, whose convergence has so far only been established assuming\naccess to exact gradient oracles. To go beyond this scenario, we propose the\nfirst set of (nearly) unbiased stochastic policy gradient estimators with\ntrajectory-level entropy regularization, with one being an unbiased visitation\nmeasure-based estimator and the other one being a nearly unbiased yet more\npractical trajectory-based estimator. We prove that although the estimators\nthemselves are unbounded in general due to the additional logarithmic policy\nrewards introduced by the entropy term, the variances are uniformly bounded. We\nthen propose a two-phase stochastic policy gradient (PG) algorithm that uses a\nlarge batch size in the first phase to overcome the challenge of the stochastic\napproximation due to the non-coercive landscape, and uses a small batch size in\nthe second phase by leveraging the curvature information around the optimal\npolicy. We establish a global optimality convergence result and a sample\ncomplexity of $\\widetilde{\\mathcal{O}}(\\frac{1}{\\epsilon^2})$ for the proposed\nalgorithm. Our result is the first global convergence and sample complexity\nresults for the stochastic entropy-regularized vanilla PG method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The usage of environment sensor models for virtual testing is a promising\napproach to reduce the testing effort of autonomous driving. However, in order\nto deduce any statements regarding the performance of an autonomous driving\nfunction based on simulation, the sensor model has to be validated to determine\nthe discrepancy between the synthetic and real sensor data. Since a certain\ndegree of divergence can be assumed to exist, the sufficient level of fidelity\nmust be determined, which poses a major challenge. In particular, a method for\nquantifying the fidelity of a sensor model does not exist and the problem of\ndefining an appropriate metric remains. In this work, we train a neural network\nto distinguish real and simulated radar sensor data with the purpose of\nlearning the latent features of real radar point clouds. Furthermore, we\npropose the classifier's confidence score for the `real radar point cloud'\nclass as a metric to determine the degree of fidelity of synthetically\ngenerated radar data. The presented approach is evaluated and it can be\ndemonstrated that the proposed deep evaluation metric outperforms conventional\nmetrics in terms of its capability to identify characteristic differences\nbetween real and simulated radar data.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Real topological phases featuring real Chern numbers and second-order\nboundary modes have been a focus of current research, but finding their\nmaterial realization remains a challenge. Here, based on first-principles\ncalculations and theoretical analysis, we reveal the already experimentally\nsynthesized three-dimensional (3D) graphdiyne as the first realistic example of\nthe recently proposed second-order real nodal-line semimetal. We show that the\nmaterial hosts a pair of real nodal rings, each protected by two topological\ncharges: a real Chern number and a 1D winding number. The two charges generate\ndistinct topological boundary modes at distinct boundaries. The real Chern\nnumber leads to a pair of hinge Fermi arcs, whereas the winding number protects\na double drumhead surface bands. We develop a low-energy model for 3D\ngraphdiyne which captures the essential topological physics. Experimental\naspects and possible topological transition to a 3D real Chern insulator phase\nare discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Graph embedding is essential for graph mining tasks. With the prevalence of\ngraph data in real-world applications, many methods have been proposed in\nrecent years to learn high-quality graph embedding vectors various types of\ngraphs. However, most existing methods usually randomly select the negative\nsamples from the original graph to enhance the training data without\nconsidering the noise. In addition, most of these methods only focus on the\nexplicit graph structures and cannot fully capture complex semantics of edges\nsuch as various relationships or asymmetry. In order to address these issues,\nwe propose a robust and generalized framework for adversarial graph embedding\nbased on generative adversarial networks. Inspired by generative adversarial\nnetwork, we propose a robust and generalized framework for adversarial graph\nembedding, named AGE. AGE generates the fake neighbor nodes as the enhanced\nnegative samples from the implicit distribution, and enables the discriminator\nand generator to jointly learn each node's robust and generalized\nrepresentation. Based on this framework, we propose three models to handle\nthree types of graph data and derive the corresponding optimization algorithms,\ni.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs,\nrespectively, and HIN-AGE for heterogeneous information networks. Extensive\nexperiments show that our methods consistently and significantly outperform\nexisting state-of-the-art methods across multiple graph mining tasks, including\nlink prediction, node classification, and graph reconstruction.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Principal components analysis has been used to reduce the dimensionality of\ndatasets for a long time. In this paper, we will demonstrate that in mode\ndetection the components of smallest variance, the pettiest components, are\nmore important. We prove that for a multivariate normal or Laplace\ndistribution, we obtain boxes of optimal volume by implementing \"pettiest\ncomponent analysis\", in the sense that their volume is minimal over all\npossible boxes with the same number of dimensions and fixed probability. This\nreduction in volume produces an information gain that is measured using active\ninformation. We illustrate our results with a simulation and a search for modal\npatterns of digitized images of hand-written numbers using the famous MNIST\ndatabase; in both cases pettiest components work better than their competitors.\nIn fact, we show that modes obtained with pettiest components generate better\nwritten digits for MNIST than principal components.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The Birkhoff polytope $\\mathcal{B}_d$ consisting of all bistochastic matrices\nof order $d$ assists researchers from many areas, including combinatorics,\nstatistical physics and quantum information. Its subset $\\mathcal{U}_d$ of\nunistochastic matrices, determined by squared moduli of unitary matrices, is of\na particular importance for quantum theory as classical dynamical systems\ndescribed by unistochastic transition matrices can be quantised. In order to\ninvestigate the problem of unistochasticity we introduce the set\n$\\mathcal{L}_d$ of bracelet matrices that forms a subset of $\\mathcal{B}_d$,\nbut a superset of $\\mathcal{U}_d$. We prove that for every dimension $d$ this\nset contains the set of factorisable bistochastic matrices $\\mathcal{F}_d$ and\nis closed under matrix multiplication by elements of $\\mathcal{F}_d$. Moreover,\nwe prove that both $\\mathcal{L}_d$ and $\\mathcal{F}_d$ are star-shaped with\nrespect to the flat matrix. We also analyse the set of $d\\times d$\nunistochastic matrices arising from circulant unitary matrices, and show that\ntheir spectra lie inside $d$-hypocycloids on the complex plane. Finally,\napplying our results to small dimensions, we fully characterise the set of\ncirculant unistochastic matrices of order $d\\leq 4$, and prove that such\nmatrices form a monoid for $d=3$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We address the problem of computing the smallest symplectic eigenvalues and\nthe corresponding eigenvectors of symmetric positive-definite matrices in the\nsense of Williamson's theorem. It is formulated as minimizing a trace cost\nfunction over the symplectic Stiefel manifold. We first investigate various\ntheoretical aspects of this optimization problem such as characterizing the\nsets of critical points, saddle points, and global minimizers as well as\nproving that non-global local minimizers do not exist. Based on our recent\nresults on constructing Riemannian structures on the symplectic Stiefel\nmanifold and the associated optimization algorithms, we then propose solving\nthe symplectic eigenvalue problem in the framework of Riemannian optimization.\nMoreover, a connection of the sought solution with the eigenvalues of a special\nclass of Hamiltonian matrices is discussed. Numerical examples are presented.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present the first census of giant molecular clouds (GMCs) complete down to\n10$^6 M_{\\odot}$ and within the inner 4 kpc of the nearest giant elliptical and\npowerful radio galaxy, Centaurus A.\n  We identified 689 GMCs using CO(1--0) data with 1\" spatial resolution ($\\sim\n20$ pc) and 2 km/s velocity resolution obtained with the Atacama Large\nMillimeter/submillimeter Array (ALMA).\n  The $I$(CO)-$N$(H$_2$) conversion factor based on the virial method is\n$X_{\\rm CO}$ = $(2 \\pm 1 )\\times10^{20}$ cm$^{-2}$(K km/s)$^{-1}$ for the\nentire molecular disk, consistent with that of the disks of spiral galaxies\nincluding the Milky Way, and $X_{\\rm CO}$ = $(5 \\pm 2)\\times10^{20}$\ncm$^{-2}$(K km/s)$^{-1}$ for the circumnuclear disk (CND, within a\ngalactocentric radius of 200 pc).\n  We obtained the GMC mass spectrum distribution and find that the\nbest-truncated power-law fit for the whole molecular disk, with index $\\gamma\n\\simeq -2.41 \\pm 0.02$ and upper cutoff mass $\\sim 1.3 \\times 10^{7}\nM_{\\odot}$, is also in agreement with that of nearby disk galaxies. A trend is\nfound in the mass spectrum index from steep to shallow as we move to inner\nradii.\n  Although the GMCs are in an elliptical galaxy, the general GMC properties in\nthe molecular disk are as in spiral galaxies. However, in the CND, large\noffsets in the line-width-size scaling relations ($\\sim$ 0.3 dex higher than\nthose in the GMCs in the molecular disk), a different $X_{\\rm CO}$ factor, and\nthe shallowest GMC mass distribution shape ($\\gamma = -1.1 \\pm 0.2$) all\nsuggest that there the GMCs are most strongly affected by the presence of the\nAGN and/or shear motions.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper, we present a new verification style reading comprehension\ndataset named VGaokao from Chinese Language tests of Gaokao. Different from\nexisting efforts, the new dataset is originally designed for native speakers'\nevaluation, thus requiring more advanced language understanding skills. To\naddress the challenges in VGaokao, we propose a novel Extract-Integrate-Compete\napproach, which iteratively selects complementary evidence with a novel query\nupdating mechanism and adaptively distills supportive evidence, followed by a\npairwise competition to push models to learn the subtle difference among\nsimilar text pieces. Experiments show that our methods outperform various\nbaselines on VGaokao with retrieved complementary evidence, while having the\nmerits of efficiency and explainability. Our dataset and code are released for\nfurther research.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Studies of twisted morie systems have been mainly focused on 2D materials\nlike graphene with Dirac points and transition-metal-dichalcogenide so far.\nHere we propose a new twisted bilayer of 2D systems which feature\nquadratic-band-touching points and find exotic physics different from\npreviously studied twisted morie systems. Specifically, we show that exactly\nflat bands can emerge at magic angles and, more interestingly, each flat band\nexhibits a high Chern number ($C=\\pm 2$) which was not realized in bilayer\nmorie systems before. We further consider the effect of Coulomb interactions in\nsuch magic-angle twisted system and find that the ground state supports the\nquantum anomalous Hall effect with quantized Hall conductivity\n$2\\frac{e^2}{hc}$ at certain filling. Furthermore, possible physical\nrealization of such twisted bilayer systems will be briefly discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate the properties of a strongly interacting imbalanced mixture of\nbosonic $^{41}$K impurities immersed in a Fermi sea of ultracold $^6$Li atoms.\nThis enables us to explore the Fermi polaron scenario for large impurity\nconcentrations including the case where they form a Bose-Einstein condensate.\nThe system is characterized by means of radio-frequency injection spectroscopy\nand interspecies interactions are widely tunable by means of a\nwell-characterized Feshbach resonance. We find that the energy of the Fermi\npolarons formed in the thermal fraction of the impurity cloud remains rather\ninsensitive to the impurity concentration, even as we approach equal densities\nfor both species. The apparent insensitivity to high concentration is\nconsistent with a theoretical prediction, based on Landau's quasiparticle\ntheory, of a weak effective interaction between the polarons. The condensed\nfraction of the bosonic $^{41}$K gas is much denser than its thermal component,\nwhich leads to a break-down of the Fermi polaron description. Instead, we\nobserve a new branch in the radio-frequency spectrum with a small energy shift,\nwhich is consistent with the presence of Bose polarons formed by $^{6}$Li\nfermions inside the $^{41}$K condensate. A closer investigation of the behavior\nof the condensate by means of Rabi oscillation measurements support this\nobservation, indicating that we have realized Fermi and Bose polarons, two\nfundamentally different quasiparticles, in one cloud.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the spirit of Lehmer's speculation that Ramanujan's tau-function never\nvanishes, it is natural to ask whether any given integer $\\alpha$ is a value of\n$\\tau(n)$. For odd $\\alpha$, Murty, Murty, and Shorey proved that $\\tau(n)\\neq\n\\alpha$ for sufficiently large $n$. Several recent papers have identified\nexplicit examples of odd $\\alpha$ which are not tau-values. Here we apply these\nresults (most notably the recent work of Bennett, Gherga, Patel, and Siksek) to\noffer the first examples of even integers that are not tau-values. Namely, for\nprimes $\\ell$ we find that $$ \\tau(n)\\not \\in \\{ \\pm 2\\ell \\ : \\ 3\\leq \\ell<\n100\\} \\cup \\{\\pm 2\\ell^2 \\ : \\ 3\\leq \\ell <100\\} \\cup \\{\\pm 2\\ell^3 \\ : \\ 3\\leq\n\\ell<100\\ {\\text {\\rm with $\\ell\\neq 59$}}\\}.$$ Moreover, we obtain such\nresults for infinitely many powers of each prime $3\\leq \\ell<100$. As an\nexample, for $\\ell=97$ we prove that $$\\tau(n)\\not \\in \\{ 2\\cdot 97^j \\ : \\\n1\\leq j\\not \\equiv 0\\pmod{44}\\}\\cup \\{-2\\cdot 97^j \\ : \\ j\\geq 1\\}.$$ The\nmethod of proof applies mutatis mutandis to newforms with residually reducible\nmod 2 Galois representation and is easily adapted to generic newforms with\ninteger coefficients.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We consider a model for repeated stochastic matching where compatibility is\nprobabilistic, is realized the first time agents are matched, and persists in\nthe future. Such a model has applications in the gig economy, kidney exchange,\nand mentorship matching.\n  We ask whether a $decentralized$ matching process can approximate the optimal\nonline algorithm. In particular, we consider a decentralized $stable$\n$matching$ process where agents match with the most compatible partner who does\nnot prefer matching with someone else, and known compatible pairs continue\nmatching in all future rounds. We demonstrate that the above process provides a\n0.316-approximation to the optimal online algorithm for matching on general\ngraphs. We also provide a $\\frac{1}{7}$-approximation for many-to-one bipartite\nmatching, a $\\frac{1}{11}$-approximation for capacitated matching on general\ngraphs, and a $\\frac{1}{2k}$-approximation for forming teams of up to $k$\nagents. Our results rely on a novel coupling argument that decomposes the\nsuccessful edges of the optimal online algorithm in terms of their\nround-by-round comparison with stable matching.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Spectre vulnerabilities violate our fundamental assumptions about\narchitectural abstractions, allowing attackers to steal sensitive data despite\npreviously state-of-the-art countermeasures. To defend against Spectre,\ndevelopers of verification tools and compiler-based mitigations are forced to\nreason about microarchitectural details such as speculative execution. In order\nto aid developers with these attacks in a principled way, the research\ncommunity has sought formal foundations for speculative execution upon which to\nrebuild provable security guarantees.\n  This paper systematizes the community's current knowledge about software\nverification and mitigation for Spectre. We study state-of-the-art software\ndefenses, both with and without associated formal models, and use a cohesive\nframework to compare the security properties each defense provides. We explore\na wide variety of tradeoffs in the expressiveness of formal frameworks, the\ncomplexity of defense tools, and the resulting security guarantees. As a result\nof our analysis, we suggest practical choices for developers of analysis and\nmitigation tools, and we identify several open problems in this area to guide\nfuture work on grounded software defenses.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The atomic masses of the isotopes $^{206,207}$Ra have been measured via\ndecay-correlated mass spectroscopy using a multi-reflection time-of-flight mass\nspectrograph equipped with an $\\alpha$-TOF detector. The Ra isotopes were\nproduced as fusion-evaporation products in the $^{51}$V+$^{159}$Tb reaction\nsystem and delivered by the gas-filled recoil ion separator GARIS-II at RIKEN.\nThe $\\alpha$-TOF detector provides for high-accuracy mass measurements by\ncorrelating time-of-flight signals with subsequent $\\alpha$-decay events. The\nmasses of $^{206}$Ra and $^{207g,m}$Ra were directly measured using a\nmulti-reflection time-of-flight mass spectrograph equipped with an $\\alpha$-TOF\ndetector. A mass excess of ME = 3538(15) keV/c$^2$ and an excitation energy of\nE$_{\\rm ex}$ = 552(42) keV were determined. The $\\alpha$-decay branching ratio\nof $^{207m}$Ra, b$\\alpha$ = 0.26(20), was directly determined from\ndecay-correlated time-of-flight signals, and the reduced alpha width of\n$^{207m}$Ra was calculated to be $\\delta^2$ = 50+62-41 keV from the branching\nratio. The spin-parity of $^{207m}$Ra was confirmed to be $J^\\pi$ = 13/2$^-$\nfrom decay correlated mass measurement results.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  At optical wavelengths, blazar Electric Vector Position Angle (EVPA)\nrotations linked with gamma-ray activity have been the subject of intense\ninterest and systematic investigation for over a decade. One difficulty in the\ninterpretation of EVPA rotations is the inherent 180{\\deg} ambiguity in the\nmeasurements. It is therefore essential, when studying EVPA rotations, to\nensure that the typical time-interval between successive observations -- i.e.\nthe cadence -- is short enough to ensure that the correct modulo 180{\\deg}\nvalue is selected. This optimal cadence depends on the maximum intrinsic EVPA\nrotation speed in blazars, which is currently not known. In this paper we\naddress the following questions for the RoboPol sample: What range of rotation\nspeeds for rotations greater than 90{\\deg} can we expect? What observation\ncadence is required to detect such rotations? Have rapid rotations been missed\nin EVPA rotation studies thus far? What fraction of data is affected by the\nambiguity? And how likely are detected rotations affected by the ambiguity? We\nanswer these questions with three seasons of optical polarimetric observations\nof a statistical sample of blazars sampled weekly with the RoboPol instrument\nand an additional season with daily observations. We model the distribution of\nEVPA changes on time scales from 1-30 days and estimate the fraction of changes\nexceeding 90{\\deg}. We show that at least daily observations are necessary to\nmeasure >96% of optical EVPA variability in the RoboPol sample of blazars\ncorrectly and that intra-day observations are needed to measure the fastest\nrotations that have been seen thus far.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Transformer becomes prevalent in computer vision, especially for high-level\nvision tasks. However, adopting Transformer in the generative adversarial\nnetwork (GAN) framework is still an open yet challenging problem. In this\npaper, we conduct a comprehensive empirical study to investigate the properties\nof Transformer in GAN for high-fidelity image synthesis. Our analysis\nhighlights and reaffirms the importance of feature locality in image\ngeneration, although the merits of the locality are well known in the\nclassification task. Perhaps more interestingly, we find the residual\nconnections in self-attention layers harmful for learning Transformer-based\ndiscriminators and conditional generators. We carefully examine the influence\nand propose effective ways to mitigate the negative impacts. Our study leads to\na new alternative design of Transformers in GAN, a convolutional neural network\n(CNN)-free generator termed as STrans-G, which achieves competitive results in\nboth unconditional and conditional image generations. The Transformer-based\ndiscriminator, STrans-D, also significantly reduces its gap against the\nCNN-based discriminators.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop a method for describing the tropical complete intersection of a\ntropical hypersurface and a tropical plane in $\\mathbb{R}^3$. This involves a\nmethod for determining the topological type of the intersection of a tropical\nplane curve and $\\mathbb{R}_{\\leq 0}^2$ by using a polyhedral complex. As an\napplication, we study smooth tropical complete intersection curves of genus $3$\nin $\\mathbb{R}^3$. In particular, we show that there are no smooth tropical\ncomplete intersection curves in $\\mathbb{R}^3$ whose skeletons are the lollipop\ngraph of genus $3$. This gives a partial answer to a problem of Morrison.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In this paper we explore the scientific synergies between Athena and some of\nthe key multi-messenger facilities that should be operative concurrently with\nAthena. These facilities include LIGO A+, Advanced Virgo+ and future detectors\nfor ground-based observation of gravitational waves (GW), LISA for space-based\nobservations of GW, IceCube and KM3NeT for neutrino observations, and CTA for\nvery high energy observations. These science themes encompass pressing issues\nin astrophysics, cosmology and fundamental physics such as: the central engine\nand jet physics in compact binary mergers, accretion processes and jet physics\nin Super-Massive Binary Black Holes (SMBBHs) and in compact stellar binaries,\nthe equation of state of neutron stars, cosmic accelerators and the origin of\nCosmic Rays (CRs), the origin of intermediate and high-Z elements in the\nUniverse, the Cosmic distance scale and tests of General Relativity and the\nStandard Model. Observational strategies for implementing the identified\nscience topics are also discussed.\n  A significant part of the sources targeted by multi-messenger facilities is\nof transient nature. We have thus also discussed the synergy of \\textsl{Athena}\nwith wide-field high-energy facilities, taking THESEUS as a case study for\ntransient discovery. This discussion covers all the Athena science goals that\nrely on follow-up observations of high-energy transients identified by external\nobservatories, and includes also topics that are not based on multi-messenger\nobservations, such as the search for missing baryons or the observation of\nearly star populations and metal enrichment at the cosmic dawn with Gamma-Ray\nBursts (GRBs).\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  While Deep Neural Networks (DNNs) excel in many tasks, the huge training\nresources they require become an obstacle for practitioners to develop their\nown models. It has become common to collect data from the Internet or hire a\nthird party to train models. Unfortunately, recent studies have shown that\nthese operations provide a viable pathway for maliciously injecting hidden\nbackdoors into DNNs. Several defense methods have been developed to detect\nmalicious samples, with the common assumption that the latent representations\nof benign and malicious samples extracted by the infected model exhibit\ndifferent distributions. However, a comprehensive study on the distributional\ndifferences is missing. In this paper, we investigate such differences\nthoroughly via answering three questions: 1) What are the characteristics of\nthe distributional differences? 2) How can they be effectively reduced? 3) What\nimpact does this reduction have on difference-based defense methods? First, the\ndistributional differences of multi-level representations on the regularly\ntrained backdoored models are verified to be significant by introducing Maximum\nMean Discrepancy (MMD), Energy Distance (ED), and Sliced Wasserstein Distance\n(SWD) as the metrics. Then, ML-MMDR, a difference reduction method that adds\nmulti-level MMD regularization into the loss, is proposed, and its\neffectiveness is testified on three typical difference-based defense methods.\nAcross all the experimental settings, the F1 scores of these methods drop from\n90%-100% on the regularly trained backdoored models to 60%-70% on the models\ntrained with ML-MMDR. These results indicate that the proposed MMD\nregularization can enhance the stealthiness of existing backdoor attack\nmethods. The prototype code of our method is now available at\nhttps://github.com/xpf/Multi-Level-MMD-Regularization.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Local available quantum correlations (LAQC), as defined by Mundarain et al.,\nare analyzed for 2-qubit X states with local Bloch vectors of equal magnitude.\nSymmetric X-states are invariant under the exchange of subsystems, hence having\nthe same {local} Bloch vector. On the other hand, anti-symmetric X states have\n{local} Bloch vectors with an equal magnitude but opposite direction\n{(anti-parallel)}. In both cases, we obtain exact analytical expressions for\ntheir LAQC quantifier. We present some examples and compare this quantum\ncorrelation to concurrence and quantum discord. We have also included Markovian\ndecoherence, with Werner states under amplitude damping decoherence. As is the\ncase for depolarization and phase damping, no sudden death behavior occurs for\nthe LAQC of these states with this quantum channel.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Numerous algorithms have been proposed for detecting anomalies (outliers,\nnovelties) in an unsupervised manner. Unfortunately, it is not trivial, in\ngeneral, to understand why a given sample (record) is labelled as an anomaly\nand thus diagnose its root causes. We propose the following\nreduced-dimensionality, surrogate model approach to explain detector decisions:\napproximate the detection model with another one that employs only a small\nsubset of features. Subsequently, samples can be visualized in this\nlow-dimensionality space for human understanding. To this end, we develop\nPROTEUS, an AutoML pipeline to produce the surrogate model, specifically\ndesigned for feature selection on imbalanced datasets. The PROTEUS surrogate\nmodel can not only explain the training data, but also the out-of-sample\n(unseen) data. In other words, PROTEUS produces predictive explanations by\napproximating the decision surface of an unsupervised detector. PROTEUS is\ndesigned to return an accurate estimate of out-of-sample predictive performance\nto serve as a metric of the quality of the approximation. Computational\nexperiments confirm the efficacy of PROTEUS to produce predictive explanations\nfor different families of detectors and to reliably estimate their predictive\nperformance in unseen data. Unlike several ad-hoc feature importance methods,\nPROTEUS is robust to high-dimensional data.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  GNNs are a paradigm-shifting neural architecture to facilitate the learning\nof complex multi-agent behaviors. Recent work has demonstrated remarkable\nperformance in tasks such as flocking, multi-agent path planning and\ncooperative coverage. However, the policies derived through GNN-based learning\nschemes have not yet been deployed to the real-world on physical multi-robot\nsystems. In this work, we present the design of a system that allows for fully\ndecentralized execution of GNN-based policies. We create a framework based on\nROS2 and elaborate its details in this paper. We demonstrate our framework on a\ncase-study that requires tight coordination between robots, and present\nfirst-of-a-kind results that show successful real-world deployment of GNN-based\npolicies on a decentralized multi-robot system relying on Adhoc communication.\nA video demonstration of this case-study, as well as the accompanying source\ncode repository, can be found online.\nhttps://www.youtube.com/watch?v=COh-WLn4iO4\nhttps://github.com/proroklab/ros2_multi_agent_passage\nhttps://github.com/proroklab/rl_multi_agent_passage\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It is widely believed that super-Eddington accretion flow can produce\npowerful outflow, but where it originates from and how much mass and energy are\ncarried away to which directions? To answer to these questions, we newly\nperform a large-box, two-dimensional radiation hydrodynamic simulation, paying\nspecial attention lest the results should depend on adopted initial and\nboundary conditions. We could achieve a quasi-steady state in an\nunprecedentedly large range, $r=2~r_{\\rm S}$-$600~r_{\\rm S}$ (with $r_{\\rm S}$\nbeing the Schwarzschild radius) from the black hole. The accretion rate onto\nthe central $10 ~M_{\\odot}$ black hole is $\\dot{M}_{\\rm BH} \\sim 180 ~L_{\\rm\nEdd}/c^{2}$, whereas the mass outflow rate is ${\\dot M}_{\\rm outflow} \\sim 24\n~L_{\\rm Edd}/c^2$ (where $L_{\\rm Edd}$ and $c$ are the Eddington luminosity and\nthe speed of light, respectively). The ratio (${\\dot M}_{\\rm outflow}/{\\dot\nM}_{\\rm BH} \\sim 0.14$) is much less than those reported previously. By careful\ninspection we find that most of outflowing gas which reach the outer boundary\noriginates from the region at $R\\lesssim140~r_{\\rm S}$, while gas at\n$140~r_{\\rm S}$-$230 ~r_{\\rm S}$ forms failed outflow. Therefore, significant\noutflow occurs inside the trapping radius $\\sim 450 ~r_{\\rm S}$. The mechanical\nenergy flux (or mass flux) reaches its maximum in the direction of $\\sim\n15^\\circ$ ($\\sim 80^\\circ$) from the rotation axis. The total mechanical\nluminosity is $L_{\\rm mec}\\sim 0.16~L_{\\rm Edd}$, while the isotropic X-ray\nluminosity varies from $L_{\\rm X}^{\\rm ISO}\\sim 2.9~L_{\\rm Edd}$, (for a\nface-on observer) to $\\sim 2.1~L_{\\rm Edd}$ (for a nearly edge-on observer).\nThe power ratio is $L_{\\rm mec}/L_{\\rm X}^{\\rm ISO}\\sim 0.05$-$0.08$, in good\nagreement with the observations of Ultra-Luminous X-ray sources surrounded by\noptical nebulae.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We establish adjunction and inversion of adjunction for log canonical centers\nof arbitrary codimension in full generality.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The process of hand washing involves complex hand movements. There are six\nprincipal sequential steps for washing hands as per the World Health\nOrganisation (WHO) guidelines. In this work, a detailed description of an\naluminium rig construction for creating a robust hand-washing dataset is\ndiscussed. The preliminary results with the help of image processing and\ncomputer vision algorithms for hand pose extraction and feature detection such\nas Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene\npose- Rub hands palm to palm was captured as an input image for running all the\nexperiments. The future work will focus upon processing the video recordings of\nhand movements captured and applying deep-learning solutions for the\nclassification of hand-hygiene stages.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  When a dual-arm robot clamps a rigid object in an environment for human\nbeings, the environment or the collaborating human will impose incidental\ndisturbance on the operated object or the robot arm, leading to clamping\nfailure, damaging the robot even hurting the human. This research proposes a\nprioritized hierarchical compliance control to simultaneously deal with the two\ntypes of disturbances in the dual-arm robot clamping. First, we use\nhierarchical quadratic programming (HQP) to solve the robot inverse kinematics\nunder the joint constraints and prioritize the compliance for the disturbance\non the object over that on the robot arm. Second, we estimate the disturbance\nforces throughout the momentum observer with the F/T sensors and adopt\nadmittance control to realize the compliances. Finally, we perform the verify\nexperiments on a 14-DOF position-controlled dual-arm robot WalkerX, clamping a\nrigid object stably while realizing the compliance against the disturbances.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Ivermectin is an antiparasitic drug that some have claimed is an effective\ntreatment for reducing Covid-19 deaths. To test this claim, two recent peer\nreviewed papers both conducted a meta-analysis on a similar set of randomized\ncontrolled trials data, applying the same classical statistical approach.\nAlthough the statistical results were similar, one of the papers (Bryant et al,\n2021) concluded that ivermectin was effective for reducing Covid-19 deaths,\nwhile the other (Roman et al, 2021) concluded that there was insufficient\nquality of evidence to support the conclusion Ivermectin was effective. This\npaper applies a Bayesian approach, to a subset of the same trial data, to test\nseveral causal hypotheses linking Covid-19 severity and ivermectin to mortality\nand produce an alternative analysis to the classical approach. Applying diverse\nalternative analysis methods which reach the same conclusions should increase\noverall confidence in the result. We show that there is strong evidence to\nsupport a causal link between ivermectin, Covid-19 severity and mortality, and:\ni) for severe Covid-19 there is a 90.7% probability the risk ratio favours\nivermectin; ii) for mild/moderate Covid-19 there is an 84.1% probability the\nrisk ratio favours ivermectin. To address concerns expressed about the veracity\nof some of the studies we evaluate the sensitivity of the conclusions to any\nsingle study by removing one study at a time. In the worst case, where\n(Elgazzar 2020) is removed, the results remain robust, for both severe and mild\nto moderate Covid-19. The paper also highlights advantages of using Bayesian\nmethods over classical statistical methods for meta-analysis. All studies\nincluded in the analysis were prior to data on the delta variant.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Stateful and stateless web tracking gathered much attention in the last\ndecade, however they were always measured separately. To the best of our\nknowledge, our study is the first to detect and measure cookie respawning with\nbrowser and machine fingerprinting. We develop a detection methodology that\nallows us to detect cookies dependency on browser and machine features. Our\nresults show that 1,150 out of the top 30, 000 Alexa websites deploy this\ntracking mechanism. We further uncover how domains collaborate to respawn\ncookies through fingerprinting. We find out that this technique can be used to\ntrack users across websites even when third-party cookies are deprecated.\nTogether with a legal scholar, we conclude that cookie respawning with browser\nfingerprinting lacks legal interpretation under the GDPR and the ePrivacy\ndirective, but its use in practice may breach them, thus subjecting it to fines\nup to 20 million euro.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Higher-order topological insulators (HOTI) are a novel topological phase\nbeyond the framework of the conventional bulk-boundary correspondence. In these\npeculiar systems, the topologically nontrivial boundary modes are characterized\nby a co-dimension of at least two. Despite several promising preliminary\nconsiderations regarding the impact of nonlinearity in such systems, the\nflourishing field of experimental HOTI research has thus far been confined to\nthe linear evolution of topological states. As such, the observation of the\ninterplay between nonlinearity and the dynamics of higher-order topological\nphases in conservative systems remains elusive. In our work, we experimentally\ndemonstrate nonlinear higher-order topological corner states. Our photonic\nplatform enables us to observe nonlinear topological corner states as well as\nthe formation of solitons in such topological structures. Our work paves the\nway towards the exploration of topological properties of matter in the\nnonlinear regime, and may herald a new class of compact devices that harnesses\nthe intriguing features of topology in an on-demand fashion.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Weak $B^-\\rightarrow D^0, \\pi^0$ and $D^-\\rightarrow {K}^0, \\pi^0$ transition\nform factors are described in both the space- and time-like momentum transfer\nregions, within a constituent-quark model. Neutrino-meson scattering and\nsemileptonic weak decays are formulated within the point form of relativistic\nquantum mechanics to end up with relativistic invariant process amplitudes from\nwhich meson transition currents and form factors are extracted in an\nunambiguous way. For space-like momentum transfers, form factors depend on the\nframe in which the $W M M^\\prime$ vertex is considered. Such a frame dependence\nis expected from a pure valence-quark picture, since a complete, frame\nindependent description of form factors is supposed to include non-valence\ncontributions. The most important of such contributions are the $Z$-graphs,\nwhich are, however, suppressed in the infinite-momentum frame ($q^2<0$). On the\nother hand, they can play a significant role in the Breit frame ($q^2<0$) and\nin the direct decay calculation ($q^2>0$), as a comparison with the\ninfinite-momentum-frame form factors (analytically continued to $q^2>0$)\nreveals. Numerical results for the analytically continued\ninfinite-momentum-frame form factors agree very well with lattice data in the\ntime-like momentum transfer region and the experimental value for the slope of\nthe $F^+_{B\\rightarrow D}$ transition form factor at zero recoil is reproduced\nsatisfactorily. These predictions satisfy heavy-quark-symmetry constraints and\ntheir $q^2$ dependence is well approximated by a pole fit, reminiscent of a\nvector-meson-dominance-like decay mechanism. We discuss how such a decay\nmechanism can be accommodated within an extension of our constituent-quark\nmodel, by allowing for a non-valence component in the meson wave functions. We\nalso address the question of wrong cluster properties inherent in the\nBakamjian-Thomas formulation.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  For any linear inequality in three variables $\\mathcal{L}$, we determine (if\nit exist) the smallest integer $R(\\mathcal{L}, \\mathbb{Z}/3\\mathbb{Z})$ such\nthat: for every mapping $\\chi :[1,n] \\to \\{0,1,2\\}$, with $n\\geq R(\\mathcal{L},\n\\mathbb{Z}/3\\mathbb{Z})$, there is a solution $(x_1,x_2,x_3)\\in [1,n]^3$ of\n$\\mathcal{L}$ with $\\chi(x_1)+\\chi(x_2)+\\chi(x_3)\\equiv 0$ (mod $3$). Moreover,\nwe prove that $R(\\mathcal{L}, \\mathbb{Z}/3\\mathbb{Z})=R(\\mathcal{L}, 2)$, where\n$R(\\mathcal{L}, 2)$ denotes the classical $2$-color Rado number, that is, the\nsmallest integer (provided it exist) such that for every $2$-coloring of\n$[1,n]$, with $n\\geq R(\\mathcal{L}, 2)$, there exist a monochromatic solution\nof $\\mathcal{L}$. Thus, we get an Erd\\H{o}s-Ginzburg-Ziv type generalization\nfor all lineal inequalities in three variables having a solution in the\npositive integers. We also show a number of families of linear equations in\nthree variables $\\mathcal{L}$ such that they do not admit such\nErd\\H{o}s-Ginzburg-Ziv type generalization, named $R(\\mathcal{L},\n\\mathbb{Z}/3\\mathbb{Z})\\neq R(\\mathcal{L}, 2)$. At the end of this paper some\nquestions are proposed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We propose and experimentally evaluate a novel secure aggregation algorithm\ntargeted at cross-organizational federated learning applications with a fixed\nset of participating learners. Our solution organizes learners in a chain and\nencrypts all traffic to reduce the controller of the aggregation to a mere\nmessage broker. We show that our algorithm scales better and is less resource\ndemanding than existing solutions, while being easy to implement on constrained\nplatforms.\n  With 36 nodes our method outperforms state-of-the-art secure aggregation by\n70x, and 56x with and without failover, respectively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In the present paper we deal with a quasilinear problem involving a singular\nterm and a parametric superlinear perturbation. We are interested in the\nexistence, nonexistence and multiplicity of positive solutions as the parameter\n$\\lambda>0$ varies. In our first result, the superlinear perturbation has an\narbitrary growth and we obtain the existence of a solution for the problem by\nusing the sub-supersolution method. For the second result, the superlinear\nperturbation has subcritical growth and we employ the Mountain Pass Theorem to\nshow the existence of a second solution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The amplitude of ground state superconducting energy gap $\\Delta(0)$ and\nrelative jump in electronic specific heat at the transition temperature,\n$\\Delta$$C$${/}$$\\gamma$$T_c$, are primary fundamental parameters of any\nsuperconductor. There are several well-established techniques to measure these\nvalues for bulk samples. However, there is limited number of techniques which\ncan be applied to measure these parameters in atomically thin superconductors.\nHere we proposed a new approach to extract $\\Delta(0)$ and\n$\\Delta$$C$${/}$$\\gamma$$T_c$ in atomically thin superconductors by utilizing\nperpendicular, Bc2,perp(T) (when magnetic field is applied in perpendicular\ndirection to the film surface), and parallel, Bc2,||(T) (when magnetic field is\napplied in parallel direction to the film surface), upper critical field data.\nDeduced parameters for few layers thick Al, Sn, NbSe2, MoS2, magic angle\ntwisted trilayer graphene (MATTG), and WTe2 are well matched values expected\nfor strong- and moderately strong-coupled electron-phonon mediated\nsuperconductors. Observed, in many atomically thin superconductors, an\nenhancement of Bc2,||(0) above the Pauli-Clogston-Chandrasekhar limiting field\n(i.e., magnetic field required to break the Cooper pair) is explained based on\nthe sample geometry, without an assumption that some exotic pairing mechanism,\nfor instance, Ising-type, is emergent in these materials.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Stability is one of the most fundamental requirements for systems synthesis.\nIn this paper, we address the stabilization problem for unknown linear systems\nvia policy gradient (PG) methods. We leverage a key feature of PG for Linear\nQuadratic Regulator (LQR), i.e., it drives the policy away from the boundary of\nthe unstabilizing region along the descent direction, provided with an initial\npolicy with finite cost. To this end, we discount the LQR cost with a factor,\nby adaptively increasing which gradient leads the policy to the stabilizing set\nwhile maintaining a finite cost. Based on the Lyapunov theory, we design an\nupdate rule for the discount factor which can be directly computed from data,\nrendering our method purely model-free. Compared to recent work\n\\citep{perdomo2021stabilizing}, our algorithm allows the policy to be updated\nonly once for each discount factor. Moreover, the number of sampled\ntrajectories and simulation time for gradient descent is significantly reduced\nto $\\mathcal{O}(\\log(1/\\epsilon))$ for the desired accuracy $\\epsilon$.\nFinally, we conduct simulations on both small-scale and large-scale examples to\nshow the efficiency of our discount PG method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We mapped all boulders larger than 105 m on the surface of dwarf planet Ceres\nusing images of the Dawn framing camera acquired in the Low Altitude Mapping\nOrbit (LAMO). We find that boulders on Ceres are more numerous towards high\nlatitudes and have a maximum lifetime of $150 \\pm 50$ Ma, based on crater\ncounts. These characteristics are distinctly different from those of boulders\non asteroid (4) Vesta, an earlier target of Dawn, which implies that Ceres\nboulders are mechanically weaker. Clues to their properties can be found in the\ncomposition of Ceres' complex crust, which is rich in phyllosilicates and\nsalts. As water ice is though to be present only meters below the surface, we\nsuggest that boulders also harbor ice. Furthermore, the boulder size-frequency\ndistribution is best fit by a Weibull distribution rather than the customary\npower law, just like for Vesta boulders. This finding is robust in light of\npossible types of size measurement error.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Chan, Har-Peled, and Jones [2020] recently developed locality-sensitive\nordering (LSO), a new tool that allows one to reduce problems in the Euclidean\nspace $\\mathbb{R}^d$ to the $1$-dimensional line. They used LSO's to solve a\nhost of problems. Later, Buchin, Har-Peled, and Ol{\\'{a}}h [2019,2020] used the\nLSO of Chan {\\em et al. } to construct very sparse \\emph{reliable spanners} for\nthe Euclidean space. A highly desirable feature of a reliable spanner is its\nability to withstand a massive failure: the network remains functioning even if\n90\\% of the nodes fail. In a follow-up work, Har-Peled, Mendel, and Ol{\\'{a}}h\n[2021] constructed reliable spanners for general and topologically structured\nmetrics. Their construction used a different approach, and is based on sparse\ncovers.\n  In this paper, we develop the theory of LSO's in non-Euclidean metrics by\nintroducing new types of LSO's suitable for general and topologically\nstructured metrics. We then construct such LSO's, as well as constructing\nconsiderably improved LSO's for doubling metrics. Afterwards, we use our new\nLSO's to construct reliable spanners with improved stretch and sparsity\nparameters. Most prominently, we construct $\\tilde{O}(n)$-size reliable\nspanners for trees and planar graphs with the optimal stretch of $2$. Along the\nway to the construction of LSO's and reliable spanners, we introduce and\nconstruct ultrametric covers, and construct $2$-hop reliable spanners for the\nline.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present the contributions to electric dipole moments (EDMs) induced by the\nYukawa couplings of an additional electroweak doublet of colour-octet scalars.\nThe full set of one-loop diagrams and the enhanced higher-order effects from\nBarr-Zee diagrams are computed for the quark (chromo-)EDM, along with the\ntwo-loop contributions to the Weinberg operator. Using the stringent\nexperimental upper limits on the neutron EDM, constraints on the parameter\nspace of this model are derived.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Long-duration quantum memories for photonic qubits are essential components\nfor achieving long-distance quantum networks and repeaters. The mapping of\noptical states onto coherent spin-waves in rare earth ensembles is a\nparticularly promising approach to quantum storage. However, it remains\nchallenging to achieve long-duration storage at the quantum level due to\nread-out noise caused by the required spin-wave manipulation. In this work, we\napply dynamical decoupling techniques and a small magnetic field to achieve the\nstorage of six temporal modes for 20, 50 and 100 ms in a\n$^{151}$Eu$^{3+}$:Y$_2$SiO$_5$ crystal, based on an atomic frequency comb\nmemory, where each temporal mode contains around one photon on average. The\nquantum coherence of the memory is verified by storing two time-bin qubits for\n20 ms, with an average memory output fidelity of $F=(85\\pm 2)\\%$ for an average\nnumber of photons per qubit of $\\mu_\\text{in}$ = 0.92$\\pm$0.04. The qubit\nanalysis is done at the read-out of the memory, using a type of composite\nadiabatic read-out pulse we developed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Empowering conventional materials with unexpected magnetoelectric properties\nis appealing to the multi-functionalization of existing devices and the\nexploration of future electronics. Recently, owing to its unique effect in\nmodulating a matter's properties, ultra-small dopants, e.g. H, D, and Li,\nattract enormous attention in creating emergent functionalities, such as\nsuperconductivity, and metal-insulator transition. Here, we report an\nobservation of bipolar conduction accompanied by a giant positive\nmagnetoresistance in D-doped metallic Ti oxide (TiOxDy) films. To overcome the\nchallenges in intercalating the D into a crystalline oxide, a series of TiOxDy\nwere formed by sequentially doping Ti with D and surface/interface oxidation.\nIntriguingly, while the electron mobility of the TiOxDy increases by an order\nof magnitude larger after doping, the emergent holes also exhibit high\nmobility. Moreover, the bipolar conduction induces a giant magnetoresistance up\nto 900% at 6 T, which is ~6 times higher than its conventional phase. Our study\npaves a way to empower conventional materials in existing electronics and\ninduce novel electronic phases.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We are considering a two-stage optimal scheduling problem, which involves two\nsimilar projects with the same starting times for workers and the same\ndeadlines for tasks. It is required that the starting times for workers and\ndeadlines for tasks should be optimal for the first-stage project and, under\nthis condition, also for the second-stage project. Optimality is measured with\nrespect to the maximal lateness (or maximal delay) of tasks, which has to be\nminimized. We represent this problem as a problem of tropical pseudoquadratic\noptimization and show how the existing methods of tropical optimization and\ntropical linear algebra yield a full and explicit solution for this problem.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper is concerned with the linear theory of thermoelasticity with\nmicrotemperatures, based on the entropy balance proposed by Green and Naghdi,\nwhich permits the transmission of heat as thermal waves of finite speed. We\nanalyze the behavior of Rayleigh waves in an unbounded isotropic homogeneous\nstrongly elliptic thermoelastic material with microtemperatures. The related\nsolution of the Rayleigh surface wave problem is expressed as a linear\ncombination of the elements of the bases of the kernels of appropriate\nmatrices. The secular equation is established and afterwards an explicit form\nis written when some coupling constitutive coefficients vanish. Then, we solve\nnumerically the secular equation by mean of a grafical metod and by taking\narbitrary data for strongly elliptic thermoelastic material.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  H2-conic controller design seeks to minimize the closed-loop H2-norm for a\nnominal linear system while satisfying the Conic Sector Theorem for nonlinear\nstability. This problem has only been posed with limited design freedom, as\nopposed to fixed-order design where all controller parameters except the number\nof state estimates are free variables. Here, the fixed-order H2-conic design\nproblem is reformulated as a convergent series of convex approximations using\niterative convex overbounding. A synthesis algorithm and various\ninitializations are proposed. The synthesis is applied to a passivity-violated\nsystem with uncertain parameters and compared to benchmark controller designs.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The blazar Mrk 421 shows frequent, short flares in the TeV energy regime. Due\nto the fast nature of such episodes, we often fail to obtain sufficient\nsimultaneous information about flux variations in several energy bands. To\novercome this lack of multi-wavelength (MWL) coverage, especially for the pre-\nand post-flare periods, we have set up a monitoring program with the FACT\ntelescope (TeV energies) and the Neil Gehrels Swift Observatory (X-rays). On\n2019 June 9, Mrk 421 showed a TeV outburst reaching a flux level of more than\ntwo times the flux of the Crab Nebula at TeV energies. We acquired simultaneous\ndata in the X-rays with additional observations by XMM-Newton and INTEGRAL. For\nthe first time, we can study a TeV blazar in outburst taking advantage of\nhighly sensitive X-ray data from XMM-Newton and INTEGRAL combined. Our dataset\nis complemented by pointed radio observations by Effelsberg at GHz frequencies.\nWe present our first results, including the {\\gamma}-ray and X-ray light\ncurves, a timing analysis of the X-ray data obtained with XMM-Newton , as well\nas the radio spectra before, during and after the flare.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We establish a novel framework for learning a directed acyclic graph (DAG)\nwhen data are generated from a Gaussian, linear structural equation model. It\nconsists of two parts: (1) introduce a permutation matrix as a new parameter\nwithin a regularized Gaussian log-likelihood to represent variable ordering;\nand (2) given the ordering, estimate the DAG structure through sparse Cholesky\nfactor of the inverse covariance matrix. For permutation matrix estimation, we\npropose a relaxation technique that avoids the NP-hard combinatorial problem of\norder estimation. Given an ordering, a sparse Cholesky factor is estimated\nusing a cyclic coordinatewise descent algorithm which decouples row-wise. Our\nframework recovers DAGs without the need for an expensive verification of the\nacyclicity constraint or enumeration of possible parent sets. We establish\nnumerical convergence of the algorithm, and consistency of the Cholesky factor\nestimator when the order of variables is known. Through several simulated and\nmacro-economic datasets, we study the scope and performance of the proposed\nmethodology.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Legged robot navigation in extreme environments can hinder the use of cameras\nand laser scanners due to darkness, air obfuscation or sensor damage. In these\nconditions, proprioceptive sensing will continue to work reliably. In this\npaper, we propose a purely proprioceptive localization algorithm which fuses\ninformation from both geometry and terrain class, to localize a legged robot\nwithin a prior map. First, a terrain classifier computes the probability that a\nfoot has stepped on a particular terrain class from sensed foot forces. Then, a\nMonte Carlo-based estimator fuses this terrain class probability with the\ngeometric information of the foot contact points. Results are demonstrated\nshowing this approach operating online and onboard a ANYmal B300 quadruped\nrobot traversing a series of terrain courses with different geometries and\nterrain types over more than 1.2km. The method keeps the localization error\nbelow 20cm using only the information coming from the feet, IMU, and joints of\nthe quadruped.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  To benefit the learning of a new task, meta-learning has been proposed to\ntransfer a well-generalized meta-model learned from various meta-training\ntasks. Existing meta-learning algorithms randomly sample meta-training tasks\nwith a uniform probability, under the assumption that tasks are of equal\nimportance. However, it is likely that tasks are detrimental with noise or\nimbalanced given a limited number of meta-training tasks. To prevent the\nmeta-model from being corrupted by such detrimental tasks or dominated by tasks\nin the majority, in this paper, we propose an adaptive task scheduler (ATS) for\nthe meta-training process. In ATS, for the first time, we design a neural\nscheduler to decide which meta-training tasks to use next by predicting the\nprobability being sampled for each candidate task, and train the scheduler to\noptimize the generalization capacity of the meta-model to unseen tasks. We\nidentify two meta-model-related factors as the input of the neural scheduler,\nwhich characterize the difficulty of a candidate task to the meta-model.\nTheoretically, we show that a scheduler taking the two factors into account\nimproves the meta-training loss and also the optimization landscape. Under the\nsetting of meta-learning with noise and limited budgets, ATS improves the\nperformance on both miniImageNet and a real-world drug discovery benchmark by\nup to 13% and 18%, respectively, compared to state-of-the-art task schedulers.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Thermodynamical description of the system created during high energy\ncollision requires a proper thermodynamical framework to study the distribution\nof particles. In this work, we have attempted to explain the transverse\nmomentum spectra of charged hadrons formed in $pp$ collision at different\nenergies using the Pearson statistical framework. This formalism has been\nproved to nicely explain the spectra of particles produced in soft processes as\nwell hard scattering processes in a consistent manner. For this analysis, we\nhave used the highest available range of $p_T$ published by experiments to\nverify the applicability of Pearson statistical framework at large $p_T$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Given two graph families $\\mathcal H_1$ and $\\mathcal H_2$, a size Ramsey\ngame is played on the edge set of $K_\\mathbb{N}$. In every round, Builder\nselects an edge and Painter colours it red or blue. Builder is trying to force\nPainter to create as soon as possible a red copy of a graph from $\\mathcal H_1$\nor a blue copy of a graph from $\\mathcal H_2$. The online (size) Ramsey number\n$\\tilde{r}(\\mathcal H_1,\\mathcal H_2)$ is the smallest number of rounds in the\ngame provided Builder and Painter play optimally. We prove that if $\\mathcal\nH_1$ is the family of all odd cycles and $\\mathcal H_2$ is the family of all\nconnected graphs on $n$ vertices and $m$ edges, then $\\tilde{r}(\\mathcal\nH_1,\\mathcal H_2)\\ge \\varphi n + m-2\\varphi+1$, where $\\varphi$ is the golden\nratio, and for $n\\ge 3$, $m\\le (n-1)^2/4$ we have $\\tilde{r}(\\mathcal\nH_1,\\mathcal H_2)\\le n+2m+O(\\sqrt{m-n+1})$. We also show that\n$\\tilde{r}(C_3,P_n)\\le 3n-4$ for $n\\ge 3$. As a consequence we get $2.6n-3\\le\n\\tilde{r}(C_3,P_n)\\le 3n-4$ for every $n\\ge 3$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  It is often challenging to solve a complex problem from scratch, but much\neasier if we can access other similar problems with their solutions -- a\nparadigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR\napproach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA\nconsists of a nonparametric memory that stores cases (question and logical\nforms) and a parametric model that can generate a logical form for a new\nquestion by retrieving cases that are relevant to it. On several KBQA datasets\nthat contain complex questions, CBR-KBQA achieves competitive performance. For\nexample, on the ComplexWebQuestions dataset, CBR-KBQA outperforms the current\nstate of the art by 11\\% on accuracy. Furthermore, we show that CBR-KBQA is\ncapable of using new cases \\emph{without} any further training: by\nincorporating a few human-labeled examples in the case memory, CBR-KBQA is able\nto successfully generate logical forms containing unseen KB entities as well as\nrelations.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present the design of the prototype telescope and spectrograph system for\nthe Affordable Multiple Aperture Spectroscopy Explorer (AMASE) project. AMASE\nis a planned project that will pair 100 identical multi-fiber spectrographs\nwith a large array of telephoto lenses to achieve a large area integral field\nspectroscopy survey of the sky at the spatial resolution of half an arcminute\nand a spectral resolution of R=15,000, covering important emission lines in the\noptical for studying the ionized gas in the Milky Way and beyond. The project\nwill be enabled by a significant reduction in the cost of each spectrograph\nunit, which is achieved by reducing the beam width and the use of small-pixel\nCMOS detectors, 50um-core optical fibers, and commercial photographic lenses in\nthe spectrograph. Although constrained by the challenging high spectral\nresolution requirement, we realize a 40% reduction in cost per fiber at\nconstant etendue relative to, e.g., DESI. As the reduction of cost is much more\nsignificant than the reduction in the amount of light received per fiber,\nreplicating such a system many times is more cost effective than building a\nsingle large spectrograph that achieves the same survey speed. We present the\ndesign of the prototype telescope and instrument system and the study of its\ncost effectiveness.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Extremely variable quasars (EVQs) are a population of sources showing large\noptical photometric variability revealed by time-domain surveys. The physical\norigin of such extreme variability is yet unclear. In this first paper of a\nseries, we construct the largest-ever sample of 14,012 EVQs using photometric\ndata spanning over $>$ 15 years from SDSS and Pan-STARRS1. We divide them into\nfive sub-samples according to the relative brightness of an EVQ during SDSS\nspectroscopic observation compared to the mean brightness from photometric\nobservations. Corresponding control samples of normal quasars are built with\nmatched redshift, bolometric luminosity and supermassive black hole mass. We\nobtain the composite SDSS spectra of EVQs in various states and their\ncorresponding control samples. We find EVQs exhibit clearly bluer (redder) SDSS\nspectra during bright (dim) states, consistent with the \"bluer-when-brighter\"\ntrend widely seen in normal quasars. We further find the line EWs of broad\nMgII, CIV and OIII (but not broad Hb which is yet puzzling) gradually decrease\nfrom dim state to bright state, similar to the so-called intrinsic Baldwin\neffect commonly seen in normal AGNs. Meanwhile, EVQs have systematically larger\nline EWs compared with the control samples. We also see that EVQs exhibit\nsubtle excess in the very broad line component compared with control samples.\nPossible explanations for the discoveries are discussed. Our findings support\nthe hypothesis that EVQs are in the tail of a broad distribution of quasar\nproperties, but not a distinct population.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  A system composed of two-level systems interacting with a single excitation\nof a one-dimensional boson field with continuous spectrum, described by a\nFriedrichs (or Friedrichs-Lee) model, can exhibit bound states and resonances;\nthe latter can be characterized by computing the so-called self-energy of the\nmodel. We evaluate an analytic expression, valid for a large class of\ndispersion relations and coupling functions, for the self-energy of such\nmodels. Afterwards, we focus on the case of identical two-level systems, and we\nrefine our analysis by distinguishing between dominant and suppressed\ncontributions to the associated self-energy; we finally examine the\nphenomenology of bound states in the presence of a single dominant\ncontribution.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Image segmentation is a fundamental and challenging task in image processing\nand computer vision. The color image segmentation is attracting more attention\ndue to the color image provides more information than the gray image. In this\npaper, we propose a variational model based on a convex K-means approach to\nsegment color images. The proposed variational method uses a combination of\n$l_1$ and $l_2$ regularizers to maintain edge information of objects in images\nwhile overcoming the staircase effect. Meanwhile, our one-stage strategy is an\nimproved version based on the smoothing and thresholding strategy, which\ncontributes to improving the accuracy of segmentation. The proposed method\nperforms the following steps. First, we specify the color set which can be\ndetermined by human or the K-means method. Second, we use a variational model\nto obtain the most appropriate color for each pixel from the color set via\nconvex relaxation and lifting. The Chambolle-Pock algorithm and simplex\nprojection are applied to solve the variational model effectively. Experimental\nresults and comparison analysis demonstrate the effectiveness and robustness of\nour method.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The COVID-19 pandemic has recently emerged as a worldwide health emergency\nthat necessitates coordinated international measures. To contain the virus's\nspread, governments and health organisations raced to develop vaccines that\nwould lower Covid-19 morbidity, relieve pressure on healthcare systems, and\nallow economies to open. As a way forward after the COVID-19 vaccination, the\nVaccination certificate has been adopted to help the authorities formulate\npolicies by controlling cross-border travelling. To resolve significant privacy\nconcerns and remove the need for relying on third parties to maintain trust and\ncontrol the user's data, in this paper, we leverage blockchain technologies in\ndeveloping a secure and verifiable vaccination certificate. Our approach has\nthe advantage of utilising a hybrid architecture that implements different\nadvanced technologies, such as smart contracts, interPlanetary File System\n(IPFS), and Self-sovereign Identity (SSI). We will rely on verifiable\ncredentials paired with smart contracts to implement on-chain access control\ndecisions and provide on-chain verification and validation of the user and\nissuer DIDs. The usability of this approach was further analysed, particularly\nconcerning performance and security. Our analysis proved that our approach\nsatisfies vaccination certificate security requirements.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Nonlocal cross-diffusion systems on the torus, arising in population dynamics\nand neuroscience, are analyzed. The global existence of weak solutions, the\nweak-strong uniqueness, and the localization limit are proved. The kernels are\nassumed to be positive definite and in detailed balance. The proofs are based\non entropy estimates coming from Shannon-type and Rao-type entropies, while the\nweak-strong uniqueness result follows from the relative entropy method. The\nexistence and uniqueness theorems hold for nondifferentiable kernels. The\nassociated local cross-diffusion system, derived in the localization limit, is\nalso discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We develop the kinetic theory of collisionless relaxation for systems with\nlong-range interactions in relation to the statistical theory of Lynden-Bell.\nWe treat the multi-level case. We make the connection between the kinetic\nequation obtained from the quasilinear theory of the Vlasov equation and the\nrelaxation equation obtained from a maximum entropy production principle. We\npropose a method to close the infinite hierarchy of kinetic equations for the\nphase level moments and obtain a kinetic equation for the coarse-grained\ndistribution function in the form of a generalized Landau, Lenard-Balescu or\nKramers equation associated with a generalized form of entropy [P.H. Chavanis,\nPhysica A {\\bf 332}, 89 (2004)]. This allows us to go beyond the two-level case\nassociated with a Fermi-Dirac-type entropy. We discuss the numerous analogies\nwith two-dimensional turbulence. We also mention possible applications of the\npresent formalism to fermionic and bosonic dark matter halos.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  \"Changing-look\" quasars are a new class of highly variable active galactic\nnuclei that have changed their spectral type over surprisingly short timescales\nof just a few years. The origin of this phenomenon is debated, but is likely to\nreflect some change in the accretion flow. To investigate the disk-corona\nsystems in these objects, we measure optical/UV-X-ray spectral indices\n($\\alpha_{\\rm OX}$) and Eddington ratios ($\\lambda_{\\rm Edd}$) of ten\npreviously-discovered changing-look quasars at two or more epochs. By comparing\nthese data with simulated results based on the behavior of X-ray binaries, we\nfind possible similarities in spectral indices below 1% Eddington ratio. We\nfurther investigate the Eddington ratios of changing-look quasars before and\nafter their spectral type changes, and find that changing-look quasars cross\nthe 1% Eddington ratio boundary when their broad emission lines\ndisappear/emerge. This is consistent with the disk-wind model as the origin of\nbroad emission lines.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Using a hydrodynamic approach, we show that charge diffusion in two\ndimensional Coulomb interacting liquids with broken momentum conservation is\nintrinsically anomalous. The charge relaxation is governed by an overdamped,\nsuperdiffusive plasmon mode. We demonstrate that the diffusing particles follow\nL\\'evy flight trajectories, and study the hydrodynamic collective modes under\nthe influence of magnetic fields. The latter are shown to slow down the\nsuperdiffusive process. The results are argued to be relevant to electron\nliquids in solids, as well as plasmas.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Although recent advances in deep learning accelerated an improvement in a\nweakly supervised object localization (WSOL) task, there are still challenges\nto identify the entire body of an object, rather than only discriminative\nparts. In this paper, we propose a novel residual fine-grained attention (RFGA)\nmodule that autonomously excites the less activated regions of an object by\nutilizing information distributed over channels and locations within feature\nmaps in combination with a residual operation. To be specific, we devise a\nseries of mechanisms of triple-view attention representation, attention\nexpansion, and feature calibration. Unlike other attention-based WSOL methods\nthat learn a coarse attention map, having the same values across elements in\nfeature maps, our proposed RFGA learns fine-grained values in an attention map\nby assigning different attention values for each of the elements. We validated\nthe superiority of our proposed RFGA module by comparing it with the recent\nmethods in the literature over three datasets. Further, we analyzed the effect\nof each mechanism in our RFGA and visualized attention maps to get insights.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  The concept of a covering system was first introduced by Erd\\H{o}s in 1950.\nSince their introduction, a lot of the research regarding covering systems has\nfocused on the existence of covering systems with certain restrictions on the\nmoduli. Arguably, the most famous open question regarding covering systems is\nthe odd covering problem. In this paper, we explore a variation of the odd\ncovering problem, allowing a single odd prime to appear as a modulus in the\ncovering more than once, while all other moduli are distinct, odd, and greater\nthan $1$. We also consider this variation while further requiring the moduli of\nthe covering system to be square-free.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Several Type Ia supernova analyses make use of non-simultaneous regressions\nbetween observed supernova and host galaxy properties and supernova luminosity:\nfirst the supernova magnitudes are corrected for their light curve shape and\ncolor, and then they are separately corrected for their host galaxy masses.\nThis two-step regression methodology does not introduce any biases when there\nare no correlations between the variables regressed in each correction step.\nHowever, correlations between these covariates will bias estimates of the size\nof the corrections, as well as estimates of the variance of the final\nresiduals. In this work, we analyze the general case of non-simultaneous\nregression with correlated covariates to derive the functional forms of these\nbiases. We also simulate this effect on data from the literature to provide\ncorrections to remove these biases from the data sets studied. The biases\nexamined here can be entirely avoided by using simultaneous regression\ntechniques.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Indeed, the global production (as a system of creating values) is eventually\nforming like a gigantic and complex network/web of value chains that explains\nthe transitional structures of global trade and development of the global\neconomy. It's truly a new wave of globalisation, and we term it as the global\nvalue chains (GVCs), creating the nexus among firms, workers and consumers\naround the globe. The emergence of this new scenario asks: how an economy's\nfirms, producers and workers connect in the global economy. And how are they\ncapturing the gains out of it in terms of different dimensions of economic\ndevelopment? This GVC approach is very crucial for understanding the\norganisation of the global industries and firms. It requires the statics and\ndynamics of diverse players involved in this complex global production network.\nIts broad notion deals with different global issues (including regional value\nchains also) from the top down to the bottom up, founding a scope for policy\nanalysis (Gereffi & Fernandez-Stark 2011). But it is true that, as Feenstra\n(1998) points out, any single computational framework is not sufficient to\nquantification this whole range of economic activities. We should adopt an\nintegrative framework for accurate projection of this dynamic multidimensional\nphenomenon.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Lyapunov exponent is widely used in natural science to find chaotic signal,\nbut its existence is seldom discussed. In the present paper, we consider the\nproblem of whether the set of points at which Lyapunov exponent fails to exist,\ncalled the Lyapunov irregular set, has positive Lebesgue measure. The only\nknown example with the Lyapunov irregular set of positive Lebesgue measure is a\nfigure-8 attractor by the work of Ott and Yorke [OY2008], whose key mechanism\n(homoclinic loop) is easy to be broken by small perturbations. In this paper,\nwe show that surface diffeomorphisms with a robust homoclinic tangency given by\nColli and Vargas [CV2001], as well as other several known nonhyperbolic\ndynamics, has the Lyapunov irregular set of positive Lebesgue measure. We can\nconstruct such positive Lebesgue measure sets both as the time averages exist\nand do not exist on it.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  In Bayesian Networks (BNs), the direction of edges is crucial for causal\nreasoning and inference. However, Markov equivalence class considerations mean\nit is not always possible to establish edge orientations, which is why many BN\nstructure learning algorithms cannot orientate all edges from purely\nobservational data. Moreover, latent confounders can lead to false positive\nedges. Relatively few methods have been proposed to address these issues. In\nthis work, we present the hybrid mFGS-BS (majority rule and Fast Greedy\nequivalence Search with Bayesian Scoring) algorithm for structure learning from\ndiscrete data that involves an observational data set and one or more\ninterventional data sets. The algorithm assumes causal insufficiency in the\npresence of latent variables and produces a Partial Ancestral Graph (PAG).\nStructure learning relies on a hybrid approach and a novel Bayesian scoring\nparadigm that calculates the posterior probability of each directed edge being\nadded to the learnt graph. Experimental results based on well-known networks of\nup to 109 variables and 10k sample size show that mFGS-BS improves structure\nlearning accuracy relative to the state-of-the-art and it is computationally\nefficient.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This article proposes a generalized notion of extreme multivariate dependence\nbetween two random vectors which relies on the extremality of the\ncross-covariance matrix between these two vectors. Using a partial ordering on\nthe cross-covariance matrices, we also generalize the notion of positive upper\ndependence. We then proposes a means to quantify the strength of the dependence\nbetween two given multivariate series and to increase this strength while\npreserving the marginal distributions. This allows for the design of\nstress-tests of the dependence between two sets of financial variables, that\ncan be useful in portfolio management or derivatives pricing.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Combinatorial problems are formulated to find optimal designs within a fixed\nset of constraints. They are commonly found across diverse engineering and\nscientific domains. Understanding how to best use quantum computers for\ncombinatorial optimization is to date an open problem. Here we propose new\nmethods for producing approximate solutions for the maximum cut problem and its\nweighted version, which are based on relaxations to local quantum Hamiltonians.\nThese relaxations are defined through commutative maps, which in turn are\nconstructed borrowing ideas from quantum random access codes. We establish\nrelations between the spectra of the relaxed Hamiltonians and optimal cuts of\nthe original problems, via two quantum rounding protocols. The first one is\nbased on projections to random magic states. It produces average cuts that\napproximate the optimal one by a factor of least 0.555 or 0.625, depending on\nthe relaxation chosen, if given access to a quantum state with energy between\nthe optimal classical cut and the maximal relaxed energy. The second rounding\nprotocol is deterministic and it is based on estimation of Pauli observables.\nThe proposed quantum relaxations inherit memory compression from quantum random\naccess codes, which allowed us to test the performances of the methods\npresented for 3-regular random graphs and a design problem motivated by\nindustry for sizes up to 40 nodes, on superconducting quantum processors.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  (Abridged) Recent surveys of young star formation regions have shown that the\naverage Class II object does not have enough dust mass to make the cores of\ngiant planets. Younger Class 0/I objects have enough dust in their embedded\ndisk, which begs the questions: can the first steps of planet formation occur\nin these younger systems? The first step is building the first planetesimals,\ngenerally believed to be the product of the streaming instability. Hence the\nquestion can be restated: are the physical conditions of embedded disks\nconducive to the growth of the streaming instability? Here we model the\ncollapse of a `dusty' proto-stellar cloud to show that if there is sufficient\ndrift between the falling gas and dust, regions of the embedded disk can become\nsufficiently enhanced in dust to drive the streaming instability. We include\nfour models, three with different dust grain sizes and one with a different\ninitial cloud angular momentum to test a variety of collapse trajectories. We\nfind a `sweet spot' for planetesimal formation for grain sizes of a few 10s of\nmicron since they fall sufficiently fast relative to the gas to build a high\ndust-to-gas ratio along the disk midplane, but have slow enough radial drift\nspeeds in the embedded disk to maintain the high dust-to-gas ratio. Unlike the\ngas, which is held in hydrostatic equilibrium for a time due to gas pressure,\nthe dust can begin collapsing from all radii at a much earlier time. The\nstreaming instability can produce at least between 7-35 M$_\\oplus$ of\nplanetesimals in the Class 0/I phase of our smooth embedded disks, depending on\nthe size of the falling dust grains. This first generation of planetesimals\ncould represent the first step in planet formation, and occurs earlier in the\nlifetime of the young star than is traditionally thought.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Spin transport via magnon diffusion in magnetic insulators is important for a\nbroad range of spin-based phenomena and devices. However, the absence of the\nmagnon equivalent of an electric force is a bottleneck. In this work, we\ndemonstrate the controlled generation of magnon drift currents in yttrium iron\ngarnet/platinum heterostructures. By performing electrical injection and\ndetection of incoherent magnons, we find magnon drift currents that stem from\nthe interfacial Dzyaloshinskii-Moriya interaction. We can further control the\nmagnon drift by the orientation of the magnetic field. The drift current\nchanges the magnon propagation length by up to $\\pm$ 6 % relative to diffusion.\nWe generalize the magnonic spin transport theory to include a finite drift\nvelocity resulting from any inversion asymmetric interaction, and obtain\nresults consistent with our experiments.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Superscatterers are known to expand the rigid boundary of an object thereby\nenhancing the scattering cross section of the object. The design philosophy of\nthe acoustic superscatterers is based on partially-resonant systems in which a\ncoating material or double-negative metamaterial complementary to the host\nmedium is provided on top of the rigid object. When the source lies within the\nenhanced boundary of the complementary media, the interaction between the\nradiated wavefront and the enhanced virtual boundary gets stronger and results\nin suppression of the total forward radiated sound at far-field. An analytical\nframework is shown in this paper on suppression of the total acoustic pressure\nat far field in the forward direction when a cylindrical source, both monopole\nand dipole, lies within the virtual rigid boundary of the superscatterer. Total\nextinction cross section of the scatterer as a function of the distance between\nthe source and scatterer is derived to substantiate the earlier results.\nAdditionally, the effect of the area of cross-section of the cylindrical source\non the total forward radiated sound pressure at far-field is discussed.\nFinally, the effectiveness of the analytical results is verified numerically\nand the prospects for practical applications are discussed.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study isotropic and slowly-rotating stars made of dark energy adopting the\nextended Chaplygin equation-of-state. We compute the moment of inertia as a\nfunction of the mass of the stars, both for rotating and non-rotating objects.\nThe solution for the non-diagonal metric component as a function of the radial\ncoordinate for three different star masses is shown as well. We find that i)\nthe moment of inertia increases with the mass of the star, ii) in the case of\nnon-rotating objects the moment of inertia grows faster, and iii) the curve\ncorresponding to rotation lies below the one corresponding to non-rotating\nstars.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We study the environmental dependence of ultralight scalar dark matter (DM)\nwith linear interactions to the standard model particles. The solution to the\nDM field turns out to be a sum of the cosmic harmonic oscillation term and the\nlocal exponential fluctuation term. The amplitude of the first term depends on\nthe local DM density and the mass of the DM field. The second term is induced\nby the local distribution of matter, such as the Earth. And it depends not only\non the mass of the Earth, but also the density of the Earth. Then, we compute\nthe phase shift induced by the DM field in atom interferometers (AIs), through\nsolving the trajectories of atoms. Especially, the AI signal for the violation\nof weak equivalence principle (WEP) caused by the DM field is calculated.\nDepending on the values of the DM coupling parameters, contributions to the WEP\nviolation from the first and second terms of the DM field can be either\ncomparable or one larger than the other. Finally, we give some constraints to\nDM coupling parameters using results from the terrestrial atomic WEP tests.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We classify, up to isometric congruence, the homogeneous hypersurfaces in the\nRiemannian symmetric spaces $\\mathrm{SL}(3,\\mathbb{H})/\\mathrm{Sp}(3),\n\\hspace{1pt} \\mathrm{SO}(5,\\mathbb{C})/\\mathrm{SO}(5),$ and\n$\\mathrm{Gr}^*(2,\\mathbb{C}^{n+4}) =\n\\mathrm{SU}(n+2,2)/\\mathrm{S}(\\mathrm{U}(n+2)\\mathrm{U}(2)), \\, n \\geqslant 1$.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present a computational framework for analyzing and quantifying system\nflexibility. Our framework incorporates new features that include: general\nuncertainty characterizations that are constructed using composition of sets,\nprocedures for computing well-centered nominal points, and a procedure for\nidentifying and ranking flexibility-limiting constraints and critical parameter\nvalues. These capabilities allow us to analyze the flexibility of complex\nsystems such as distribution networks.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Consider the restriction of the directed landscape $\\mathcal L(x, s; y, t)$\nto a set of the form $\\{x_1, \\dots, x_k\\} \\times \\{s_0\\} \\times \\mathbb R\n\\times \\{t_0\\}$. We show that on any such set, the directed landscape is given\nby a last passage problem across $k$ locally Brownian functions. The $k$\nfunctions in this last passage isometry are built from certain marginals of the\nextended directed landscape. As applications of this construction, we show that\nthe Airy difference profile is locally absolutely continuous with respect to\nBrownian local time, that the KPZ fixed point started from two narrow wedges\nhas a Brownian-Bessel decomposition around its cusp point, and that the\ndirected landscape is a function of its geodesic shapes.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Low mass star formation inside massive clusters is crucial to understand the\neffect of cluster environment on processes like circumstellar disk evolution,\nplanet and brown dwarf formation. The young massive association of Cygnus OB2,\nwith a strong feedback from massive stars, is an ideal target to study the\neffect of extreme environmental conditions on its extensive low-mass\npopulation. We aim to perform deep multi-wavelength studies to understand the\nrole of stellar feedback on the IMF, brown dwarf fraction and circumstellar\ndisk properties in the region. We introduce here, the deepest and widest\noptical photometry of 1.5$^\\circ$ diameter region centred at Cygnus OB2 in\nr$_{2}$, i$_{2}$, z and Y-filters using Subaru Hyper Suprime-Cam (HSC). This\nwork presents the data reduction, source catalog generation, data quality\nchecks and preliminary results about the pre-main sequence sources. We obtain\n713,529 sources in total, with detection down to $\\sim$ 28 mag, 27 mag, 25.5\nmag and 24.5 mag in r$_{2}$, i$_{2}$, z and Y-band respectively, which is\n$\\sim$ 3 - 5 mag deeper than the existing Pan-STARRS and GTC/OSIRIS photometry.\nWe confirm the presence of a distinct pre-main sequence branch by statistical\nfield subtraction of the central 18$^\\prime$ region. We find the median age of\nthe region as $\\sim$ 5 $\\pm$ 2 Myrs with an average disk fraction of $\\sim$\n9$\\%$. At this age, combined with A$_V$ $\\sim$ 6 - 8 mag, we detect sources\ndown to a mass range $\\sim$ 0.01 - 0.17 M$_\\odot$. The deep HSC catalog will\nserve as the groundwork for further studies on this prominent active young\ncluster.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper proposes and demonstrates a PHY-layer design of a real-time\nprototype that supports Ultra-Reliable Communication (URC) in wireless\ninfrastructure networks. The design makes use of Orthogonal Frequency Division\nMultiple Access (OFDMA) as a means to achieve URC. Compared with Time-Division\nMultiple Access (TDMA), OFDMA concentrates the transmit power to a narrower\nbandwidth, resulting in higher effective SNR. Compared with Frequency-Division\nMultiple Access (FDMA), OFDMA has higher spectrum efficiency thanks to the\nsmaller subcarrier spacing. Although OFDMA has been introduced in 802.11ax, the\npurpose was to add flexibility in spectrum usage. Our Reliable OFDMA design,\nreferred to as ROFA, is a clean-slate design with a single goal of\nultra-reliable packet delivery. ROFA solves a number of key challenges to\nensure the ultra-reliability: (1) a downlink-coordinated time-synchronization\nmechanism to synchronize the uplink transmission of users, with at most $0.1us$\ntiming offset; (2) an \"STF-free\" packet reception synchronization method that\nmakes use of the property of synchronous systems to avoid packet misdetection;\nand (3) an uplink precoding mechanism to reduce the CFOs between users and the\nAP to a negligible level. We implemented ROFA on the Universal Software Radio\nPeripheral (USRP) SDR platform with real-time signal processing. Extensive\nexperimental results show that ROFA can achieve ultra-reliable packet delivery\n($PER<10^5$) with $11.5dB$ less transmit power compared with OFDM-TDMA when\nthey use $3$ and $52$ subcarriers respectively.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Machine learning (ML) is expected to play a major role in 5G edge computing.\nVarious studies have demonstrated that ML is highly suitable for optimizing\nedge computing systems as rapid mobility and application-induced changes occur\nat the edge. For ML to provide the best solutions, it is important to\ncontinually train the ML models to include the changing scenarios. The sudden\nchanges in data distributions caused by changing scenarios (e.g., 5G base\nstation failures) is referred to as concept drift and is a major challenge to\ncontinual learning. The ML models can present high error rates while the drifts\ntake place and the errors decrease only after the model learns the\ndistributions. This problem is more pronounced in a distributed setting where\nmultiple ML models are being used for different heterogeneous datasets and the\nfinal model needs to capture all concept drifts. In this paper, we show that\nusing Attention in Federated Learning (FL) is an efficient way of handling\nconcept drifts. We use a 5G network traffic dataset to simulate concept drift\nand test various scenarios. The results indicate that Attention can\nsignificantly improve the concept drift handling capability of FL.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  Feature attribution is widely used in interpretable machine learning to\nexplain how influential each measured input feature value is for an output\ninference. However, measurements can be uncertain, and it is unclear how the\nawareness of input uncertainty can affect the trust in explanations. We propose\nand study two approaches to help users to manage their perception of\nuncertainty in a model explanation: 1) transparently show uncertainty in\nfeature attributions to allow users to reflect on, and 2) suppress attribution\nto features with uncertain measurements and shift attribution to other features\nby regularizing with an uncertainty penalty. Through simulation experiments,\nqualitative interviews, and quantitative user evaluations, we identified the\nbenefits of moderately suppressing attribution uncertainty, and concerns\nregarding showing attribution uncertainty. This work adds to the understanding\nof handling and communicating uncertainty for model interpretability.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  This paper investigates the design and performance of delayed bit-interleaved\ncoded modulation (DBICM) with low-density parity-check (LDPC) codes. For Gray\nlabeled square $M$-ary quadrature amplitude modulation (QAM) constellations, we\ninvestigate the optimal delay scheme with the largest spectrum efficiency of\nDBICM for a fixed maximum number of delayed time slots and a given\nsignal-to-noise ratio. When analyzing the capacity of DBICM, we find two\nimportant properties: the capacity improvement due to delayed coded bits being\nmapped to the real and imaginary parts of the transmitted symbols are\nindependent of each other; a pair of delay schemes with delayed coded bits\nhaving identical bit-channel capacity lead to equivalent DBICM capacity. Using\nthese two properties, we efficiently optimize the delay scheme for any uniform\nGray-QAM systems. Furthermore, these two properties enable efficient LDPC code\ndesigns regarding unequal error protection via bit-channel type\nclassifications. Moreover, we use protograph-based extrinsic information\ntransfer charts to jointly optimize degree distributions and channel\nassignments of LDPC codes and propose a constrained progressive edge growth\nlike algorithm to jointly construct LDPC codes and bit-interleavers for DBICM,\ntaking distinctive bit-channel's capacity into account. Simulation results\ndemonstrate that the designed LDPC coded DBICM systems significantly outperform\nLDPC coded BICM systems.\n\n\n###\n\n", "completion": " 21\n"}
{"prompt": "  We present Neural Descriptor Fields (NDFs), an object representation that\nencodes both points and relative poses between an object and a target (such as\na robot gripper or a rack used for hanging) via category-level descriptors. We\nemploy this representation for object manipulation, where given a task\ndemonstration, we want to repeat the same task on a new object instance from\nthe same category. We propose to achieve this objective by searching (via\noptimization) for the pose whose descriptor matches that observed in the\ndemonstration. NDFs are conveniently trained in a self-supervised fashion via a\n3D auto-encoding task that does not rely on expert-labeled keypoints. Further,\nNDFs are SE(3)-equivariant, guaranteeing performance that generalizes across\nall possible 3D object translations and rotations. We demonstrate learning of\nmanipulation tasks from few (5-10) demonstrations both in simulation and on a\nreal robot. Our performance generalizes across both object instances and 6-DoF\nobject poses, and significantly outperforms a recent baseline that relies on 2D\ndescriptors. Project website: https://yilundu.github.io/ndf/.\n\n\n###\n\n", "completion": " 21\n"}
