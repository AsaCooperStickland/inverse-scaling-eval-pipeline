{"prompt": "  Colors of Type Ia supernovae in the first few days after explosion provide a\npotential discriminant between different models. In this paper, we present\n$g-r$ colors of 65 Type Ia supernovae discovered within 5 days from first light\nby the Zwicky Transient Facility in 2018, a sample that is about three times\nlarger than that in the literature. We find that $g-r$ colors are intrinsically\nrather homogeneous at early phases, with about half of the dispersion\nattributable to photometric uncertainties\n($\\sigma_\\mathrm{noise}\\sim\\sigma_\\mathrm{int}\\sim$ 0.18 mag). Colors are\nnearly constant starting from 6 days after first light ($g-r\\sim-0.15$ mag),\nwhile the time evolution at earlier epochs is characterized by a continuous\nrange of slopes, from events rapidly transitioning from redder to bluer colors\n(slope of $\\sim-0.25$ mag day$^{-1}$) to events with a flatter evolution. The\ncontinuum in the slope distribution is in good agreement both with models\nrequiring some amount of $^{56}$Ni mixed in the outermost regions of the ejecta\nand with \"double-detonation\" models having thin helium layers\n($M_\\mathrm{He}=0.01\\,M_\\odot$) and varying carbon-oxygen core masses. At the\nsame time, six events show evidence for a distinctive \"red bump\" signature\npredicted by \"double-detonation\" models with larger helium masses. We finally\nidentify a significant correlation between the early-time $g-r$ slopes and\nsupernova brightness, with brighter events associated to flatter color\nevolution (p-value=0.006). The distribution of slopes, however, is consistent\nwith being drawn from a single population, with no evidence for two components\nas claimed in the literature based on $B-V$ colors.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Recently, backpropagation through time inspired learning algorithms are\nwidely introduced into SNNs to improve the performance, which brings the\npossibility to attack the models accurately given Spatio-temporal gradient\nmaps. We propose two approaches to address the challenges of gradient input\nincompatibility and gradient vanishing. Specifically, we design a gradient to\nspike converter to convert continuous gradients to ternary ones compatible with\nspike inputs. Then, we design a gradient trigger to construct ternary gradients\nthat can randomly flip the spike inputs with a controllable turnover rate, when\nmeeting all zero gradients. Putting these methods together, we build an\nadversarial attack methodology for SNNs trained by supervised algorithms.\nMoreover, we analyze the influence of the training loss function and the firing\nthreshold of the penultimate layer, which indicates a \"trap\" region under the\ncross-entropy loss that can be escaped by threshold tuning. Extensive\nexperiments are conducted to validate the effectiveness of our solution.\nBesides the quantitative analysis of the influence factors, we evidence that\nSNNs are more robust against adversarial attack than ANNs. This work can help\nreveal what happens in SNN attack and might stimulate more research on the\nsecurity of SNN models and neuromorphic devices.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  For an inner function u we discuss the dual operator for the well-known\ncompressed shift. We establish conditions for two dual compressed shifts to be\nunitarily equivalent/similar and we describe the invariant subspace structure\nfor the dual.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We show the Morse-Novikov number of knots in $S^3$ is additive under\nconnected sum and unchanged by cabling.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  NASA's Parker Solar Probe (PSP) mission is currently investigating the local\nplasma environment of the inner-heliosphere ($< $0.25$R_\\odot$) using both\n{\\em{in-situ}} and remote sensing instrumentation. Connecting signatures of\nmicrophysical particle heating and acceleration processes to macro-scale\nheliospheric structure requires sensitive measurements of electromagnetic\nfields over a large range of physical scales. The FIELDS instrument, which\nprovides PSP with {\\em{in-situ}} measurements of electromagnetic fields of the\ninner heliosphere and corona, includes a set of three vector magnetometers: two\nfluxgate magnetometers (MAGs), and a single inductively coupled search-coil\nmagnetometer (SCM). Together, the three FIELDS magnetometers enable\nmeasurements of the local magnetic field with a bandwidth ranging from DC to 1\nMHz. This manuscript reports on the development of a merged data set combining\nSCM and MAG (SCaM) measurements, enabling the highest fidelity data product\nwith an optimal signal to noise ratio. On-ground characterization tests of\ncomplex instrumental responses and noise floors are discussed as well as\napplication to the in-flight calibration of FIELDS data. The algorithm used on\nPSP/FIELDS to merge waveform observations from multiple sensors with optimal\nsignal to noise characteristics is presented. In-flight analysis of\ncalibrations and merging algorithm performance demonstrates a timing accuracy\nto well within the survey rate sample period of $\\sim340 \\mu s$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The CHaracterising ExOPlanet Satellite (CHEOPS) is a mission dedicated to the\nsearch for exoplanetary transits through high precision photometry of bright\nstars already known to host planets. The telescope will provide the unique\ncapability of determining accurate radii for planets whose masses have already\nbeen measured from ground-based spectroscopic surveys. This will allow a\nfirst-order characterisation of the planets' internal structure through the\ndetermination of the bulk density, providing direct insight into their\ncomposition. The CHEOPS simulator has been developed to perform detailed\nsimulations of the data which is to be received from the CHEOPS satellite. It\ngenerates accurately simulated images that can be used to explore design\noptions and to test the on-ground data processing, in particular, the pipeline\nproducing the photometric time series. It is, thus, a critical tool for\nestimating the photometric performance expected in flight and to guide\nphotometric analysis. It can be used to prepare observations, consolidate the\nnoise budget, and asses the performance of CHEOPS in realistic astrophysical\nfields that are difficult to reproduce in the laboratory. Images generated by\nCHEOPSim take account of many detailed effects, including variations of the\nincident signal flux and backgrounds, and detailed modelling of the satellite\norbit, pointing jitter and telescope optics, as well as the CCD response, noise\nand readout. The simulator results presented in this paper have been used in\nthe context of validating the data reduction processing chain, in which image\ntime series generated by CHEOPSim were used to generate light curves for\nsimulated planetary transits across real and simulated targets. Independent\nanalysts were successfully able to detect the planets and measure their radii\nto an accuracy within the science requirements of the mission.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In programmatic advertising, ad slots are usually sold using second-price\n(SP) auctions in real-time. The highest bidding advertiser wins but pays only\nthe second-highest bid (known as the winning price). In SP, for a single item,\nthe dominant strategy of each bidder is to bid the true value from the bidder's\nperspective. However, in a practical setting, with budget constraints, bidding\nthe true value is a sub-optimal strategy. Hence, to devise an optimal bidding\nstrategy, it is of utmost importance to learn the winning price distribution\naccurately. Moreover, a demand-side platform (DSP), which bids on behalf of\nadvertisers, observes the winning price if it wins the auction. For losing\nauctions, DSPs can only treat its bidding price as the lower bound for the\nunknown winning price. In literature, typically censored regression is used to\nmodel such partially observed data. A common assumption in censored regression\nis that the winning price is drawn from a fixed variance (homoscedastic)\nuni-modal distribution (most often Gaussian). However, in reality, these\nassumptions are often violated. We relax these assumptions and propose a\nheteroscedastic fully parametric censored regression approach, as well as a\nmixture density censored network. Our approach not only generalizes censored\nregression but also provides flexibility to model arbitrarily distributed\nreal-world data. Experimental evaluation on the publicly available dataset for\nwinning price estimation demonstrates the effectiveness of our method.\nFurthermore, we evaluate our algorithm on one of the largest demand-side\nplatforms and significant improvement has been achieved in comparison with the\nbaseline solutions.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The longitudinal current along a bar conductor is accompanied by self induced\ntransverse magnetic field. In absence of transverse current in sample bulk a\nnonzero electric field (Hall) must be present to compensate Lorentz force\naction. The longitudinal current itself can be viewed as collective drift of\ncarriers in crossed magnetic and electric fields. At low temperatures the\nenhanced carrier viscosity leads to nonuniform longitudinal current flow whose\ntransverse profile is sensitive to presence of $collinear$ diamagnetic currents\nnearby the sample inner wall. Both the longitudinal current and transverse\nmagnetic field are squeezed out from the bulk towards the inner walls of a bar.\nMagnetic properties of a sample resembles those expected for ideal diamagnet.\nAt certain critical temperature a former dissipative current becomes purely\ndiamagnetic providing a zero resistance state. Temperature threshold of\nzero-resistance state is found for arbitrary disorder strength, sample size and\nmagnetic field. Sample-size and magnetic field driven transition from zero\nresistance state to normal metal has been studied.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  An approach for mapping an electric vehicle EV driver s travel time\nconstraints and risk taking behavior to real time routing in a probabilistic\ntime-dependent or stochastic network is proposed in this paper. The proposed\napproach is based on a heuristic algorithm that finds the shortest path\naccording to the driver s preferences. Accounting for en route delays and\nalternate routes, the EV routing problem in stochastic networks is shown to\nexhibit other than the First In First Out FIFO property i.e. the traveling time\nfor those who depart earlier may not sooner of those who depart later or wait\nen route in the charging stations. The proposed approach provides EV drivers\nthe option to manage their trip and reach the destination on time, while by\ntaking advantage of the non FIFO characteristics of the traffic network, charge\ntheir cars en route. The proposed routing algorithm is tested on a given\nstochastic transportation network. The best routes based on the driver s\npreferences are identified while accounting for the best-planned delays at the\ncharging stations or en-route.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We examine the stacked thermal Sunyaev-Zel\\text{'}dovich (SZ) signals for a\nsample of galaxy cluster candidates from the Spitzer-HETDEX Exploratory Large\nArea (SHELA) Survey, which are identified in combined optical and infrared\nSHELA data using the redMaPPer algorithm. We separate the clusters into three\nrichness bins, with average photometric redshifts ranging from 0.70 to 0.80.\nThe richest bin shows a clear temperature decrement at 148 GHz in the Atacama\nCosmology Telescope data, which we attribute to the SZ effect. All richness\nbins show an increment at 220 GHz, which we attribute to dust emission from\ncluster galaxies. We correct for dust emission using stacked profiles from\nHerschel Stripe 82 data, and allow for synchrotron emission using stacked\nprofiles created by binning source fluxes from NVSS data. We see dust emission\nin all three richness bins, but can only confidently detect the SZ decrement in\nthe highest richness bin, finding $M_{500}$ = $8.7^{+1.7}_{-1.3} \\times 10^{13}\nM_\\odot$. Neglecting the correction for dust depresses the inferred mass by 26\npercent, indicating a partial fill-in of the SZ decrement from thermal dust and\nsynchrotron emission by the cluster member galaxies. We compare our corrected\nSZ masses to two redMaPPer mass--richness scaling relations and find that the\nSZ mass is lower than predicted by the richness. We discuss possible\nexplanations for this discrepancy, and note that the SHELA richnesses may\ndiffer from previous richness measurements due to the inclusion of IR data in\nredMaPPer.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We construct the first examples of normal subgroups of mapping class groups\nthat are isomorphic to non-free right-angled Artin groups. Our construction\nalso gives normal, non-free right-angled Artin subgroups of other groups, such\nas braid groups and pure braid groups, as well as many subgroups of the mapping\nclass group, such as the Torelli subgroup. Our work recovers and generalizes\nthe seminal result of Dahmani-Guirardel-Osin, which gives free, purely\npseudo-Anosov normal subgroups of mapping class groups. We give two\napplications of our methods: (1) we produce an explicit proper normal subgroup\nof the mapping class group that is not contained in any level $m$ congruence\nsubgroup, and (2) we produce an explicit example of a pseudo-Anosov mapping\nclass with the property that all of its even powers have free normal closure\nand its odd powers normally generate the entire mapping class group. The\ntechnical theorem at the heart of our work is a new version of the windmill\napparatus of Dahmani-Guirardel-Osin, which is tailored to the setting of group\nactions on the projection complexes of Bestvina-Bromberg-Fujiwara.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper a mathematical model is given for the scattering of an incident\nwave from a surface covered with microscopic small Helmholtz resonators, which\nare cavities with small openings. More precisely, the surface is built upon a\nfinite number of Helmholtz resonators in a unit cell and that unit cell is\nrepeated periodically. To solve the scattering problem, the mathematical\nframework elaborated in [Ammari et al., Asympt. Anal., 2019] is used. The main\nresult is an approximate formula for the scattered wave in terms of the lengths\nof the openings. Our framework provides analytic expressions for the scattering\nwave vector and angle and the phase-shift. It justifies the apparent\nabsorption. Moreover, it shows that at specific lengths for the openings and a\nspecific frequency there is an abrupt shift of the phase of the scattered wave\ndue to the subwavelength resonances of the Helmholtz resonators. A numerically\nfast implementation is given to identify a region of those specific values of\nthe openings and the frequencies.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Although the challenge of the device connection is much relieved in 5G\nnetworks, the training latency is still an obstacle preventing Federated\nLearning (FL) from being largely adopted. One of the most fundamental problems\nthat lead to large latency is the bad candidate-selection for FL. In the\ndynamic environment, the mobile devices selected by the existing reactive\ncandidate-selection algorithms very possibly fail to complete the training and\nreporting phases of FL, because the FL parameter server only knows the\ncurrently-observed resources of all candidates. To this end, we study the\nproactive candidate-selection for FL in this paper. We first let each candidate\ndevice predict the qualities of both its training and reporting phases locally\nusing LSTM. Then, the proposed candidateselection algorithm is implemented by\nthe Deep Reinforcement Learning (DRL) framework. Finally, the real-world\ntrace-driven experiments prove that the proposed approach outperforms the\nexisting reactive algorithms\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Super-aligned carbon nanotube (CNT) films have intriguing anisotropic thermal\ntransport properties due to the anisotropic nature of individual nanotubes and\nthe important role of nanotube alignment. However, the relationship between the\nalignment and the anisotropic thermal conductivities was not well understood\ndue to the challenges in both the preparation of high-quality super-aligned CNT\nfilm samples and the thermal characterization of such highly anisotropic and\nporous thin films. Here, super-aligned CNT films with different alignment\nconfigurations are designed and their anisotropic thermal conductivities are\nmeasured using time-domain thermoreflectance (TDTR) with an elliptical-beam\napproach. The results suggest that the alignment configuration could tune the\ncross-plane thermal conductivity k_z from 6.4 to 1.5 W/mK and the in-plane\nanisotropic ratio from 1.2 to 13.5. This work confirms the important role of\nCNT alignment in tuning the thermal transport properties of super-aligned CNT\nfilms and provides an efficient way to design thermally anisotropic films for\nthermal management.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We review recent works on the possibility for eternal existence of thin-shell\nwormholes on Einstein and Einstein-Gauss-Bonnet gravity. We introduce\nthin-shell wormholes that are categorized into a class of traversable wormhole\nsolutions. After that, we discuss stable thin-shell wormholes with\nnegative-tension branes in Reissner-Nordstr\\\"om-(anti) de Sitter spacetimes in\n$d$ dimensional Einstein gravity. Imposing $Z_2$ symmetry, we construct and\nclassify traversable static thin-shell wormholes in spherical, planar and\nhyperbolic symmetries. It is found that the spherical wormholes are stable\nagainst spherically symmetric perturbations. It is also found that some classes\nof wormholes in planar and hyperbolic symmetries with a negative cosmological\nconstant are stable against perturbations preserving symmetries. In most cases,\nstable wormholes are found with the appropriate combination of an electric\ncharge and a negative cosmological constant. However, as special cases, there\nare stable wormholes even with a vanishing cosmological constant in spherical\nsymmetry and with a vanishing electric charge in hyperbolic symmetry.\nSubsequently, the existence and dynamical stability of traversable thin-shell\nwormholes with electrically neutral negative-tension branes is discussed in\nEinstein-Gauss-Bonnet theory of gravitation. We consider radial perturbations\nagainst the shell for the solutions, which have the $Z_2$ symmetry. The effect\nof the Gauss-Bonnet term on the stability depends on the spacetime symmetry.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We propose a one-step constrained (OSC) beam search to accelerate recurrent\nneural network (RNN) transducer (RNN-T) inference. The original RNN-T beam\nsearch has a while-loop leading to speed down of the decoding process. The OSC\nbeam search eliminates this while-loop by vectorizing multiple hypotheses. This\nvectorization is nontrivial as the expansion of the hypotheses within the\noriginal RNN-T beam search can be different from each other. However, we found\nthat the hypotheses expanded only once at each decoding step in most cases;\nthus, we constrained the maximum expansion number to one, thereby allowing\nvectorization of the hypotheses. For further acceleration, we assign\nconstraints to the prefixes of the hypotheses to prune the redundant search\nspace. In addition, OSC beam search has duplication check among hypotheses\nduring the decoding process as duplication can undesirably shrink the search\nspace. We achieved significant speedup compared with other RNN-T beam search\nmethods with lower phoneme and word error rate.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Recent work showed that stabilizing affine control systems to desired (sets\nof) states while optimizing quadratic costs and observing state and control\nconstraints can be reduced to quadratic programs (QP) by using control barrier\nfunctions (CBF) and control Lyapunov functions. In our own recent work, we\ndefined high order CBFs (HOCBFs) to accommodating systems and constraints with\narbitrary relative degrees, and a penalty method to increase the feasibility of\nthe corresponding QPs. In this paper, we introduce adaptive CBF (AdaCBFs) that\ncan accommodate time-varying control bounds and dynamics noise, and also\naddress the feasibility problem. Central to our approach is the introduction of\npenalty functions in the definition of an AdaCBF and the definition of\nauxiliary dynamics for these penalty functions that are HOCBFs and are\nstabilized by CLFs. We demonstrate the advantages of the proposed method by\napplying it to a cruise control problem with different road surfaces, tires\nslipping, and dynamics noise.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper studies the regular stochastic block model comprising\n\\emph{several} communities: each of the $k$ non-overlapping communities, for $k\n\\geqslant 3$, possesses $n$ vertices, each of which has total degree $d$. The\nvalues of the intra-cluster degrees (i.e.\\ the number of neighbours of a vertex\ninside the cluster it belongs to) and the inter-cluster degrees (i.e.\\ the\nnumber of neighbours of a vertex inside a cluster different from its own) are\nallowed to vary across clusters. We discuss two main results: the first\ncompares the probability measure induced by our model with the uniform measure\non the space of $d$-regular graphs on $kn$ vertices, and the second establishes\nthat the clusters, under rather weak assumptions, are unique asymptotically\nalmost surely as $n \\rightarrow \\infty$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper studies regulated state synchronization of discrete-time\nhomogeneous networks of non-introspective agents in presence of unknown\nnon-uniform input delays. A scale free protocol is designed based on additional\ninformation exchange, which does not need any knowledge of the directed network\ntopology and the spectrum of the associated Laplacian matrix. The proposed\nprotocol is scalable and achieves state synchronization for any arbitrary\nnumber of agents. Meanwhile, an upper bound for the input delay tolerance is\nobtained, which explicitly depends on the agent dynamics.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Quantum gas microscopy has emerged as a powerful new way to probe quantum\nmany-body systems at the microscopic level. However, layered or efficient\nspin-resolved readout methods have remained scarce as they impose strong\ndemands on the specific atomic species and constrain the simulated lattice\ngeometry and size. Here we present a novel high-fidelity bilayer readout, which\ncan be used for full spin- and density-resolved quantum gas microscopy of\ntwo-dimensional systems with arbitrary geometry. Our technique makes use of an\ninitial Stern-Gerlach splitting into adjacent layers of a highly-stable\nvertical superlattice and subsequent charge pumping to separate the layers by\n$21\\,\\mu$m. This separation enables independent high-resolution images of each\nlayer. We benchmark our method by spin- and density-resolving two-dimensional\nFermi-Hubbard systems. Our technique furthermore enables the access to advanced\nentropy engineering schemes, spectroscopic methods or the realization of\ntunable bilayer systems.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Faceted browsing is a commonly supported feature of user interfaces for\naccess to information. Existing interfaces generally treat facet values\nselected by a user as hard filters and respond to the user by only displaying\ninformation items strictly satisfying the filters and in their original ranking\norder. We propose a novel alternative strategy for faceted browsing, called\nsoft faceted browsing, where the system also includes some possibly relevant\nitems outside the selected filter in a non-intrusive way and re-ranks the items\nto better satisfy the user's information need. Such a soft faceted browsing\nstrategy can be beneficial when the user does not have a very confident and\nstrict preference for the selected facet values, and is especially appropriate\nfor applications such as e-commerce search where the user would like to explore\na larger space before finalizing a purchasing decision. We propose a\nprobabilistic framework for modeling and solving the soft faceted browsing\nproblem, and apply the framework to study the case of facet filter selection in\ne-commerce search engines. Preliminary experiment results demonstrate the soft\nfaceted browsing scheme is better than the traditional faceted browsing scheme\nin terms of its efficiency in helping users navigate in the information space.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper presents a soft-robotic platform for exploring the ecological\nrelevance of non-locomotory movements via animal-robot interactions. Coral\nsnakes (genus Micrurus) and their mimics use vigorous, non-locomotory, and\narrhythmic thrashing to deter predation. There is variation across snake\nspecies in the duration and curvature of anti-predator thrashes, and it is\nunclear how these aspects of motion interact to contribute to snake survival.\nIn this work, soft robots composed of fiber-reinforced elastomeric enclosures\n(FREEs) are developed to emulate the anti-predator behaviors of three genera of\nsnake. Curvature and duration of motion are estimated for both live snakes and\nrobots, providing a quantitative assessment of the robots' ability to emulate\nsnake poses. The curvature values of the fabricated soft-robotic head,\nmidsection, and tail segments are found to overlap with those exhibited by live\nsnakes. Soft robot motion durations were less than or equal to those of snakes\nfor all three genera. Additionally, combinations of segments were selected to\nemulate three specific snake genera with distinct anti-predatory behavior,\nproducing curvature values that aligned well with live snake observations.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper investigates highly mobile vehicular networks from users'\nperspectives in highway transportation. Particularly, a centralized\nsoftware-defined architecture is introduced in which centralized resources can\nbe assigned, programmed, and controlled using the anchor nodes (ANs) of the\nedge servers. Unlike the legacy networks, where a typical user is served from\nonly one access point (AP), in the proposed system model, a vehicle user is\nserved from multiple APs simultaneously. While this increases the reliability\nand the spectral efficiency of the assisted users, it also necessitates an\naccurate power allocation in all transmission time slots. As such, a joint user\nassociation and power allocation problem is formulated to achieve enhanced\nreliability and weighted user sum rate. However, the formulated problem is a\ncomplex combinatorial problem, remarkably hard to solve. Therefore,\nfine-grained machine learning algorithms are used to efficiently optimize joint\nuser associations and power allocations of the APs in a highly mobile vehicular\nnetwork. Furthermore, a distributed single-agent reinforcement learning\nalgorithm, namely SARL-MARL, is proposed which obtains nearly identical\ngenie-aided optimal solutions within a nominal number of training episodes than\nthe baseline solution. Simulation results validate that our solution\noutperforms existing schemes and can attain genie-aided optimal performances.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Finding a set of nested partitions of a dataset is useful to uncover relevant\nstructure at different scales, and is often dealt with a data-dependent\nmethodology. In this paper, we introduce a general two-step methodology for\nmodel-based hierarchical clustering. Considering the integrated classification\nlikelihood criterion as an objective function, this work applies to every\ndiscrete latent variable models (DLVMs) where this quantity is tractable. The\nfirst step of the methodology involves maximizing the criterion with respect to\nthe partition. Addressing the known problem of sub-optimal local maxima found\nby greedy hill climbing heuristics, we introduce a new hybrid algorithm based\non a genetic algorithm efficiently exploring the space of solutions. The\nresulting algorithm carefully combines and merges different solutions, and\nallows the joint inference of the number $K$ of clusters as well as the\nclusters themselves. Starting from this natural partition, the second step of\nthe methodology is based on a bottom-up greedy procedure to extract a hierarchy\nof clusters. In a Bayesian context, this is achieved by considering the\nDirichlet cluster proportion prior parameter $\\alpha$ as a regularization term\ncontrolling the granularity of the clustering. A new approximation of the\ncriterion is derived as a log-linear function of $\\alpha$, enabling a simple\nfunctional form of the merge decision criterion. This second step allows the\nexploration of the clustering at coarser scales. The proposed approach is\ncompared with existing strategies on simulated as well as real settings, and\nits results are shown to be particularly relevant. A reference implementation\nof this work is available in the R package greed accompanying the paper.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this article, we elaborate further on the $\\Lambda$CDM \"tension\",\nsuggested recently by the authors \\cite{Lusso:2019akb,Risaliti:2018reu}. We\ncombine Supernovae type Ia (SNIa) with quasars (QSO) and Gamma Ray Bursts (GRB)\ndata in order to reconstruct a model independent Hubble diagram to as high\nredshifts as possible. Specifically, in the case of either SNIa or SNIa/QSO\ndata, we find that current values of the cosmokinetic parameters extracted from\nthe Gaussian process are consistent with those of $\\Lambda$CDM. Including GRBs,\nin the analysis, we find a tension, which however is not as significant as that\nmentioned in \\cite{Lusso:2019akb, Risaliti:2018reu}. Finally, we argue that the\nchoice of the kernel function used in extracting the luminosity distance might\naffect the amount of tension.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The goal of this study is to improve the accuracy of millimeter wave received\npower prediction by utilizing camera images and radio frequency (RF) signals,\nwhile gathering image inputs in a communication-efficient and\nprivacy-preserving manner. To this end, we propose a distributed multimodal\nmachine learning (ML) framework, coined multimodal split learning (MultSL), in\nwhich a large neural network (NN) is split into two wirelessly connected\nsegments. The upper segment combines images and received powers for future\nreceived power prediction, whereas the lower segment extracts features from\ncamera images and compresses its output to reduce communication costs and\nprivacy leakage. Experimental evaluation corroborates that MultSL achieves\nhigher accuracy than the baselines utilizing either images or RF signals.\nRemarkably, without compromising accuracy, compressing the lower segment output\nby 16x yields 16x lower communication latency and 2.8% less privacy leakage\ncompared to the case without compression.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We re-explore the symmetries of a weakly isolated horizon (WIH) from the\nperspective of freedom in the choice of intrinsic data. The supertranslations\nare realized as additional symmetries. Further, it is shown that all smooth\nvector fields tangent to the cross-sections are Hamiltonian. We show that\njoining two WIHs which differ in these Hamiltonians and boundary data, under\nthe action of a supertranslation, necessarily require the inclusion of an\nintermediate dynamical phase, possibly with the inclusion of a stress energy\ntensor. This phase of the boundary is non-expanding but not a WIH and\ninvariably leads to a violation of the dominant energy condition. The\nassumptions made allow us to reconstruct the (classically) pathological stress\nenergy tensor also.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  SentenceMIM is a probabilistic auto-encoder for language data, trained with\nMutual Information Machine (MIM) learning to provide a fixed length\nrepresentation of variable length language observations (i.e., similar to VAE).\nPrevious attempts to learn VAEs for language data faced challenges due to\nposterior collapse. MIM learning encourages high mutual information between\nobservations and latent variables, and is robust against posterior collapse. As\nsuch, it learns informative representations whose dimension can be an order of\nmagnitude higher than existing language VAEs. Importantly, the SentenceMIM loss\nhas no hyper-parameters, simplifying optimization. We compare sentenceMIM with\nVAE, and AE on multiple datasets. SentenceMIM yields excellent reconstruction,\ncomparable to AEs, with a rich structured latent space, comparable to VAEs. The\nstructured latent representation is demonstrated with interpolation between\nsentences of different lengths. We demonstrate the versatility of sentenceMIM\nby utilizing a trained model for question-answering and transfer learning,\nwithout fine-tuning, outperforming VAE and AE with similar architectures.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  State-of-the-art neural dialogue systems excel at syntactic and semantic\nmodelling of language, but often have a hard time establishing emotional\nalignment with the human interactant during a conversation. In this work, we\nbring Affect Control Theory (ACT), a socio-mathematical model of emotions for\nhuman-human interactions, to the neural dialogue generation setting. ACT makes\npredictions about how humans respond to emotional stimuli in social situations.\nDue to this property, ACT and its derivative probabilistic models have been\nsuccessfully deployed in several applications of Human-Computer Interaction,\nincluding empathetic tutoring systems, assistive healthcare devices and\ntwo-person social dilemma games. We investigate how ACT can be used to develop\naffect-aware neural conversational agents, which produce emotionally aligned\nresponses to prompts and take into consideration the affective identities of\nthe interactants.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The majority of learning-based semantic segmentation methods are optimized\nfor daytime scenarios and favorable lighting conditions. Real-world driving\nscenarios, however, entail adverse environmental conditions such as nighttime\nillumination or glare which remain a challenge for existing approaches. In this\nwork, we propose a multimodal semantic segmentation model that can be applied\nduring daytime and nighttime. To this end, besides RGB images, we leverage\nthermal images, making our network significantly more robust. We avoid the\nexpensive annotation of nighttime images by leveraging an existing daytime\nRGB-dataset and propose a teacher-student training approach that transfers the\ndataset's knowledge to the nighttime domain. We further employ a domain\nadaptation method to align the learned feature spaces across the domains and\npropose a novel two-stage training scheme. Furthermore, due to a lack of\nthermal data for autonomous driving, we present a new dataset comprising over\n20,000 time-synchronized and aligned RGB-thermal image pairs. In this context,\nwe also present a novel target-less calibration method that allows for\nautomatic robust extrinsic and intrinsic thermal camera calibration. Among\nothers, we employ our new dataset to show state-of-the-art results for\nnighttime semantic segmentation.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We investigate how the Kitaev spin liquid state described by Majorana\nfermions coupled with $Z_2$ gauge fields is affected by non-Kitaev interactions\nwhich exist in real candidate materials of the Kitaev magnet. It is found that\nthe off-diagonal exchange interaction referred to as the $\\Gamma'$ term\ndramatically changes the Majorana band structure in the case of the\nantiferromagnetic Kitaev interaction, and gives rise to a topological phase\ntransition from a non-Abelian topological phase with the Chern number equal to\n$\\pm 1$ to an Abelian phase with the Chern number equal to $\\pm 2$, in which\nthe $Z_{2}$ vortices behave as Abelian anyons. On the other hand, other\nnon-Kitaev interactions such as the Heisenberg interaction and the $\\Gamma$\nterm, only affect the bandwidth of the Majorana band as long as the spin liquid\nstate is not destabilized.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We prove that a cuspidal automorphic representation of GL(3) over any number\nfield is determined by the quadratic twists of its central value. In the case\nof a non-Gelbart-Jacquet lift, the result is conditional on the analytic\nbehavior of a certain Euler product. We deduce the nonvanishing of infinitely\nmany quadratic twists of central values. This generalizes a result of Chinta\nand Diaconu that was valid only over the field of rational numbers and explored\nonly for Gelbart-Jacquet lifts.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We give an exact criterion of a conjecture of L.M.Kelly to hold true which is\nstated as follows. If there is a finite family $\\Sigma$ of mutually skew lines\nin $\\mathbb{R}^l,l\\geq 4$ such that the three dimensional affine span (hull) of\nevery two lines in $\\Sigma$, contains at least one more line of $\\Sigma$, then\nwe have that $\\Sigma$ is entirely contained in a three dimensional space if and\nonly if the arrangement of affine hulls is central. Finally, this article leads\nto an analogous question for higher dimensional skew affine spaces, that is,\nfor $(2,5)$-representations of sylvester-gallai designs in $\\mathbb{R}^6$,\nwhich is answered in the last section.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  With the arrival of several face-swapping applications such as FaceApp,\nSnapChat, MixBooth, FaceBlender and many more, the authenticity of digital\nmedia content is hanging on a very loose thread. On social media platforms,\nvideos are widely circulated often at a high compression factor. In this work,\nwe analyze several deep learning approaches in the context of deepfakes\nclassification in high compression scenario and demonstrate that a proposed\napproach based on metric learning can be very effective in performing such a\nclassification. Using less number of frames per video to assess its realism,\nthe metric learning approach using a triplet network architecture proves to be\nfruitful. It learns to enhance the feature space distance between the cluster\nof real and fake videos embedding vectors. We validated our approaches on two\ndatasets to analyze the behavior in different environments. We achieved a\nstate-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of\n90.71% on a highly compressed Neural Texture dataset. Our approach is\nespecially helpful on social media platforms where data compression is\ninevitable.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Due to its extremely rich phase diagram, the two-dimensional electron gas\nexposed to perpendicular magnetic field has been the subject of intense and\nsustained study. One particularly interesting problem in this system is that of\nthe half-filled Landau level, where the Fermi sea of composite fermions, a\nfractional quantum Hall state arising from a pairing instability of the\ncomposite fermions, and the quantum Hall nematic were observed in the\nhalf-filled $N=0$, $N=1$, and $N \\geq 2$ Landau levels, respectively. Thus\ndifferent ground states developed in different half-filled Landau levels. This\nsituation has recently changed, when evidence for both the paired fractional\nquantum Hall state and the quantum Hall nematic was reported in the half-filled\n$N=1$ Landau level. Furthermore, a direct quantum phase transition between\nthese two ordered states was found. These results highlight an intimate\nconnection between pairing and nematicity, a topic of current interest in\nseveral strongly correlated systems, in a well-understood and low disorder\nenvironment.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We show relation between sign of Gaussian curvature of cuspidal edge and\ngeometric invariants through types of singularities of Gauss map. Moreover, we\ndefine and characterize positivity/negativity of cusps of Gauss maps by\ngeometric invariants of cuspidal edges, and show relation between sign of cusps\nand of the Gaussian curvature.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Word2Vec is a prominent model for natural language processing (NLP) tasks.\nSimilar inspiration is found in distributed embeddings for new state-of-the-art\n(SotA) deep neural networks. However, wrong combination of hyper-parameters can\nproduce poor quality vectors. The objective of this work is to empirically show\noptimal combination of hyper-parameters exists and evaluate various\ncombinations. We compare them with the released, pre-trained original word2vec\nmodel. Both intrinsic and extrinsic (downstream) evaluations, including named\nentity recognition (NER) and sentiment analysis (SA) were carried out. The\ndownstream tasks reveal that the best model is usually task-specific, high\nanalogy scores don't necessarily correlate positively with F1 scores and the\nsame applies to focus on data alone. Increasing vector dimension size after a\npoint leads to poor quality or performance. If ethical considerations to save\ntime, energy and the environment are made, then reasonably smaller corpora may\ndo just as well or even better in some cases. Besides, using a small corpus, we\nobtain better human-assigned WordSim scores, corresponding Spearman correlation\nand better downstream performances (with significance tests) compared to the\noriginal model, trained on 100 billion-word corpus.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  When we place a colored filter in front of a camera the effective camera\nresponse functions are equal to the given camera spectral sensitivities\nmultiplied by the filter spectral transmittance. In this paper, we solve for\nthe filter which returns the modified sensitivities as close to being a linear\ntransformation from the color matching functions of human visual system as\npossible. When this linearity condition - sometimes called the Luther condition\n- is approximately met, the `camera+filter' system can be used for accurate\ncolor measurement. Then, we reformulate our filter design optimisation for\nmaking the sensor responses as close to the CIEXYZ tristimulus values as\npossible given the knowledge of real measured surfaces and illuminants spectra\ndata. This data-driven method in turn is extended to incorporate constraints on\nthe filter (smoothness and bounded transmission). Also, because how the\noptimisation is initialised is shown to impact on the performance of the\nsolved-for filters, a multi-initialisation optimisation is developed.\n  Experiments demonstrate that, by taking pictures through our optimised color\nfilters we can make cameras significantly more colorimetric.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The effects of collisional processes in the hot QCD medium to thermal\ndilepton production from $q\\overline{q}$ annihilation in relativistic heavy-ion\ncollisions have been investigated. The non-equilibrium corrections to the\nmomentum distribution function have been estimated within the framework of\nensemble-averaged diffusive Vlasov-Boltzmann equation, encoding the effects of\ncollisional processes and turbulent chromo-fields in the medium. The\ncontributions from the $2\\rightarrow2$ elastic scattering processes have been\nquantified for the thermal dilepton production rate. It is seen that the\ncollisional corrections enhance the equilibrium dilepton spectra at high $p_T$\nand suppress at lower $p_T$. A comparative study between collisional and\nanomalous contributions to the dilepton production rates has also been\nexplored. The collisional contributions are seen to be marginal over that due\nto collisionless anomalous transport.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Occluded person re-identification is a challenging task as the appearance\nvaries substantially with various obstacles, especially in the crowd scenario.\nTo address this issue, we propose a Pose-guided Visible Part Matching (PVPM)\nmethod that jointly learns the discriminative features with pose-guided\nattention and self-mines the part visibility in an end-to-end framework.\nSpecifically, the proposed PVPM includes two key components: 1) pose-guided\nattention (PGA) method for part feature pooling that exploits more\ndiscriminative local features; 2) pose-guided visibility predictor (PVP) that\nestimates whether a part suffers the occlusion or not. As there are no ground\ntruth training annotations for the occluded part, we turn to utilize the\ncharacteristic of part correspondence in positive pairs and self-mining the\ncorrespondence scores via graph matching. The generated correspondence scores\nare then utilized as pseudo-labels for visibility predictor (PVP). Experimental\nresults on three reported occluded benchmarks show that the proposed method\nachieves competitive performance to state-of-the-art methods. The source codes\nare available at https://github.com/hh23333/PVPM\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Spontaneous pattern formation is a fundamental scientific problem that has\nreceived much attention since the seminal theoretical work of Turing on\nreaction-diffusion systems. In molecular biophysics, this phenomena often takes\nplace under the influence of large fluctuations. It is then natural to inquire\nabout the precision of such pattern. In particular, spontaneous pattern\nformation is a nonequilibrium phenomenon, and the relation between the\nprecision of a pattern and the thermodynamic cost associated with it remains\nunexplored. Here, we analyze this relation with a paradigmatic stochastic\nreaction-diffusion model, the Brusselator in one spatial dimension. We find\nthat the precision of the pattern is maximized for an intermediate\nthermodynamic cost, i.e., increasing the thermodynamic cost beyond this value\nmakes the pattern less precise. Even though fluctuations get less pronounced\nwith an increase in thermodynamic cost, we argue that larger fluctuations can\nalso have a positive effect on the precision of the pattern.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The mechanical response of naturally abundant amorphous solids such as gels,\njammed grains, and biological tissues are not described by the conventional\nparadigm of broken symmetry that defines crystalline elasticity. In contrast,\nthe response of such athermal solids are governed by local conditions of\nmechanical equilibrium, i.e., force and torque balance of its constituents.\nHere we show that these constraints have the mathematical structure of a\ngeneralized electromagnetism, where the electrostatic limit successfully\ncaptures the anisotropic elasticity of amorphous solids. The emergence of\nelasticity from local mechanical constraints offers a new paradigm for systems\nwith no broken symmetry, analogous to emergent gauge theories of quantum spin\nliquids. Specifically, our $U(1)$ rank-2 symmetric tensor gauge theory of\nelasticity translates to the electromagnetism of fractonic phases of matter\nwith the stress mapped to electric displacement and forces to vector charges.\nWe corroborate our theoretical results with numerical simulations of soft\nfrictionless disks in both two and three dimensions, and experiments on\nfrictional disks in two dimensions. We also present experimental evidence\nindicating that force chains in granular media are sub-dimensional excitations\nof amorphous elasticity similar to fractons.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We derive upper bounds for the eigenvalues of the Kirchhoff Laplacian on a\ncompact metric graph depending on the graph's genus g. These bounds can be\nfurther improved if $g = 0$, i.e. if the metric graph is planar. Our results\nare based on a spectral correspondence between the Kirchhoff Laplacian and a\nparticular a certain combinatorial weighted Laplacian. In order to take\nadvantage of this correspondence, we also prove new estimates for the\neigenvalues of the weighted combinatorial Laplacians that were previously known\nonly in the weighted case.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Exomoons are predicted to produce transit timing variations (TTVs) upon their\nhost planet. Unfortunately, so are many other astrophysical phenomena - most\nnotably other planets in the system. In this work, an argument of reductio ad\nabsurdum is invoked, by deriving the transit timing effects that are impossible\nfor a single exomoon to produce. Our work derives three key analytic tests.\nFirst, one may exploit the fact that a TTV signal from an exomoon should be\naccompanied by transit duration variations (TDVs), and that one can derive a\nTDV floor as a minimum expected level of variability. Cases for which the TDV\nupper limit is below this floor can thus be killed as exomoon candidates.\nSecond, formulae are provided for estimating whether moons are expected to be\n'killable' when no TDVs presently exist, thus enabling the community to\nestimate whether it's even worth deriving TDVs in the first place. Third, a TTV\nceiling is derived, above which exomoons should never be able to produce TTV\namplitudes. These tools are applied to a catalog of TTVs and TDVs for two and\nhalf thousand Kepler Objects Interest, revealing over two hundred cases that\ncannot be due to a moon. These tests are also applied to the exomoon candidate\nKepler-1625b i, which comfortably passes the criteria. These simple analytic\nresults should provide a means of rapidly rejecting putative exomoons and\nstreamlining the search for satellites.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Let P a locally finite partially ordered set, F a field, G a group, and\nI(P,F) the incidence algebra of P over F. We describe all the inequivalent\nelementary G-gradings on this algebra. If P is bounded, F is a infinite field\nof characteristic zero, and A, B are both elementary G-graded incidence\nalgebras satisfying the same G-graded polynomial identities, and the\nautomorphisms group of P acts transitively on the maximal chains of P , we show\nthat A and B are graded isomorphic.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Contextual bandits often provide simple and effective personalization in\ndecision making problems, making them popular tools to deliver personalized\ninterventions in mobile health as well as other health applications. However,\nwhen bandits are deployed in the context of a scientific study -- e.g. a\nclinical trial to test if a mobile health intervention is effective -- the aim\nis not only to personalize for an individual, but also to determine, with\nsufficient statistical power, whether or not the system's intervention is\neffective. It is essential to assess the effectiveness of the intervention\nbefore broader deployment for better resource allocation. The two objectives\nare often deployed under different model assumptions, making it hard to\ndetermine how achieving the personalization and statistical power affect each\nother. In this work, we develop general meta-algorithms to modify existing\nalgorithms such that sufficient power is guaranteed while still improving each\nuser's well-being. We also demonstrate that our meta-algorithms are robust to\nvarious model mis-specifications possibly appearing in statistical studies,\nthus providing a valuable tool to study designers.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We study the orbital stability of a non-zero mass, close-in circular orbit\nplanet around an eccentric orbit binary for various initial values of the\nbinary eccentricity, binary mass fraction, planet mass, planet semi--major\naxis, and planet inclination by means of numerical simulations that cover $5\n\\times 10^4$ binary orbits. For small binary eccentricity, the stable orbits\nthat extend closest to the binary (most stable orbits) are nearly retrograde\nand circulating. For high binary eccentricity, the most stable orbits are\nhighly inclined and librate near the so-called generalised polar orbit which is\na stationary orbit that is fixed in the frame of the binary orbit. For more\nextreme mass ratio binaries, there is a greater variation in the size of the\nstability region (defined by initial orbital radius and inclination) with\nplanet mass and initial inclination, especially for low binary eccentricity.\nFor low binary eccentricity, inclined planet orbits may be unstable even at\nlarge orbital radii (separation $> 5 \\,a_{\\rm b}$). The escape time for an\nunstable planet is generally shorter around an equal mass binary compared with\nan unequal mass binary. Our results have implications for circumbinary planet\nformation and evolution and will be helpful for understanding future\ncircumbinary planet observations.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Recent NICER observation data on PSR J0030+0451 has recently added a unique\nmass-radius constraint on the properties of the superdense nuclear matter\nexisting in the interior of compact stars. Such a macroscopic data restrict\nfurther the microscopical models, elementary interactions, and their\nparameters, however, with reasonable margin because of the masquarade problem.\nHere our goal is to identify the origin and quantify the magnitude of the\ntheoretical uncertainties of the nuclear matter models.\n  A detailed study on the effect of different interaction terms and the nuclear\nparameter values in the Lagrangian of the extended $\\sigma$-$\\omega$ model is\npresented here. The equation of state was inserted to the\nTolman--Oppenheimer--Volkoff equation and observable parameters of the neutron\nstar were calculated.\n  We identified, that the optimal Landau effective mass is the most relevant\nphysical parameter modifying the macroscopic observable values. Moreover, the\ncompressibility and symmetry energy terms just generate one-one order of\nmagnitude smaller effect to this, respectively. We calculated the linear\nrelations between the maximal mass of a compact star and these microscopic\nnuclear parameter values within the physical relevant parameters range. Based\non the mass observational data we estimated the magnitude of the radii of PSR\nJ1614-2230, PSR J0348+0432, and PSR J0740+6620 including theoretical\nuncertainties arising from the models' interaction terms and their parameter\nvalues choices.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The genesis of the stand genetic code is considered as a result of a fusion\nof two AU- and GC-codes distributed in two dominant and two recessive domains.\nThe fusion of these codes is described with simple empirical rules. This formal\napproach explains the number of the proteinogenic amino acids and the codon\nassignment in the resulting standard genetic code. It shows how norleucine,\npyrrolysine, selenocysteine and two other unknown amino acids, included into\nthe prebiotic codes, disappeared after the fusion. The properties of these two\nmissing amino acids were described. The ambiguous translation observed in\nmitochondria is explained. The internal structure of the codes allows a more\ndetailed insights into molecular evolution in prebiotic time. In particular,\nthe structure of the oldest single base-pair code is presented. The fusion\nconcept reveals the appearance of the DNA machinery on the level of the single\ndominant AU-code. The time before the appearance of standard genetic code is\ndivided into four epochs: pre-DNA, 2-code, pre-fusion, and after-fusion epochs.\nThe prebiotic single-base-pair codes may help design novel peptide-based\ncatalysts.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We present 3, 15, and 33 GHz imaging towards galaxy nuclei and extranuclear\nstar-forming regions using the Karl G. Jansky Very Large Array as part of the\nStar Formation in Radio Survey. With $3-33$ GHz radio spectra, we measured the\nspectral indices and corresponding thermal (free-free) emission fractions for a\nsample of 335 discrete regions having significant detections in at least two\nradio bands. After removing 14 likely background galaxies, we find that the\nmedian thermal fraction at 33 GHz is $92 \\pm 0.8\\%$ with a median absolute\ndeviation of $11\\%$, when a two-component power-law model is adopted to fit the\nradio spectrum. Limiting the sample to 238 sources that are confidently\nidentified as star-forming regions, and not affected by potential AGN\ncontamination (i.e., having galactocentric radii $r_{\\rm G} \\geq 250$ pc),\nresults in a median thermal fraction of $93 \\pm 0.8 \\%$ with a median absolute\ndeviation of $10\\%$. We further measure the thermal fraction at 33 GHz for 163\nregions identified at 7\" resolution to be $94 \\pm 0.8 \\%$ with a median\nabsolute deviation of $8\\%$. Together, these results confirm that free-free\nemission dominates the radio spectra of star-forming regions on scales up to\n$\\sim$500 pc in normal star-forming galaxies. We additionally find a factor of\n$\\sim$1.6 increase in the scatter of the measured spectral index and thermal\nfraction distributions as a function of decreasing galactocentric radius. This\ntrend is likely reflective of the continuous star-formation activity occurring\nin the galaxy centers, resulting a larger contribution of diffuse nonthermal\nemission relative to star-forming regions in the disk.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The Leiden Atomic and Molecular Database (LAMDA) collects spectroscopic\ninformation and collisional rate coefficients for molecules, atoms, and ions of\nastrophysical and astrochemical interest. We describe the developments of the\ndatabase since its inception in 2005, and outline our plans for the near\nfuture. Such a database is constrained both by the nature of its uses and by\nthe availability of accurate data: we suggest ways to improve the synergies\namong users and suppliers of data. We summarize some recent developments in\ncomputation of collisional cross sections and rate coefficients. We consider\natomic and molecular data that are needed to support astrophysics and\nastrochemistry with upcoming instruments that operate in the mid- and\nfar-infrared parts of the spectrum.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Pairs of graded graphs, together with the Fomin property of graded graph\nduality, are rich combinatorial structures providing among other a framework\nfor enumeration. The prototypical example is the one of the Young graded graph\nof integer partitions, allowing us to connect number of standard Young tableaux\nand numbers of permutations. Here, we use operads, that algebraic devices\nabstracting the notion of composition of combinatorial objects, to build pairs\nof graded graphs. For this, we first construct a pair of graded graphs where\nvertices are syntax trees, the elements of free nonsymmetric operads. This pair\nof graphs is dual for a new notion of duality called $\\phi$-diagonal duality,\nsimilar to the ones introduced by Fomin. We also provide a general way to build\npairs of graded graphs from operads, wherein underlying posets are analogous to\nthe Young lattice. Some examples of operads leading to new pairs of graded\ngraphs involving integer compositions, Motzkin paths, and $m$-trees are\nconsidered.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Many important problems can be formulated as reasoning in knowledge graphs.\nRepresentation learning has proved extremely effective for transductive\nreasoning, in which one needs to make new predictions for already observed\nentities. This is true for both attributed graphs(where each entity has an\ninitial feature vector) and non-attributed graphs (where the only initial\ninformation derives from known relations with other entities). For\nout-of-sample reasoning, where one needs to make predictions for entities that\nwere unseen at training time, much prior work considers attributed graph.\nHowever, this problem is surprisingly under-explored for non-attributed graphs.\nIn this paper, we study the out-of-sample representation learning problem for\nnon-attributed knowledge graphs, create benchmark datasets for this task,\ndevelop several models and baselines, and provide empirical analyses and\ncomparisons of the proposed models and baselines.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The curse of dimensionality causes the well-known and widely discussed\nproblems for machine learning methods. There is a hypothesis that using of the\nManhattan distance and even fractional quasinorms lp (for p less than 1) can\nhelp to overcome the curse of dimensionality in classification problems. In\nthis study, we systematically test this hypothesis. We confirm that fractional\nquasinorms have a greater relative contrast or coefficient of variation than\nthe Euclidean norm l2, but we also demonstrate that the distance concentration\nshows qualitatively the same behaviour for all tested norms and quasinorms and\nthe difference between them decays as dimension tends to infinity. Estimation\nof classification quality for kNN based on different norms and quasinorms shows\nthat a greater relative contrast does not mean better classifier performance\nand the worst performance for different databases was shown by different norms\n(quasinorms). A systematic comparison shows that the difference of the\nperformance of kNN based on lp for p=2, 1, and 0.5 is statistically\ninsignificant.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The attention mechanism of the Listen, Attend and Spell (LAS) model requires\nthe whole input sequence to calculate the attention context and thus is not\nsuitable for online speech recognition. To deal with this problem, we propose\nmulti-head monotonic chunk-wise attention (MTH-MoChA), an improved version of\nMoChA. MTH-MoChA splits the input sequence into small chunks and computes\nmulti-head attentions over the chunks. We also explore useful training\nstrategies such as LSTM pooling, minimum world error rate training and\nSpecAugment to further improve the performance of MTH-MoChA. Experiments on\nAISHELL-1 data show that the proposed model, along with the training\nstrategies, improve the character error rate (CER) of MoChA from 8.96% to 7.68%\non test set. On another 18000 hours in-car speech data set, MTH-MoChA obtains\n7.28% CER, which is significantly better than a state-of-the-art hybrid system.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The atomic and electronic structure of the p-type transparent amorphous\nsemiconductor CuI is calculated by ab-initio molecular dynamics. It is found to\nconsist of a random tetrahedrally bonded network. The hole effective mass is\nfound to be quite low, as in the crystal. The valence band maximum (VBM) state\nhas a mixed I(p)-Cu(t2g)-I(p) character, and its energy is relatively\ninsensitive to disorder. An iodine excess creates holes that move the Fermi\nlevel into the valence band, but it does not pin the Fermi level above the VBM\nmobility edge. Thus the Fermi level can easily enter the valence band if\np-doped, similar to the behavior of electrons in In-Ga-Zn oxide semiconductors\nbut opposite to that of electrons in a-Si:H. This suggests that amorphous CuI\ncould make an effective p-type transparent semiconductor.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The right to be forgotten states that a data owner has the right to erase\ntheir data from an entity storing it. In the context of machine learning (ML),\nthe right to be forgotten requires an ML model owner to remove the data owner's\ndata from the training set used to build the ML model, a process known as\nmachine unlearning. While originally designed to protect the privacy of the\ndata owner, we argue that machine unlearning may leave some imprint of the data\nin the ML model and thus create unintended privacy risks. In this paper, we\nperform the first study on investigating the unintended information leakage\ncaused by machine unlearning. We propose a novel membership inference attack\nthat leverages the different outputs of an ML model's two versions to infer\nwhether a target sample is part of the training set of the original model but\nout of the training set of the corresponding unlearned model. Our experiments\ndemonstrate that the proposed membership inference attack achieves strong\nperformance. More importantly, we show that our attack in multiple cases\noutperforms the classical membership inference attack on the original ML model,\nwhich indicates that machine unlearning can have counterproductive effects on\nprivacy. We notice that the privacy degradation is especially significant for\nwell-generalized ML models where classical membership inference does not\nperform well. We further investigate four mechanisms to mitigate the newly\ndiscovered privacy risks and show that releasing the predicted label only,\ntemperature scaling, and differential privacy are effective. We believe that\nour results can help improve privacy protection in practical implementations of\nmachine unlearning. Our code is available at\nhttps://github.com/MinChen00/UnlearningLeaks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  A Non-terrestrial Network (NTN) comprising Low Earth Orbit (LEO) satellites\ncan enable connectivity to underserved areas, thus complementing existing\ntelecom networks. The high-speed satellite motion poses several challenges at\nthe physical layer such as large Doppler frequency shifts. In this paper, an\nanalytical framework is developed for statistical characterization of Doppler\nshift in an NTN where LEO satellites provide communication services to\nterrestrial users. Using tools from stochastic geometry, the users within a\ncell are grouped into disjoint clusters to limit the differential Doppler\nacross users. Under some simplifying assumptions, the cumulative distribution\nfunction (CDF) and the probability density function are derived for the Doppler\nshift magnitude at a random user within a cluster. The CDFs are also provided\nfor the minimum and the maximum Doppler shift magnitude within a cluster.\nLeveraging the analytical results, the interplay between key system parameters\nsuch as the cluster size and satellite altitude is examined. Numerical results\nvalidate the insights obtained from the analysis.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The functionals of double phase type \\[ \\mathcal{H} (u):= \\int \\left(|Du|^{p}\n+ a(x)|Du|^{q} \\right) dx, ( q > p > 1, a(x)\\geq 0) \\] are introduced in the\nepoch-making paper by Colombo-Mingione for constants $p$ and $q$, and\ninvestigated by them and Baroni. They obtained sharp regularity results for\nminimizers of such functionals. In this paper we treat the case that the\nexponents are functions of $x$ and partly generalize their regularity results.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  One of the key science goals for the most sensitive telescopes, both current\nand upcoming, is the detection of the redshifted 21-cm signal from the Cosmic\nDawn and Epoch of Reionization. The success of detection relies on accurate\nforeground modeling for their removal from data sets. This paper presents the\ncharacterization of astrophysical sources in the Lockman Hole region. Using 325\nMHz data obtained from the GMRT, a $6^\\circ \\times 6^\\circ$ mosaiced map is\nproduced with an RMS reaching 50 $\\mu$Jy $\\mathrm{beam}^{-1}$. A source catalog\ncontaining 6186 sources is created, and the Euclidean normalized differential\nsource counts have been derived from it, consistent with previous observations\nas well as simulations. A detailed comparison of the source catalog is also\nmade with previous findings - at both lower and higher frequencies. The angular\npower spectrum (APS) of the diffuse Galactic synchrotron emission is determined\nfor three different galactic latitudes using the Tapered Gridded Estimator. The\nvalues of the APS lie between $\\sim$1 mK$^2$ to $\\sim$100 mK$^2$. Fitting a\npower law of the form $A\\ell^{-\\beta}$ gives values of $A$ and $\\beta$ varying\nacross the latitudes considered. This paper demonstrates, for the first time,\nthe variation of the power-law index for diffuse emission at very high galactic\nlocations. It follows the same trend that is seen at locations near the\ngalactic plane, thus emphasizing the need for low-frequency observations for\ndeveloping better models of the diffuse emission.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We give a new, direct proof of the formulas for the conic intrinsic volumes\nof the Weyl chambers of types $A_{n-1}, B_n$ and $D_n$. These formulas express\nthe conic intrinsic volumes in terms of the Stirling numbers of the first kind\nand their $B$- and $D$-analogues. The proof involves an explicit determination\nof the internal and external angles of the faces of the Weyl chambers.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We study the quantum phase transition upon variation of the fermionic density\n$\\nu$ in a solvable model with random Yukawa interactions between $N$ bosons\nand $M$ fermions, dubbed the Yukawa-SYK model. We show that there are two\ndistinct phases in the model: an incompressible state with gapped excitations\nand an exotic quantum-critical, non-Fermi liquid state with exponents varying\nwith $\\nu$. We show analytically and numerically that the quantum phase\ntransition between these two states is first-order, as for some range of $\\nu$\nthe NFL state has a negative compressibility. In the limit $N/M\\to \\infty$ the\nfirst-order transition gets weaker and asymptotically becomes second-order,\nwith an exotic quantum-critical behavior. We show that fermions and bosons\ndisplay highly unconventional spectral behavior in the transition region.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Typical random codes (TRC) in a communication scenario of source coding with\nside information at the decoder is the main subject of this work. We study the\nsemi-deterministic code ensemble, which is a certain variant of the ordinary\nrandom binning code ensemble. In this code ensemble, the relatively small type\nclasses of the source are deterministically partitioned into the available bins\nin a one-to-one manner. As a consequence, the error probability decreases\ndramatically. The random binning error exponent and the error exponent of the\nTRC are derived and proved to be equal to one another in a few important\nspecial cases. We show that the performance under optimal decoding can be\nattained also by certain universal decoders, e.g., the stochastic likelihood\ndecoder with an empirical entropy metric. Moreover, we discuss the trade-offs\nbetween the error exponent and the excess-rate exponent for the typical random\nsemi-deterministic code and characterize its optimal rate function. We show\nthat for any pair of correlated information sources, both error and excess-rate\nprobabilities are exponentially vanishing when the blocklength tends to\ninfinity.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Graham and Winkler derived a formula for the determinant of the distance\nmatrix of a full-dimensional set of $n + 1$ points $\\{ x_{0}, x_{1}, \\ldots ,\nx_{n} \\}$ in the Hamming cube $H_{n} = ( \\{ 0,1 \\}^{n}, \\ell_{1} )$. In this\narticle we derive a formula for the determinant of the distance matrix $D$ of\nan arbitrary set of $m + 1$ points $\\{ x_{0}, x_{1}, \\ldots , x_{m} \\}$ in\n$H_{n}$. It follows from this more general formula that $\\det (D) \\not= 0$ if\nand only if the vectors $x_{0}, x_{1}, \\ldots , x_{m}$ are affinely\nindependent. Specializing to the case $m = n$ provides new insights into the\noriginal formula of Graham and Winkler. A significant difference that arises\nbetween the cases $m < n$ and $m = n$ is noted. We also show that if $D$ is the\ndistance matrix of an unweighted tree on $n + 1$ vertices, then $\\langle D^{-1}\n\\mathbf{1}, \\mathbf{1} \\rangle = 2/n$ where $\\mathbf{1}$ is the column vector\nall of whose coordinates are $1$. Finally, we derive a new proof of Murugan's\nclassification of the subsets of $H_{n}$ that have strict $1$-negative type.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this article, the theoretical model on heat and momentum transfer for\nRayleigh-B\\'enard convection in a vertical magnetic field by Z\\\"urner et al.\n(Phys. Rev. E 94, 043108 (2016)) is revisited. Using new data from recent\nexperimental and numerical studies the model is simplified and extended to the\nfull range of Hartmann numbers, reproducing the results of the Grossmann-Lohse\ntheory in the limit of vanishing magnetic fields. The revised model is compared\nto experimental results in liquid metal magnetoconvection and shows that the\nheat transport is described satisfactorily. The momentum transport, represented\nby the Reynolds number, agrees less well which reveals some shortcomings in the\ntheoretical treatment of magnetoconvection.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The Cauchy problem for non-isentropic compressible Navier-Stokes/Allen-Cahn\nsystem with degenerate heat-conductivity\n$\\kappa(\\theta)=\\tilde{\\kappa}\\theta^\\beta$ in 1-d is discussed in this paper.\nThis system is widely used to describe the motion of immiscible two-phase flow\nin numerical simulation. The wellposedness for strong solution of this problem\nis established with the $H^1$ initial data for density, temperature, velocity,\nand the $H^2$ initial data for phase field. The result shows that no\ndiscontinuity of the phase field, vacuum, shock wave, mass or heat\nconcentration will be developed at any finite time in the whole space. From the\nhydrodynamic point of view, this means that no matter how complex the\ninteraction between the hydrodynamic and phase-field effects, phase separation\nwill not occur, but the phase transition is possible.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Quantum confinement of graphene Dirac-like electrons in artificially crafted\nnanometer structures is a long sought goal that would provide a strategy to\nselectively tune the electronic properties of graphene, including bandgap\nopening or quantization of energy levels However, creating confining structures\nwith nanometer precision in shape, size and location, remains as an\nexperimental challenge, both for top-down and bottom-up approaches. Moreover,\nKlein tunneling, offering an escape route to graphene electrons, limits the\nefficiency of electrostatic confinement. Here, a scanning tunneling microscope\n(STM) is used to create graphene nanopatterns, with sub-nanometer precision, by\nthe collective manipulation of a large number of H atoms. Individual graphene\nnanostructures are built at selected locations, with predetermined orientations\nand shapes, and with dimensions going all the way from 2 nanometers up to 1\nmicron. The method permits to erase and rebuild the patterns at will, and it\ncan be implemented on different graphene substrates. STM experiments\ndemonstrate that such graphene nanostructures confine very efficiently graphene\nDirac quasiparticles, both in zero and one dimensional structures. In graphene\nquantum dots, perfectly defined energy band gaps up to 0.8 eV are found, that\nscale as the inverse of the dots linear dimension, as expected for massless\nDirac fermions\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The interplay among topology, disorder, and non-Hermiticity can induce some\nexotic topological and localization phenomena. Here we investigate this\ninterplay in a two-dimensional non-Hermitian disordered Chern-insulator model\nwith two typical kinds of non-Hermiticities, the nonreciprocal hopping and\non-site gain-and-loss effects. The topological phase diagrams are obtained by\nnumerically calculating two topological invariants in the real space, which are\nthe disorder-averaged open-bulk Chern number and the generalized Bott index,\nrespectively. We reveal that the nonreciprocal hopping (the gain-and-loss\neffect) can enlarge (reduce) the topological regions and the topological\nAnderson insulators induced by disorders can exist under both kinds of\nnon-Hermiticities. Furthermore, we study the localization properties of the\nsystem in the topologically nontrivial and trivial regions by using the inverse\nparticipation ratio and the expansion of single particle density distribution.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Motivated by the possible presence of deconfined quark matter in neutron\nstars and their mergers and the important role of transport phenomena in these\nsystems, we perform the first-ever systematic study of different viscosities\nand conductivities of dense quark matter using the gauge/gravity duality.\nUtilizing the V-QCD model, we arrive at results that are in qualitative\ndisagreement with the predictions of perturbation theory, which highlights the\ndiffering transport properties of the system at weak and strong coupling and\ncalls for caution in the use of the perturbative results in neutron-star\napplications.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We demonstrate terahertz time-domain spectroscopy (THz-TDS) to be an\naccurate, rapid and scalable method to probe the interaction-induced Fermi\nvelocity renormalization {\\nu}F^* of charge carriers in graphene. This allows\nthe quantitative extraction of all electrical parameters (DC conductivity\n{\\sigma}DC, carrier density n, and carrier mobility {\\mu}) of large-scale\ngraphene films placed on arbitrary substrates via THz-TDS. Particularly\nrelevant are substrates with low relative permittivity (< 5) such as polymeric\nfilms, where notable renormalization effects are observed even at relatively\nlarge carrier densities (> 10^12 cm-2, Fermi level > 0.1 eV). From an\napplication point of view, the ability to rapidly and non-destructively\nquantify and map the electrical ({\\sigma}DC, n, {\\mu}) and electronic ({\\nu}F^*\n) properties of large-scale graphene on generic substrates is key to utilize\nthis material in applications such as metrology, flexible electronics as well\nas to monitor graphene transfers using polymers as handling layers.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper, a large class of time-varying Riccati equations arising in\nstochastic dynamic games is considered. The problem of the existence and\nuniqueness of some globally defined solution, namely the bounded and\nstabilizing solution, is investigated. As an application of the obtained\nexistence results, we address in a second step the problem of infinite-horizon\nzero-sum two players linear quadratic (LQ) dynamic game for a stochastic\ndiscrete-time dynamical system subject to both random switching of its\ncoefficients and multiplicative noise. We show that in the solution of such an\noptimal control problem, a crucial role is played by the unique bounded and\nstabilizing solution of the considered class of generalized Riccati equations.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Due to their strong and tunable interactions, Rydberg atoms can be used to\nrealize fast two-qubit entangling gates. We propose a generalization of a\ngeneric two-qubit Rydberg-blockade gate to multi-qubit Rydberg-blockade gates\nwhich involve both many control qubits and many target qubits simultaneously.\nThis is achieved by using strong microwave fields to dress nearby Rydberg\nstates, leading to asymmetric blockade in which control-target interactions are\nmuch stronger than control-control and target-target interactions. The\nimplementation of these multi-qubit gates can drastically simplify both quantum\nalgorithms and state preparation. To illustrate this, we show that a 25-atom\nGHZ state can be created using only three gates with an error of 7.8%.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This work proves that semantic segmentation on minimally invasive surgical\ninstruments can be improved by using training data that has been augmented\nthrough domain adaptation. The benefit of this method is twofold. Firstly, it\nsuppresses the need of manually labeling thousands of images by transforming\nsynthetic data into realistic-looking data. To achieve this, a CycleGAN model\nis used, which transforms a source dataset to approximate the domain\ndistribution of a target dataset. Secondly, this newly generated data with\nperfect labels is utilized to train a semantic segmentation neural network,\nU-Net. This method shows generalization capabilities on data with variability\nregarding its rotation- position- and lighting conditions. Nevertheless, one of\nthe caveats of this approach is that the model is unable to generalize well to\nother surgical instruments with a different shape from the one used for\ntraining. This is driven by the lack of a high variance in the geometric\ndistribution of the training data. Future work will focus on making the model\nmore scale-invariant and able to adapt to other types of surgical instruments\npreviously unseen by the training.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Shock wave gives back reaction to spacetime and the information is stored in\nthe memory of background. Quantum memory effect of localised shock wave on\nMinkowski metric is investigated here. We find that the Wightman function for\nmassless scalar field, on both sides of the wave location, turns out to be the\nsame for usual Minkowski spacetime. Therefore the observables obtained from\nthis, for any kind of observer, do not show the signature of the shock.\nMoreover, the vacuum states of field on both sides are equivalent. On the\ncontrary, the correlator for the non-vacuum state does memorise the classical\nshock wave and hence the effect of it is visible in the observables, even for\nany frame. We argue that rather than vacuum state, the non-vacuum ones are\nrelevant to retrieve the quantum information of classical memory.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We provide an error bound for approximating the time evolution of N bosons by\na generalized nonlinear Hartree equation. The bosons are assumed to interact\nvia permutation symmetric bounded many-particle potentials and the initial\nwave-function is a product state. We show that the error between the actual\nevolution of a single particle derived from tracing out the full N-particle\nSchrodinger equation and the solution to the mean field approximate generalized\nnonlinear Hartree equation scales as 1/N for all times. Our result is a\ngeneralization of rigorous error bounds previously given for the case of\nbounded 2-particle potentials\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The Brownian bees model is a branching particle system with spatial\nselection. It is a system of $N$ particles which move as independent Brownian\nmotions in $\\mathbb{R}^d$ and independently branch at rate 1, and, crucially,\nat each branching event, the particle which is the furthest away from the\norigin is removed to keep the population size constant. In the present work we\nprove that as $N \\to \\infty$ the behaviour of the particle system is well\napproximated by the solution of a free boundary problem (which is the subject\nof a companion paper), the hydrodynamic limit of the system. We then show that\nfor this model the so-called selection principle holds, i.e. that as $N \\to\n\\infty$ the equilibrium density of the particle system converges to the steady\nstate solution of the free boundary problem.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Edge connectivity of a graph is one of the most fundamental graph-theoretic\nconcepts. The celebrated tree packing theorem of Tutte and Nash-Williams from\n1961 states that every $k$-edge connected graph $G$ contains a collection\n$\\cal{T}$ of $\\lfloor k/2 \\rfloor$ edge-disjoint spanning trees, that we refer\nto as a tree packing; the diameter of the tree packing $\\cal{T}$ is the largest\ndiameter of any tree in $\\cal{T}$. A desirable property of a tree packing, that\nis both sufficient and necessary for leveraging the high connectivity of a\ngraph in distributed communication, is that its diameter is low. Yet, despite\nextensive research in this area, it is still unclear how to compute a tree\npacking, whose diameter is sublinear in $|V(G)|$, in a low-diameter graph $G$,\nor alternatively how to show that such a packing does not exist.\n  In this paper we provide first non-trivial upper and lower bounds on the\ndiameter of tree packing. First, we show that, for every $k$-edge connected\n$n$-vertex graph $G$ of diameter $D$, there is a tree packing $\\cal{T}$ of size\n$\\Omega(k)$, diameter $O((101k\\log n)^D)$, that causes edge-congestion at most\n$2$. Second, we show that for every $k$-edge connected $n$-vertex graph $G$ of\ndiameter $D$, the diameter of $G[p]$ is $O(k^{D(D+1)/2})$ with high\nprobability, where $G[p]$ is obtained by sampling each edge of $G$\nindependently with probability $p=\\Theta(\\log n/k)$. This provides a packing of\n$\\Omega(k/\\log n)$ edge-disjoint trees of diameter at most $O(k^{(D(D+1)/2)})$\neach. We then prove that these two results are nearly tight. Lastly, we show\nthat if every pair of vertices in a graph has $k$ edge-disjoint paths of length\nat most $D$ connecting them, then there is a tree packing of size $k$, diameter\n$O(D\\log n)$, causing edge-congestion $O(\\log n)$. We also provide several\napplications of low-diameter tree packing in distributed computation.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Literature-based discovery (LBD) aims to discover valuable latent\nrelationships between disparate sets of literatures. This paper presents the\nfirst inclusive scientometric overview of LBD research. We utilize a\ncomprehensive scientometric approach incorporating CiteSpace to systematically\nanalyze the literature on LBD from the last four decades (1986-2020). After\nmanual cleaning, we have retrieved a total of 409 documents from six\nbibliographic databases and two preprint servers. The 35 years' history of LBD\ncould be partitioned into three phases according to the published papers per\nyear: incubation (1986-2003), developing (2004-2008), and mature phase\n(2009-2020). The annual production of publications follows Price's law. The\nco-authorship network exhibits many subnetworks, indicating that LBD research\nis composed of many small and medium-sized groups with little collaboration\namong them. Science mapping reveals that mainstream research in LBD has shifted\nfrom baseline co-occurrence approaches to semantic-based methods at the\nbeginning of the new millennium. In the last decade, we can observe the leaning\nof LBD towards modern network science ideas. In an applied sense, the LBD is\nincreasingly used in predicting adverse drug reactions and drug repurposing.\nBesides theoretical considerations, the researchers have put a lot of effort\ninto the development of Web-based LBD applications. Nowadays, LBD is becoming\nincreasingly interdisciplinary and involves methods from information science,\nscientometrics, and machine learning. Unfortunately, LBD is mainly limited to\nthe biomedical domain. The cascading citation expansion announces deep learning\nand explainable artificial intelligence as emerging topics in LBD. The results\nindicate that LBD is still growing and evolving.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Although model-agnostic meta-learning (MAML) is a very successful algorithm\nin meta-learning practice, it can have high computational cost because it\nupdates all model parameters over both the inner loop of task-specific\nadaptation and the outer-loop of meta initialization training. A more efficient\nalgorithm ANIL (which refers to almost no inner loop) was proposed recently by\nRaghu et al. 2019, which adapts only a small subset of parameters in the inner\nloop and thus has substantially less computational cost than MAML as\ndemonstrated by extensive experiments. However, the theoretical convergence of\nANIL has not been studied yet. In this paper, we characterize the convergence\nrate and the computational complexity for ANIL under two representative\ninner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our\nresults show that such a geometric property can significantly affect the\noverall convergence performance of ANIL. For example, ANIL achieves a faster\nconvergence rate for a strongly-convex inner-loop loss as the number $N$ of\ninner-loop gradient descent steps increases, but a slower convergence rate for\na nonconvex inner-loop loss as $N$ increases. Moreover, our complexity analysis\nprovides a theoretical quantification on the improved efficiency of ANIL over\nMAML. The experiments on standard few-shot meta-learning benchmarks validate\nour theoretical findings.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We develop the theory of Kim-independence in the context of NSOP$_{1}$\ntheories satsifying the existence axiom. We show that, in such theories,\nKim-independence is transitive and that $\\ind^{K}$-Morley sequences witness\nKim-dividing. As applications, we show that, under the assumption of existence,\nin a low NSOP$_{1}$ theory, Shelah strong types and Lascar strong types\ncoincide and, additionally, we introduce a notion of rank for NSOP$_{1}$\ntheories.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Vehicle re-identification (reID) aims at identifying vehicles across\ndifferent non-overlapping cameras views. The existing methods heavily relied on\nwell-labeled datasets for ideal performance, which inevitably causes fateful\ndrop due to the severe domain bias between the training domain and the\nreal-world scenes; worse still, these approaches required full annotations,\nwhich is labor-consuming. To tackle these challenges, we propose a novel\nprogressive adaptation learning method for vehicle reID, named PAL, which\ninfers from the abundant data without annotations. For PAL, a data adaptation\nmodule is employed for source domain, which generates the images with similar\ndata distribution to unlabeled target domain as ``pseudo target samples''.\nThese pseudo samples are combined with the unlabeled samples that are selected\nby a dynamic sampling strategy to make training faster. We further proposed a\nweighted label smoothing (WLS) loss, which considers the similarity between\nsamples with different clusters to balance the confidence of pseudo labels.\nComprehensive experimental results validate the advantages of PAL on both\nVehicleID and VeRi-776 dataset.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  High-dimensional generative models have many applications including image\ncompression, multimedia generation, anomaly detection and data completion.\nState-of-the-art estimators for natural images are autoregressive, decomposing\nthe joint distribution over pixels into a product of conditionals parameterized\nby a deep neural network, e.g. a convolutional neural network such as the\nPixelCNN. However, PixelCNNs only model a single decomposition of the joint,\nand only a single generation order is efficient. For tasks such as image\ncompletion, these models are unable to use much of the observed context. To\ngenerate data in arbitrary orders, we introduce LMConv: a simple modification\nto the standard 2D convolution that allows arbitrary masks to be applied to the\nweights at each location in the image. Using LMConv, we learn an ensemble of\ndistribution estimators that share parameters but differ in generation order,\nachieving improved performance on whole-image density estimation (2.89 bpd on\nunconditional CIFAR10), as well as globally coherent image completions. Our\ncode is available at https://ajayjain.github.io/lmconv.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Sparse neural networks are shown to give accurate predictions competitive to\ndenser versions, while also minimizing the number of arithmetic operations\nperformed. However current hardware like GPU's can only exploit structured\nsparsity patterns for better efficiency. Hence the run time of a sparse neural\nnetwork may not correspond to the arithmetic operations required.\n  In this work, we propose RBGP( Ramanujan Bipartite Graph Product) framework\nfor generating structured multi level block sparse neural networks by using the\ntheory of Graph products. We also propose to use products of Ramanujan graphs\nwhich gives the best connectivity for a given level of sparsity. This\nessentially ensures that the i.) the networks has the structured block sparsity\nfor which runtime efficient algorithms exists ii.) the model gives high\nprediction accuracy, due to the better expressive power derived from the\nconnectivity of the graph iii.) the graph data structure has a succinct\nrepresentation that can be stored efficiently in memory. We use our framework\nto design a specific connectivity pattern called RBGP4 which makes efficient\nuse of the memory hierarchy available on GPU. We benchmark our approach by\nexperimenting on image classification task over CIFAR dataset using VGG19 and\nWideResnet-40-4 networks and achieve 5-9x and 2-5x runtime gains over\nunstructured and block sparsity patterns respectively, while achieving the same\nlevel of accuracy.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  It has so far proven impossible to reproduce all aspects of the solar plage\nchromosphere in quasi-realistic numerical models. The magnetic field\nconfiguration in the lower atmosphere is one of the few free parameters in such\nsimulations. The literature only offers proxy-based estimates of the field\nstrength, as it is difficult to obtain observational constraints in this\nregion. Sufficiently sensitive spectro-polarimetric measurements require a high\nsignal-to-noise ratio, spectral resolution, and cadence, which are at the limit\nof current capabilities. We use critically sampled spectro-polarimetric\nobservations of the \\cair line obtained with the CRISP instrument of the\nSwedish 1-m Solar Telescope to study the strength and inclination of the\nchromospheric magnetic field of a plage region. This will provide direct\nphysics-based estimates of these values, which could aid modelers to put\nconstraints on plage models. We increased the signal-to-noise ratio of the data\nby applying several methods including deep learning and PCA. We estimated the\nnoise level to be $1\\cdot10^{-3} I_c$. We then used STiC, a non-local\nthermodynamic equilibrium (NLTE) inversion code to infer the atmospheric\nstructure and magnetic field pixel by pixel. We are able to infer the magnetic\nfield strength and inclination for a plage region and for fibrils in the\nsurrounding canopy. In the plage we report an absolute field strength of $|B|\n=440 \\pm 90$ G, with an inclination of $10^\\circ \\pm 16^\\circ$ with respect to\nthe local vertical. This value for $|B|$ is roughly double of what was reported\npreviously, while the inclination matches previous studies done in the\nphotosphere. In the fibrillar region we found $|B| = 300 \\pm 50$ G, with an\ninclination of $50^\\circ \\pm 13^\\circ$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We propose a transductive Laplacian-regularized inference for few-shot tasks.\nGiven any feature embedding learned from the base classes, we minimize a\nquadratic binary-assignment function containing two terms: (1) a unary term\nassigning query samples to the nearest class prototype, and (2) a pairwise\nLaplacian term encouraging nearby query samples to have consistent label\nassignments. Our transductive inference does not re-train the base model, and\ncan be viewed as a graph clustering of the query set, subject to supervision\nconstraints from the support set. We derive a computationally efficient bound\noptimizer of a relaxation of our function, which computes independent\n(parallel) updates for each query sample, while guaranteeing convergence.\nFollowing a simple cross-entropy training on the base classes, and without\ncomplex meta-learning strategies, we conducted comprehensive experiments over\nfive few-shot learning benchmarks. Our LaplacianShot consistently outperforms\nstate-of-the-art methods by significant margins across different models,\nsettings, and data sets. Furthermore, our transductive inference is very fast,\nwith computational times that are close to inductive inference, and can be used\nfor large-scale few-shot tasks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  By assuming that tau protein can be in seven kinetic states, we developed a\nmodel of tau protein transport in the axon and in the axon initial segment\n(AIS). Two separate sets of kinetic constants were determined, one in the axon\nand the other in the AIS. This was done by fitting the model predictions in the\naxon with experimental results and by fitting the model predictions in the AIS\nwith the assumed linear increase of the total tau concentration in the AIS. The\ncalibrated model was used to make predictions about tau transport in the axon\nand in the AIS. To the best of our knowledge, this is the first paper that\npresents a mathematical model of tau transport in the AIS. Our modeling results\nsuggest that binding of free tau to MTs creates a negative gradient of free tau\nin the AIS. This leads to diffusion-driven tau transport from the soma into the\nAIS. The model further suggests that slow axonal transport and diffusion-driven\ntransport of tau work together in the AIS, moving tau anterogradely. Our\nnumerical results predict an interplay between these two mechanisms: as the\ndistance from the soma increases, the diffusion-driven transport decreases,\nwhile motor-driven transport becomes larger. Thus, the machinery in the AIS\nworks as a pump, moving tau into the axon.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Electrical energy storage is considered essential for the future energy\nsystems. Among all the energy storage technologies, battery systems may provide\nflexibility to the power grid in a more distributed and decentralized way. In\ncountries with deregulated electricity markets, grid-connected battery systems\nshould be operated under the specific market design of the country. In this\nwork, using the Spanish electricity market as an example, the barriers to\ngrid-connected battery systems are investigated using utilization analysis. The\nconcept of \"potentially profitable utilization time\" is proposed and introduced\nto identify and evaluate future potential grid applications for battery\nsystems. The numerical and empirical analysis suggests that the high cycle cost\nfor battery systems is still the main barrier for grid-connected battery\nsystems. In Spain, for energy arbitrage within the day-ahead market, it is\nrequired that the battery wear cost decreases to 15 Euro/MWh to make the\npotentially profitable utilization rate higher than 20%. Nevertheless, the\npotentially profitable utilization of batteries is much higher in the\napplications when higher flexibility is demanded. The minimum required battery\nwear cost corresponding to 20% potentially profitable utilization time\nincreases to 35 Euro/MWh for energy arbitrage within the day-ahead market and\nancillary services, and 50 Euro/MWh for upward secondary reserve. The results\nof this study contribute to the awareness of battery storage technology and its\nflexibility in grid applications. The findings also have significant\nimplications for policy makers and market operators interested in promoting\ngrid-connected battery storage under a deregulated power market.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper, we propose a novel network training mechanism called \"dynamic\nchannel propagation\" to prune the neural networks during the training period.\nIn particular, we pick up a specific group of channels in each convolutional\nlayer to participate in the forward propagation in training time according to\nthe significance level of channel, which is defined as channel utility. The\nutility values with respect to all selected channels are updated simultaneously\nwith the error back-propagation process and will adaptively change.\nFurthermore, when the training ends, channels with high utility values are\nretained whereas those with low utility values are discarded. Hence, our\nproposed scheme trains and prunes neural networks simultaneously. We\nempirically evaluate our novel training scheme on various representative\nbenchmark datasets and advanced convolutional neural network (CNN)\narchitectures, including VGGNet and ResNet. The experiment results verify the\nsuperior performance and robust effectiveness of our approach.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Overparametrized neural networks trained by gradient descent (GD) can\nprovably overfit any training data. However, the generalization guarantee may\nnot hold for noisy data. From a nonparametric perspective, this paper studies\nhow well overparametrized neural networks can recover the true target function\nin the presence of random noises. We establish a lower bound on the $L_2$\nestimation error with respect to the GD iterations, which is away from zero\nwithout a delicate scheme of early stopping. In turn, through a comprehensive\nanalysis of $\\ell_2$-regularized GD trajectories, we prove that for\noverparametrized one-hidden-layer ReLU neural network with the $\\ell_2$\nregularization: (1) the output is close to that of the kernel ridge regression\nwith the corresponding neural tangent kernel; (2) minimax {optimal} rate of\n$L_2$ estimation error can be achieved. Numerical experiments confirm our\ntheory and further demonstrate that the $\\ell_2$ regularization approach\nimproves the training robustness and works for a wider range of neural\nnetworks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Security metrics present the security level of a system or a network in both\nqualitative and quantitative ways. In general, security metrics are used to\nassess the security level of a system and to achieve security goals. There are\na lot of security metrics for security analysis, but there is no systematic\nclassification of security metrics that are based on network reachability\ninformation. To address this, we propose a systematic classification of\nexisting security metrics based on network reachability information. Mainly, we\nclassify the security metrics into host-based and network-based metrics. The\nhost-based metrics are classified into metrics ``without probability\" and \"with\nprobability\", while the network-based metrics are classified into \"path-based\"\nand \"non-path based\". Finally, we present and describe an approach to develop\ncomposite security metrics and it's calculations using a Hierarchical Attack\nRepresentation Model (HARM) via an example network. Our novel classification of\nsecurity metrics provides a new methodology to assess the security of a system.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this work, we study some novel applications of conformal inference\ntechniques to the problem of providing machine learning procedures with more\ntransparent, accurate, and practical performance guarantees. We provide a\nnatural extension of the traditional conformal prediction framework, done in\nsuch a way that we can make valid and well-calibrated predictive statements\nabout the future performance of arbitrary learning algorithms, when passed an\nas-yet unseen training set. In addition, we include some nascent empirical\nexamples to illustrate potential applications.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The Brouwer fixed-point theorem in topology states that for any continuous\nmapping $f$ on a compact convex set into itself admits a fixed point, i.e., a\npoint $x_0$ such that $f(x_0)=x_0$. Under certain conditions, this fixed point\ncorresponds to the throat of a traversable wormhole, i.e., $b(r_0)=r_0$ for the\nshape function $b=b(r)$. The possible existence of wormholes can therefore be\ndeduced from purely mathematical considerations without going beyond the\nexisting physical requirements.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Speech recognition is a well developed research field so that the current\nstate of the art systems are being used in many applications in the software\nindustry, yet as by today, there still does not exist such robust system for\nthe recognition of words and sentences from singing voice. This paper proposes\na complete pipeline for this task which may commonly be referred as automatic\nlyrics transcription (ALT). We have trained convolutional time-delay neural\nnetworks with self-attention on monophonic karaoke recordings using a sequence\nclassification objective for building the acoustic model. The dataset used in\nthis study, DAMP - Sing! 300x30x2 [1] is filtered to have songs with only\nEnglish lyrics. Different language models are tested including MaxEnt and\nRecurrent Neural Networks based methods which are trained on the lyrics of pop\nsongs in English. An in-depth analysis of the self-attention mechanism is held\nwhile tuning its context width and the number of attention heads. Using the\nbest settings, our system achieves notable improvement to the state-of-the-art\nin ALT and provides a new baseline for the task.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The world of linear radio broadcasting is characterized by a wide variety of\nstations and played content. That is why finding stations playing the preferred\ncontent is a tough task for a potential listener, especially due to the\noverwhelming number of offered choices. Here, recommender systems usually step\nin but existing content-based approaches rely on metadata and thus are\nconstrained by the available data quality. Other approaches leverage user\nbehavior data and thus do not exploit any domain-specific knowledge and are\nfurthermore disadvantageous regarding privacy concerns. Therefore, we propose a\nnew pipeline for the generation of audio-based radio station fingerprints\nrelying on audio stream crawling and a Deep Autoencoder. We show that the\nproposed fingerprints are especially useful for characterizing radio stations\nby their audio content and thus are an excellent representation for meaningful\nand reliable radio station recommendations. Furthermore, the proposed modules\nare part of the HRADIO Communication Platform, which enables hybrid radio\nfeatures to radio stations. It is released with a flexible open source license\nand enables especially small- and medium-sized businesses, to provide\ncustomized and high quality radio services to potential listeners.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We present a novel formalism to describe the $in$ $vacuo$ conversion between\npolarization states of propagating radiation, also known as generalized Faraday\neffect (GFE), in a cosmological context. Thinking of GFE as a potential tracer\nof new, isotropy- and/or parity-violating physics, we apply our formalism to\nthe cosmic microwave background (CMB) polarized anisotropy power spectra,\nproviding a simple framework to easily compute their observed modifications. In\nso doing, we re-interpret previously known results, namely the $in$ $vacuo$\nrotation of the linear polarization plane of CMB photons (or cosmic\nbirefringence) but also point out that GFE could lead to the partial conversion\nof linear into circular polarization. We notice that GFE can be seen as an\neffect of light propagating in an anisotropic and/or chiral medium (a \"dark\ncrystal\") and recast its parameters as the components of an effective \"cosmic\nsusceptibility tensor\". For a wave number-independent susceptibility tensor,\nthis allows us to set an observational bound on a GFE-induced CMB circularly\npolarized power spectrum, or $VV$, at $C_{\\ell}^{VV} < 2 \\times 10^{-5} \\mu\nK^2$ (95 \\% C.L.), at its peak $\\ell\\simeq 370$, which is some 3 orders of\nmagnitude better than presently available direct $VV$ measurements. We argue\nthat, unless dramatic technological improvements will arise in direct $V$-modes\nmeasurements, cosmic variance-limited linear polarization surveys expected\nwithin this decade should provide, as a byproduct, superior bounds on\nGFE-induced circular polarization of the CMB.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  A non-trivial $\\mathcal{S}$-matrix generally implies a production of\nentanglement: starting with an incoming pure state the scattering generally\nreturns an outgoing state with non-vanishing entanglement entropy. It is then\ninteresting to ask if there exists a non-trivial $\\mathcal{S}$-matrix that\ngenerates no entanglement. In this letter, we argue that the answer is the\nscattering of classical black holes. We study the spin-entanglement in the\nscattering of arbitrary spinning particles. Augmented with Thomas-Wigner\nrotation factors, we derive the entanglement entropy from the gravitational\ninduced $2\\rightarrow 2$ amplitude. In the Eikonal limit, we find that the\nrelative entanglement entropy, defined here as the \\textit{difference} between\nthe entanglement entropy of the \\textit{in} and \\textit{out}-states, is nearly\nzero for minimal coupling irrespective of the \\textit{in}-state, and increases\nsignificantly for any non-vanishing spin multipole moments. This suggests that\nminimal couplings of spinning particles, whose classical limit corresponds to\nKerr black hole, has the unique feature of generating near zero entanglement.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This article contains a series of analyses done for the SARS-CoV-2 outbreak\nin Rio Grande do Sul (RS) in the south of Brazil. These analyses are focused on\nthe high-incidence cities such as the state capital Porto Alegre and at the\nstate level. We provide methodological details and estimates for the effective\nreproduction number $R_t$, a joint analysis of the mobility data together with\nthe estimated $R_t$ as well as ICU simulations and ICU LoS (length of stay)\nestimation for hospitalizations in Porto Alegre/RS.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this article we study the equations $x^4+dy^2=z^p$ and $x^2+dy^6=z^p$ for\npositive square-free values of $d$. A Frey curve over $\\mathbb{Q}(\\sqrt{-d})$\nis attached to each primitive solution, which happens to be a\n$\\mathbb{Q}$-curve. Our main result is the construction of a Hecke character\n$\\chi$ satisfying that the Frey elliptic curve representation twisted by $\\chi$\nextends to $\\text{Gal}_\\mathbb{Q}$, therefore (by Serre's conjectures)\ncorresponds to a newform in $S_2(n,\\varepsilon)$ for explicit values of $n$ and\n$\\varepsilon$. Following some well known results and elimination techniques\n(together with some improvements) it provides a systematic procedure to study\nsolutions of the above equations and allows us to prove non-existence of\nnon-trivial primitive solutions for large values of $p$ of both equations for\nnew values of $d$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Let us consider two sequences of closed convex sets $\\{A_n\\}$ and $\\{B_n\\}$\nconverging with respect to the Attouch-Wets convergence to $A$ and $B$,\nrespectively. Given a starting point $a_0$, we consider the sequences of points\nobtained by projecting on the \"perturbed\" sets, i.e., the sequences $\\{a_n\\}$\nand $\\{b_n\\}$ defined inductively by $b_n=P_{B_n}(a_{n-1})$ and\n$a_n=P_{A_n}(b_n)$.\n  Suppose that $A\\cap B$ (or a suitable substitute if $A \\cap B=\\emptyset$) is\nbounded, we prove that if the couple $(A,B)$ is (boundedly) regular then the\ncouple $(A,B)$ is $d$-stable, i.e., for each $\\{a_n\\}$ and $\\{b_n\\}$ as above\nwe have $\\mathrm{dist}(a_n,A\\cap B)\\to 0$ and $\\mathrm{dist}(b_n,A\\cap B)\\to\n0$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Reinforcement learning algorithms such as hindsight experience replay (HER)\nand hindsight goal generation (HGG) have been able to solve challenging robotic\nmanipulation tasks in multi-goal settings with sparse rewards.\n  HER achieves its training success through hindsight replays of past\nexperience with heuristic goals, but under-performs in challenging tasks in\nwhich goals are difficult to explore.\n  HGG enhances HER by selecting intermediate goals that are easy to achieve in\nthe short term and promising to lead to target goals in the long term.\n  This guided exploration makes HGG applicable to tasks in which target goals\nare far away from the object's initial position.\n  However, HGG is not applicable to manipulation tasks with obstacles because\nthe euclidean metric used for HGG is not an accurate distance metric in such\nenvironments.\n  In this paper, we propose graph-based hindsight goal generation (G-HGG), an\nextension of HGG selecting hindsight goals based on shortest distances in an\nobstacle-avoiding graph, which is a discrete representation of the environment.\n  We evaluated G-HGG on four challenging manipulation tasks with obstacles,\nwhere significant enhancements in both sample efficiency and overall success\nrate are shown over HGG and HER.\n  Videos can be viewed at https://sites.google.com/view/demos-g-hgg/.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper presents a self-improving lifelong learning framework for a mobile\nrobot navigating in different environments. Classical static navigation methods\nrequire environment-specific in-situ system adjustment, e.g. from human\nexperts, or may repeat their mistakes regardless of how many times they have\nnavigated in the same environment. Having the potential to improve with\nexperience, learning-based navigation is highly dependent on access to training\nresources, e.g. sufficient memory and fast computation, and is prone to\nforgetting previously learned capability, especially when facing different\nenvironments. In this work, we propose Lifelong Learning for Navigation (LLfN)\nwhich (1) improves a mobile robot's navigation behavior purely based on its own\nexperience, and (2) retains the robot's capability to navigate in previous\nenvironments after learning in new ones. LLfN is implemented and tested\nentirely onboard a physical robot with a limited memory and computation budget.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Deep learning methods are being increasingly used for urban traffic\nprediction where spatiotemporal traffic data is aggregated into sequentially\norganized matrices that are then fed into convolution-based residual neural\nnetworks. However, the widely known modifiable areal unit problem within such\naggregation processes can lead to perturbations in the network inputs. This\nissue can significantly destabilize the feature embeddings and the predictions,\nrendering deep networks much less useful for the experts. This paper approaches\nthis challenge by leveraging unit visualization techniques that enable the\ninvestigation of many-to-many relationships between dynamically varied\nmulti-scalar aggregations of urban traffic data and neural network predictions.\nThrough regular exchanges with a domain expert, we design and develop a visual\nanalytics solution that integrates 1) a Bivariate Map equipped with an advanced\nbivariate colormap to simultaneously depict input traffic and prediction errors\nacross space, 2) a Morans I Scatterplot that provides local indicators of\nspatial association analysis, and 3) a Multi-scale Attribution View that\narranges non-linear dot plots in a tree layout to promote model analysis and\ncomparison across scales. We evaluate our approach through a series of case\nstudies involving a real-world dataset of Shenzhen taxi trips, and through\ninterviews with domain experts. We observe that geographical scale variations\nhave important impact on prediction performances, and interactive visual\nexploration of dynamically varying inputs and outputs benefit experts in the\ndevelopment of deep traffic prediction models.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We construct numerically solitary wave solutions of the Rosenau equation\nusing the Petviashvili iteration method. We first summarize the theoretical\nresults available in the literature for the existence of solitary wave\nsolutions. We then apply two numerical algorithms based on the Petviashvili\nmethod for solving the Rosenau equation with single or double power law\nnonlinearity. Numerical calculations rely on a uniform discretization of a\nfinite computational domain. Through some numerical experiments we observe that\nthe algorithm converges rapidly and it is robust to very general forms of the\ninitial guess.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this work, semi-discrete and fully-discrete error estimates are derived\nfor the Biot's consolidation model described using a three-field finite element\nformulation. The fields include displacements, total stress and pressure. The\nmodel is implemented using a backward Euler discretization in time for the\nfully-discrete scheme and validated for benchmark examples. Computational\nexperiments presented verifies the convergence orders for the lowest order\nfinite elements with discontinuous and continuous finite element appropriation\nfor the total stress.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We examine the spectroscopic signatures of tunneling through a Kitaev quantum\nspin liquid (QSL) barrier in a number of experimentally relevant geometries. We\ncombine contributions from elastic and inelastic tunneling processes and find\nthat spin-flip scattering at the itinerant spinon modes gives rise to a gaped\ncontribution to the tunneling conductance spectrum. We address the spectral\nmodifications that arise in a magnetic field necessary to drive the candidate\nmaterial $\\alpha$-RuCl$_3$ into a QSL phase, and we propose a lateral 1D tunnel\njunction as a viable setup in this regime. The characteristic spin gap is an\nunambiguous signature of the fractionalized QSL excitations, distinguishing it\nfrom magnons or phonons. The results of our analysis are generically applicable\nto a wide variety of topological QSL systems.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  With the use of the data from archives, we studied the correlations between\nthe equivalent widths of four diffuse interstellar bands (4430$\\r{A}$,\n5780$\\r{A}$, 5797$\\r{A}$, 6284$\\r{A}$) and properties of the target stars\n(colour excess values, distances and Galactic coordinates). Many different\nplots of the diffuse interstellar bands and their maps were produced and\nfurther analysed. There appears to be a structure in the plot of equivalent\nwidths of 5780$\\r{A}$ DIB (and 6284$\\r{A}$ DIB) against the Galactic\n$x$-coordinate. The structure is well defined below $\\sim150$ m$\\r{A}$ and\nwithin $|x|<250$ pc, peaking around $x=170$ pc. We argue that the origin of\nthis structure is not a statistical fluctuation. Splitting the data in the\nGalactic longitude into several subregions improves or lowers the well known\nlinear relation between the equivalent widths and the colour excess, which was\nexpected. However, some of the lines of sight display drastically different\nbehaviour. The region within $150^\\circ<l<200^\\circ$ shows scatter in the\ncorrelation plots with the colour excess for all of the four bands with\ncorrelation coefficients $\\textrm{R}<0.58$. We suspect that the variation of\nphysical conditions in the nearby molecular clouds could be responsible.\nFinally, the area $250^\\circ<l<300^\\circ$ displays (from the statistical point\nof view) significantly lower values of equivalent widths than the other regions\n-- this tells us that there is either a significant underabundance of carriers\n(when compared with the other regions) or that this has to be a result of an\nobservational bias.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We introduce an extension to local principal component analysis for learning\nsymmetric manifolds. In particular, we use a spectral method to approximate the\nLie algebra corresponding to the symmetry group of the underlying manifold. We\nderive the sample complexity of our method for a variety of manifolds before\napplying it to various data sets for improved density estimation.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We propose a new form of the Second Law inequality that defines a tight bound\nfor extractable work from the non-equilibrium quantum state. In classical\nthermodynamics, the optimal work is given by the difference of free energy,\nwhat according to the result of Skrzypczyk \\emph{et al.} can be generalized for\nindividual quantum systems. The saturation of this bound, however, requires an\ninfinite bath and an ideal energy storage that is able to extract work from\ncoherences. The new inequality, defined in terms of the ergotropy (rather than\nfree energy), incorporates both of those important microscopic effects. In\nparticular, we derive a formula for the locked energy in coherences, i.e. a\nquantum contribution that cannot be extracted as a work, and we find out its\nthermodynamic limit. Furthermore, we establish a general relation between\nergotropy and free energy of the arbitrary quantum system coupled to the heat\nbath, what reveals that the latter is indeed the ultimate thermodynamic bound\nregarding work extraction, and shows that ergotropy can be interpreted as the\ngeneralization of the free energy for the finite-size heat baths.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Optimisation and simulation models for the design and operation of\ngrid-connected distributed energy systems (DES) often exclude the inherent\nnonlinearities related to power flow and generation and storage units, to\nmaintain an accuracy-complexity balance. Such models may provide sub-optimal or\neven infeasible designs and dispatch schedules. In DES, optimal power flow\n(OPF) is often misrepresented and treated as a standalone problem. OPF consists\nof highly nonlinear and nonconvex constraints related to the underlying\nalternating current (AC) distribution network. This aspect of the optimisation\nproblem has often been overlooked by researchers in the process systems and\noptimisation area. In this review we address the disparity between OPF and DES\nmodels, highlighting the importance of including elements of OPF in DES design\nand operational models to ensure that the design and operation of microgrids\nmeet the requirements of the electrical grid. By analysing foundational models\nfor both DES and OPF, we identify detailed technical power flow constraints\nthat have been typically represented using oversimplified linear approximations\nin DES models. We also identify a subset of models, labelled DES-OPF, which\ninclude these detailed constraints and use innovative optimisation approaches\nto solve them. Results of these studies suggest that achieving feasible\nsolutions with high-fidelity models is more important than achieving globally\noptimal solutions using less-detailed DES models. Recommendations for future\nwork include the need for more comparisons between high-fidelity models and\nmodels with linear approximations, and the use of simulation tools to validate\nDES-OPF models. The review is aimed at a multidisciplinary audience of\nresearchers and stakeholders who are interested in modelling DES to support the\ndevelopment of more robust and accurate optimisation models for the future.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Research in population and public health focuses on the mechanisms between\ndifferent cultural, social, and environmental factors and their effect on the\nhealth, of not just individuals, but communities as a whole. We present here a\nvery brief introduction into research in these fields, as well as connections\nto existing machine learning work to help activate the machine learning\ncommunity on such topics and highlight specific opportunities where machine\nlearning, public and population health may synergize to better achieve health\nequity.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Skin lesion segmentation is a crucial step in the computer-aided diagnosis of\ndermoscopic images. In the last few years, deep learning based semantic\nsegmentation methods have significantly advanced the skin lesion segmentation\nresults. However, the current performance is still unsatisfactory due to some\nchallenging factors such as large variety of lesion scale and ambiguous\ndifference between lesion region and background. In this paper, we propose a\nsimple yet effective framework, named Dual Objective Networks (DONet), to\nimprove the skin lesion segmentation. Our DONet adopts two symmetric decoders\nto produce different predictions for approaching different objectives.\nConcretely, the two objectives are actually defined by different loss\nfunctions. In this way, the two decoders are encouraged to produce\ndifferentiated probability maps to match different optimization targets,\nresulting in complementary predictions accordingly. The complementary\ninformation learned by these two objectives are further aggregated together to\nmake the final prediction, by which the uncertainty existing in segmentation\nmaps can be significantly alleviated. Besides, to address the challenge of\nlarge variety of lesion scales and shapes in dermoscopic images, we\nadditionally propose a recurrent context encoding module (RCEM) to model the\ncomplex correlation among skin lesions, where the features with different scale\ncontexts are efficiently integrated to form a more robust representation.\nExtensive experiments on two popular benchmarks well demonstrate the\neffectiveness of the proposed DONet. In particular, our DONet achieves 0.881\nand 0.931 dice score on ISIC 2018 and $\\text{PH}^2$, respectively. Code will be\nmade public available.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We establish Sobolev type inequalities in the noncommutative settings by\ngeneralizing monotone metrics in the space of quantum states, such as\nmatrix-valued Beckner inequalities. We also discuss examples such as random\ntranspositions and Bernoulli-Laplace models.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Under mild topological restrictions, this article establishes that a smooth,\nclosed, simply connected manifold of dimension at most seven which can be\ndecomposed as the union of two disk bundles must be rationally elliptic. In\ndimension five, such manifolds are classified up to diffeomorphism, while the\nsame is true in dimension six when either the second Betti number vanishes or\nthe third Betti number is non-trivial.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In a modern vehicle, there are over seventy Electronics Control Units (ECUs).\nFor an in-vehicle network, ECUs communicate with each other by following a\nstandard communication protocol, such as Controller Area Network (CAN).\nHowever, an attacker can easily access the in-vehicle network to compromise\nECUs through a WLAN or Bluetooth. Though there are various deep learning (DL)\nmethods suggested for securing in-vehicle networks, recent studies on\nadversarial examples have shown that attackers can easily fool DL models. In\nthis research, we further explore adversarial examples in an in-vehicle\nnetwork. We first discover and implement two adversarial attack models that are\nharmful to a Long Short Term Memory (LSTM)-based detection model used in the\nin-vehicle network. Then, we propose an Adversarial Attack Defending System\n(AADS) for securing an in-vehicle network. Specifically, we focus on\nbrake-related ECUs in an in-vehicle network. Our experimental results\ndemonstrate that adversaries can easily attack the LSTM-based detection model\nwith a success rate of over 98%, and the proposed AADS achieves over 99%\naccuracy for detecting adversarial attacks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Measuring the F\\\"{o}rster resonance energy transfer (FRET) efficiency of\nfreely diffusing single molecules provides information about the sampled\nconformational states of the molecules. Under equilibrium conditions, the\ndistribution of the conformational states is independent of time, whereas it\ncan vary over time under non-equilibrium conditions. In this work, we consider\nthe problem of parameter inference on non-equilibrium solution-based\nsingle-molecule FRET data. With a non-equilibrium model for the conformational\ndynamics and a model for the conformation-dependent FRET efficiency\ndistribution, the likelihood function could be constructed. The model\nparameters, such as the rate constants of the non-equilibrium conformational\ndynamics model and the average FRET efficiencies of the different\nconformational states, have been estimated from the data by maximizing the\nappropriate likelihood function via the Expectation-Maximization algorithm. We\nillustrate the likelihood method for a few simple non-equilibrium models and\nvalidated the method by simulations. The likelihood method could be applied to\nstudy protein folding, macromolecular complex formation, protein conformational\ndynamics and other non-equilibrium processes at the single-molecule level and\nin solution.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Inthispaperwedescribeaconcept-wisemulti-preferencesemantics for description\nlogic which has its root in the preferential approach for modeling defeasible\nreasoning in knowledge representation. We argue that this proposal, beside\nsatisfying some desired properties, such as KLM postulates, and avoiding the\ndrowning problem, also defines a plausible notion of semantics. We motivate the\nplausibility of the concept-wise multi-preference semantics by developing a\nlogical semantics of self-organising maps, which have been proposed as possible\ncandidates to explain the psychological mechanisms underlying category\ngeneralisation, in terms of multi-preference interpretations.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Recommender systems are often optimised for short-term reward: a\nrecommendation is considered successful if a reward (e.g. a click) can be\nobserved immediately after the recommendation. The advantage of this framework\nis that with some reasonable (although questionable) assumptions, it allows\nfamiliar supervised learning tools to be used for the recommendation task.\nHowever, it means that long-term business metrics, e.g. sales or retention are\nignored. In this paper we introduce a framework for modeling long-term rewards\nin the RecoGym simulation environment. We use this newly introduced\nfunctionality to showcase problems introduced by the last-click attribution\nscheme in the case of conversion-optimized recommendations and propose a simple\nextension that leads to state-of-the-art results.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Most modern communication networks include fast rerouting mechanisms,\nimplemented entirely in the data plane, to quickly recover connectivity after\nlink failures. By relying on local failure information only, these data plane\nmechanisms provide very fast reaction times, but at the same time introduce an\nalgorithmic challenge in case of multiple link failures: failover routes need\nto be robust to additional but locally unknown failures downstream. This paper\npresents local fast rerouting algorithms which not only provide a high degree\nof resilience against multiple link failures, but also ensure a low congestion\non the resulting failover paths. We consider a randomized approach and focus on\nnetworks which are highly connected before the failures occur. Our main\ncontribution are three simple algorithms which come with provable guarantees\nand provide interesting resilience-load tradeoffs, significantly outperforming\nany deterministic fast rerouting algorithm with high probability.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper continues a research program on constructive investigations of\nnon-commutative Ore localizations, initiated in our previous papers, and\nparticularly touches the constructiveness of arithmetics within such\nlocalizations. Earlier we have introduced monoidal, geometric and rational\ntypes of localizations of domains as objects of our studies. Here we extend\nthis classification to rings with zero divisors and consider Ore sets of the\nmentioned types which are commutative enough: such a set either belongs to a\ncommutative algebra or it is central or its elements commute pairwise. By using\nthe systematic approach we have developed before, we prove that arithmetic\nwithin the localization of a commutative polynomial algebra is constructive and\ngive the necessary algorithms. We also address the important question of\ncomputing the local closure of ideals which is also known as the\ndesingularization, and present an algorithm for the computation of the symbolic\npower of a given ideal in a commutative ring. We also provide algorithms to\ncompute local closures for certain non-commutative rings with respect to Ore\nsets with enough commutativity.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The search and follow-up observation of electromagnetic (EM) counterparts of\ngravitational waves (GW) is a current hot topic of GW cosmology. Due to the\nlimitation of the accuracy of the GW observation facility at this stage, we can\nonly get a rough sky-localization region for the GW event, and the typical area\nof the region is between 200 and 1500 square degrees. Since GW events occur in\nor near galaxies, limiting the observation target to galaxies can significantly\nspeedup searching for EM counterparts. Therefore, how to efficiently select\nhost galaxy candidates in such a large GW localization region, how to arrange\nthe observation sequence, and how to efficiently identify the GW source from\nobservational data are the problems that need to be solved. International\nVirtual Observatory Alliance has developed a series of technical standards for\ndata retrieval, interoperability and visualization. Based on the application of\nVO technologies, we construct the GW follow-up Observation Planning System\n(GWOPS). It consists of three parts: a pipeline to select host candidates of GW\nand sort their priorities for follow-up observation, an identification module\nto find the transient from follow-up observation data, and a visualization\nmodule to display GW-related data. GWOPS can rapidly respond to GW events. With\nGWOPS, the operations such as follow-up observation planning, data storage,\ndata visualization, and transient identification can be efficiently\ncoordinated, which will promote the success searching rate for GWs EM\ncounterparts.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Let $f:R\\to C$ be the characteristic function of a probability measure. We\nstudy the following question: Is it true that for any closed interval $I$ on\n$R$, which does not contain the origin, there exists a characteristic function\n$g$ such that $g$ coincides with $f$ on $I$ but $g \\not\\equiv f$ on $R$?\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The Fourier algebra of the affine group of the real line has a natural\nidentification, as a Banach space, with the space of trace-class operators on\n$L^2({\\mathbb R}^\\times, dt/ |t|)$. In this paper we study the \"dual\nconvolution product\" of trace-class operators that corresponds to pointwise\nproduct in the Fourier algebra. Answering a question raised in work of Eymard\nand Terp, we provide an intrinsic description of this operation which does not\nrely on the identification with the Fourier algebra, and obtain a similar\nresult for the connected component of this affine group. In both cases we\nconstruct explicit derivations on the corresponding Banach algebras, verifying\nthe derivation identity directly without requiring the inverse Fourier\ntransform. We also initiate the study of the analogous Banach algebra structure\nfor trace-class operators on $L^p({\\mathbb R}^\\times, dt/ |t|)$ for $p\\in\n(1,2)\\cup(2,\\infty)$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Cluster computing was introduced to replace the superiority of super\ncomputers. Cluster computing is able to overcome the problems that cannot be\neffectively dealt with supercomputers. In this paper, we are going to evaluate\nthe performance of cluster computing by executing one of data mining techniques\nin the cluster environment. The experiment will attempt to predict the flight\ndelay by using linear regression algorithm with apache spark as a framework for\ncluster computing. The result shows that, by involving 5 PCs in cluster\nenvironment with equal specifications can increase the performance of\ncomputation up to 39.76% compared to the standalone one. Attaching more nodes\nto the cluster can make the process become faster significantly.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Logical forgetting is NP-complete even in the simple case of propositional\nHorn formulae, and may exponentially increase their size. A way to forget is to\nreplace each variable to forget with the body of each clause whose head is the\nvariable. It takes polynomial time in the single-head case: each variable is at\nmost the head of a clause. Some formulae are not single-head but can be made so\nto simplify forgetting. They are single-head equivalent. The first contribution\nof this article is the study of a semantical characterization of single-head\nequivalence. Two necessary conditions are given. They are sufficient when the\nformula is inequivalent: it makes two sets of variables equivalent only if they\nare also equivalent to their intersection. All acyclic formulae are\ninequivalent. The second contribution of this article is an incomplete\nalgorithm for turning a formula single-head. In case of success, forgetting\nbecomes possible in polynomial time and produces a polynomial-size formula,\nnone of which is otherwise guaranteed. The algorithm is complete on\ninequivalent formulae.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  After a surge in popularity of supervised Deep Learning, the desire to reduce\nthe dependence on curated, labelled data sets and to leverage the vast\nquantities of unlabelled data available recently triggered renewed interest in\nunsupervised learning algorithms. Despite a significantly improved performance\ndue to approaches such as the identification of disentangled latent\nrepresentations, contrastive learning, and clustering optimisations, the\nperformance of unsupervised machine learning still falls short of its\nhypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly\nbeen based on adult learners with access to labels and a vast amount of prior\nknowledge. In order to push unsupervised machine learning forward, we argue\nthat developmental science of infant cognition might hold the key to unlocking\nthe next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artificial unsupervised\nlearning, as infants too must learn useful representations from unlabelled\ndata. In contrast to machine learning, these new representations are learned\nrapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used flexibly and efficiently in a number of\ndifferent tasks and contexts. We identify five crucial factors enabling\ninfants' quality and speed of learning, assess the extent to which these have\nalready been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in\nunsupervised learning.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Continuous learning from streaming data is among the most challenging topics\nin the contemporary machine learning. In this domain, learning algorithms must\nnot only be able to handle massive volumes of rapidly arriving data, but also\nadapt themselves to potential emerging changes. The phenomenon of the evolving\nnature of data streams is known as concept drift. While there is a plethora of\nmethods designed for detecting its occurrence, all of them assume that the\ndrift is connected with underlying changes in the source of data. However, one\nmust consider the possibility of a malicious injection of false data that\nsimulates a concept drift. This adversarial setting assumes a poisoning attack\nthat may be conducted in order to damage the underlying classification system\nby forcing adaptation to false data. Existing drift detectors are not capable\nof differentiating between real and adversarial concept drift. In this paper,\nwe propose a framework for robust concept drift detection in the presence of\nadversarial and poisoning attacks. We introduce the taxonomy for two types of\nadversarial concept drifts, as well as a robust trainable drift detector. It is\nbased on the augmented Restricted Boltzmann Machine with improved gradient\ncomputation and energy function. We also introduce Relative Loss of Robustness\n- a novel measure for evaluating the performance of concept drift detectors\nunder poisoning attacks. Extensive computational experiments, conducted on both\nfully and sparsely labeled data streams, prove the high robustness and efficacy\nof the proposed drift detection framework in adversarial scenarios.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Stochastic Differential Equations (SDEs) in high dimension, having the\nstructure of finite dimensional approximation of Stochastic Partial\nDifferential Equations (SPDEs), are considered. The aim is to compute\nnumerically expected values and probabilities associated to their solutions, by\nsolving the associated Kolmogorov equations, with a partial use of Monte Carlo\nstrategy - precisely, using Monte Carlo only for the linear part of the SDE.\nThe basic idea was presented in Flandoli et al., JMAA (2020), but here we\nstrongly improve the numerical results by means of a shift of the auxiliary\nGaussian process. For relatively simple nonlinearities, we have good results in\ndimension of the order of 100.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Controlling the water contact angle on a surface is important for regulating\nits wettability in industrial applications, which involves developing ab initio\nprediction scheme of accurately predicting the angle. The scheme requires\nstructural models for the adsorption of liquid molecules on a surface, but\ntheir reliability depend on whether the surfaces comprise insulating or\nmetallic materials. Previous ab initio studies have focused on the estimation\nof the water contact angle on insulators, where the periodic-honeycomb array of\nwater molecules was adopted as the adsorption model for the water on the\ninsulating surface and succeeded in the insulating cases. This study, however,\nfocus on the water contact angle on a metallic surface, and propose a simple ab\ninitio based estimation scheme. We not only adopt the previously proposed\nstructural modeling based on the periodic-honyecomb array, but also consider an\nensemble of isolated water oligomers that have different molecular coverage\n(ML) values. We established a statistic model to predict a contact angle of the\nwater wetting on a Cu(111) surface: The coverage-dependent contact angles\nobtained from each of the isolated clusters was fit to a quadratic regression,\nand the contact angle was interpolated by referring to a ML value of water\nlayer in literature. This interpolated value lay within the deviation of\nexperimental angles. In addition, the Boltzmann-average over the isolated\nclusters was found to agree well with the interpolated one. This indicates that\nthe Boltzmann-average is useful for estimating the contact angle of other\nmetallic surfaces without knowing a ML value a priori.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  A pure state of $N$ parties with local dimension $d$ is called a $k$-uniform\nstate if all the reductions to $k$ parties are maximally mixed. Based on the\nconnections among $k$-uniform states, orthogonal arrays and linear codes, we\ngive general constructions for $k$-uniform states. We show that when $d\\geq\n4k-2$ (resp. $d\\geq 2k-1$) is a prime power, there exists a $k$-uniform state\nfor any $N\\geq 2k$ (resp. $2k\\leq N\\leq d+1$). Specially, we give the existence\nof $4,5$-uniform states for almost every $N$-qudits. Further, we generalize the\nconcept of quantum information masking in bipartite systems given by [Modi\n\\emph{et al.} {Phys. Rev. Lett. \\textbf{120}, 230501 (2018)}] to $k$-uniform\nquantum information masking in multipartite systems, and we show that\n$k$-uniform states and quantum error-correcting codes can be used for\n$k$-uniform quantum information masking.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We develop an approach to study character sums, weighted by a multiplicative\nfunction $f:\\mathbb{F}_q[t]\\to S^1$, of the form \\begin{equation} \\sum_{G\\in\n\\mathcal{M}_N}f(G)\\chi(G)\\xi(G), \\end{equation} where $\\chi$ is a Dirichlet\ncharacter and $\\xi$ is a short interval character over $\\mathbb{F}_q[t].$ We\nthen deduce versions of the Matom\\\"aki-Radziwill theorem and Tao's two-point\nlogarithmic Elliott conjecture over function fields $\\mathbb{F}_q[t]$, where\n$q$ is fixed. The former of these improves on work of Gorodetsky, and the\nlatter extends the work of Sawin-Shusterman on correlations of the M\\\"{o}bius\nfunction for various values of $q$.\n  Compared with the integer setting, we encounter a different phenomenon,\nspecifically a low characteristic issue in the case that $q$ is a power of $2$.\n  As an application of our results, we give a short proof of the function field\nversion of a conjecture of K\\'atai on classifying multiplicative functions with\nsmall increments, with the classification obtained and the proof being\ndifferent from the integer case.\n  In a companion paper, we use these results to characterize the limiting\nbehavior of partial sums of multiplicative functions in function fields and in\nparticular to solve a \"corrected\" form of the Erd\\H{o}s discrepancy problem\nover $\\mathbb{F}_q[t]$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Solar flares are sudden energy release events in the solar corona, resulting\nfrom magnetic reconnection, that accelerates particles and heats the ambient\nplasma. During a flare, there are often multiple, temporally and spatially\nseparated individual energy release episodes that can be difficult to resolve\ndepending on the observing instrument. We present multi-wavelength imaging and\nspectroscopy observations of multiple electron acceleration episodes during a\nGOES B1.7-class two-ribbon flare on 2012 February 25, observed simultaneously\nwith the Karl G. Jansky Very Large Array (VLA) at 1--2 GHz, the Reuven Ramatay\nHigh Energy Solar Spectroscopic Imager (RHESSI) in X-rays, and the Solar\nDynamics Observatory in extreme ultraviolet (EUV). During the initial phase of\nthe flare, five radio bursts were observed. A nonthermal X-ray source was seen\nco-temporal, but not co-spatial, with the first three radio bursts. Their radio\nspectra are interpreted as optically thick gyrosynchrotron emission. By fitting\nthe radio spectra with a gyrosynchrotron model, we derive the magnetic field\nstrength and nonthermal electron spectral parameters in each acceleration\nepisode. Notably, the nonthermal parameters derived from X-rays differ\nconsiderably from the nonthermal parameters inferred from the radio. The\nobservations are indicative of multiple, co-temporal acceleration episodes\nduring the impulsive phase of a solar microflare. The X-ray and radio burst\nsources likely originate from separate electron distributions in different\nmagnetic loops.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper relates the elliptic stable envelopes of a hypertoric variety $X$\nwith the K-theoretic stable envelopes of the loop hypertoric space,\n$\\widetilde{\\mathscr{L}}X$. It thus points to a possible categorification of\nelliptic stable envelopes.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Since the initial launch of Bitcoin by Satoshi Nakamoto in 2009,\ndecentralized digital currencies have long been of major interest in both the\nacademia and the industry. Till today, there are more than 3000 different\ncryptocurrencies over the internet. Each one relies on mathematical soundness\nand cryptographic wit to provide unique properties in addition to securing\nbasic correctness. A common misbelief for cryptocurrencies is that they provide\ncomplete anonymity by replacing people's real-life identity with a randomly\ngenerated wallet address in payments. However, this \"pseudonymity\" is easily\nbreakable under the public ledger. Many attacks demonstrate ways to deanonymize\npeople through observing the transaction patterns or network interactions.\nThus, cryptocurrency fungibility has become a popular topic in the research\ncommunity. This report reviews a partial list of existing schemes and describes\nan alternative implementation, Eth-Tumbler. Eth-Tumbler utilizes layered\nencryption and multiple signatures and thus efficiently hides a user under\nk-anonymity.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Deep learning (DL) and machine learning (ML) methods have recently\ncontributed to the advancement of models in the various aspects of prediction,\nplanning, and uncertainty analysis of smart cities and urban development. This\npaper presents the state of the art of DL and ML methods used in this realm.\nThrough a novel taxonomy, the advances in model development and new application\ndomains in urban sustainability and smart cities are presented. Findings reveal\nthat five DL and ML methods have been most applied to address the different\naspects of smart cities. These are artificial neural networks; support vector\nmachines; decision trees; ensembles, Bayesians, hybrids, and neuro-fuzzy; and\ndeep learning. It is also disclosed that energy, health, and urban transport\nare the main domains of smart cities that DL and ML methods contributed in to\naddress their problems.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Etiologies of tear breakup include evaporation-driven, divergent flow-driven,\nand a combination of these two. A mathematical model incorporating evaporation\nand lipid-driven tangential flow is fit to fluorescence imaging data. The\nlipid-driven motion is hypothesized to be caused by localized excess lipid, or\n\"globs.\" Tear breakup quantities such as evaporation rates and tangential flow\nrates cannot currently be directly measured during breakup. We determine such\nvariables by fitting mathematical models for tear breakup and the computed\nfluorescent intensity to experimental intensity data gathered in vivo.\nParameter estimation is conducted via least squares minimization of the\ndifference between experimental data and computed answers using either the\ntrust-region-reflective or Levenberg-Marquardt algorithm. Best-fit\ndetermination of tear breakup parameters supports the notion that evaporation\nand divergent tangential flow can cooperate to drive breakup. The resulting\ntear breakup is typically faster than purely evaporative cases. Many instances\nof tear breakup may have similar causes, which suggests that interpretation of\nexperimental results may benefit from considering multiple mechanisms.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this short note we show that the octagon Farey map introduced by Smillie\nand Ulcigrai is an acceleration of the diagonal changes algorithm introduced by\nDelecroix and Ulcigrai.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Words can have multiple senses. Compositional distributional models of\nmeaning have been argued to deal well with finer shades of meaning variation\nknown as polysemy, but are not so well equipped to handle word senses that are\netymologically unrelated, or homonymy. Moving from vectors to density matrices\nallows us to encode a probability distribution over different senses of a word,\nand can also be accommodated within a compositional distributional model of\nmeaning. In this paper we present three new neural models for learning density\nmatrices from a corpus, and test their ability to discriminate between word\nsenses on a range of compositional datasets. When paired with a particular\ncomposition method, our best model outperforms existing vector-based\ncompositional models as well as strong sentence encoders.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  The discovery of the TeV point source 2HWC J2006+341 was reported in the\nsecond HAWC gamma-ray catalog. We present a follow-up study of this source\nhere. The TeV emission is best described by an extended source with a soft\nspectrum. At GeV energies, an extended source is significantly detected in\nFermi-LAT data. The matching locations, sizes and spectra suggest that both\ngamma-ray detections correspond to the same source. Different scenarios for the\norigin of the emission are considered and we rule out an association to the\npulsar PSR J2004+3429 due to extreme energetics required, if located at a\ndistance of 10.8 kpc.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Due to the potential applications in the low-power-consumption spintronic\ndevices, the quantum anomalous Hall effect (QAHE) has attracted tremendous\nattention in past decades. However, up to now, QAHE was only observed\nexperimentally in topological insulators with Chern numbers C= 1 and 2 at very\nlow temperatures. Here, we propose three novel two-dimensional stable kagome\nferromagnets Co3Pb3S2, Co3Pb3Se2and Co3Sn3Se2that can realize QAHE with high\nChern number of |C|=3. Monolayers Co3Pb3S2, Co3Pb3Se2 and Co3Sn3Se2 possess the\nlarge band gap of 70, 77 and 63 meV with Curie temperature TC of 51, 42 and 46\nK, respectively. By constructing a heterostructure Co3Sn3Se2/MoS2, its TC is\nenhanced to 60 K and the band gap keeps about 60 meV due to the tensile strain\nof 2% at the interface. For the bilayer compound Co6Sn5Se4, it becomes a\nhalf-metal, with a relatively flat plateau in its anomalous Hall conductivity\ncorresponding to |C| = 3 near the Fermi level. Our results provide new\ntopological nontrivial systems of kagome ferromagnetic monolayers and\nheterostructrues possessing QAHE with high Chern number |C| = 3 and large band\ngaps.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Data augmentation has been demonstrated as an effective strategy for\nimproving model generalization and data efficiency. However, due to the\ndiscrete nature of natural language, designing label-preserving transformations\nfor text data tends to be more challenging. In this paper, we propose a novel\ndata augmentation framework dubbed CoDA, which synthesizes diverse and\ninformative augmented examples by integrating multiple transformations\norganically. Moreover, a contrastive regularization objective is introduced to\ncapture the global relationship among all the data samples. A momentum encoder\nalong with a memory bank is further leveraged to better estimate the\ncontrastive loss. To verify the effectiveness of the proposed framework, we\napply CoDA to Transformer-based models on a wide range of natural language\nunderstanding tasks. On the GLUE benchmark, CoDA gives rise to an average\nimprovement of 2.2% while applied to the RoBERTa-large model. More importantly,\nit consistently exhibits stronger results relative to several competitive data\naugmentation and adversarial training base-lines (including the low-resource\nsettings). Extensive experiments show that the proposed contrastive objective\ncan be flexibly combined with various data augmentation approaches to further\nboost their performance, highlighting the wide applicability of the CoDA\nframework.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  As a research community, we are still lacking a systematic understanding of\nthe progress on adversarial robustness which often makes it hard to identify\nthe most promising ideas in training robust models. A key challenge in\nbenchmarking robustness is that its evaluation is often error-prone leading to\nrobustness overestimation. Our goal is to establish a standardized benchmark of\nadversarial robustness, which as accurately as possible reflects the robustness\nof the considered models within a reasonable computational budget. To this end,\nwe start by considering the image classification task and introduce\nrestrictions (possibly loosened in the future) on the allowed models. We\nevaluate adversarial robustness with AutoAttack, an ensemble of white- and\nblack-box attacks, which was recently shown in a large-scale study to improve\nalmost all robustness evaluations compared to the original publications. To\nprevent overadaptation of new defenses to AutoAttack, we welcome external\nevaluations based on adaptive attacks, especially where AutoAttack flags a\npotential overestimation of robustness. Our leaderboard, hosted at\nhttps://robustbench.github.io/, contains evaluations of 120+ models and aims at\nreflecting the current state of the art in image classification on a set of\nwell-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models and on common\ncorruptions, with possible extensions in the future. Additionally, we\nopen-source the library https://github.com/RobustBench/robustbench that\nprovides unified access to 80+ robust models to facilitate their downstream\napplications. Finally, based on the collected models, we analyze the impact of\nrobustness on the performance on distribution shifts, calibration,\nout-of-distribution detection, fairness, privacy leakage, smoothness, and\ntransferability.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Policy networks are a central feature of deep reinforcement learning (RL)\nalgorithms for continuous control, enabling the estimation and sampling of\nhigh-value actions. From the variational inference perspective on RL, policy\nnetworks, when used with entropy or KL regularization, are a form of\n\\textit{amortized optimization}, optimizing network parameters rather than the\npolicy distributions directly. However, \\textit{direct} amortized mappings can\nyield suboptimal policy estimates and restricted distributions, limiting\nperformance and exploration. Given this perspective, we consider the more\nflexible class of \\textit{iterative} amortized optimizers. We demonstrate that\nthe resulting technique, iterative amortized policy optimization, yields\nperformance improvements over direct amortization on benchmark continuous\ncontrol tasks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Given a finite set of primes $S$ and a $m$-tuple $(a_1,\\dots,a_m)$ of\npositive, distinct integers we call the $m$-tuple $S$-Diophantine, if for each\n$1\\leq i < j\\leq m$ the quantity $a_ia_j+1$ has prime divisors coming only from\nthe set $S$. For a given set $S$ we give a practical algorithm to find all\n$S$-Diophantine quadruples, provided that $|S|=3$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We propose 3DBooSTeR, a novel method to recover a textured 3D body mesh from\na textured partial 3D scan. With the advent of virtual and augmented reality,\nthere is a demand for creating realistic and high-fidelity digital 3D human\nrepresentations. However, 3D scanning systems can only capture the 3D human\nbody shape up to some level of defects due to its complexity, including\nocclusion between body parts, varying levels of details, shape deformations and\nthe articulated skeleton. Textured 3D mesh completion is thus important to\nenhance 3D acquisitions. The proposed approach decouples the shape and texture\ncompletion into two sequential tasks. The shape is recovered by an\nencoder-decoder network deforming a template body mesh. The texture is\nsubsequently obtained by projecting the partial texture onto the template mesh\nbefore inpainting the corresponding texture map with a novel approach. The\napproach is validated on the 3DBodyTex.v2 dataset.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Accretion onto the supermassive black hole in some active galactic nuclei\n(AGN) drives relativistic jets of plasma, which dissipate a significant\nfraction of their kinetic energy into gamma-ray radiation. The location of\nenergy dissipation in powerful extragalactic jets is currently unknown, with\nimplications for particle acceleration, jet formation, jet collimation, and\nenergy dissipation. Previous studies have been unable to constrain the location\nbetween possibilities ranging from the sub-parsec-scale broad-line region to\nthe parsec-scale molecular torus, and beyond. Here we show using a simple\ndiagnostic that the more distant molecular torus is the dominant location for\npowerful jets. This diagnostic, called the seed factor, is dependent only on\nobservable quantities, and is unique to the seed photon population at the\nlocation of gamma-ray emission. Using $62$ multiwavelength, quasi-simultaneous\nspectral energy distributions of gamma-ray quasars, we find a seed factor\ndistribution which peaks at a value corresponding to the molecular torus,\ndemonstrating that energy dissipation occurs $\\sim 1$ parsec from the black\nhole (or $\\sim 10^{4}$ Schwarzchild radii for a $10^{9}M_{\\odot}$ black hole).\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We study the problem of online learning with primary and secondary losses.\nFor example, a recruiter making decisions of which job applicants to hire might\nweigh false positives and false negatives equally (the primary loss) but the\napplicants might weigh false negatives much higher (the secondary loss). We\nconsider the following question: Can we combine \"expert advice\" to achieve low\nregret with respect to the primary loss, while at the same time performing {\\em\nnot much worse than the worst expert} with respect to the secondary loss?\nUnfortunately, we show that this goal is unachievable without any bounded\nvariance assumption on the secondary loss. More generally, we consider the goal\nof minimizing the regret with respect to the primary loss and bounding the\nsecondary loss by a linear threshold. On the positive side, we show that\nrunning any switching-limited algorithm can achieve this goal if all experts\nsatisfy the assumption that the secondary loss does not exceed the linear\nthreshold by $o(T)$ for any time interval. If not all experts satisfy this\nassumption, our algorithms can achieve this goal given access to some external\noracles which determine when to deactivate and reactivate experts.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Depressive disorder is one of the most prevalent mental illnesses among the\nglobal population. However, traditional screening methods require exacting\nin-person interviews and may fail to provide immediate interventions. In this\nwork, we leverage ubiquitous personal longitudinal Google Search and YouTube\nengagement logs to detect individuals with depressive disorder. We collected\nGoogle Search and YouTube history data and clinical depression evaluation\nresults from $212$ participants ($99$ of them suffered from moderate to severe\ndepressions). We then propose a personalized framework for classifying\nindividuals with and without depression symptoms based on mutual-exciting point\nprocess that captures both the temporal and semantic aspects of online\nactivities. Our best model achieved an average F1 score of $0.77 \\pm 0.04$ and\nan AUC ROC of $0.81 \\pm 0.02$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper, we design and experiment a far-field wireless power transfer\n(WPT) architecture based on distributed antennas, so-called WPT DAS, that\ndynamically selects transmit antenna and frequency to increase the output dc\npower. Uniquely, spatial and frequency diversities are jointly exploited in the\nproposed WPT DAS with low complexity, low cost, and flexible deployment to\ncombat the wireless fading channel. A numerical experiment is designed to show\nthe benefits using antenna and frequency selections in spatially and frequency\nselective fading channels for single-user and multi-user cases. Accordingly,\nthe proposed WPT DAS for single-user and two-user cases is prototyped. At the\ntransmitter, we adopt antenna selection to exploit spatial diversity and adopt\nfrequency selection to exploit frequency diversity. A low-complexity\nover-the-air limited feedback using an IEEE 802.15.4 RF interface is designed\nfor antenna and frequency selections and reporting from the receiver to the\ntransmitter. The proposed WPT DAS prototype is demonstrated in a real indoor\nenvironment. The measurements show that WPT DAS can boost the output dc power\nby up to 30 dB in single-user case and boost the sum of output dc power by up\nto 21.8 dB in two-user case and broaden the service coverage area in a low\ncost, low complexity, and flexible manner.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We study different aspects of quantum field theory at finite density using\nmethods from quantum information theory. For simplicity we focus on massive\nDirac fermions with nonzero chemical potential, and work in $1+1$ space-time\ndimensions. Using the entanglement entropy on an interval, we construct an\nentropic $c$-function that is finite. Unlike what happens in Lorentz-invariant\ntheories, this $c$-function exhibits a strong violation of monotonicity; it\nalso encodes the creation of long-range entanglement from the Fermi surface.\nMotivated by previous works on lattice models, we next calculate numerically\nthe Renyi entropies and find Friedel-type oscillations; these are understood in\nterms of a defect operator product expansion. Furthermore, we consider the\nmutual information as a measure of correlation functions between different\nregions. Using a long-distance expansion previously developed by Cardy, we\nargue that the mutual information detects Fermi surface correlations already at\nleading order in the expansion. We also analyze the relative entropy and its\nRenyi generalizations in order to distinguish states with different charge\nand/or mass. In particular, we show that states in different superselection\nsectors give rise to a super-extensive behavior in the relative entropy.\nFinally, we discuss possible extensions to interacting theories, and argue for\nthe relevance of some of these measures for probing non-Fermi liquids.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper, we introduce Kathaka, a model trained with a novel two-stage\ntraining process for neural speech synthesis with contextually appropriate\nprosody. In Stage I, we learn a prosodic distribution at the sentence level\nfrom mel-spectrograms available during training. In Stage II, we propose a\nnovel method to sample from this learnt prosodic distribution using the\ncontextual information available in text. To do this, we use BERT on text, and\ngraph-attention networks on parse trees extracted from text. We show a\nstatistically significant relative improvement of $13.2\\%$ in naturalness over\na strong baseline when compared to recordings. We also conduct an ablation\nstudy on variations of our sampling technique, and show a statistically\nsignificant improvement over the baseline in each case.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Modern industrial applications require robots to be able to operate in\nunpredictable environments, and programs to be created with a minimal effort,\nas there may be frequent changes to the task. In this paper, we show that\ngenetic programming can be effectively used to learn the structure of a\nbehavior tree (BT) to solve a robotic task in an unpredictable environment.\nMoreover, we propose to use a simple simulator for the learning and demonstrate\nthat the learned BTs can solve the same task in a realistic simulator, reaching\nconvergence without the need for task specific heuristics. The learned solution\nis tolerant to faults, making our method appealing for real robotic\napplications.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Statistical physics provides a useful perspective for the analysis of many\ncomplex systems; it allows us to relate microscopic fluctuations to macroscopic\nobservations. Developmental biology, but also cell biology more generally, are\nexamples where apparently robust behaviour emerges from highly complex and\nstochastic sub-cellular processes. Here we attempt to make connections between\ndifferent theoretical perspectives to gain qualitative insights into the types\nof cell-fate decision making processes that are at the heart of stem cell and\ndevelopmental biology. We discuss both dynamical systems as well as statistical\nmechanics perspectives on the classical Waddington or epigenetic landscape. We\nfind that non-equilibrium approaches are required to overcome some of the\nshortcomings of classical equilibrium statistical thermodynamics or statistical\nmechanics in order to shed light on biological processes, which, almost by\ndefinition, are typically far from equilibrium.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We parametrize the space of double cosets of the group $GL(n,\\Bbbk)$ with\nrespect to two subgroups $T_-$, $T_+$ of block strictly triangular matrices. In\nAddendum, we consider the quasi-regular representation of $GL(n,\\Bbb{C})$ in\n$L^2$ on $T_- \\ GL(n,\\Bbb{C})$, observe that it admits an additional group of\nsymmetries, find the joint spectrum, and observe that it is multiplicity free.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  This paper presents a holistic approach to saliency-guided visual attention\nmodeling (SVAM) for use by autonomous underwater robots. Our proposed model,\nnamed SVAM-Net, integrates deep visual features at various scales and semantics\nfor effective salient object detection (SOD) in natural underwater images. The\nSVAM-Net architecture is configured in a unique way to jointly accommodate\nbottom-up and top-down learning within two separate branches of the network\nwhile sharing the same encoding layers. We design dedicated spatial attention\nmodules (SAMs) along these learning pathways to exploit the coarse-level and\nfine-level semantic features for SOD at four stages of abstractions. The\nbottom-up branch performs a rough yet reasonably accurate saliency estimation\nat a fast rate, whereas the deeper top-down branch incorporates a residual\nrefinement module (RRM) that provides fine-grained localization of the salient\nobjects. Extensive performance evaluation of SVAM-Net on benchmark datasets\nclearly demonstrates its effectiveness for underwater SOD. We also validate its\ngeneralization performance by several ocean trials' data that include test\nimages of diverse underwater scenes and waterbodies, and also images with\nunseen natural objects. Moreover, we analyze its computational feasibility for\nrobotic deployments and demonstrate its utility in several important use cases\nof visual attention modeling.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Hand segmentation and detection in truly unconstrained RGB-based settings is\nimportant for many applications. However, existing datasets are far from\nsufficient in terms of size and variety due to the infeasibility of manual\nannotation of large amounts of segmentation and detection data. As a result,\ncurrent methods are limited by many underlying assumptions such as constrained\nenvironment, consistent skin color and lighting. In this work, we present\nEgo2Hands, a large-scale RGB-based egocentric hand segmentation/detection\ndataset that is semi-automatically annotated and a color-invariant\ncompositing-based data generation technique capable of creating training data\nwith large quantity and variety. For quantitative analysis, we manually\nannotated an evaluation set that significantly exceeds existing benchmarks in\nquantity, diversity and annotation accuracy. We provide cross-dataset\nevaluation as well as thorough analysis on the performance of state-of-the-art\nmodels on Ego2Hands to show that our dataset and data generation technique can\nproduce models that generalize to unseen environments without domain\nadaptation.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this work, we introduce a comet dust model that incorporates multiple dust\nmorphologies along with inhomogeneous mixture of silicate minerals and\ncarbonaceous materials under power-law size distribution, to replicate the\nstandard polarization-phase curve observed in several comets in the narrow-band\ncontinuum. Following the results from Rosetta/MIDAS and COSIMA, we create high\nporosity Hierarchical Aggregates (HA) and low porosity (< 10$\\%$) Solids in the\nform of agglomerated debris. We also introduce a moderate porosity structure\nwith solids in the core, surrounded by fluffy aggregates called Fluffy Solids\n(FS). We study the mixing combinations, (HA and Solids), (HA and FS) and (HA,\nFS and Solids) for a range of power-law index n=2.0 to 3.0 for different sets\nof mixing percentage of silicate minerals and carbonaceous materials.\nPolarimetry of the short period comets 1P/Halley and 67P/Churyumov-Gerasimenko\nmatch best with the polarisation resulting from the combination of HA and\nSolids while the combinations (HA and FS) and (HA, FS and Solids) provide the\nbest fit results for the long period comets C/1995 O1 (Hale-Bopp) and C/1996 B2\n(Hyakutake). The best fit model results also recreate the observed wavelength\ndependence of polarization. Our dust model agree with the idea that the long\nperiod comets may have high percentage of loose particles (HA and FS) compared\nto those in the case of short period comets as the short period comets\nexperience more frequent and/or higher magnitude of weathering.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  'Lockdown' periods in response to COVID-19 have provided a unique opportunity\nto study the impacts of economic activity on environmental pollution (e.g.\nNO$_2$, aerosols, noise, light). The effects on NO$_2$ and aerosols have been\nvery noticeable and readily demonstrated, but that on light pollution has\nproven challenging to determine. The main reason for this difficulty is that\nthe primary source of nighttime satellite imagery of the earth is the\nSNPP-VIIRS/DNB instrument, which acquires data late at night after most human\nnocturnal activity has already occurred and much associated lighting has been\nturned off. Here, to analyze the effect of lockdown on urban light emissions,\nwe use ground and satellite data for Granada, Spain, during the COVID-19\ninduced confinement of the city's population from March 14 until May 31, 2020.\nWe find a clear decrease in light pollution due both to a decrease in light\nemissions from the city and to a decrease in anthropogenic aerosol content in\nthe atmosphere which resulted in less light being scattered. A clear\ncorrelation between the abundance of PM10 particles and sky brightness is\nobserved, such that the more polluted the atmosphere the brighter the urban\nnight sky. An empirical expression is determined that relates PM10 particle\nabundance and sky brightness at three different wavelength bands.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In this paper we propose a semi-parametric Bayesian Generalized Least Squares\nestimator. In a generic GLS setting where each error is a vector, parametric\nGLS maintains the assumption that each error vector has the same covariance\nmatrix. In reality however, the observations are likely to be heterogeneous\nregarding their distributions. To cope with such heterogeneity, a Dirichlet\nprocess prior is introduced for the covariance matrices of the errors, leading\nto the error distribution being a mixture of a variable number of normal\ndistributions. Our methods let the number of normal components be data driven.\nTwo specific cases are then presented: the semi-parametric Bayesian Seemingly\nUnrelated Regression (SUR) for equation systems; as well as the Random Effects\nModel (REM) and Correlated Random Effects Model (CREM) for panel data. A series\nof simulation experiments is designed to explore the performance of our\nmethods. The results demonstrate that our methods obtain smaller posterior\nstandard deviations than the parametric Bayesian GLS. We then apply our\nsemi-parametric Bayesian SUR and REM/CREM methods to empirical examples.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We consider the exponent of \\L ojasiewicz inequality $\\|\\partial\\,f(\\mathbf\nz)\\| \\ge c |f(\\mathbf z|^\\theta$ for two classes of analytic functions and we\nwill give an explicit estimation for $\\theta$. First we consider certain\nnon-degenerate functions which is not convenient. In \\S 3.4, we give an example\nof a polynomial for which $\\theta_0(f)$ is not constant on the moduli space and\nin \\S 3.5, we show that the behaviors of the \\L ojasiewicz exponents is not\nsimilar as the Milnor numbers by an example.\n  In the last section (\\S 4), we give also an estimation for product functions\n$f(\\mathbf z)=f_1(\\mathbf z)\\cdots f_k(\\mathbf z)$ associated to a family of a\ncertain convenient non-degenerate complete intersection varieties. In either\nclass, the singularity is not isolated. We will give explicit estimations of\nthe \\L ojasiewicz exponent $\\theta_0(f)$ using combinatorial data of the Newton\nboundary of $f$. We generalize this estimation for non-reduced function\n$g=f_1^{m_1}\\cdots f_k^{m_k}$.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  High-harmonic generation (HHG) in solids is an emerging method to probe\nultrafast electron dynamics in solids at attosecond timescale. In this work, we\nstudy HHG from monolayer and bilayer graphene. Bilayer graphenes with AA and AB\nstacking are considered in this work. It is found that the monolayer and\nbilayer graphenes exhibit significantly different harmonic spectra. The\ndifference in the spectra is attributed to the interlayer coupling between the\ntwo layers. Also, the intraband and interband contributions to the total\nharmonic spectrum play a significant role. Moreover, interesting polarization\nand ellipticity dependence are noticed in total harmonic spectrum for monolayer\nand bilayer graphene.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Solitons are coherent structures that describe the nonlinear evolution of\nwave localizations in hydrodynamics, optics, plasma and Bose-Einstein\ncondensates. While the Peregrine breather is known to amplify a single\nlocalized perturbation of a carrier wave of finite amplitude by a factor of\nthree, there is a counterpart solution on zero background known as the\ndegenerate two-soliton which also leads to high amplitude maxima. In this\nstudy, we report several observations of such multi-soliton with\ndoubly-localized peaks in a water wave flume. The data collected in this\nexperiment confirm the distinctive attainment of wave amplification by a factor\nof two in good agreement with the dynamics of the nonlinear Schr\\\"odinger\nequation solution. Advanced numerical simulations solving the problem of\nnonlinear free water surface boundary conditions of an ideal fluid quantify the\nphysical limitations of the degenerate two-soliton in hydrodynamics.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Several new applications of Katona's circle are given.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Three-dimensional Catalan numbers are a variant of the classical\n(bidimensional) Catalan numbers, that count, among other interesting objects,\nthe standard Young tableaux of shape (n,n,n). In this paper, we present a\nstructural bijection between two three-dimensional Catalan objects:\n1234-avoiding up-down permutations, and a class of weighted Dyck paths.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Vehicular fog computing (VFC) is envisioned as a promising solution to\nprocess the explosive tasks in autonomous vehicular networks. In the VFC\nsystem, task offloading is the key technique to process the\ncomputation-intensive tasks efficiently. In the task offloading, the task is\ntransmitted to the VFC system according to the 802.11p standard and processed\nby the computation resources in the VFC system. The delay of task offloading,\nconsisting of the transmission delay and computing delay, is extremely critical\nespecially for some delay-sensitive applications. Furthermore, the long-term\nreward of the system (i.e., jointly considers the transmission delay, computing\ndelay, available resources, and diversity of vehicles and tasks) becomes a\nsignificantly important issue for providers. Thus, in this article, we propose\nan optimal task offloading scheme to maximize the long-term reward of the\nsystem where 802.11p is employed as the transmission protocol for the\ncommunications between vehicles. Specifically, a task offloading problem based\non a semi-Markov decision process (SMDP) is formulated. To solve this problem,\nwe utilize an iterative algorithm based on the Bellman equation to approach the\ndesired solution. The performance of the proposed scheme has been demonstrated\nby extensive numerical results.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Massive sized survival datasets are becoming increasingly prevalent with the\ndevelopment of the healthcare industry. Such datasets pose computational\nchallenges unprecedented in traditional survival analysis use-cases. A popular\nway for coping with massive datasets is downsampling them to a more manageable\nsize, such that the computational resources can be afforded by the researcher.\nCox proportional hazards regression has remained one of the most popular\nstatistical models for the analysis of survival data to-date. This work\naddresses the settings of right censored and possibly left truncated data with\nrare events, such that the observed failure times constitute only a small\nportion of the overall sample. We propose Cox regression subsampling-based\nestimators that approximate their full-data partial-likelihood-based\ncounterparts, by assigning optimal sampling probabilities to censored\nobservations, and including all observed failures in the analysis. Asymptotic\nproperties of the proposed estimators are established under suitable regularity\nconditions, and simulation studies are carried out to evaluate the finite\nsample performance of the estimators. We further apply our procedure on\nUK-biobank colorectal cancer genetic and environmental risk factors.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Energetics of the ultrahigh-energy cosmic rays (UHECRs) generated in the\nuniverse is crucial for pinning down their candidate sources. Using the recent\nAuger data on UHECR spectra, we calculate the UHECR energy generation rate\ndensity for different species of nuclei at the injection, considering\nintermediate and heavy nuclei as well as protons, through scanning over source\nparameters on the spectral index, maximum energy and redshift evolution. We\nfind the resulting UHECR energy generation rate density to be\n$\\approx(0.2-2)\\times$10$^{44}$~erg~Mpc$^{-3}$~yr$^{-1}$ at $10^{19.5}$~eV with\na nontrivial dependence on the spectral index. Nuclei other than protons and\nirons favor hard spectral indices at the injection, and the required value of\nenergy budget is smaller for intermediate nuclei. Given significant\nuncertainties in hadronic interaction models and astrophysical models for the\nGalactic-extragalactic transition, our results can be regarded as conservative.\nThe composition data on $X_{\\rm max}$ give additional constraints, but the\nresults are consistent within the model uncertainties.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We construct a structure-preserving finite element method and time-stepping\nscheme for inhomogeneous, incompressible magnetohydrodynamics (MHD). The method\npreserves energy, cross-helicity (when the fluid density is constant), magnetic\nhelicity, mass, total squared density, pointwise incompressibility, and the\nconstraint $\\operatorname{div} B = 0$ to machine precision, both at the\nspatially and temporally discrete levels.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  We derive novel error estimates for Hybrid High-Order (HHO) discretizations\nof Leray-Lions problems set in W^(1,p) with p in (1,2]. Specifically, we prove\nthat, depending on the degeneracy of the problem, the convergence rate may vary\nbetween (k+1)(p-1) and (k+1), with k denoting the degree of the HHO\napproximation. These regime-dependent error estimates are illustrated by a\ncomplete panel of numerical experiments.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In deep learning applications, the architectures of deep neural networks are\ncrucial in achieving high accuracy. Many methods have been proposed to search\nfor high-performance neural architectures automatically. However, these\nsearched architectures are prone to adversarial attacks. A small perturbation\nof the input data can render the architecture to change prediction outcomes\nsignificantly. To address this problem, we propose methods to perform\ndifferentiable search of robust neural architectures. In our methods, two\ndifferentiable metrics are defined to measure architectures' robustness, based\non certified lower bound and Jacobian norm bound. Then we search for robust\narchitectures by maximizing the robustness metrics. Different from previous\napproaches which aim to improve architectures' robustness in an implicit way:\nperforming adversarial training and injecting random noise, our methods\nexplicitly and directly maximize robustness metrics to harvest robust\narchitectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based\nevaluation and verification-based evaluation on the robustness of our methods.\nThe experimental results show that our methods 1) are more robust to various\nnorm-bound attacks than several robust NAS baselines; 2) are more accurate than\nbaselines when there are no attacks; 3) have significantly higher certified\nlower bounds than baselines.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Automatic detecting anomalous regions in images of objects or textures\nwithout priors of the anomalies is challenging, especially when the anomalies\nappear in very small areas of the images, making difficult-to-detect visual\nvariations, such as defects on manufacturing products. This paper proposes an\neffective unsupervised anomaly segmentation approach that can detect and\nsegment out the anomalies in small and confined regions of images. Concretely,\nwe develop a multi-scale regional feature generator that can generate multiple\nspatial context-aware representations from pre-trained deep convolutional\nnetworks for every subregion of an image. The regional representations not only\ndescribe the local characteristics of corresponding regions but also encode\ntheir multiple spatial context information, making them discriminative and very\nbeneficial for anomaly detection. Leveraging these descriptive regional\nfeatures, we then design a deep yet efficient convolutional autoencoder and\ndetect anomalous regions within images via fast feature reconstruction. Our\nmethod is simple yet effective and efficient. It advances the state-of-the-art\nperformances on several benchmark datasets and shows great potential for real\napplications.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Let $n>1$, $e\\geq 0$ and a prime number $p\\geq 2^{n+2+2e}+3$, such that the\nindex of regularity of $p$ is $\\leq e$. We show that there are infinitely many\nirreducible Galois representations $\\rho:\nGal(\\bar{\\mathbb{Q}}/\\mathbb{Q})\\rightarrow {GL}_n(\\mathbb{Q}_p)$ unramified at\nall primes $l\\neq p$. Furthermore, these representations are shown to have\nimage containing a fixed finite index subgroup of ${SL}_n(\\mathbb{Z}_p)$. Such\nrepresentations are constructed by lifting suitable residual representations\n$\\bar{\\rho}$ with image in the diagonal torus in ${GL}_n(\\mathbb{F}_p)$, for\nwhich the global deformation problem is unobstructed.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Ordinary differential equation models facilitate the understanding of\ncellular signal transduction and other biological processes. However, for large\nand comprehensive models, the computational cost of simulating or calibrating\ncan be limiting. AMICI is a modular toolbox implemented in C++/Python/MATLAB\nthat provides efficient simulation and sensitivity analysis routines tailored\nfor scalable, gradient-based parameter estimation and uncertainty\nquantification.\n  AMICI is published under the permissive BSD-3-Clause license with source code\npublicly available on https://github.com/AMICI-dev/AMICI. Citeable releases are\narchived on Zenodo.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  High-resolution hyperspectral images (HSIs) contain the response of each\npixel in different spectral bands, which can be used to effectively distinguish\nvarious objects in complex scenes. While HSI cameras have become low cost,\nalgorithms based on it have not been well exploited. In this paper, we focus on\na novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.\nIt is based on the idea that high-resolution HSIs in city scenes contain rich\nspectral information, which can be easily associated to semantics without\nmanual labeling. Therefore, it enables low cost, highly reliable semantic\nsegmentation in complex scenes. Specifically, in this paper, we theoretically\nanalyze the HSIs and introduce a weakly-supervised HSI semantic segmentation\nframework, which utilizes spectral information to improve the coarse labels to\na finer degree. The experimental results show that our method can obtain highly\ncompetitive labels and even have higher edge fineness than artificial fine\nlabels in some classes. At the same time, the results also show that the\nrefined labels can effectively improve the effect of semantic segmentation. The\ncombination of HSIs and semantic segmentation proves that HSIs have great\npotential in high-level visual tasks.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Over the last two decades, the science has come a long way from relying on\nonly physical experiments and observations to experimentation using computer\nsimulators. This chapter focusses on the modelling and analysis of data arising\nfrom computer simulators. It turns out that traditional statistical metamodels\nare often not very useful for analyzing such datasets. For deterministic\ncomputer simulators, the realizations of Gaussian Process (GP) models are\ncommonly used for fitting a surrogate statistical metamodel of the simulator\noutput. The chapter starts with a quick review of the standard GP based\nstatistical surrogate model. The chapter also emphasizes on the numerical\ninstability due to near-singularity of the spatial correlation structure in the\nGP model fitting process. The authors also present a few generalizations of the\nGP model, reviews methods and algorithms specifically developed for analyzing\nbig data obtained from computer model runs, and reviews the popular analysis\ngoals of such computer experiments. A few real-life computer simulators are\nalso briefly outlined here.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  In preparation for photometric classification of transients from the Legacy\nSurvey of Space and Time (LSST) we run tests with different training data sets.\nUsing estimates of the depth to which the 4-metre Multi-Object Spectroscopic\nTelescope (4MOST) Time Domain Extragalactic Survey (TiDES) can classify\ntransients, we simulate a magnitude-limited sample reaching $r_{\\textrm{AB}}\n\\approx$ 22.5 mag. We run our simulations with the software snmachine, a\nphotometric classification pipeline using machine learning. The\nmachine-learning algorithms struggle to classify supernovae when the training\nsample is magnitude-limited, in contrast to representative training samples.\nClassification performance noticeably improves when we combine the\nmagnitude-limited training sample with a simulated realistic sample of faint,\nhigh-redshift supernovae observed from larger spectroscopic facilities; the\nalgorithms' range of average area under ROC curve (AUC) scores over 10 runs\nincreases from 0.547-0.628 to 0.946-0.969 and purity of the classified sample\nreaches 95 per cent in all runs for 2 of the 4 algorithms. By creating new,\nartificial light curves using the augmentation software avocado, we achieve a\npurity in our classified sample of 95 per cent in all 10 runs performed for all\nmachine-learning algorithms considered. We also reach a highest average AUC\nscore of 0.986 with the artificial neural network algorithm. Having `true'\nfaint supernovae to complement our magnitude-limited sample is a crucial\nrequirement in optimisation of a 4MOST spectroscopic sample. However, our\nresults are a proof of concept that augmentation is also necessary to achieve\nthe best classification results.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Existing Image Captioning (IC) systems model words as atomic units in\ncaptions and are unable to exploit the structural information in the words.\nThis makes representation of rare words very difficult and out-of-vocabulary\nwords impossible. Moreover, to avoid computational complexity, existing IC\nmodels operate over a modest sized vocabulary of frequent words, such that the\nidentity of rare words is lost. In this work we address this common limitation\nof IC systems in dealing with rare words in the corpora. We decompose words\ninto smaller constituent units 'subwords' and represent captions as a sequence\nof subwords instead of words. This helps represent all words in the corpora\nusing a significantly lower subword vocabulary, leading to better parameter\nlearning. Using subword language modeling, our captioning system improves\nvarious metric scores, with a training vocabulary size approximately 90% less\nthan the baseline and various state-of-the-art word-level models. Our\nquantitative and qualitative results and analysis signify the efficacy of our\nproposed approach.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Frieze showed that the expected weight of the minimum spanning tree (MST) of\nthe uniformly weighted graph converges to $\\zeta(3)$. Recently, this result was\nextended to a uniformly weighted simplicial complex, where the role of the MST\nis played by its higher-dimensional analogue -- the Minimum Spanning Acycle\n(MSA). In this work, we go beyond and look at the histogram of the weights in\nthis random MSA -- both in the bulk and in the extremes. In particular, we\nfocus on the `incomplete' setting, where one has access only to a fraction of\nthe potential face weights. Our first result is that the empirical distribution\nof the MSA weights asymptotically converges to a measure based on the shadow --\nthe complement of graph components in higher dimensions. As far as we know,\nthis result is the first to explore the connection between the MSA weights and\nthe shadow. Our second result is that the extremal weights converge to an\ninhomogeneous Poisson point process. A interesting consequence of our two\nresults is that we can also state the distribution of the death times in the\npersistence diagram corresponding to the above weighted complex, a result of\ninterest in applied topology.\n\n\n###\n\n", "completion": "2020"}
{"prompt": "  Geometry of the wave function is a central pillar of modern solid state\nphysics. In this work, we unveil the wave-function geometry of two-dimensional\nsemimetals with band crossing points (BCPs). We show that the Berry phase of\nBCPs are governed by the quantum metric describing the infinitesimal distance\nbetween quantum states. For generic linear BCPs, we show that the corresponding\nBerry phase is determined either by an angular integral of the quantum metric,\nor equivalently, by the maximum quantum distance of Bloch states. This\nnaturally explains the origin of the $\\pi$-Berry phase of a linear BCP. In the\ncase of quadratic BCPs, the Berry phase can take an arbitrary value between 0\nand $2\\pi$. We find simple relations between the Berry phase, maximum quantum\ndistance, and the quantum metric in two cases: (i) when one of the two crossing\nbands is flat; (ii) when the system has rotation and/or time-reversal\nsymmetries. To demonstrate the implication of the continuum model analysis in\nlattice systems, we study tight-binding Hamiltonians describing quadratic BCPs.\nWe show that, when the Berry curvature is absent, a quadratic BCP with an\narbitrary Berry phase always accompanies another quadratic BCP so that the\ntotal Berry phase of the periodic system becomes zero. This work demonstrates\nthat the quantum metric plays a critical role in understanding the geometric\nproperties of topological semimetals.\n\n\n###\n\n", "completion": "2020"}
