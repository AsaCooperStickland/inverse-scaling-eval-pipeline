{"prompt": "  As the second most common neurodegenerative disease, Parkinson's disease has\ncaused serious problems worldwide. However, the cause and mechanism of PD are\nnot clear, and no systematic early diagnosis and treatment of PD have been\nestablished. Many patients with PD have not been diagnosed or misdiagnosed. In\nthis paper, we proposed an EEG-based approach to diagnosing Parkinson's\ndisease. It mapped the frequency band energy of electroencephalogram(EEG)\nsignals to 2-dimensional images using the interpolation method and identified\nclassification using capsule network(CapsNet) and achieved 89.34%\nclassification accuracy for short-term EEG sections. A comparison of separate\nclassification accuracy across different EEG bands revealed the highest\naccuracy in the gamma bands, suggesting that we need to pay more attention to\nthe changes in gamma band changes in the early stages of PD.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In the multi-armed bandit framework, there are two formulations that are\ncommonly employed to handle time-varying reward distributions: adversarial\nbandit and nonstationary bandit. Although their oracles, algorithms, and regret\nanalysis differ significantly, we provide a unified formulation in this paper\nthat smoothly bridges the two as special cases. The formulation uses an oracle\nthat takes the best-fixed arm within time windows. Depending on the window\nsize, it turns into the oracle in hindsight in the adversarial bandit and\ndynamic oracle in the nonstationary bandit. We provide algorithms that attain\nthe optimal regret with the matching lower bound.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Temporal abstraction in reinforcement learning is the ability of an agent to\nlearn and use high-level behaviors, called options. The option-critic\narchitecture provides a gradient-based end-to-end learning method to construct\noptions. We propose an attention-based extension to this framework, which\nenables the agent to learn to focus different options on different aspects of\nthe observation space. We show that this leads to behaviorally diverse options\nwhich are also capable of state abstraction, and prevents the degeneracy\nproblems of option domination and frequent option switching that occur in\noption-critic, while achieving a similar sample complexity. We also demonstrate\nthe more efficient, interpretable, and reusable nature of the learned options\nin comparison with option-critic, through different transfer learning tasks.\nExperimental results in a relatively simple four-rooms environment and the more\ncomplex ALE (Arcade Learning Environment) showcase the efficacy of our\napproach.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  This paper is concerned with a two dimensional Whitham-Boussinesq system\nmodelling surface waves of an inviscid incompressible fluid layer. We prove\nthat the associated Cauchy problem is well-posed for initial data of low\nregularity, with existence time of scale $\\mathcal O(1/\\sqrt{\\epsilon})$, where\n$\\epsilon>0$ is a shallowness parameter measuring the ratio of the amplitude of\nthe wave to the mean depth of the fluid. The key ingredients in the proof are\nfrequency loacalised dispersive and Strichartz estimates that depend on\n$\\epsilon$ as well as bilinear estimates in some Strichartz norms.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Understanding flow and transport of bacteria in porous media is crucial to\ntechnologies such as bioremediation, biomineralization or enhanced oil\nrecovery. While physicochemical bacteria filtration is well-documented, recent\nstudies showed that bacterial motility plays a key role in the transport\nprocess. Flow and transport experiments performed in microfluidic chips\ncontaining randomly placed obstacles confirmed that the distributions of\nnon-motile particles stays compact, whereas for the motile strains, the\ndistributions are characterized by both significant retention as well as fast\ndownstream motion. For motile bacteria, the detailed microscopic study of\nindividual bacteria trajectories reveals two salient features: (i) the\nemergence of an active retention process triggered by motility, (ii)\nenhancement of dispersion due to the exchange between fast flow channels and\nlow flow regions in the vicinity of the solid grains. We propose a physical\nmodel based on a continuous time random walk approach. This approach accounts\nfor bacteria dispersion via variable pore-scale flow velocities through a\nMarkov model for equidistant particle speeds. Motility of bacteria is modeled\nby a two-rate trapping process that accounts for the motion towards and active\ntrapping at the obstacles. This approach captures the forward tails observed\nfor the distribution of bacteria displacements, and quantifies an enhanced\nhydrodynamic dispersion effect that originates in the interaction between flow\nat the pore-scale and bacterial motility. The model reproduces the experimental\nobservations, and predicts bacteria dispersion and transport at the macroscale.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The starting point of this article is a decades-old yet little-noticed\nsufficient condition, presented by Sassenfeld in 1951, for the convergence of\nthe classical Gauss-Seidel method. The purpose of the present paper is to shed\nnew light on Sassenfeld's criterion and to demonstrate that the original work\ncan be perceived as a special case of a far more extensive concept in the\ncontext of preconditioners and iterative linear solvers. Our main result is a\nclassification theorem for the set of all matrices which this general framework\napplies to.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this paper, we present a deep learning technique for data-driven\npredictions of wave propagation in a fluid medium. The technique relies on an\nattention-based convolutional recurrent autoencoder network (AB-CRAN). To\nconstruct a low-dimensional representation of wave propagation data, we employ\na denoising-based convolutional autoencoder. The AB-CRAN architecture with\nattention-based long short-term memory cells forms our deep neural network\nmodel for the time marching of the low-dimensional features. We assess the\nproposed AB-CRAN framework against the standard recurrent neural network for\nthe low-dimensional learning of wave propagation. To demonstrate the\neffectiveness of the AB-CRAN model, we consider three benchmark problems,\nnamely, one-dimensional linear convection, the nonlinear viscous Burgers\nequation, and the two-dimensional Saint-Venant shallow water system. Using the\nspatial-temporal datasets from the benchmark problems, our novel AB-CRAN\narchitecture accurately captures the wave amplitude and preserves the wave\ncharacteristics of the solution for long time horizons. The attention-based\nsequence-to-sequence network increases the time-horizon of prediction compared\nto the standard recurrent neural network with long short-term memory cells. The\ndenoising autoencoder further reduces the mean squared error of prediction and\nimproves the generalization capability in the parameter space.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Using some extensions of a theorem of Heppes on finitely supported discrete\nprobability measures, we address the problems of classification and testing\nbased on projections. In particular, when the support of the distributions is\nknown in advance (as for instance for multivariate Bernoulli distributions), a\nsingle suitably chosen projection determines the distribution. Several\napplications of these results are considered.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We show that for almost every translation surface the number of pairs of\nsaddle connections with bounded virtual area has asymptotic growth like $c R^2$\nwhere the constant $c$ depends only on the area and the connected component of\nthe stratum. The proof techniques combine classical results for counting saddle\nconnections with the crucial result that the Siegel-Veech transform is in\n$L^2$. In order to capture information about pairs of saddle connections, we\nconsider pairs with bounded virtual area since the set of such pairs can be\napproximated by a fibered set which is equivariant under geodesic flow. In the\ncase of lattice surfaces, small virtual area is equivalent to counting parallel\npairs of saddle connections, which also have a quadratic growth of $c R^2$\nwhere $c$ depends in this case on the given lattice surface.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Self-trapped droplets stabilized by quantum fluctuations have been\nexperimentally realized in dipolar gases and binary Boson mixtures. We propose\nspinor Bose gases as another candidate for droplet formation in this work. For\nspin-1 gas, we find that spin fluctuations give a dilute but self-trapped state\nfor two different order parameters where the mean-field picture predicts\ncollapse. A polar droplet phase can be stabilized by spin fluctuations for both\nantiferromagnetic and ferromagnetic spin-dependent coupling. An\nantiferromagnetic droplet phase can be stabilized similarly with a negative\nquadratic Zeeman shift. Furthermore, the beyond mean-field energy of the system\ndepends on the quadratic Zeeman coupling, which provides a mechanism to tune\nthe droplet formation and its density. We discuss the parameters necessary for\nthe experimental realization of such spinor droplets.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In the Next-to-Minimal Supersymmetric Standard Model there is a strong\ncorrelation between the mass terms corresponding to the singlet Higgs and the\nsinglino interaction states, both of which are proportional to the parameter\n$\\kappa$. If this parameter is complex, explicit CP-violation occurs in the\nHiggs as well as the neutralino sectors of the model at the tree level, unlike\nin the minimal scenario. A small magnitude of $\\kappa$ typically yields a\n$\\cal{O}$(10) GeV lightest neutralino with a dominant singlino component. In\nsuch a scenario, the phase of $\\kappa$, beside modifying the properties of the\nfive Higgs bosons, can also have a crucial impact on the phenomenology of the\nneutralino dark matter. In this study we perform a first investigation of this\nimpact on the relic abundance of the dark matter solutions with sub-100 GeV\nmasses, obtained for parameter space configurations of the model that are\nconsistent with a variety of current experimental data.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The Internet has become a prime subject to security attacks and intrusions by\nattackers. These attacks can lead to system malfunction, network breakdown,\ndata corruption or theft. A network intrusion detection system (IDS) is a tool\nused for identifying unauthorized and malicious behavior by observing the\nnetwork traffic. State-of-the-art intrusion detection systems are designed to\ndetect an attack by inspecting the complete information about the attack. This\nmeans that an IDS would only be able to detect an attack after it has been\nexecuted on the system under attack and might have caused damage to the system.\nIn this paper, we propose an end-to-end early intrusion detection system to\nprevent network attacks before they could cause any more damage to the system\nunder attack while preventing unforeseen downtime and interruption. We employ a\ndeep neural network-based classifier for attack identification. The network is\ntrained in a supervised manner to extract relevant features from raw network\ntraffic data instead of relying on a manual feature selection process used in\nmost related approaches. Further, we introduce a new metric, called earliness,\nto evaluate how early our proposed approach detects attacks. We have\nempirically evaluated our approach on the CICIDS2017 dataset. The results show\nthat our approach performed well and attained an overall 0.803 balanced\naccuracy.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Topological quantum state described by the global invariant has been\nextensively studied in theory and experiment. In this letter, we investigate\nthe relationship between \\emph{Zitterbewegung} and the topology of systems that\nreflect the properties of the local and whole energy bands, respectively. We\ngeneralize the usual two-band effective Hamiltonian to characterize the\ntopological phase transition of the spin-$J$ topological insulator. By studying\n\\emph{Zitterbewegung} dynamics before and after topological phase transition,\nwe find that the direction of quasiparticles' oscillation can well reflect\ntopological properties. Furthermore, we develop a quantitative calculation\nformula for the topological invariant in the spin-$J$ Chern insulator and give\nthe selection rule of the corresponding dynamics. Finally, we demonstrate that\nour theory is valid in different topological systems. The topological invariant\ncan be represented by local dynamical properties of the high-symmetry points in\nthe first Brillouin zone, which provides a new measurement method from the\ndynamical perspective.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  UAV-based wireless systems, such as wireless relay and remote sensing, have\nattracted great attentions from academia and industry. To realize them, a\nhigh-performance wireless aerial communication system, which bridges UAVs and\nground stations, is one of the key enablers. However, there are still issues\nhindering its development, such as the severe co-channel interference among\nUAVs, and the limited payload/battery-life of UAVs. To address the challenges,\nwe propose an aerial communication system which enables system-level\nfull-duplex communication of multiple UAVs with lower hardware complexities\nthan ideal full-duplex communication systems. In the proposed system, each\nchannel is re-assigned to the uplink and downlink of a pair of UAVs, and each\nUAV employ a pair of separated channels for its uplink and downlink. The\nco-channel interference between UAVs that reuse same channels is eliminated by\nexploiting advantages of UAVs' maneuverability and high-gain directional\nantennas equipped in UAVs and ground stations, so that dedicated cancellers are\nnot necessary in the proposed system. The system design and performance\nanalysis are given, and the simulation results well agree with the designs.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The continuity hypothesis of dreams suggests that the content of dreams is\ncontinuous with the dreamer's waking experiences. Given the unprecedented\nnature of the experiences during COVID-19, we studied the continuity hypothesis\nin the context of the pandemic. We implemented a deep-learning algorithm that\ncan extract mentions of medical conditions from text and applied it to two\ndatasets collected during the pandemic: 2,888 dream reports (dreaming life\nexperiences), and 57M tweets mentioning the pandemic (waking life experiences).\nThe health expressions common to both sets were typical COVID-19 symptoms\n(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's\nreal-world experiences. The health expressions that distinguished the two sets\nreflected differences in thought processes: expressions in waking life\nreflected a linear and logical thought process and, as such, described\nrealistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those\nin dreaming life reflected a thought process closer to the visual and emotional\nspheres and, as such, described either conditions unrelated to the virus (e.g.,\nmaggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth\nfalling out, body crumbling into sand). Our results confirm that dream reports\nrepresent an understudied yet valuable source of people's health experiences in\nthe real world.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recent experimental and theoretical works have uncovered nontrivial quantum\ndynamics due to external dissipation. Using an exact numerical method and a\nrenormalization-group-based analytical technique, we theoretically elucidate\nthat dissipation drastically alters universal particle-number-fluctuation\ndynamics related to surface-roughness growth in non-interacting fermions and\nbosons. In a system under dephasing that causes loss of spatial coherence, we\nfind that a universality class of surface-roughness dynamics changes from the\nballistic class to a class with the Edwards-Wilkinson scaling exponents and an\nunconventional scaling function. On the other hand, in a system under\ndissipation with in- and out-flow of particles that breaks particle-number\nconservation, the universal dynamics is lost.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  360{\\deg} cameras have gained popularity over the last few years. In this\npaper, we propose two fundamental techniques -- Field-of-View IoU (FoV-IoU) and\n360Augmentation for object detection in 360{\\deg} images. Although most object\ndetection neural networks designed for the perspective images are applicable to\n360{\\deg} images in equirectangular projection (ERP) format, their performance\ndeteriorates owing to the distortion in ERP images. Our method can be readily\nintegrated with existing perspective object detectors and significantly\nimproves the performance. The FoV-IoU computes the intersection-over-union of\ntwo Field-of-View bounding boxes in a spherical image which could be used for\ntraining, inference, and evaluation while 360Augmentation is a data\naugmentation technique specific to 360{\\deg} object detection task which\nrandomly rotates a spherical image and solves the bias due to the\nsphere-to-plane projection. We conduct extensive experiments on the 360indoor\ndataset with different types of perspective object detectors and show the\nconsistent effectiveness of our method.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We analyze a large corpus of police incident narrative documents in\nunderstanding the spatial distribution of the topics. The motivation for doing\nthis is that police narratives in each incident report contains very\nfine-grained information that is richer than the category that is manually\nassigned by the police. Our approach is to split the corpus into topics using\ntwo different unsupervised machine learning algorithms - Latent Dirichlet\nAllocation and Non-negative Matrix Factorization. We validate the performance\nof each learned topic model using model coherence. Then, using a k-nearest\nneighbors density ratio estimation (kNN-DRE) approach that we propose, we\nestimate the spatial density ratio per topic and use this for data discovery\nand analysis of each topic, allowing for insights into the described incidents\nat scale. We provide a qualitative assessment of each topic and highlight some\nkey benefits for using our kNN-DRE model for estimating spatial trends.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  During the early stage of the COVID-19 pandemic, many countries implemented\nnon-pharmaceutical interventions (NPIs) to control the transmission of\nSARS-CoV-2, the causative pathogen of COVID-19. Among those NPIs, quarantine\nmeasures were widely adopted and enforced through stay-at-home and\nshelter-in-place orders. Understanding the effectiveness of quarantine measures\ncan inform decision-making and control planning during the ongoing COVID-19\npandemic and for future disease outbreaks. In this study, we use mathematical\nmodels to evaluate the impact of quarantine measures on COVID-19 spread in four\ncities that experienced large-scale outbreaks in the spring of 2020: Wuhan, New\nYork, Milan, and London. We develop a susceptible-exposed-infected-removed\n(SEIR)-type model with a component of quarantine and couple this disease\ntransmission model with a data assimilation method. By calibrating the model to\ncase data, we estimate key epidemiological parameters before lockdown in each\ncity. We further examine the impact of quarantine rates on COVID-19 spread\nafter lockdown using model simulations. Results indicate that quarantine of\nsusceptible and exposed individuals and undetected infections is necessary to\ncontain the outbreak; however, the quarantine rates for these populations can\nbe reduced through faster isolation of confirmed cases. We generate\ncounterfactual simulations to estimate effectiveness of quarantine measures.\nWithout quarantine measures, the cumulative confirmed cases could be 73, 22, 43\nand 93 times higher than reported numbers within 40 days after lockdown in\nWuhan, New York, Milan, and London. Our findings underscore the essential role\nof quarantine during the early phase of the pandemic.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the genuine multipartite entanglement in tripartite quantum systems\nby using the principal basis matrix representations of density matrices. Using\nthe Schmidt decomposition and local unitary transformation, we first convert\nthe general states to simpler forms and then construct some special matrices\nfrom the correlation tensors of the simplified density matrices. Based on the\ndifferent linear combinations of these matrices, necessary conditions are\npresented to detect genuine multipartite entanglement of tripartite states.\nDetailed examples show that our method can detect more entangled states than\nprevious ones.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  This paper proposes a new data-driven method for the reliable prediction of\npower system post-fault trajectories. The proposed method is based on the\nfundamentally new concept of Deep Operator Networks (DeepONets). Compared to\ntraditional neural networks that learn to approximate functions, DeepONets are\ndesigned to approximate nonlinear operators. Under this operator framework, we\ndesign a DeepONet to (1) take as inputs the fault-on trajectories collected,\nfor example, via simulation or phasor measurement units, and (2) provide as\noutputs the predicted post-fault trajectories. In addition, we endow our method\nwith a much-needed ability to balance efficiency with reliable/trustworthy\npredictions via uncertainty quantification. To this end, we propose and compare\ntwo methods that enable quantifying the predictive uncertainty. First, we\npropose a \\textit{Bayesian DeepONet} (B-DeepONet) that uses stochastic gradient\nHamiltonian Monte-Carlo to sample from the posterior distribution of the\nDeepONet parameters. Then, we propose a \\textit{Probabilistic DeepONet}\n(Prob-DeepONet) that uses a probabilistic training strategy to equip DeepONets\nwith a form of automated uncertainty quantification, at virtually no extra\ncomputational cost. Finally, we validate the predictive power and uncertainty\nquantification capability of the proposed B-DeepONet and Prob-DeepONet using\nthe IEEE 16-machine 68-bus system.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  One of the difficulties of artificial intelligence is to ensure that model\ndecisions are fair and free of bias. In research, datasets, metrics,\ntechniques, and tools are applied to detect and mitigate algorithmic unfairness\nand bias. This study aims to examine existing knowledge on bias and unfairness\nin Machine Learning models, identifying mitigation methods, fairness metrics,\nand supporting tools. A Systematic Literature Review found 40 eligible articles\npublished between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and\nGoogle Scholar knowledge bases. The results show numerous bias and unfairness\ndetection and mitigation approaches for ML technologies, with clearly defined\nmetrics in the literature, and varied metrics can be highlighted. We recommend\nfurther research to define the techniques and metrics that should be employed\nin each case to standardize and ensure the impartiality of the machine learning\nmodel, thus, allowing the most appropriate metric to detect bias and unfairness\nin a given context.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We suggest a new hardcore Poisson-type distribution for Young diagrams with\nthe row lengths from some finite list. A discrete variant of the time-ordered\nMat\\'{e}rn II process in 1D is employed. This approach is related to that based\non the interlacing sequences due to Kerov and others, but we restrict the\nnumber of rows. The basic lengths are assumed comparable with the total order\nof the diagram in the quasi-classical limit, which results in new methods and\nnew formulas. An interesting application is to random walks where the steps are\nat the points satisfying the classical Poisson distribution or our\ntruncatedone. In the simplest case, one obtains the distribution in terms of\nBessel I-functions, which provides some probabilistic interpretation of its\nmany properties. An immediate application of our truncated Poisson\ndistributions is to modeling reinfections in epidemics.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recent advances in ultrafast pump-probe spectroscopy provide access to hidden\nphases of correlated matter, including light-induced superconducting states,\nbut the theoretical understanding of these nonequilibrium phases remains\nlimited. Here we report how a new type of chiral superconducting phase can be\nstabilized in photodoped frustrated Mott insulators. The metastable phase\nfeatures a spatially varying order parameter with a $120^\\circ$ phase twist\nwhich breaks both time-reversal and inversion symmetry. Under an external\nelectric pulse, the $120^\\circ$ chiral superconducting state can exhibit a\nsecond-order supercurrent perpendicular to the field in addition to a\nfirst-order parallel response, similar to a nonlinear anomalous Hall effect.\nThis phase can be tuned by artificial gauge fields when the system is dressed\nby high-frequency periodic driving. The mechanism revealed in this study\napplies to Mott insulators on various frustrated lattices and the hidden\nsuperconducting phase can be realized in both cold-atom quantum simulators and\ncorrelated solids.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  On the world wide web, toxic content detectors are a crucial line of defense\nagainst potentially hateful and offensive messages. As such, building highly\neffective classifiers that enable a safer internet is an important research\narea. Moreover, the web is a highly multilingual, cross-cultural community that\ndevelops its own lingo over time. As such, it is crucial to develop models that\nare effective across a diverse range of languages, usages, and styles. In this\npaper, we present the fundamentals behind the next version of the Perspective\nAPI from Google Jigsaw. At the heart of the approach is a single multilingual\ntoken-free Charformer model that is applicable across a range of languages,\ndomains, and tasks. We demonstrate that by forgoing static vocabularies, we\ngain flexibility across a variety of settings. We additionally outline the\ntechniques employed to make such a byte-level model efficient and feasible for\nproductionization. Through extensive experiments on multilingual toxic comment\nclassification benchmarks derived from real API traffic and evaluation on an\narray of code-switching, covert toxicity, emoji-based hate, human-readable\nobfuscation, distribution shift, and bias evaluation settings, we show that our\nproposed approach outperforms strong baselines. Finally, we present our\nfindings from deploying this system in production.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The Energy-Based Model (EBM) framework is a very general approach to\ngenerative modeling that tries to learn and exploit probability distributions\nonly defined though unnormalized scores. It has risen in popularity recently\nthanks to the impressive results obtained in image generation by parameterizing\nthe distribution with Convolutional Neural Networks (CNN). However, the\nmotivation and theoretical foundations behind modern EBMs are often absent from\nrecent papers and this sometimes results in some confusion. In particular, the\ntheoretical justifications behind the popular MCMC-based learning algorithm\nContrastive Divergence (CD) are often glossed over and we find that this leads\nto theoretical errors in recent influential papers (Du & Mordatch, 2019; Du et\nal., 2020). After offering a first-principles introduction of MCMC-based\ntraining, we argue that the learning algorithm they use can in fact not be\ndescribed as CD and reinterpret theirs methods in light of a new\ninterpretation. Finally, we discuss the implications of our new interpretation\nand provide some illustrative experiments.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  An $r$-uniform supertree is a connected and acyclic hypergraph of which each\nedge has $r$ vertices, where $r\\geq 3$. We propose the concept of matching\nenergy for an $r$-uniform hypergraph, which is defined as the sum of the\nabsolute value of all the eigenvalues of its matching polynomial. With the aid\nof the matching polynomial of an $r$-uniform supertree, three pairs of\n$r$-uniform supertrees with the same spectral radius and the same matching\nenergy are constructed, and two infinite families of $r$-uniform supertrees\nwith the same spectral radius and the same matching energy are characterized.\nSome known results about the graphs with the same spectra regarding to their\nadjacency matrices can be naturally deduced from our new results.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Environment perception including detection, classification, tracking, and\nmotion prediction are key enablers for automated driving systems and\nintelligent transportation applications. Fueled by the advances in sensing\ntechnologies and machine learning techniques, LiDAR-based sensing systems have\nbecome a promising solution. The current challenges of this solution are how to\neffectively combine different perception tasks into a single backbone and how\nto efficiently learn the spatiotemporal features directly from point cloud\nsequences. In this research, we propose a novel spatiotemporal attention\nnetwork based on a transformer self-attention mechanism for joint semantic\nsegmentation and motion prediction within a point cloud at the voxel level. The\nnetwork is trained to simultaneously outputs the voxel level class and\npredicted motion by learning directly from a sequence of point cloud datasets.\nThe proposed backbone includes both a temporal attention module (TAM) and a\nspatial attention module (SAM) to learn and extract the complex spatiotemporal\nfeatures. This approach has been evaluated with the nuScenes dataset, and\npromising performance has been achieved.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Single quarks moving in the vacuum of confining gauge theories are stopped by\na drag force. The holographic description relates the confining scale in the\nbulk geometry with a range of physical values for the drag force in the vacuum.\nThe vacuum drag force acting on the isolated quark directly manifests quark\nconfinement since it prevents the quark from walking freely in the vacuum.\nHowever, analytical expressions for the drag force as a function of the quark\nvelocity were lacking. In the present work, we propose that the vacuum drag\nforce is given by the regularized zero-temperature limit of the corresponding\nthermal drag force. Within this approach, we obtain the desired analytic\nexpressions in two different holographic models: the quadratic dilaton and the\nD-instanton. In both cases, we find well-behaved functions belonging to their\nphysical range of values.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Stacking order in van der Waals materials determines the coupling between\natomic layers and is therefore key to the materials' properties. By exploring\ndifferent stacking orders, many novel physical phenomena have been realized in\nartificial vdW stacks. Recently, 2D ferroelectricity has been observed in\nzero-degree aligned hBN and graphene-hBN heterostructures, holding promise in a\nrange of electronic applications. In those artificial stacks, however, the\nsingle domain size is limited by the stacking-angle misalignment to about 0.1\nto 1 $\\mu$m, which is incompatible with most optical or optoelectronic\napplications. Here we show MoS$_2$ in the rhombohedral phase can host a\nhomogeneous spontaneous polarization throughout few-$\\mu$m-sized exfoliated\nflakes, as it is a natural crystal requiring no stacking and is, therefore free\nof misalignment. Utilizing this homogeneous polarization and its induced\ndepolarization field (DEP), we build a graphene-MoS$_2$ based photovoltaic\ndevice with high efficiency. The few-layer MoS$_2$ is thinner than most\noxide-based ferroelectric films, which allows us to maximize the DEP and study\nits impact at the atomically thin limit, while the highly uniform polarization\nachievable in the commensurate crystal enables a tangible path for up-scaling.\nThe external quantum efficiency of our device is up to 16% at room temperature,\nover one order larger than the highest efficiency observed in bulk photovoltaic\ndevices, owing to the reduced screening in graphene, the exciton-enhanced\nlight-matter interaction, and the ultrafast interlayer relaxation in MoS$_2$.\nIn view of the wide range of bandgap energy in other TMDs, our findings make\nrhombohedral TMDs a promising and versatile candidate for applications such as\nenergy-efficient photo-detection with high speed and programmable polarity.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Despite the long history of spectroscopic studies of the C$_2$ molecule,\nfundamental questions about its chemical bonding are still being hotly debated.\nThe complex electronic structure of C$_2$ is a consequence of its dense\nmanifold of near-degenerate, low-lying electronic states. A global multi-state\ndiabatic model is proposed here to disentangle the numerous configuration\ninteractions within four symmetry manifolds of C$_2$ ($^{1}\\Pi_g$, $^{3}\\Pi_g$,\n$^{1}\\Sigma_u^+$, and $^{3}\\Sigma_u^+$). The key concept of our model is the\nexistence of two \"valence-hole\" configurations,\n$2\\sigma_g^22\\sigma_u^11\\pi_{u}^33\\sigma_g^2$ for $^{1,3}\\Pi_g$ states and\n$2\\sigma_g^22\\sigma_u^11\\pi_{u}^43\\sigma_g^1$ for $^{1,3}\\Sigma_u^+$ states\nthat derive from $3\\sigma_g\\leftarrow2\\sigma_u$ electron promotion. The\nlowest-energy state from each of the four C$_2$ symmetry species is dominated\nby this type of valence-hole configuration at its equilibrium internuclear\nseparation. As a result of their large binding energy (nominal bond order of 3)\nand correlation with the 2s$^2$2p$^2$+2s2p$^3$ separated-atom configurations,\nthe presence of these valence-hole configurations has a profound impact on the\n$global$ electronic structure and unimolecular dynamics of C$_2$.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  An effective numerical method is presented for optimizing model parameters\nthat can be applied to any type of system of non-linear equations and any\nnumber of data-points, which does not require explicit formulation of the\nobjective function or its partial derivatives. The numerics are reduced to\nsolving a non-linear least squares problem, which uses the Levenberg-Marquardt\nalgorithm and the Jacobian is approximated by applying rank-one updates using\nBroyden's method. An advantage of this methodology over conventional approaches\nis that the partial derivatives of the objective function do not have to be\nanalytically calculated. For instance, there may be situations where one cannot\nformulate the partial derivatives, such as cases involving an objective\nfunction that itself contains a nested optimization problem. Moreover, a line\nsearch algorithm is also described that ensures that the Armijo conditions are\nsatisfied and that convergence is assured, which makes the success of the\napproach insensitive to the initial estimates of the model parameters. The\nforegoing numerical methods are described with respect to the development of\nthe Optima software to solve inverse problems, which are reduced to non-linear\nleast squares problems. This computational approach has proven to be\nparticularly useful at solving inverse problems of very complex physical models\nthat cannot be optimized directly in a practical way.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  An anomalous magneto-optical spectrum is discovered for dipolar valley\nexcitons in twisted double layer transition metal dichalcogenides (TMD), where\nin-plane magnetic field induces a sizable multiplet splitting of exciton states\ninside the light cone. Chiral dispersions of the split branches make possible\nefficient optical injection of unidirectional exciton current. We also find an\nanalog effect with a modest heterostrain replacing the magnetic field for\nintroducing large splitting and chiral dispersions in the light cone. Angular\norientation of photo-injected exciton flow can be controlled by strain, with\nleft-right unidirectionality selected by circular polarisation.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  String edit distances have been used for decades in applications ranging from\nspelling correction and web search suggestions to DNA analysis. Most string\nedit distances are variations of the Levenshtein distance and consider only\nsingle-character edits. In forensic applications polymorphic genetic markers\nsuch as short tandem repeats (STRs) are used. At these repetitive motifs the\nDNA copying errors consist of more than just single base differences. More\noften the phenomenon of ``stutter'' is observed, where the number of repeated\nunits differs (by whole units) from the template. To adapt the Levenshtein\ndistance to be suitable for forensic applications where DNA sequence similarity\nis of interest, a generalized string edit distance is defined that accommodates\nthe addition or deletion of whole motifs in addition to single-nucleotide\nedits. A dynamic programming implementation is developed for computing this\ndistance between sequences. The novelty of this algorithm is in handling the\ncomplex interactions that arise between multiple- and single-character edits.\nForensic examples illustrate the purpose and use of the Restricted Forensic\nLevenshtein (RFL) distance measure, but applications extend to sequence\nalignment and string similarity in other biological areas, as well as dynamic\nprogramming algorithms more broadly.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The vulnerability of convolutional neural networks (CNNs) to image\nperturbations such as common corruptions and adversarial perturbations has\nrecently been investigated from the perspective of frequency. In this study, we\ninvestigate the effect of the amplitude and phase spectra of adversarial images\non the robustness of CNN classifiers. Extensive experiments revealed that the\nimages generated by combining the amplitude spectrum of adversarial images and\nthe phase spectrum of clean images accommodates moderate and general\nperturbations, and training with these images equips a CNN classifier with more\ngeneral robustness, performing well under both common corruptions and\nadversarial perturbations. We also found that two types of overfitting\n(catastrophic overfitting and robust overfitting) can be circumvented by the\naforementioned spectrum recombination. We believe that these results contribute\nto the understanding and the training of truly robust classifiers.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Cryo-electron microscopy (cryo-EM) has become a tool of fundamental\nimportance in structural biology, helping us understand the basic building\nblocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the\nunknown 3D poses and the 3D electron scattering potential of a biomolecule from\nmillions of extremely noisy 2D images. Existing reconstruction algorithms,\nhowever, cannot easily keep pace with the rapidly growing size of cryo-EM\ndatasets due to their high computational and memory cost. We introduce cryoAI,\nan ab initio reconstruction algorithm for homogeneous conformations that uses\ndirect gradient-based optimization of particle poses and the electron\nscattering potential from single-particle cryo-EM data. CryoAI combines a\nlearned encoder that predicts the poses of each particle image with a\nphysics-based decoder to aggregate each particle image into an implicit\nrepresentation of the scattering potential volume. This volume is stored in the\nFourier domain for computational efficiency and leverages a modern coordinate\nnetwork architecture for memory efficiency. Combined with a symmetrized loss\nfunction, this framework achieves results of a quality on par with\nstate-of-the-art cryo-EM solvers for both simulated and experimental data, one\norder of magnitude faster for large datasets and with significantly lower\nmemory requirements than existing methods.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Knowledge-based visual question answering requires the ability of associating\nexternal knowledge for open-ended cross-modal scene understanding. One\nlimitation of existing solutions is that they capture relevant knowledge from\ntext-only knowledge bases, which merely contain facts expressed by first-order\npredicates or language descriptions while lacking complex but indispensable\nmultimodal knowledge for visual understanding. How to construct vision-relevant\nand explainable multimodal knowledge for the VQA scenario has been less\nstudied. In this paper, we propose MuKEA to represent multimodal knowledge by\nan explicit triplet to correlate visual objects and fact answers with implicit\nrelations. To bridge the heterogeneous gap, we propose three objective losses\nto learn the triplet representations from complementary views: embedding\nstructure, topological relation and semantic space. By adopting a pre-training\nand fine-tuning learning strategy, both basic and domain-specific multimodal\nknowledge are progressively accumulated for answer prediction. We outperform\nthe state-of-the-art by 3.35% and 6.08% respectively on two challenging\nknowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the\ncomplementary benefits of the multimodal knowledge with existing knowledge\nbases and the advantages of our end-to-end framework over the existing pipeline\nmethods. The code is available at https://github.com/AndersonStra/MuKEA.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We prove that the normal metric on the homogeneous space\n$E_7/\\mathrm{PSO}(8)$ is stable with respect to the Einstein-Hilbert action,\nthereby exhibiting the first known example of a non-symmetric metric of\npositive scalar curvature with this property.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  A Head Related Transfer Function (HRTF) characterizes how a human ear\nreceives sounds from a point in space, and depends on the shapes of one's head,\npinna, and torso. Accurate estimations of HRTFs for human subjects are crucial\nin enabling binaural acoustic applications such as sound localization and 3D\nsound spatialization. Unfortunately, conventional approaches for HRTF\nestimation rely on specialized devices or lengthy measurement processes. This\nwork proposes a novel lightweight method for HRTF individualization that can be\nimplemented using commercial-off-the-shelf components and performed by average\nusers in home settings. The proposed method has two key components: a\ngenerative neural network model that can be individualized to predict HRTFs of\nnew subjects from sparse measurements, and a lightweight measurement procedure\nthat collects HRTF data from spatial locations. Extensive experiments using a\npublic dataset and in house measurement data from 10 subjects of different ages\nand genders, show that the individualized models significantly outperform a\nbaseline model in the accuracy of predicted HRTFs. To further demonstrate the\nadvantages of individualized HRTFs, we implement two prototype applications for\nbinaural localization and acoustic spatialization. We find that the performance\nof a localization model is improved by 15 degree after trained with\nindividualized HRTFs. Furthermore, in hearing tests, the success rate of\ncorrectly identifying the azimuth direction of incoming sounds increases by\n183% after individualization.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Many modern cyber physical systems incorporate computer vision technologies,\ncomplex sensors and advanced control software, allowing them to interact with\nthe environment autonomously. Testing such systems poses numerous challenges:\nnot only should the system inputs be varied, but also the surrounding\nenvironment should be accounted for. A number of tools have been developed to\ntest the system model for the possible inputs falsifying its requirements.\nHowever, they are not directly applicable to autonomous cyber physical systems,\nas the inputs to their models are generated while operating in a virtual\nenvironment. In this paper, we aim to design a search based framework, named\nAmbieGen, for generating diverse fault revealing test scenarios for autonomous\ncyber physical systems. The scenarios represent an environment in which an\nautonomous agent operates. The framework should be applicable to generating\ndifferent types of environments. To generate the test scenarios, we leverage\nthe NSGA II algorithm with two objectives. The first objective evaluates the\ndeviation of the observed system behaviour from its expected behaviour. The\nsecond objective is the test case diversity, calculated as a Jaccard distance\nwith a reference test case. We evaluate AmbieGen on three scenario generation\ncase studies, namely a smart-thermostat, a robot obstacle avoidance system, and\na vehicle lane keeping assist system. We compared three configurations of\nAmbieGen: based on a single objective genetic algorithm, multi objective, and\nrandom search. Both single and multi objective configurations outperform the\nrandom search. Multi objective configuration can find the individuals of the\nsame quality as the single objective, producing more unique test scenarios in\nthe same time budget.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The Lagrangian formalism is developed for the population dynamics of\ninteracting species that are described by several well-known models. The\nformalism is based on standard Lagrangians, which represent differences between\nthe physical kinetic and potential energy-like terms. A method to derive these\nLagrangians is presented and applied to selected theoretical models of the\npopulation dynamics. The role of the derived Lagrangians and the energy-like\nterms in the population dynamics is investigated and discussed. It is suggested\nthat the obtained standard Lagrangians can be used to identify physical\nsimilarities between different population models.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We consider Hodgkin-Huxley-type model that is a stiff ODE system with two\nfast and one slow variables. For the parameter ranges under consideration the\noriginal version of the model has unstable fixed point and the oscillating\nattractor that demonstrates bifurcation from bursting to spiking dynamics. Also\na modified version is considered where the bistability occurs such that an area\nin the parameter space appears where the fixed point becomes stable and\ncoexists with the bursting attractor. For these two systems we create\nartificial neural networks that are able to reproduce their dynamics. The\ncreated networks operate as recurrent maps and are trained on trajectory cuts\nsampled at random parameter values within a certain range. Although the\nnetworks are trained only on oscillatory trajectory cuts, it also discover the\nfixed point of the considered systems. The position and even the eigenvalues\ncoincide very well with the fixed point of the initial ODEs. For the bistable\nmodel it means that the network being trained only on one brunch of the\nsolutions recovers another brunch without seeing it during the training. These\nresults, as we see it, are able to trigger the development of new approaches to\ncomplex dynamics reconstruction and discovering. From the practical point of\nview reproducing dynamics with the neural network can be considered as a sort\nof alternative method of numerical modeling intended for use with contemporary\nparallel hard- and software.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The motivation for PF calorimetry is to experimentally measure the energy of\nhadron jets with excellent resolution. In particle flow designs, sigma(E)/E <\n5% should be possible for a range of jet energies from 50 GeV to 250 GeV,\nimportant particularly for experiments at electron-positron colliders (ILC,\nCLIC, FCCee, CEPC). The high granularity, which is essential for PF\ncalorimetry, can also be very beneficial for removal of background from pile-up\non an event-by-event basis making such calorimeters an attractive approach for\nhadron collider experiments, for example the HGCAL under construction for CMS\nat the CERN HL-LHC.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recent studies reveal that a double-quantum-dot system hosting more than two\nelectrons may be superior in certain aspects as compared to the traditional\ncase in which only two electrons are confined (a singlet-triplet qubit). We\nstudy the electron-phonon dephasing occurring in a GaAs multi-electron\ndouble-quantum-dot system, in a biased case in which the singlet state is\nhybridized, as well as in an unbiased case in which the hybridization is\nabsent. We have found that while the electron-phonon dephasing rate increases\nwith the number of electrons confined in the unbiased case, this does not hold\nin the biased case. We define a merit figure as a ratio between the exchange\nenergy and the dephasing rate, and have shown that in experimentally relevant\nrange of the exchange energy, the merit figure actually increases with the\nnumber of electrons in the biased case. Our results show that the\nmulti-electron quantum-dot system has another advantage in mitigating the\neffect of electron-phonon dephasing, which is previously under-appreciated in\nthe literature.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We investigate the use of prior knowledge of human and animal movement to\nlearn reusable locomotion skills for real legged robots. Our approach builds\nupon previous work on imitating human or dog Motion Capture (MoCap) data to\nlearn a movement skill module. Once learned, this skill module can be reused\nfor complex downstream tasks. Importantly, due to the prior imposed by the\nMoCap data, our approach does not require extensive reward engineering to\nproduce sensible and natural looking behavior at the time of reuse. This makes\nit easy to create well-regularized, task-oriented controllers that are suitable\nfor deployment on real robots. We demonstrate how our skill module can be used\nfor imitation, and train controllable walking and ball dribbling policies for\nboth the ANYmal quadruped and OP3 humanoid. These policies are then deployed on\nhardware via zero-shot simulation-to-reality transfer. Accompanying videos are\navailable at https://bit.ly/robot-npmp.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Predicting pedestrian crossing intention is an indispensable aspect of\ndeploying advanced driving systems (ADS) or advanced driver-assistance systems\n(ADAS) to real life. State-of-the-art methods in predicting pedestrian crossing\nintention often rely on multiple streams of information as inputs, each of\nwhich requires massive computational resources and heavy network architectures\nto generate. However, such reliance limits the practical application of the\nsystems. In this paper, driven the the real-world demands of pedestrian\ncrossing intention prediction models with both high efficiency and accuracy, we\nintroduce a network with only frames of pedestrians as the input. Every\ncomponent in the introduced network is driven by the goal of light weight.\nSpecifically, we reduce the multi-source input dependency and employ light\nneural networks that are tailored for mobile devices. These smaller neural\nnetworks can fit into computer memory and can be transmitted over a computer\nnetwork more easily, thus making them more suitable for real-life deployment\nand real-time prediction. To compensate the removal of the multi-source input,\nwe enhance the network effectiveness by adopting a multi-task learning\ntraining, named \"side task learning\", to include multiple auxiliary tasks to\njointly learn the feature extractor for improved robustness. Each head handles\na specific task that potentially shares knowledge with other heads. In the\nmeantime, the feature extractor is shared across all tasks to ensure the\nsharing of basic knowledge across all layers. The light weight but high\nefficiency characteristics of our model endow it the potential of being\ndeployed on vehicle-based systems. Experiments validate that our model\nconsistently delivers outstanding performances.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The discovery of graphene and its fascinating capabilities have triggered an\nunprecedented interest in inorganic two-dimensional (2D) materials. Van der\nWaals (vdW) layered materials as graphene, hexagonal boron nitride (hBN),\ntransition metal dichalcogenides (TMDs), and the more recently re-discovered\nblack phosphorus (BP) indeed display an exceptional technological potential for\nengineering nano-electronic and nano-photonic devices and components by design,\noffering a unique platform for developing new devices with a variety of ad-hoc\nproperties. In this perspective article, we provide a vision on the key\ntransformative applications of 2D nanomaterials for the developments of\nnanoelectronic, nanophotonic, optical and plasmonic devices, at terahertz\nfrequencies, highlighting how the rich physical phenomena enabled by their\nunique band-structure engineering can allow those devices to boost the vibrant\nfield of quantum science and quantum technologies.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  According to the quantum adiabatic theorem, we can in principle obtain a true\nvacuum of a quantum system starting from a trivial vacuum of a simple\nHamiltonian. In actual adiabatic digital quantum simulation with finite time\nlength and non-infinitesimal time steps, we can only obtain an approximate\nvacuum that is supposed to be a superposition of a true vacuum and excited\nstates. We propose a procedure to improve the approximate vacuum.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Over the last decade, Twitter has emerged as one of the most influential\nforums for social, political, and health discourse. In this paper, we introduce\na massive dataset of more than 45 million geo-located tweets posted between\n2015 and 2021 from US and Canada (TUSC), especially curated for natural\nlanguage analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to\ncapture patterns of emotions associated with tweets over time. We use TED and\nTUSC to explore the use of emotion-associated words across US and Canada;\nacross 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the\nsecond year of the pandemic); and across individual tweeters. We show that\nCanadian tweets tend to have higher valence, lower arousal, and higher\ndominance than the US tweets. Further, we show that the COVID-19 pandemic had a\nmarked impact on the emotional signature of tweets posted in 2020, when\ncompared to the adjoining years. Finally, we determine metrics of TED for\n170,000 tweeters to benchmark characteristics of TED metrics at an aggregate\nlevel. TUSC and the metrics for TED will enable a wide variety of research on\nstudying how we use language to express ourselves, persuade, communicate, and\ninfluence, with particularly promising applications in public health, affective\nscience, social science, and psychology.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Most motion retargeting methods in human action video synthesis decompose the\ninput video to motion (dynamic information) and shape (static information).\nHowever, we observe if the dynamic information is directly transferred to\nanother subject, it will result in unnatural synthesised motion. This\nphenomenon is mainly caused by neglecting subject-dependent information in\nmotion. To solve the problem, we propose a novel motion retargeting method\nwhich can combine both subject-independent (common motion content) information\nfrom a source video and subject-dependent (individualized identity motion)\ninformation from a target video. So it can synthesize videos with a much\nnatural appearance along with identity-preserved motion. In the proposed method\ntwo encoders are employed to extract identity and motion content\nrepresentations respectively. We employ the adaptive instance normalization\n(AdaIN) layer in the generator and the instance normalization (IN) layer in the\nmotion content encoder to synthesize the new motion. Besides, we also collected\na dataset, named $Chuang101$, with 101 subjects in total. Each subject performs\nidentical dancing movement, and so it is convenient for feature disentanglement\namong motion and identity of each subject. Furthermore, an efficient\nquantitative metric for identify information is designed by gait recognition.\nThe experiments show the proposed method can synthesize videos more naturally\nwhen the subject's identity is preserved.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The GHM multi-level discrete wavelet transform is proposed as preprocessing\nfor image super resolution with convolutional neural networks. Previous works\nperform analysis with the Haar wavelet only. In this work, 37 single-level\nwavelets are experimentally analyzed from Haar, Daubechies, Biorthogonal,\nReverse Biorthogonal, Coiflets, and Symlets wavelet families. All single-level\nwavelets report similar results indicating that the convolutional neural\nnetwork is invariant to choice of wavelet in a single-level filter approach.\nHowever, the GHM multi-level wavelet achieves higher quality reconstructions\nthan the single-level wavelets. Three large data sets are used for the\nexperiments: DIV2K, a dataset of textures, and a dataset of satellite images.\nThe approximate high resolution images are compared using seven objective error\nmeasurements. A convolutional neural network based approach using wavelet\ntransformed images has good results in the literature.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The MAVERIC survey is the first deep radio continuum imaging survey of Milky\nWay globular clusters, with a central goal of finding and classifying accreting\ncompact binaries, including stellar-mass black holes. Here we present radio\nsource catalogs for 25 clusters with ultra-deep Karl G. Jansky Very Large Array\nobservations. The median observing time was 10 hr per cluster, resulting in\ntypical rms sensitivities of 2.3 and 2.1 uJy per beam at central frequencies of\n5.0 and 7.2 GHz, respectively. We detect nearly 1300 sources in our survey at 5\nsigma, and while many of these are likely to be background sources, we also\nfind strong evidence for an excess of radio sources in some clusters. The radio\nspectral index distribution of sources in the cluster cores differs from the\nbackground, and shows a bimodal distribution. We tentatively classify the\nsteep-spectrum sources (those much brighter at 5.0 GHz) as millisecond pulsars\nand the flat-spectrum sources as compact or other kinds of binaries. These\nprovisional classifications will be solidified with the future addition of\nX-ray and optical data. The outer regions of our images represent a deep,\nrelatively wide field (~ 0.4/sq. deg) and high resolution C band background\nsurvey, and we present source counts calculated for this area. We also release\nradio continuum images for these 25 clusters to the community.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Semiparametric Bayesian inference has so far relied on models for the\nobservable that partition into two parts, one being parametric and the other\nnonparametric, with the target parameter being dependent on the parametric\ncomponent. While a partitioned structure makes specification of the marginal\nprior on the target parameter simple to perform, it often arises from\nconditional modelling which is subject to misspecification and ultimately a\nlack of consistency. We introduce a new type of semiparametric model to allow\neasy prior specification for a parameter that is defined as a functional of the\ndistribution for the observable. Our semiparametric model is obtained as an\nextension of nonparametric models that are consistent under very general\nconditions. This type of Bayesian semiparametric model can be used to obtain\nBayesian versions of Frequentist estimators that are defined as functionals of\nthe empirical distribution. This gives us new opportunities to conduct Bayesian\nanalysis in problems where Frequentist estimators exist but not well-accepted\nlikelihoods.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The paper considers the Hilbert space $\\hat{H}_r$ of real functions summable\nwith the square $L^2(a,b)_r$ on any interval $\\{(a,b)_r\\}_{r=1}^{\\infty}\\in\n\\mathbb{R}$. It is shown on the basis of the theorem on zeros of real\northogonal polynomials if in $\\hat{H}_r$ there exists a complete orthonormal\nbasis $\\{f(x)_k\\}_{k=1}^{\\infty}$ and the function\n$f(x)\\in\\{f(x)_k\\}_{k=1}^{\\infty}$ has zeros, then these zeros are simple and\nreal. The generalized Hardy function\n$Z(\\sigma,t)=\\Re\\zeta(\\sigma+it)e^{i\\theta(t)}$ is considered. It is shown that\nin the Hilbert space $\\hat{H}_r$ there exists a complete basis\n$\\{Z(\\lambda_k,t\\}_{k=1}^{\\infty}$ where $\\lambda_k\\in\\mathbb{Q}$ and\n$Z(t)\\in\\{Z(\\lambda_k,t\\}_{k=1}^{\\infty}$ when $\\lambda_k=1/2$, hence the Hardy\nfunction $Z(t)=\\zeta(1/2+it)e^{i\\theta(t)}$ has all simple and real zeros.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Starting from first principles, we study radiative transfer by new\nfeebly-interacting bosons (FIBs) such as axions, axion-like particles (ALPs),\ndark photons, and others. Our key simplification is to include only boson\nemission or absorption (including decay), but not scattering between different\nmodes of the radiation field. Based on a given distribution of temperature and\nFIB absorption rate in a star, we derive explicit volume-integral expressions\nfor the boson luminosity, reaching from the free-streaming to the\nstrong-trapping limit. The latter is seen explicitly to correspond to\nquasi-thermal emission from a \"FIB sphere\" according to the Stefan-Boltzmann\nlaw. Our results supersede expressions and approximations found in the recent\nliterature on FIB emission from a supernova core and, for radiatively unstable\nFIBs, provide explicit expressions for the nonlocal (\"ballistic\") transfer of\nenergy recently discussed in horizontal-branch stars.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recent numerical explorations of extremely intense circulation fluctuations\nat high Reynolds number flows have brought to light novel aspects of turbulent\nintermittency. Vortex gas modeling ideas, introduced alongside such\ndevelopments, have led to accurate descriptions of the core and the\nintermediate tails of circulation probability distribution functions (cPDFs),\nas well as the scaling exponents associated to statistical moments of\ncirculation. We extend the predictive reach of the vortex gas picture of\nturbulence, by emphasizing that multifractality breaking, one of its salient\nphenomenological ingredients, is the key concept to disclose the asymptotic\nform of cPDF tails. A remarkable analytical agreement is found with previous\nresults derived within the framework of the instanton approach to circulation\nintermittency.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Objective: Model based deep learning (MBDL) has been challenging to apply to\nthe reconstruction of 3D non-Cartesian MRI acquisitions due to extreme GPU\nmemory demand (>250 GB using traditional backpropagation) primarily because the\nentire volume is needed for data-consistency steps embedded in the model. The\ngoal of this work is to develop and apply a memory efficient method called\nblock-wise learning that combines gradient checkpointing with patch-wise\ntraining to allow for fast and high-quality 3D non-Cartesian reconstructions\nusing MBDL. Approach: Block-wise learning applied to a single unroll decomposes\nthe input volume into smaller patches, gradient checkpoints each patch, passes\neach patch iteratively through a neural network regularizer, and then rebuilds\nthe full volume from these output patches for data-consistency. This method is\napplied across unrolls during training. Block-wise learning significantly\nreduces memory requirements by tying GPU memory to user selected patch size\ninstead of the full volume. This algorithm was used to train a MBDL\narchitecture to reconstruct highly undersampled, 1.25mm isotropic, pulmonary\nmagnetic resonance angiography volumes with matrix sizes varying from 300-450 x\n200-300 x 300-450 on a single GPU. We compared block-wise learning\nreconstructions against L1 wavelet compressed reconstructions and proxy ground\ntruth images. Main results: MBDL with block-wise learning significantly\nimproved image quality relative to L1 wavelet compressed sensing while\nsimultaneously reducing average reconstruction time 38x. Significance:\nBlock-wise learning allows for MBDL to be applied to high spatial resolution,\n3D non-Cartesian datasets with improved image quality and significant\nreductions in reconstruction time relative to traditional iterative methods\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Prophet inequalities consist of many beautiful statements that establish\ntight performance ratios between online and offline allocation algorithms.\nTypically, tightness is established by constructing an algorithmic guarantee\nand a worst-case instance separately, whose bounds match as a result of some\n\"ingenuity\". In this paper, we instead formulate the construction of the\nworst-case instance as an optimization problem, which directly finds the tight\nratio without needing to construct two bounds separately.\n  Our analysis of this complex optimization problem involves identifying\nstructure in a new \"Type Coverage\" dual problem. It can be seen as akin to the\ncelebrated Magician and OCRS (Online Contention Resolution Scheme) problems,\nexcept more general in that it can also provide tight ratios relative to the\noptimal offline allocation, whereas the earlier problems only establish tight\nratios relative to the ex-ante relaxation of the offline problem.\n  Through this analysis, our paper provides a unified framework that derives\nnew results and recovers many existing ones. First, we show that the\n\"oblivious\" method of setting a static threshold due to Chawla et al. (2020)\nis, surprisingly, best-possible among all static threshold algorithms, for any\nnumber $k$ of starting units. We emphasize that this result is derived without\nneeding to explicitly find any counterexample instances. We establish similar\n\"no separation\" results for static thresholds in the IID setting, which\nalthough previously known, required the construction of complicated\ncounterexamples. Finally, our framework and in particular our Type Coverage\nproblem yields a simplified derivation of the tight 0.745 ratio when $k=1$ in\nthe IID setting.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We introduce SparcAssist, a general-purpose risk assessment tool for the\nmachine learning models trained for language tasks. It evaluates models' risk\nby inspecting their behavior on counterfactuals, namely out-of-distribution\ninstances generated based on the given data instance. The counterfactuals are\ngenerated by replacing tokens in rational subsequences identified by ExPred,\nwhile the replacements are retrieved using HotFlip or\nMasked-Language-Model-based algorithms. The main purpose of our system is to\nhelp the human annotators to assess the model's risk on deployment. The\ncounterfactual instances generated during the assessment are the by-product and\ncan be used to train more robust NLP models in the future.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the stochastic gravitational wave background (SGWB) produced by\nfreely decaying vortical turbulence in the early Universe. We thoroughly\ninvestigate the time correlation of the velocity field, and hence of the\nanisotropic stresses producing the gravitational waves. With hydrodynamical\nsimulations, we show that the unequal time correlation function (UETC) of the\nFourier components of the velocity field is Gaussian in the time difference, as\npredicted by the \"sweeping\" decorrelation model. We introduce a decorrelation\nmodel that can be extended to wavelengths around the integral scale of the\nflow. Supplemented with the evolution laws of the kinetic energy and of the\nintegral scale, this provides a new model UETC of the turbulent velocity field\nconsistent with the simulations. We discuss the UETC as a positive definite\nkernel, and propose to use the Gibbs kernel for the velocity UETC as a natural\nway to ensure positive definiteness of the SGWB. The SGWB is given by a\n4-dimensional integration of the resulting anisotropic stress UETC with the\ngravitational wave Green's function. We perform this integration using a Monte\nCarlo algorithm based on importance sampling, and find that the result matches\nthat of the simulations. Furthermore, the SGWB obtained from the numerical\nintegration and from the simulations show close agreement with a model in which\nthe source is constant in time and abruptly turns off after a few eddy turnover\ntimes. Based on this assumption, we provide an approximate analytical form for\nthe SGWB spectrum and its scaling with the initial kinetic energy and integral\nscale. Finally, we use our model and numerical integration algorithm to show\nthat including an initial growth phase for the turbulent flow heavily\ninfluences the spectral shape of the SGWB. This highlights the importance of a\ncomplete understanding of the turbulence generation mechanism.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this article, we focus on the left (translation) actions on noncommutative\ncompact connected Lie groups ${\\rm SU}(2) \\times T^n$. We define the rotation\nvectors of the left actions induced by the elements in the maximal tori of\n${\\rm SU}(2) \\times T^n$, and utilize rotation vectors to give the complete\ntopologically conjugate classifications of left actions. Algebraic conjugacy\nand smooth conjugacy are also considered.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Despite intense experimental efforts, the nature of the unconventional\nsuperconducting order parameter of UTe$_2$ remains elusive. This puzzle stems\nfrom different reported numbers of superconducting transitions at ambient\npressure, as well as a complex pressure-temperature phase diagram. To bring new\ninsights into the superconducting properties of UTe$_2$, we measured the heat\ncapacity and electrical resistivity of single crystals under compressive\nuniaxial stress $\\sigma$ applied along different crystallographic directions.\nWe find that the critical temperature $T_{\\rm c}$ of the single observed bulk\nsuperconducting transition decreases with $\\sigma$ along $[100]$ and $[110]$\nbut increases with $\\sigma$ along $[001]$. Aside from its effect on $T_{\\rm\nc}$, we notice that $c$-axis stress leads to a significant piezoresistivity,\nwhich we associate with the shift of the zero-pressure resistivity peak at\n$T^\\star \\approx 15\\, \\rm K$ to lower temperatures under stress. Finally, we\nshow that an in-plane shear stress $\\sigma_{xy}$ does not induce any observable\nsplitting of the superconducting transition over a stress range of\n$\\sigma_{xy}\\approx 0.17 \\, \\rm GPa$. This result suggests that the\nsuperconducting order parameter of UTe$_2$ may be single-component at ambient\npressure.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the action generalization ability of deep Q-learning in discrete\naction spaces. Generalization is crucial for efficient reinforcement learning\n(RL) because it allows agents to use knowledge learned from past experiences on\nnew tasks. But while function approximation provides deep RL agents with a\nnatural way to generalize over state inputs, the same generalization mechanism\ndoes not apply to discrete action outputs. And yet, surprisingly, our\nexperiments indicate that Deep Q-Networks (DQN), which use exactly this type of\nfunction approximator, are still able to achieve modest action generalization.\nOur main contribution is twofold: first, we propose a method of evaluating\naction generalization using expert knowledge of action similarity, and\nempirically confirm that action generalization leads to faster learning;\nsecond, we characterize the action-generalization gap (the difference in\nlearning performance between DQN and the expert) in different domains. We find\nthat DQN can indeed generalize over actions in several simple domains, but that\nits ability to do so decreases as the action space grows larger.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the Yamabe equation in the Euclidean half-space. We prove that any\nsign-changing solution has at least twice the energy of a standard bubble.\nMoreover, a sharper energy lower bound of the sign-changing solution set is\nalso established via the method of moving planes. This bound increases the\nenergy range for which Palais-Smale sequences of related variational problem\nhas a non-trivial weak limit.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Examined in this paper, is the Gray and Wyner achievable lossy rate region\nfor a tuple of correlated multivariate Gaussian random variables (RVs) $X_1 :\n\\Omega \\rightarrow {\\mathbb R}^{p_1}$ and $X_2 : \\Omega \\rightarrow {\\mathbb\nR}^{p_2}$ with respect to square-error distortions at the two decoders. It is\nshown that among all joint distributions induced by a triple of RVs $(X_1,X_2,\nW)$, such that $W : \\Omega \\rightarrow {\\mathbb W} $ is the auxiliary RV taking\ncontinuous, countable, or finite values, the Gray and Wyner achievable rate\nregion is characterized by jointly Gaussian RVs $(X_1,X_2, W)$ such that $W $\nis an $n$-dimensional Gaussian RV. It then follows that the achievable rate\nregion is parametrized by the three conditional covariances $Q_{X_1,X_2|W},\nQ_{X_1|W}, Q_{X_2|W}$ of the jointly Gaussian RVs. Furthermore, if the RV $W$\nmakes $X_1$ and $X_2$ conditionally independent, then the corresponding subset\nof the achievable rate region, is simpler, and parametrized by only the two\nconditional covariances $Q_{X_1|W}, Q_{X_2|W}$. The paper also includes the\ncharacterization of the Pangloss plane of the Gray-Wyner rate region along with\nthe characterizations of the corresponding rate distortion functions, their\ntest-channel distributions, and structural properties of the realizations which\ninduce these distributions.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Faced with massive data, subsampling is a commonly used technique to improve\ncomputational efficiency, and using nonuniform subsampling probabilities is an\neffective approach to improve estimation efficiency. For computational\nefficiency, subsampling is often implemented with replacement or through\nPoisson subsampling. However, no rigorous investigation has been performed to\nstudy the difference between the two subsampling procedures such as their\nestimation efficiency and computational convenience. This paper performs a\ncomparative study on these two different sampling procedures. In the context of\nmaximizing a general target function, we first derive asymptotic distributions\nfor estimators obtained from the two sampling procedures. The results show that\nthe Poisson subsampling may have a higher estimation efficiency. Based on the\nasymptotic distributions for both subsampling with replacement and Poisson\nsubsampling, we derive optimal subsampling probabilities that minimize the\nvariance functions of the subsampling estimators. These subsampling\nprobabilities further reveal the similarities and differences between\nsubsampling with replacement and Poisson subsampling. The theoretical\ncharacterizations and comparisons on the two subsampling procedures provide\nguidance to select a more appropriate subsampling approach in practice.\nFurthermore, practically implementable algorithms are proposed based on the\noptimal structural results, which are evaluated through both theoretical and\nempirical analyses.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  To better understand catastrophic forgetting, we study fitting an\noverparameterized linear model to a sequence of tasks with different input\ndistributions. We analyze how much the model forgets the true labels of earlier\ntasks after training on subsequent tasks, obtaining exact expressions and\nbounds. We establish connections between continual learning in the linear\nsetting and two other research areas: alternating projections and the Kaczmarz\nmethod. In specific settings, we highlight differences between forgetting and\nconvergence to the offline solution as studied in those areas. In particular,\nwhen T tasks in d dimensions are presented cyclically for k iterations, we\nprove an upper bound of T^2 * min{1/sqrt(k), d/k} on the forgetting. This\nstands in contrast to the convergence to the offline solution, which can be\narbitrarily slow according to existing alternating projection results. We\nfurther show that the T^2 factor can be lifted when tasks are presented in a\nrandom ordering.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  With the rapid development of Internet technology and the comprehensive\npopularity of Internet applications, online activities have gradually become an\nindispensable part of people's daily life. The original recommendation learning\nalgorithm is mainly based on user-microvideo interaction for learning, modeling\nthe user-micro-video connection relationship, which is difficult to capture the\nmore complex relationships between nodes. To address the above problems, we\npropose a personalized recommendation model based on graph neural network,\nwhich utilizes the feature that graph neural network can tap deep information\nof graph data more effectively, and transforms the input user rating\ninformation and item side information into graph structure, for effective\nfeature extraction, based on the importance sampling strategy. The\nimportance-based sampling strategy measures the importance of neighbor nodes to\nthe central node by calculating the relationship tightness between the neighbor\nnodes and the central node, and selects the neighbor nodes for recommendation\ntasks based on the importance level, which can be more targeted to select the\nsampling neighbors with more influence on the target micro-video nodes. The\npooling aggregation strategy, on the other hand, trains the aggregation weights\nby inputting the neighborhood node features into the fully connected layer\nbefore aggregating the neighborhood features, and then introduces the pooling\nlayer for feature aggregation, and finally aggregates the obtained neighborhood\naggregation features with the target node itself, which directly introduces a\nsymmetric trainable function to fuse the neighborhood weight training into the\nmodel to better capture the different neighborhood nodes' differential features\nin a learnable manner to allow for a more accurate representation of the\ncurrent node features.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this work, we explore whether modeling recurrence into the Transformer\narchitecture can both be beneficial and efficient, by building an extremely\nsimple recurrent module into the Transformer. We compare our model to baselines\nfollowing the training and evaluation recipe of BERT. Our results confirm that\nrecurrence can indeed improve Transformer models by a consistent margin,\nwithout requiring low-level performance optimizations, and while keeping the\nnumber of parameters constant. For example, our base model achieves an absolute\nimprovement of 2.1 points averaged across 10 tasks and also demonstrates\nincreased stability in fine-tuning over a range of learning rates.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We reanalyze the problem of a 1D Dirac single-particle colliding with the\nelectrostatic potential step of height $V_{0}$ with an incoming energy that\ntends to the limit point of the so-called Klein energy zone, i.e.,\n$E\\rightarrow V_{0}-\\mathrm{m}c^{2}$, for a given $V_{0}$. In this situation,\nthe particle is actually colliding with an impenetrable barrier. In fact,\n$V_{0}\\rightarrow E+\\mathrm{m}c^{2}$, for a given relativistic energy\n$E\\,(<V_{0})$, is the maximum value that the height of the step can reach and\nthat ensures the perfect impenetrability of the barrier. Nevertheless, we\nnotice that, unlike the nonrelativistic case, the entire eigensolution does not\ncompletely vanish, either at the barrier or in the region under the step, but\nits upper component does satisfy the Dirichlet boundary condition at the\nbarrier. More importantly, by calculating the mean value of the force exerted\nby the impenetrable wall on the particle in this eigenstate and taking its\nnonrelativistic limit, we recover the required result. We use two different\napproaches to obtain the latter results.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study transcendental meromorphic functions with essential singularities on\nRiemann surfaces. Every function $\\Psi_X$ has associated a complex vector field\n$X$. In the converse direction, vector fields $X$ provide single valued or\nmultivalued functions $\\Psi_X$. Our goal is to understand the relationship\nbetween the analytical properties of $\\Psi_X$, the singularities of its inverse\n$\\Psi_X^{-1}$ and the geometric behavior of $X$. As first result, by examining\nthe containment properties of the neighborhoods of the singularities of\n$\\Psi_X^{-1}$, we characterize when a singularity of $\\Psi_X^{-1}$ over a\nsingular value $a$, is either algebraic, logarithmic or non logarithmic.\nSecondly, to make precise the cooperative aspects between analysis and\ngeometry, we present the systematic study of two holomorphic families of\ntranscendental functions with an essential singularity at infinity, as well as\nsome sporadic examples. As third stage, we study the incompleteness of the\ntrajectories of the associated vector field $X$ with essential singularities on\na Riemann surface $M_{\\mathfrak g}$ of genus ${\\mathfrak g}$. As an\napplication, we provide conditions under which there exists an infinite number\nof (real) incomplete trajectories of $X$ localized at the essential\nsingularities. Furthermore, removing the incomplete trajectories decomposes the\nRiemann surface into real flow invariant canonical pieces.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We employ two models of the nucleon-nucleon force: the OPE-Gaussian as well\nas the chiral N4LO and N4LO+ interactions with semilocal regularization in\nmomentum space to study correlations among two-nucleon and three-nucleon\nelastic scattering observables. These models contain a number of free\nparameters whose values and covariance matrices are evaluated from a fit to the\ntwo-nucleon data. Such detailed knowledge of parameters allows us to create,\nusing various sets of statistically generated parameters, numerous versions of\nthese potentials and next apply them to two- and three-nucleon scattering to\nmake predictions of various observables at the reaction energies up to 200 MeV.\nThis permits a systematic analysis of correlations among two-nucleon and\nthree-nucleon observables, basing on a relatively big sample of predictions. We\nfound that most observables in neutron-proton and neutron-deuteron systems are\nuncorrelated, but there are exceptions revealing strong correlations, which\ndepend on the reaction energy and scattering angle. This information may be\nuseful for precise fixing free parameters of two-nucleon and three-nucleon\nforces and for understanding dependencies and correlations between potential\nparameters and observables.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Quantum parameter estimation promises a high-precision measurement in theory,\nhowever, how to design the optimal scheme in a specific scenario, especially\nunder a practical condition, is still a serious problem that needs to be solved\ncase by case due to the existence of multiple mathematical bounds and\noptimization methods. Depending on the scenario considered, different bounds\nmay be more or less suitable, both in terms of computational complexity and the\ntightness of the bound itself. At the same time, the metrological schemes\nprovided by different optimization methods need to be tested against\nrealization complexity, robustness, etc. Hence, a comprehensive toolkit\ncontaining various bounds and optimization methods is essential for the scheme\ndesign in quantum metrology. To fill this vacancy, here we present a\nPython-Julia-based open-source toolkit for quantum parameter estimation, which\nincludes many well-used mathematical bounds and optimization methods. Utilizing\nthis toolkit, all procedures in the scheme design, such as the optimizations of\nthe probe state, control and measurement, can be readily and efficiently\nperformed.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recent studies in biometric-based identification tasks have shown that deep\nlearning methods can achieve better performance. These methods generally\nextract the global features as descriptor to represent the original image.\nNonetheless, it does not perform well for biometric identification under\nfine-grained tasks. The main reason is that the single image descriptor\ncontains insufficient information to represent image. In this paper, we present\na dual global descriptor model, which combines multiple global descriptors to\nexploit multi level image features. Moreover, we utilize a contrastive loss to\nenlarge the distance between image representations of confusing classes. The\nproposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet\nBiometric Challenge. The source code and trained models are publicly available\nat: https://github.com/flyingsheepbin/pet-biometrics\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Conventional kernel-based machine learning models for ab initio potential\nenergy surfaces, while accurate and convenient in small data regimes, suffer\nimmense computational cost as training set sizes increase. We introduce\nQML-Lightning, a PyTorch package containing GPU-accelerated approximate kernel\nmodels, which reduces the training time by several orders of magnitude,\nyielding trained models within seconds. QML-Lightning includes a cost-efficient\nGPU implementation of FCHL19, which together can yield energy and force\npredictions with competitive accuracy on a microsecond-per-atom timescale.\nUsing modern GPU hardware, we report learning curves of energies and forces as\nwell as timings as numerical evidence for select legacy benchmarks from\natomisitic simulation including QM9, MD-17, and 3BPA.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The recently discovered kagome superconductors $A$V$_3$Sb$_5$ ($A$ = K, Rb,\nCs) possess a unique band structure with van Hove singularities and Dirac\ndispersions, in which unusual charge-density-wave (CDW) orders with\ntime-reversal and rotational symmetry breaking have been reported. One of the\nmost crucial unresolved issues is identifying the symmetry of the\nsuperconductivity that develops inside the CDW phase. Theory predicts a variety\nof unconventional superconducting symmetries, including exotic states with\nchiral and topological properties accompanied by a sign-changing\nsuperconducting gap. Experimentally, however, the phase information on the\nsuperconducting gap in $A$V$_3$Sb$_5$ is still lacking. Here we report the\nelectron irradiation effects in CsV$_3$Sb$_5$ using introduced impurities as a\nphase-sensitive probe of superconductivity. Our magnetic penetration depth\nmeasurements reveal that with increasing impurities, a highly anisotropic\nfully-gapped state changes gradually to an isotropic full-gap state without\npassing through a nodal state. Furthermore, transport measurements under high\npressure show that the double superconducting dome in the pressure-temperature\nphase diagram survives against sufficient impurities. These results are strong\nbulk evidence that CsV$_3$Sb$_5$ is a non-chiral, anisotropic $s$-wave\nsuperconductor with no sign change both at ambient and high pressure, which\nprovides a clue to understanding the relationship between CDW and\nsuperconductivity in kagome superconductors.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The Internet of Things (IoT) is an idea that intends to interface arranged\ndata frameworks to actual items. The Internet of Things (IoT) has applications\nin pretty much every part of life in this day and age, and stock administration\nis no special case. IoT gives an answer for this issue by making it simpler to\ninterface every one of the various organizations in a strategic framework\nutilizing Wireless Sensor Networks. An Interactive Shopping Model and an\nAutomated Inventory Intelligent Management System that uses the Internet of\nThings to give constant item following, the board, and observing. A study and\nexamination of the commonness of IoT among assembling SMEs is introduced, just\nas the current impediments and possibilities for permitting prescient\ninvestigation. The four examination capacities are depicted alongside an\noutline of the IoT empowering agents. Future patterns and difficulties in\narising innovative work subjects are featured, for example, making IoT advances\navailable to SMEs. The motivation behind this paper is to look at how the\nInternet of Things is changing our lives and work spaces, just as to feature\nprobably the best strategic approaches, insights, and patterns. Considering the\ndeveloping significance of big business IoT and the exploration hole in this\nfield, an IoT design and the IoT administration industry will be examined. A\nmodel is needed to choose and send IoT administrations in different\nauthoritative settings.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We present a chemodynamical study of the Grus I ultra-faint dwarf galaxy\n(UFD) from medium-resolution ($R\\sim11,000$) Magellan/IMACS spectra of its\nindividual member stars. We identify eight confirmed members of Grus I, based\non their low metallicities and coherent radial velocities, and four candidate\nmembers for which only velocities are derived. In contrast to previous work, we\nfind that Grus I has a very low mean metallicity of $\\langle$[Fe/H]$\\rangle =\n-2.62 \\pm 0.11$ dex, making it one of the most metal-poor UFDs. Grus I has a\nsystemic radial velocity of $-143.5\\pm1.2$ km s$^{-1}$ and a velocity\ndispersion of $\\sigma_{\\text{rv}} = 2.5^{+1.3}_{-0.8}$ km s$^{-1}$ which\nresults in a dynamical mass of $M_{1/2} (r_h) = 8^{+12}_{-4} \\times 10^5$\nM$_{\\odot}$ and a mass-to-light ratio of M/L$_V$ = $440^{+650}_{-250}$\nM$_\\odot$/L$_\\odot$. Under the assumption of dynamical equilibrium, our\nanalysis confirms that Grus I is a dark-matter-dominated UFD (M/L $> 80$\nM$_\\odot$/L$_\\odot$). However, we do not resolve a metallicity dispersion\n($\\sigma_{\\text{[Fe/H]}} < 0.44$ dex). Our results indicate that Grus I is a\nfairly typical UFD with parameters that agree with mass-metallicity and\nmetallicity-luminosity trends for faint galaxies. This agreement suggests that\nGrus I has not lost an especially significant amount of mass from tidal\nencounters with the Milky Way, in line with its orbital parameters.\nIntriguingly, Grus I has among the lowest central density ($\\rho_{1/2} \\sim\n3.5_{-2.1}^{+5.7} \\times 10^7$ M$_\\odot$ kpc$^{-3}$) of the UFDs that are not\nknown to be tidally disrupting. Models of the formation and evolution of UFDs\nwill need to explain the diversity of these central densities, in addition to\nany diversity in the outer regions of these relic galaxies.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Twisted bilayer graphene gives rise to large moir\\'{e} patterns that form a\ntriangular network upon mechanical relaxation. If gating is included, each\ntriangular region has gapped electronic Dirac points that behave as bulk\ntopological insulators with topological indices depending on valley index and\nthe type of stacking. Since each triangle has two oppositely charged valleys,\nthey remain topologically trivial.\n  In this work, we address several questions related to the edge currents of\nthis system by analysis and computation of continuum PDE models. Firstly, we\nderive the bulk invariants corresponding to a single valley, and then apply a\nbulk-interface correspondence to quantify asymmetric transport along the\ninterface. Secondly, we introduce a valley-coupled continuum model to show how\nvalleys are approximately decoupled in the presence of small perturbations\nusing a multiscale expansion, and how valleys couple for larger defects.\nThirdly, we present a method to prove for a large class of continuum\n(pseudo-)differential models that a quantized asymmetric current is preserved\nthrough a junction such as a triangular network vertex. We support all of these\narguments with numerical simulations using spectral methods to compute relevant\ncurrents and wavepacket propagation.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We consider the statistics of the extreme eigenvalues of sparse random\nmatrices, a class of random matrices that includes the normalized adjacency\nmatrices of the Erd{\\H o}s-R{\\'e}nyi graph $G(N,p)$. Recently, it was shown by\nLee, up to an explicit random shift, the optimal rigidity of extreme\neigenvalues holds, provided the averaged degree grows with the size of the\ngraph, $pN>N^\\varepsilon$. We prove in the same regime, (i) Optimal rigidity\nholds for all eigenvalues with respect to an explicit random measure. (ii) Up\nto an explicit random shift, the fluctuations of the extreme eigenvalues are\ngiven the Tracy-Widom distribution.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this contribution we use an ensemble deep-learning method for combining\nthe prediction of two individual one-stage detectors (i.e., YOLOv4 and Yolact)\nwith the aim to detect artefacts in endoscopic images. This ensemble strategy\nenabled us to improve the robustness of the individual models without harming\ntheir real-time computation capabilities. We demonstrated the effectiveness of\nour approach by training and testing the two individual models and various\nensemble configurations on the \"Endoscopic Artifact Detection Challenge\"\ndataset. Extensive experiments show the superiority, in terms of mean average\nprecision, of the ensemble approach over the individual models and previous\nworks in the state of the art.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  For $m \\geq 3$ and $n \\geq 1$, the $m$-cycle book graph $B(m,n)$ consists of\n$n$ copies of the cycle $C_m$ with one common edge. In this paper, we prove\nthat (a) the number of switching non-isomorphic signed $B(m,n)$ is $n+1$, and\n(b) the chromatic number of a signed $B(m,n)$ is either 2 or 3. We also obtain\nexplicit formulas for the chromatic polynomials and the zero-free chromatic\npolynomials of switching non-isomorphic signed book graphs.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this article the quantized matrix algebras as in the title have been\nstudied at a root of unity. A full classification of simple modules over such\nquantized matrix algebras of rank $2$ along with a class of finite dimensional\nindecomposable modules are given.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study non-abelian systems of Painlev\\'e type. To derive them, we introduce\nan auxiliary autonomous system with the frozen independent variable and\npostulate its integrability in the sense of the existence of a non-abelian\nfirst integral that generalizes the Okamoto Hamiltonian. All non-abelian\nP6-P2-systems with such integrals are found. A coalescence limiting scheme is\nconstructed for these non-abelian Painlev\\'e systems. This allows us to\nconstruct an isomonodromic Lax pair for each of them.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Incorporation of Bi into GaAs-(Ga,Al)As-Ga(As,Bi) core-shell nanowires grown\nby molecular beam epitaxy is studied with transmission electron microscopy.\nNanowires are grown on GaAs(111)B substrates with Au-droplet assisted mode.\nBi-doped shells are grown at low temperature (300 {\\deg}C) with a close to\nstoichiometric Ga/As flux ratio. At low Bi fluxes, the Ga(As,Bi) shells are\nsmooth, with Bi completely incorporated into the shells. Higher Bi fluxes\n(Bi/As flux ratio ~ 4%) led to partial segregation of Bi as droplets on the\nnanowires sidewalls, preferentially located at the nanowire segments with\nwurtzite structure. We demonstrate that such Bi droplets on the sidewalls act\nas catalysts for the growth of branches perpendicular to the GaAs trunks. Due\nto the tunability between zinc-blende and wurtzite polytypes by changing the\nnanowire growth conditions, this effect enables fabrication of branched\nnanowire architectures with branches generated from selected (wurtzite)\nnanowire segments.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  An optical field will undergo coherent diffusion when it is mapped into\nthermal-motioned atoms, e.g., in a slow or storage light process. As was\ndemonstrated before, such diffusion effect is equivalent to a spatial low-pass\nfilter attenuating the high spatial frequency (SF) components of the optical\nfield. Here, employing electromagnetically induced transparency (EIT) based\nlight storage in hot atomic vapor, we demonstrate that the angular deviation\nbetween the control and probe beams could be utilized as a degree of freedom to\nmodulate the SF of the probe beam. The principle is to change the\ndiffusion-induced low-pass filter into a band-pass filter, whose SF response\ncan be tuned by varying the direction and magnitude of the angular deviation.\nTransverse multimode light fields, such as optical images and Laguerre-Gaussian\nmodes are utilized to study such SF modulation. Our findings could be broadly\napplied to the fields of quantum information processing, all-optical image\nmanipulation and imaging through diffusive media.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The task of ranking individuals or teams, based on a set of comparisons\nbetween pairs, arises in various contexts, including sporting competitions and\nthe analysis of dominance hierarchies among animals and humans. Given data on\nwhich competitors beat which others, the challenge is to rank the competitors\nfrom best to worst. Here we study the problem of computing rankings when there\nare multiple, potentially conflicting modes of comparison, such as multiple\ntypes of dominance behaviors among animals. We assume that we do not know a\npriori what information each behavior conveys about the ranking, or even\nwhether they convey any information at all. Nonetheless we show that it is\npossible to compute a ranking in this situation and present a fast method for\ndoing so, based on a combination of an expectation-maximization algorithm and a\nmodified Bradley-Terry model. We give a selection of example applications to\nboth animal and human competition.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Dual-encoder structure successfully utilizes two language-specific encoders\n(LSEs) for code-switching speech recognition. Because LSEs are initialized by\ntwo pre-trained language-specific models (LSMs), the dual-encoder structure can\nexploit sufficient monolingual data and capture the individual language\nattributes. However, most existing methods have no language constraints on LSEs\nand underutilize language-specific knowledge of LSMs. In this paper, we propose\na language-specific characteristic assistance (LSCA) method to mitigate the\nabove problems. Specifically, during training, we introduce two\nlanguage-specific losses as language constraints and generate corresponding\nlanguage-specific targets for them. During decoding, we take the decoding\nabilities of LSMs into account by combining the output probabilities of two\nLSMs and the mixture model to obtain the final predictions. Experiments show\nthat either the training or decoding method of LSCA can improve the model's\nperformance. Furthermore, the best result can obtain up to 15.4% relative error\nreduction on the code-switching test set by combining the training and decoding\nmethods of LSCA. Moreover, the system can process code-switching speech\nrecognition tasks well without extra shared parameters or even retraining based\non two pre-trained LSMs by using our method.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Multi-wavelength observations indicate that some starburst galaxies show a\ndominant non-thermal contribution from their central region. These active\ngalactic nuclei (AGN)-starburst composites are of special interest, as both\nphenomena on their own are potential sources of highly-energetic cosmic rays\nand associated gamma-ray and neutrino emission. In this work, a homogeneous,\nsteady-state two-zone multi-messenger model of the non-thermal emission from\nthe AGN corona as well as the circumnuclear starburst region is developed and\nsubsequently applied to the case of NGC 1068, which has recently shown some\nfirst indications of high-energy neutrino emission. Here, we show that the\nentire spectrum of multi-messenger data - from radio to gamma-rays including\nthe neutrino constraint - can be described very well if both, starburst and AGN\ncorona, are taken into account. Using only a single emission region is not\nsufficient.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Multi-channel Multi-tone Active Noise Equalizers can achieve different\nuser-selected noise spectrum profiles even at different space positions. They\ncan apply a different equalization factor at each noise frequency component and\neach control point. Theoretically, the value of the transfer function at the\nfrequencies where the noise signal has energy is determined by the equalizer\nconfiguration. In this work, we show how to calculate these transfer functions\nwith a double aim: to verify that at the frequencies of interest the values\nimposed by the equalizer settings are obtained, and to characterize the\nbehavior of these transfer functions in the rest of the spectrum, as well as to\nget clues to predict the convergence behaviour of the algorithm. The\ninformation provided thanks to these transfer functions serves as a practical\nalternative to the cumbersome statistical analysis of convergence, whose\nresults are often of no practical use.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Most recent network failure diagnosis systems focused on data center networks\nwhere complex measurement systems can be deployed to derive routing information\nand ensure network coverage in order to achieve accurate and fast fault\nlocalization. In this paper, we target wide-area networks that support\ndata-intensive distributed applications. We first present a new multi-output\nprediction model that directly maps the application level observations to\nlocalize the system component failures. In reality, this application-centric\napproach may face the missing data challenge as some input (feature) data to\nthe inference models may be missing due to incomplete or lost measurements in\nwide area networks. We show that the presented prediction model naturally\nallows the {\\it multivariate} imputation to recover the missing data. We\nevaluate multiple imputation algorithms and show that the prediction\nperformance can be improved significantly in a large-scale network. As far as\nwe know, this is the first study on the missing data issue and applying\nimputation techniques in network failure localization.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We improve the current upper bound on the axion-photon coupling derived from\nstellar evolution using the $R_2$ parameter, the ratio of stellar populations\non the Asymptotic Giant Branch to Horizontal Branch in Globular Clusters. We\ncompare this with data from simulations using the stellar evolution code MESA\nwhich include the effects of axion production. Particular attention is given to\nquantifying in detail the effects of uncertainties on the $R$ and $R_2$\nparameters due to the modelling of convective core boundaries. Using a\nsemiconvective mixing scheme we constrain the axion-photon coupling to be\n$g_{a\\gamma\\gamma} < 0.47 \\times 10^{-10}~\\mathrm{GeV}^{-1}$. This rules out\nnew regions of QCD axion and axion-like particle parameter space. Complementary\nevidence from asteroseismology suggests that this could improve to as much as\n$g_{a\\gamma\\gamma} < 0.34 \\times 10^{-10}~\\mathrm{GeV}^{-1}$ as the\nuncertainties surrounding mixing across convective boundaries are better\nunderstood.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the effective SNR behavior of various enumerative amplitude shaping\nalgorithms. We show that their relative behavior can be explained via the\ntemporal autocorrelation function or via the energy dispersion index.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the stability of fluctuations around a homogeneous non-Abelian\nelectric field background that is of a form that is protected from Schwinger\npair production. Our analysis identifies the unstable modes and we find a\nlimiting set of parameters for which there are no instabilities. We discuss\npotential implications of our analysis for confining strings in non-Abelian\ngauge theories.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Medical data involves a large amount of personal information and is highly\nprivacy sensitive. In the age of big data, the increasing informatization of\nhealthcare makes it vital that medical information is stored securely and\naccurately. However, current medical information is subject to the risk of\nprivacy leakage and difficult to share. To address these issues, this paper\nproposes a healthcare information security storage solution based on\nHyperledger Fabric and the Attribute-Based Access Control (ABAC) framework. The\nscheme first utilizes attribute-based access control, which allows dynamic and\nfine-grained access to medical information, and then stores the medical\ninformation in the blockchain, which can be secured and tamper-proof by\nformulating corresponding smart contracts. In addition, this solution also\nincorporates IPFS technology to relieve the storage pressure of the blockchain.\nExperiments show that the proposed scheme combining access control of\nattributes and blockchain technology in this paper can not only ensure the\nsecure storage and integrity of medical information but also has a high\nthroughput when accessing medical information.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The first deep field images from the James Webb Space Telescope (JWST) of the\ngalaxy cluster SMACS~J0723.3-7327 reveal a wealth of new lensed images at\nuncharted infrared wavelengths, with unprecedented depth and resolution. Here\nwe securely identify 14 new sets of multiply imaged galaxies totalling 42\nimages, adding to the five sets of bright and multiply-imaged galaxies already\nknown from Hubble Space Telescope data. We find examples of arcs crossing\ncritical curves, allowing detailed community follow-up, such as JWST\nspectroscopy for precise redshift determinations, and measurements of the\nchemical abundances and of the detailed internal gas dynamics of very distant,\nyoung galaxies. One such arc contains compact knots of magnification\n$\\mu\\sim$750, and features a microlensed transient. We also detect an Einstein\ncross candidate only visible thanks to JWST's superb resolution. Our parametric\nlens model is available at\nhttps://www.dropbox.com/sh/gwup2lvks0jsqe5/AAC2RRSKce0aX-lIFCc9vhBXa?dl=0 , and\nwill be regularly updated using additional spectroscopic redshifts. The model\nreproduces the multiple images to better than an rms of $0.5^{\\prime \\prime}$,\nand allows for accurate magnification estimates of high-redshift galaxies. The\nintracluster light extends beyond the cluster members, exhibiting large-scale\nfeatures that suggest a significant past dynamical disturbance. This work\nrepresents a first taste of the enhanced power JWST will have for\nlensing-related science.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In spite of remarkable developments in the field of advanced materials,\nsilicon remains one of the foremost semiconductors of the day. Of enduring\nrelevance to science and technology is silicon's nanomechanical behaviour\nincluding phase transformation, amorphization and dislocations generation,\nparticularly in the context of molecular dynamics and materials research. So\nfar, comprehensive modelling of the whole cycle of events in silicon during\nnanoscale deformation has not been possible, however, due to the limitations\ninherent in the existing interatomic potentials. This paper examines how well\nan unconventional combination of two well-known potentials - the Tersoff and\nStillinger-Weber - can perform in simulating that complexity. Our model\nindicates that an irreversible deformation of silicon (Si-I) is set in motion\nby a transformation to a non-diamond structure (Si-nd), and followed by a\nsubsequent transition to the Si-II and Si-XII' phases\n(Si-I->Si-nd->Si-II->Si-XII'). This leads to the generation of dislocations\nspreading outwards from the incubation zone. In effect, our simulations\nparallel each and every one of the structural changes detected experimentally\nin the deformed material. This includes both the sequence of phase transitions\nand dislocation activity, which - taken together - neither the Tersoff nor\nStillinger-Weber, or indeed any other available Si interatomic potential, is\nable to achieve in its own right. We have sought to additionally validate our\nmethod of merging atomic potentials by applying it to germanium, and found it\ncan equally well predict germanium's transformation from a liquid to amorphous\nstate.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the identity testing problem for high-dimensional distributions.\nGiven as input an explicit distribution $\\mu$, an $\\varepsilon>0$, and access\nto sampling oracle(s) for a hidden distribution $\\pi$, the goal in identity\ntesting is to distinguish whether the two distributions $\\mu$ and $\\pi$ are\nidentical or are at least $\\varepsilon$-far apart. When there is only access to\nfull samples from the hidden distribution $\\pi$, it is known that exponentially\nmany samples (in the dimension) may be needed for identity testing, and hence\nprevious works have studied identity testing with additional access to various\n\"conditional\" sampling oracles. We consider a significantly weaker conditional\nsampling oracle, which we call the $\\mathsf{Coordinate\\ Oracle}$, and provide a\ncomputational and statistical characterization of the identity testing problem\nin this new model.\n  We prove that if an analytic property known as approximate tensorization of\nentropy holds for an $n$-dimensional visible distribution $\\mu$, then there is\nan efficient identity testing algorithm for any hidden distribution $\\pi$ using\n$\\tilde{O}(n/\\varepsilon)$ queries to the $\\mathsf{Coordinate\\ Oracle}$.\nApproximate tensorization of entropy is a pertinent condition as recent works\nhave established it for a large class of high-dimensional distributions. We\nalso prove a computational phase transition: for a well-studied class of\n$n$-dimensional distributions, specifically sparse antiferromagnetic Ising\nmodels over $\\{+1,-1\\}^n$, we show that in the regime where approximate\ntensorization of entropy fails, there is no efficient identity testing\nalgorithm unless $\\mathsf{RP}=\\mathsf{NP}$. We complement our results with a\nmatching $\\Omega(n/\\varepsilon)$ statistical lower bound for the sample\ncomplexity of identity testing in the $\\mathsf{Coordinate\\ Oracle}$ model.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Broad Absorption Line Quasars (BALQSOs) show strong signatures of powerful\noutflows, with the potential to alter the cosmic history of their host\ngalaxies. These signatures are only seen in ~10% of optically selected quasars,\nalthough the fraction significantly increases in IR and radio selected samples.\nA proven physical explanation for this observed fraction has yet to be found,\nalong with a determination of why this fraction increases at radio wavelengths.\nWe present the largest sample of radio matched BALQSOs using the LOFAR\nTwo-metre Sky Survey Data Release 2 and employ it to investigate radio\nproperties of BALQSOs. Within the DR2 footprint, there are 3537 BALQSOs from\nSloan Digital Sky Survey DR12 with continuum signal to noise >5. We find\nradio-detections for 1108 BALQSOs, with an important sub-population of 120\nLoBALs, an unprecedented sample size for radio matched BALQSOs given the LoTSS\nsky coverage to date. BALQSOs are a radio-quiet population that show an\nincrease of $\\times 1.50$ radio-detection fraction compared to non-BALQSOs.\nLoBALs show an increase of $\\times 2.22$ that of non-BALQSO quasars. We show\nthat this detection fraction correlates with wind-strength, reddening and\nC_{IV} emission properties of BALQSOs and that these features may be connected,\nalthough no single property can fully explain the enhanced radio detection\nfraction. We create composite spectra for sub-classes of BALQSOs based on wind\nstrength and colour, finding differences in the absorption profiles of\nradio-detected and radio-undetected sources, particularly for LoBALs. Overall,\nwe favour a wind-ISM interaction explanation for the increased radio-detection\nfraction of BALQSOs.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Optical coherence tomography angiography (OCTA) can non-invasively image the\neye's circulatory system. In order to reliably characterize the retinal\nvasculature, there is a need to automatically extract quantitative metrics from\nthese images. The calculation of such biomarkers requires a precise semantic\nsegmentation of the blood vessels. However, deep-learning-based methods for\nsegmentation mostly rely on supervised training with voxel-level annotations,\nwhich are costly to obtain. In this work, we present a pipeline to synthesize\nlarge amounts of realistic OCTA images with intrinsically matching ground truth\nlabels; thereby obviating the need for manual annotation of training data. Our\nproposed method is based on two novel components: 1) a physiology-based\nsimulation that models the various retinal vascular plexuses and 2) a suite of\nphysics-based image augmentations that emulate the OCTA image acquisition\nprocess including typical artifacts. In extensive benchmarking experiments, we\ndemonstrate the utility of our synthetic data by successfully training retinal\nvessel segmentation algorithms. Encouraged by our method's competitive\nquantitative and superior qualitative performance, we believe that it\nconstitutes a versatile tool to advance the quantitative analysis of OCTA\nimages.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  This article discusses the reasons for the choice of the sexagesimal system\nby ancient Sumerians. It is shown that Sumerians chose this specific numeral\nsystem based on logical and practical reasons which enabled them to deal with\nbig numbers easily and even perform the multiplications and divisions in this\nsystem.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Patchy cosmic reionization resulted in the ionizing UV background\nasynchronous rise across the Universe. The latter might have left imprints\nvisible in present day observations. Several numerical simulation-based studies\nshow correlations between reionization time and overdensities and object masses\ntoday. To remove the mass from the study, as it may not be the sole important\nparameter, this paper focuses solely on the properties of paired halos within\nthe same mass range as the Milky Way. For this purpose, it uses CoDaII, a\nfully-coupled radiation hydrodynamics reionization simulation of the local\nUniverse. This simulation holds a halo pair representing the Local Group, in\naddition to other pairs, sharing similar mass, mass ratio, distance separation\nand isolation criteria but in other environments, alongside isolated halos\nwithin the same mass range. Investigations of the paired halo reionization\nhistories reveal a wide diversity although always inside-out given our\nreionization model. Within this model, halos in a close pair tend to be\nreionized at the same time but being in a pair does not bring to an earlier\ntime their mean reionization. The only significant trend is found between the\ntotal energy at z = 0 of the pairs and their mean reionization time: pairs with\nthe smallest total energy (bound) are reionized up to 50 Myr earlier than\nothers (unbound). Above all, this study reveals the variety of reionization\nhistories undergone by halo pairs similar to the Local Group, that of the Local\nGroup being far from an average one. In our model, its reionization time is\n~625 Myr against 660+/-4 Myr (z~8.25 against 7.87+/-0.02) on average.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Let $S_n$ be the symmetric group of all permutations of $\\{1, \\cdots, n\\}$\nwith two generators: the transposition switching $1$ with $2$ and the cyclic\npermutation sending $k$ to $k+1$ for $1\\leq k\\leq n-1$ and $n$ to $1$ (denoted\nby $\\sigma$ and $\\tau$). In this article, we study quantum complexity of\npermutations in $S_n$ using $\\{\\sigma, \\tau, \\tau^{-1}\\}$ as logic gates. We\ngive an explicit construction of permutations in $S_n$ with quadratic quantum\ncomplexity lower bound $\\frac{n^2-2n-7}{4}$. We also prove that all\npermutations in $S_n$ have quadratic quantum complexity upper bound $3(n-1)^2$.\nFinally, we show that almost all permutations in $S_n$ have quadratic quantum\ncomplexity lower bound when $n\\rightarrow \\infty$.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Information engines can convert thermal fluctuations of a bath at temperature\n$T$ into work at rates of order $k_\\mathrm{B}T$ per relaxation time of the\nsystem. We show experimentally that such engines, when in contact with a bath\nthat is out of equilibrium, can extract much more work. We place a heavy,\nmicron-scale bead in a harmonic potential that ratchets up to capture favorable\nfluctuations. Adding a fluctuating electric field increases work extraction up\nto ten times, limited only by the strength of applied field. Our results\nconnect Maxwell's demon with energy harvesting and an estimate of efficiency\nshows that information engines in nonequilibrium baths can greatly outperform\nconventional engines.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Two-dimensional (2D) crystals' edge structures not only influence their\noverall properties but also dictate their formation due to edge-mediated\nsynthesis and etching processes. Edges must be carefully examined because they\noften display complex, unexpected features at the atomic scale, such as\nreconstruction, functionalization, and uncontrolled contamination. Here, we\nexamine atomic-scale edge structures and uncover reconstruction behavior in\nbilayer phosphorene. We use in situ transmission electron microscopy (TEM) of\nphosphorene/graphene specimens at elevated temperatures to minimize surface\ncontamination and reduce e-beam damage, allowing us to observe intrinsic edge\nconfigurations. Bilayer zigzag (ZZ) edge was found the most stable edge\nconfiguration under e-beam irradiation. Through first-principles calculations\nand TEM image analysis under various tilting and defocus conditions, we find\nthat bilayer ZZ edges undergo edge reconstruction and so acquire closed,\nself-passivated edge configurations. The extremely low formation energy of the\nclosed bilayer ZZ edge and its high stability against e-beam irradiation are\nconfirmed by first-principles calculations. Moreover, we fabricate bilayer\nphosphorene nanoribbons with atomically-sharp closed ZZ edges. The identified\nbilayer ZZ edges will aid in the fundamental understanding of the synthesis,\ndegradation, reconstruction, and applications of phosphorene and related\nstructures.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Interstellar chemistry is important for galaxy formation, as it determines\nthe rate at which gas can cool, and enables us to make predictions for\nobservable spectroscopic lines from ions and molecules. We explore two central\naspects of modelling the chemistry of the interstellar medium (ISM): (1) the\neffects of local stellar radiation, which ionises and heats the gas, and (2)\nthe depletion of metals onto dust grains, which reduces the abundance of metals\nin the gas phase. We run high-resolution (400 M$_\\odot$ per baryonic particle)\nsimulations of isolated disc galaxies, from dwarfs to Milky Way-mass, using the\nFIRE galaxy formation models together with the CHIMES non-equilibrium chemistry\nand cooling module. In our fiducial model, we couple the chemistry to the\nstellar fluxes calculated from star particles using an approximate radiative\ntransfer scheme, and we implement an empirical density-dependent prescription\nfor metal depletion. For comparison, we also run simulations with a spatially\nuniform radiation field, and without metal depletion. Our fiducial model\nbroadly reproduces observed trends in HI and H2 mass with stellar mass, and in\nline luminosity versus star formation rate for [CII] 158$\\mu$m, [OI] 63$\\mu$m,\n[OIII] 88$\\mu$m, [NII] 122$\\mu$m and H$\\alpha$ 6563A. Our simulations with a\nuniform radiation field predict fainter luminosities, by up to an order of\nmagnitude for [OIII] 88$\\mu$m and H$\\alpha$ 6563A, while ignoring metal\ndepletion increases the luminosity of carbon and oxygen lines by a factor\n$\\approx$2. However, the overall evolution of the galaxy is not strongly\naffected by local stellar fluxes or metal depletion, except in dwarf galaxies\nwhere the inclusion of local fluxes leads to weaker outflows and hence higher\ngas fractions.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Over the past few years, there has been a significant improvement in the\ndomain of few-shot learning. This learning paradigm has shown promising results\nfor the challenging problem of anomaly detection, where the general task is to\ndeal with heavy class imbalance. Our paper presents a new approach to few-shot\nclassification, where we employ the knowledge-base of multiple pre-trained\nconvolutional models that act as the backbone for our proposed few-shot\nframework. Our framework uses a novel ensembling technique for boosting the\naccuracy while drastically decreasing the total parameter count, thus paving\nthe way for real-time implementation. We perform an extensive hyperparameter\nsearch using a power-line defect detection dataset and obtain an accuracy of\n92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our model\non competing standards with the existing state-of-the-art methods and\noutperform them.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We consider the problem of designing efficient particle filters for twisted\nFeynman--Kac models. Particle filters using twisted models can deliver low\nerror approximations of statistical quantities and such twisting functions can\nbe learnt iteratively. Practical implementations of these algorithms are\ncomplicated by the need to (i) sample from the twisted transition dynamics, and\n(ii) calculate the twisted potential functions. We expand the class of\napplicable models using rejection sampling for (i) and unbiased approximations\nfor (ii) using a random weight particle filter. We characterise the average\nacceptance rates within the particle filter in order to control the\ncomputational cost, and analyse the asymptotic variance. Empirical results show\nthe mean squared error of the normalising constant estimate in our method is\nsmaller than a memory-equivalent particle filter but not a\ncomputation-equivalent filter. Both comparisons are improved when more\nefficient sampling is possible which we demonstrate on a stochastic volatility\nmodel.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We show that under $\\BMM$ and \"there exists a Woodin cardinal$\"$, the\nnonstationary ideal on $\\omega_1$ can not be defined by a $\\Sigma_1$ formula\nwith parameter $A \\subset \\omega_1$. We show that the same conclusion holds\nunder the assumption of Woodin's $(\\ast)$-axiom. We further show that there are\nuniverses where $\\BPFA$ holds and $\\NS$ is $\\Sigma_1(\\omega_1)$-definable. Last\nwe show that if the canonical inner model with one Woodin cardinal $M_1$\nexists, there is a universe where $\\NS$ is saturated,\n$\\Sigma_1(\\omega_1)$-definable and $\\MA$ holds.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study spaces that can be mapped onto the Baire space (i.e. the countable\npower of the countable discrete space) by a continuous quasi-open bijection. We\ngive a characterization of such spaces in terms of Souslin schemes and call\nthese spaces $\\pi$-spaces. We show that every space that has a Lusin $\\pi$-base\nis a $\\pi$-space and that every second-countable $\\pi$-space has a Lusin\n$\\pi$-base. The main result of this paper is a characterization of continuous\nopen images of $\\pi$-space.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the universal scaling limit of random partitions obeying the Schur\nmeasure. Extending our previous analysis [arXiv:2012.06424], we obtain the\nhigher-order Pearcey kernel describing the multi-critical behavior in the cusp\nscaling limit. We explore the gap probability associated with the higher\nPearcey kernel, and derive the coupled nonlinear differential equation and the\nasymptotic behavior in the large gap limit.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Metal artifact correction is a challenging problem in cone beam computed\ntomography (CBCT) scanning. Metal implants inserted into the anatomy cause\nsevere artifacts in reconstructed images. Widely used inpainting-based metal\nartifact reduction (MAR) methods require segmentation of metal traces in the\nprojections as a first step which is a challenging task. One approach is to use\na deep learning method to segment metals in the projections. However, the\nsuccess of deep learning methods is limited by the availability of realistic\ntraining data. It is challenging and time consuming to get reliable ground\ntruth annotations due to unclear implant boundary and large number of\nprojections. We propose to use X-ray simulations to generate synthetic metal\nsegmentation training dataset from clinical CBCT scans. We compare the effect\nof simulations with different number of photons and also compare several\ntraining strategies to augment the available data. We compare our model's\nperformance on real clinical scans with conventional threshold-based MAR and a\nrecent deep learning method. We show that simulations with relatively small\nnumber of photons are suitable for the metal segmentation task and that\ntraining the deep learning model with full size and cropped projections\ntogether improves the robustness of the model. We show substantial improvement\nin the image quality affected by severe motion, voxel size under-sampling, and\nout-of-FOV metals. Our method can be easily implemented into the existing\nprojection-based MAR pipeline to get improved image quality. This method can\nprovide a novel paradigm to accurately segment metals in CBCT projections.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Under the introduction of any interface in its trajectory, an optical beam\nexperiences polarization-dependent deflections in the longitudinal and\ntransverse directions with respect to the plane of incidence. The physics of\nsuch optical beam shifts is connected to profound universal wave phenomena\ngoverned by the fine interference effects of wave packets and has opened up\navenues towards metrological applications. Here, we reveal the inherent\nnon-separability of the longitudinal and transverse beam shifts by considering\na rather simple case of a partially reflecting Gaussian laser beam from a\ndielectric interface. This non-separability appears substantially in some\nparticular regions in the corresponding parameter space. We further show that\nsuch non-separability manifests as a position-position classically entangled\nstate of light. The tunability of the related experimental parameters offers\ncontrol over the degree of entanglement. Uncovering of the inherent\nnon-separability of the two types of beam shifts is expected to enrich the\nphysical origin of this fundamental effect, impact the understanding of\nnumerous analogous effects, and might find useful applications by exploiting\nthe position-position-polarization classical entanglement in a fundamental\nGaussian beam.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We prove that in any Banach space the set of windows in which a rectifiable\ncurve resembles two or more straight line segments is quantitatively small with\nconstants that are independent of the curve, the dimension of the space, and\nthe choice of norm. Together with Part I, this completes the proof of the\nnecessary half of the Analyst's Traveling Salesman theorem with sharp exponent\nin uniformly convex spaces.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We construct a higher-order adaptive method for strong approximations of exit\ntimes of It\\^o stochastic differential equations (SDE). The method employs a\nstrong It\\^o--Taylor scheme for simulating SDE paths, and adaptively decreases\nthe step-size in the numerical integration as the solution approaches the\nboundary of the domain. These techniques turn out to complement each other\nnicely: adaptive time-stepping improves the accuracy of the exit time by\nreducing the magnitude of the overshoot of the numerical solution when it exits\nthe domain, and higher-order schemes improve the approximation of the state of\nthe diffusion process. We present two versions of the higher-order adaptive\nmethod. The first one uses the Milstein scheme as numerical integrator and two\nstep-sizes for adaptive time-stepping: $h$ when far away from the boundary and\n$h^2$ when close to the boundary. The second method is an extension of the\nfirst one using the strong It\\^o--Taylor scheme of order 1.5 as numerical\nintegrator and three step-sizes for adaptive time-stepping. For any $\\xi>0$, we\nprove that the strong error is bounded by $\\mathcal{O}(h^{1-\\xi})$ and\n$\\mathcal{O}(h^{3/2-\\xi})$ for the first and second method, respectively, and\nthe expected computational cost for both methods is $\\mathcal{O}(h^{-1}\n\\log(h^{-1}))$. Theoretical results are supported by numerical examples, and we\ndiscuss the potential for extensions that improve the strong convergence rate\neven further.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We devise neuro-dynamic state estimation (Neuro-DSE), a learning-based\ndynamic state estimation (DSE) algorithm for networked microgrids (NMs) under\nunknown subsystems. Our contributions include: 1) a data-driven Neuro-DSE\nalgorithm for NMs DSE with partially unidentified dynamic models, which\nincorporates the neural-ordinary-differential-equations (ODE-Net) into Kalman\nfilters; 2) a self-refining Neuro-DSE algorithm (Neuro-DSE+) which enables\ndata-driven DSE under limited and noisy measurements by establishing an\nautomatic filtering, augmenting and correcting framework; 3) a\nNeuro-KalmanNet-DSE algorithm which further integrates KalmanNet with Neuro-DSE\nto relieve the model mismatch of both neural- and physics-based dynamic models;\nand 4) an augmented Neuro-DSE for joint estimation of NMs states and unknown\nparameters (e.g., inertia). Extensive case studies demonstrate the efficacy of\nNeuro-DSE and its variants under different noise levels, control modes, power\nsources, observabilities and model knowledge, respectively.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  A railway is a complex system comprising multiple infrastructure and rolling\nstock assets. To operate the system safely, reliably, and efficiently, the\ncondition many components needs to be monitored. To automate this process,\ndata-driven fault detection and diagnostics models can be employed. In\npractice, however, the performance of data-driven models can be compromised if\nthe training dataset is not representative of all possible future conditions.\nWe propose to approach this problem by learning a feature representation that\nis, on the one hand, invariant to operating or environmental factors but, on\nthe other hand, sensitive to changes in the asset's health condition. We\nevaluate how contrastive learning can be employed on supervised and\nunsupervised fault detection and diagnostics tasks given real condition\nmonitoring datasets within a railway system - one image dataset from\ninfrastructure assets and one time-series dataset from rolling stock assets.\nFirst, we evaluate the performance of supervised contrastive feature learning\non a railway sleeper defect classification task given a labeled image dataset.\nSecond, we evaluate the performance of unsupervised contrastive feature\nlearning without access to faulty samples on an anomaly detection task given a\nrailway wheel dataset. Here, we test the hypothesis of whether a feature\nencoder's sensitivity to degradation is also sensitive to novel fault patterns\nin the data. Our results demonstrate that contrastive feature learning improves\nthe performance on the supervised classification task regarding sleepers\ncompared to a state-of-the-art method. Moreover, on the anomaly detection task\nconcerning the railway wheels, the detection of shelling defects is improved\ncompared to state-of-the-art methods.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this work, we propose a data generation pipeline by leveraging the 3D\nsuite Blender to produce synthetic RGBD image datasets with 6D poses for\nrobotic picking. The proposed pipeline can efficiently generate large amounts\nof photo-realistic RGBD images for the object of interest. In addition, a\ncollection of domain randomization techniques is introduced to bridge the gap\nbetween real and synthetic data. Furthermore, we develop a real-time two-stage\n6D pose estimation approach by integrating the object detector YOLO-V4-tiny and\nthe 6D pose estimation algorithm PVN3D for time sensitive robotics\napplications. With the proposed data generation pipeline, our pose estimation\napproach can be trained from scratch using only synthetic data without any\npre-trained models. The resulting network shows competitive performance\ncompared to state-of-the-art methods when evaluated on LineMod dataset. We also\ndemonstrate the proposed approach in a robotic experiment, grasping a household\nobject from cluttered background under different lighting conditions.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Conformal field theories (CFTs) in Euclidean signature satisfy well-accepted\nrules, such as conformal invariance and the convergent Euclidean operator\nproduct expansion (OPE). Nowadays, it is common to assume that CFT correlators\nexist and have various properties in the Lorentzian signature. Some of these\nproperties may represent extra assumptions, and it is an open question if they\nhold for familiar statistical-physics CFTs such as the critical 3d Ising model.\nIn this thesis, we clarify that at the level of four-point functions, the\nEuclidean CFT axioms imply the standard quantum field theory axioms such as\nOsterwalder-Schrader axioms (in Euclidean) and Wightman axioms (in Lorentzian).\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Academic research has highlighted the failure of many Internet of Things\n(IoT) product manufacturers to follow accepted practices, while IoT security\nbest practices have recently attracted considerable attention worldwide from\nindustry and governments. Given current examples of security advice, confusion\nis evident from guidelines that conflate desired outcomes with security\npractices to achieve those outcomes. We explore a surprising lack of clarity,\nand void in the literature, on what (generically) best practice means,\nindependent of identifying specific individual practices or highlighting\nfailure to follow best practices. We consider categories of security advice,\nand analyze how they apply over the lifecycle of IoT devices. For concreteness\nin discussion, we use iterative inductive coding to code and systematically\nanalyze a set of 1013 IoT security best practices, recommendations, and\nguidelines collated from industrial, government, and academic sources. Among\nour findings, of all analyzed items, 68% fail to meet our definition of an\n(actionable) practice, and 73% of all actionable advice relates to the software\ndevelopment lifecycle phase, highlighting the critical position of\nmanufacturers and developers. We hope that our work provides a basis for the\ncommunity to better understand best practices, identify and reach consensus on\nspecific practices, and find ways to motivate relevant stakeholders to follow\nthem.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In this paper, a novel and effective image quality assessment (IQA) algorithm\nbased on frequency disparity for high dynamic range (HDR) images is proposed,\ntermed as local-global frequency feature-based model (LGFM). Motivated by the\nassumption that the human visual system is highly adapted for extracting\nstructural information and partial frequencies when perceiving the visual\nscene, the Gabor and the Butterworth filters are applied to the luminance of\nthe HDR image to extract local and global frequency features, respectively. The\nsimilarity measurement and feature pooling are sequentially performed on the\nfrequency features to obtain the predicted quality score. The experiments\nevaluated on four widely used benchmarks demonstrate that the proposed LGFM can\nprovide a higher consistency with the subjective perception compared with the\nstate-of-the-art HDR IQA methods. Our code is available at:\n\\url{https://github.com/eezkni/LGFM}.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We report an update and enhancement of the ACONFL (conformer energies of\nlarge alkanes [Ehlert, S.; Grimme, S.; Hansen, A. J. Phys. Chem. A 2022, 126,\n3521-3535]) dataset. For the ACONF12 (n-dodecane) subset, we report basis set\nlimit canonical CCSD(T) reference data obtained from MP2-F12/cc-pV{T,Q}Z-F12\nextrapolation, [CCSD(F12*)-MP2-F12]/aug-cc-pVTZ-F12, and a (T) correction from\nconventional CCSD(T)/aug-cc-pV{D,T}Z calculations. Then we explored the\nperformance of a variety of single and composite localized-orbital CCSD(T)\napproximations, ultimately finding an affordable LNO-CCSD(T)-based post-MP2\ncorrection that agrees to 0.008 kcal/mol MAD (mean absolute deviation) with the\nrevised canonical reference data. In tandem with canonical MP2-F12/CBS\nextrapolation, this was then used to re-evaluate the ACONF16 and ACONF20\nsubsets for n-hexadecane and n-icosane, respectively. A revised ACONFL set was\nthus obtained. It was then used to assess the performance of different\nlocalized-orbital coupled cluster approaches, such as PNO-LCCSD(T) as\nimplemented in MOLPRO, DLPNO-CCSD (T1) as implemented in ORCA, and LNO-CCSD(T)\nas implemented in MRCC, at their various accuracy settings. A three-tier\nLNO-CCSD(T)-based composite scheme disagrees by only 0.02 kcal/mol from the\nrevised ACONFL reference data. When extrapolated to the complete PNO space\nlimit, DLPNO-CCSD(T1, Tight) and a composite method are the best picks among\nall the localized coupled cluster methods tested for the dodecane conformers.\nDispersion-corrected dRPA-based double hybrids perform remarkably well for the\nACONFL set. While the revised reference data do not affect any conclusions on\nthe less accurate methods, they may upend orderings for more accurate methods\nwith error statistics on the same order as the difference between reference\ndatasets.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  When modelling turbulent flows, it is often the case that information on the\nforcing terms or the boundary conditions is either not available or overly\ncomplicated and expensive to implement. Instead, some flow features, such as\nthe mean velocity profile or its statistical moments, may be accessible through\nexperiments or observations. We present a method based on physics-informed\nneural networks to generate turbulent states subject to a set of given\nconditions. The physics-informed method ensures the final state approximates a\nvalid flow. We show examples of different statistical conditions that can be\nused to prepare states, motivated by experimental and atmospheric problems.\nLastly, we show two ways of scaling the resolution of the prepared states. One\nis through the use of multiple and parallel neural networks. The other uses\nnudging, a synchronization-based data assimilation technique that leverages the\npower of specialized numerical solvers.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Inertial-aided systems require continuous motion excitation among other\nreasons to characterize the measurement biases that will enable accurate\nintegration required for localization frameworks. This paper proposes the use\nof informative path planning to find the best trajectory for minimizing the\nuncertainty of IMU biases and an adaptive traces method to guide the planner\ntowards trajectories which aid convergence. The key contribution is a novel\nregression method based on Gaussian Process (GP) to enforce continuity and\ndifferentiability between waypoints from a variant of the RRT* planning\nalgorithm. We employ linear operators applied to the GP kernel function to\ninfer not only continuous position trajectories, but also velocities and\naccelerations. The use of linear functionals enable velocity and acceleration\nconstraints given by the IMU measurements to be imposed on the position GP\nmodel. The results from both simulation and real world experiments show that\nplanning for IMU bias convergence helps minimize localization errors in state\nestimation frameworks.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Which volume to annotate next is a challenging problem in building medical\nimaging datasets for deep learning. One of the promising methods to approach\nthis question is active learning (AL). However, AL has been a hard nut to crack\nin terms of which AL algorithm and acquisition functions are most useful for\nwhich datasets. Also, the problem is exacerbated with which volumes to label\nfirst when there is zero labeled data to start with. This is known as the cold\nstart problem in AL. We propose two novel strategies for AL specifically for 3D\nimage segmentation. First, we tackle the cold start problem by proposing a\nproxy task and then utilizing uncertainty generated from the proxy task to rank\nthe unlabeled data to be annotated. Second, we craft a two-stage learning\nframework for each active iteration where the unlabeled data is also used in\nthe second stage as a semi-supervised fine-tuning strategy. We show the promise\nof our approach on two well-known large public datasets from medical\nsegmentation decathlon. The results indicate that the initial selection of data\nand semi-supervised framework both showed significant improvement for several\nAL strategies.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The United Nations (UN) Sustainable Development Goals (SDGs) challenge the\nglobal community to build a world where no one is left behind. Recognizing that\nresearch plays a fundamental part in supporting these goals, attempts have been\nmade to classify research publications according to their relevance in\nsupporting each of the UN's SDGs. In this paper, we outline the methodology\nthat we followed when mapping research articles to SDGs and which is adopted by\nTimes Higher Education in their Social Impact rankings. We also discuss various\naspects in which the methodology can be improved and generalized to other types\nof content apart from research articles. The results presented in this paper\nare the outcome of the SDG Research Mapping Initiative that was established as\na partnership between the University of Southern Denmark, the Aurora European\nUniversities Alliance (represented by Vrije Universiteit Amsterdam), the\nUniversity of Auckland, and Elsevier to bring together broad expertise and\nshare best practices on identifying research contributions to UN's Sustainable\nDevelopment Goals.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Conventional works generally employ a two-phase model in which a generator\nselects the most important pieces, followed by a predictor that makes\npredictions based on the selected pieces. However, such a two-phase model may\nincur the degeneration problem where the predictor overfits to the noise\ngenerated by a not yet well-trained generator and in turn, leads the generator\nto converge to a sub-optimal model that tends to select senseless pieces. To\ntackle this challenge, we propose Folded Rationalization (FR) that folds the\ntwo phases of the rationale model into one from the perspective of text\nsemantic extraction. The key idea of FR is to employ a unified encoder between\nthe generator and predictor, based on which FR can facilitate a better\npredictor by access to valuable information blocked by the generator in the\ntraditional two-phase model and thus bring a better generator. Empirically, we\nshow that FR improves the F1 score by up to 10.3% as compared to\nstate-of-the-art methods.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We theoretically study bright and dark solitons in an experimentally relevant\nhybrid system characterized by strong light-matter coupling. We find that the\ncorresponding two-component model supports a variety of coexisting moving\nsolitons including bright solitons on zero and nonzero background, dark-gray\nand gray-gray dark solitons. The solutions are found in the analytical form by\nreducing the two-component problem to a single stationary equation with\ncubic-quintic nonlinearity. All found solutions coexist under the same set of\nthe model parameters, but, in a properly defined linear limit, approach\ndifferent branches of the polariton dispersion relation for linear waves.\nBright solitons with zero background feature an oscillatory-instability\nthreshold which can be associated with a resonance between the edges of the\ncontinuous spectrum branches. `Half-topological' dark-gray and nontopological\ngray-gray solitons are stable in wide parametric ranges below the modulational\ninstability threshold, while bright solitons on the constant-amplitude pedestal\nare unstable.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  WiFi-based smart human sensing technology enabled by Channel State\nInformation (CSI) has received great attention in recent years. However,\nCSI-based sensing systems suffer from performance degradation when deployed in\ndifferent environments. Existing works solve this problem by domain adaptation\nusing massive unlabeled high-quality data from the new environment, which is\nusually unavailable in practice. In this paper, we propose a novel augmented\nenvironment-invariant robust WiFi gesture recognition system named AirFi that\ndeals with the issue of environment dependency from a new perspective. The\nAirFi is a novel domain generalization framework that learns the critical part\nof CSI regardless of different environments and generalizes the model to unseen\nscenarios, which does not require collecting any data for adaptation to the new\nenvironment. AirFi extracts the common features from several training\nenvironment settings and minimizes the distribution differences among them. The\nfeature is further augmented to be more robust to environments. Moreover, the\nsystem can be further improved by few-shot learning techniques. Compared to\nstate-of-the-art methods, AirFi is able to work in different environment\nsettings without acquiring any CSI data from the new environment. The\nexperimental results demonstrate that our system remains robust in the new\nenvironment and outperforms the compared systems.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Many dynamical systems, from quantum many-body systems to evolving\npopulations to financial markets, are described by stochastic processes.\nParameters characterizing such processes can often be inferred using\ninformation integrated over stochastic paths. However, estimating\ntime-integrated quantities from real data with limited time resolution is\nchallenging. Here, we propose a framework for accurately estimating\ntime-integrated quantities using B\\'ezier interpolation. We applied our\napproach to two dynamical inference problems: determining fitness parameters\nfor evolving populations and inferring forces driving Ornstein-Uhlenbeck\nprocesses. We found that B\\'ezier interpolation reduces the estimation bias for\nboth dynamical inference problems. This improvement was especially noticeable\nfor data sets with limited time resolution. Our method could be broadly applied\nto improve accuracy for other dynamical inference problems using finitely\nsampled data.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We develop a resilient binary hypothesis testing framework for decision\nmaking in adversarial multi-robot crowdsensing tasks. This framework exploits\nstochastic trust observations between robots to arrive at tractable, resilient\ndecision making at a centralized Fusion Center (FC) even when i) there exist\nmalicious robots in the network and their number may be larger than the number\nof legitimate robots, and ii) the FC uses one-shot noisy measurements from all\nrobots. We derive two algorithms to achieve this. The first is the Two Stage\nApproach (2SA) that estimates the legitimacy of robots based on received trust\nobservations, and provably minimizes the probability of detection error in the\nworst-case malicious attack. Here, the proportion of malicious robots is known\nbut arbitrary. For the case of an unknown proportion of malicious robots, we\ndevelop the Adversarial Generalized Likelihood Ratio Test (A-GLRT) that uses\nboth the reported robot measurements and trust observations to estimate the\ntrustworthiness of robots, their reporting strategy, and the correct hypothesis\nsimultaneously. We exploit special problem structure to show that this approach\nremains computationally tractable despite several unknown problem parameters.\nWe deploy both algorithms in a hardware experiment where a group of robots\nconducts crowdsensing of traffic conditions on a mock-up road network similar\nin spirit to Google Maps, subject to a Sybil attack. We extract the trust\nobservations for each robot from actual communication signals which provide\nstatistical information on the uniqueness of the sender. We show that even when\nthe malicious robots are in the majority, the FC can reduce the probability of\ndetection error to 30.5% and 29% for the 2SA and the A-GLRT respectively.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The Large High Altitude Air Shower Observatory has reported the detection of\ncosmic-ray sources in Milky Way that can accelerate particles up to PeV (=\n10$^{15}$ eV) energies. These sources, so called ``PeVatrons'', are mostly\nunidentified. Several classes of sources, such as supernova remnants, pulsar\nwind nebula, or young stellar clusters can potentially be the counterparts of\nthese PeVatrons. The aim of this work is to study a pulsar wind nebula\ninterpretation of one of these PeVatrons, LHAASO J2226+6057, which has a\nrelatively well covered multi-frequency spectrum. We have performed a leptonic,\ntime-dependent modeling of the pulsar wind nebula (PWN) associated with PSR\nJ2229+6114 considering a time-energy-dependent diffusion-loss equation.\nInjection, energy losses, as well as escape of particles were considered to\nbalance the time-dependent lepton population. We have also included the\ndynamics of the PWN and the associated supernova remnant (SNR) and their\ninteraction via the reverse shock to study the reverberation phase of the\nsystem. We have considered different values of braking index ($n$) and true age\n($t_{age}$) for the fitting of the multi-wavelength (MWL) spectral energy\ndistribution (SED) of LHAASO J2226+6057. The best-fit PWN model parameters and\ntheir 1$\\sigma$ confidence intervals were evaluated. We have also demonstrated\nthe impact of reverberation on the MWL SED with increasing time. Additionally,\nwe have discussed the resultant large radius and low magnetic field associated\nwith the PWN in question, as caveats for the possible physical connection of\nthe pulsar as the origin of this high energy source.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We obtain the gluon parton distribution functions (PDFs) in the proton within\nthe extended light-front holographic QCD framework, where the proton couples\nwith the spin-two Pomeron in Anti-de Sitter space, together with constraints\nimposed by the Veneziano model. The gluon helicity asymmetry, after satisfying\nthe perturbative QCD constraints at small and large longitudinal momentum\nregions, agrees with existing experimental measurements. The polarized gluon\ndistribution is consistent with global analyses. We predict the gluon helicity\ncontribution to the proton spin, $\\Delta G=0.221^{+0.009}_{-0.010}$, close to\nthe recent analysis with updated data sets and PHENIX measurement and the\nlattice QCD simulations. We also present the unpolarized and polarized gluon\ngeneralized parton distributions.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  With the recent advance in neural machine translation demonstrating its\nimportance, research on quality estimation (QE) has been steadily progressing.\nQE aims to automatically predict the quality of machine translation (MT) output\nwithout reference sentences. Despite its high utility in the real world, there\nremain several limitations concerning manual QE data creation: inevitably\nincurred non-trivial costs due to the need for translation experts, and issues\nwith data scaling and language expansion. To tackle these limitations, we\npresent QUAK, a Korean-English synthetic QE dataset generated in a fully\nautomatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and\nQUAK-H, produced through three strategies that are relatively free from\nlanguage constraints. Since each strategy requires no human effort, which\nfacilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M\nfor QUAK-M. As an experiment, we quantitatively analyze word-level QE results\nin various ways while performing statistical analysis. Moreover, we show that\ndatasets scaled in an efficient way also contribute to performance improvements\nby observing meaningful performance gains in QUAK-M, P when adding data up to\n1.58M.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  With the emerging connected-vehicle technologies and smart roads, the need\nfor intelligent adaptive traffic signal controls is more than ever before. This\npaper proposes a novel Economic-driven Adaptive Traffic Signal Control (eATSC)\nmodel with a hyper control variable - interest rate defined in economics for\ntraffic signal control at signalized intersections. The eATSC uses a continuous\ncompounding function that captures both the total number of vehicles and the\naccumulated waiting time of each vehicle to compute penalties for different\ndirections. The computed penalties grow with waiting time and is used for\nsignal control decisions. Each intersection is assigned two intelligent agents\nadjusting interest rate and signal length for different directions according to\nthe traffic patterns, respectively. The problem is formulated as a Markov\nDecision Process (MDP) problem to reduce congestions, and a two-agent Double\nDueling Deep Q Network (DDDQN) is utilized to solve the problem. Under the\noptimal policy, the agents can select the optimal interest rates and signal\ntime to minimize the likelihood of traffic congestion. To evaluate the\nsuperiority of our method, a VISSIM simulation model with classic four-leg\nsignalized intersections is developed. The results indicate that the proposed\nmodel is adequately able to maintain healthy traffic flow at the intersection.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We address the unsolved task of robotic bin packing with irregular objects,\nsuch as groceries, where the underlying constraints on object placement and\nmanipulation, and the diverse objects' physical properties make preprogrammed\nstrategies unfeasible. Our approach is to learn directly from expert\ndemonstrations in order to extract implicit task knowledge and strategies to\nachieve an efficient space usage, safe object positioning and to generate\nhuman-like behaviors that enhance human-robot trust. We collect and make\navailable a novel and diverse dataset, BoxED, of box packing demonstrations by\nhumans in virtual reality. In total, 263 boxes were packed with\nsupermarket-like objects by 43 participants, yielding 4644 object\nmanipulations. We use the BoxED dataset to learn a Markov chain to predict the\nobject packing sequence for a given set of objects and compare it with human\nperformance. Our experimental results show that the model surpasses human\nperformance by generating sequence predictions that humans classify as\nhuman-like more frequently than human-generated sequences.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  By performing two local displacement operations (LDOs) inside an SU(1,1)\ninterferometer, called as the displacement-assisted SU(1,1) [DSU(1,1)], both\nthe phase sensitivity based on homodyne detection and quantum Fisher\ninformation (QFI) with and without photon losses are investigated in this\npaper. In this DSU(1,1) interferometer, we focus our attention on the extent to\nwhich the introduced LDO affects the phase sensitivity and the QFI, even in the\nrealistic scenario. Our analyses show that the estimation performance of\nDSU(1,1) interferometer is always better than that of SU(1,1) interferometer\nwithout the LDO, especially the phase precision of the former in the ideal\nscenario gradually approaching to the Heisenberg limit via the increase of the\nLDO strength. More significantly, different from the latter, the robustness of\nthe former can be enhanced markedly by regulating and controlling the LDO. Our\nfindings would open an useful view for quantum-improved phase estimation of\noptical interferometers.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We report the properties of more than 800 bursts detected from the repeating\nfast radio burst (FRB) source FRB 20201124A with the Five-hundred-meter\nAperture Spherical radio Telescope (FAST) during an extremely active episode on\nUTC September 25-28, 2021 in a series of four papers. In this second paper of\nthe series, we mainly focus on the energy distribution of the detected bursts.\nThe event rate initially increased exponentially but the source activity\nstopped within 24 hours after the 4th day. The detection of 542 bursts in one\nhour during the fourth day marked the highest event rate detected from one\nsingle FRB source so far. The bursts have complex structures in the\ntime-frequency space. We find a double-peak distribution of the waiting time,\nwhich can be modeled with two log-normal functions peaking at 51.22 ms and\n10.05 s, respectively. Compared with the emission from a previous active\nepisode of the source detected with FAST, the second distribution peak time is\nsmaller, suggesting that this peak is defined by the activity level of the\nsource. We calculate the isotropic energy of the bursts using both a partial\nbandwidth and a full bandwidth and find that the energy distribution is not\nsignificantly changed. We find that an exponentially connected broken-power-law\nfunction can fit the cumulative burst energy distribution well, with the lower\nand higher-energy indices being $-1.22\\pm0.01$ and $-4.27\\pm0.23$,\nrespectively. Assuming a radio radiative efficiency of $\\eta_r = 10^{-4}$, the\ntotal isotropic energy of the bursts released during the four days when the\nsource was active is already $3.9\\times10^{46}$ erg, exceeding $\\sim 23\\%$ of\nthe available magnetar dipolar magnetic energy. This challenges the magnetar\nmodels invoking an inefficient radio emission (e.g. synchrotron maser models).\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  The purpose of this paper is two-fold, first, to review a recent method\nintroduced by S. Becker, P. Cheridito, and P. Jentzen, for solving\nhigh-dimensional optimal stopping problems using deep Neural Networks, second,\nto propose an alternative algorithm replacing Neural Networks by CART-trees\nwhich allow for more interpretation of the estimated stopping rules. We in\nparticular compare the performance of the two algorithms with respect to the\nBermudan max-call benchmark example concluding that the Bermudan max-call may\nnot be suitable to serve as a benchmark example for high-dimensional optimal\nstopping problems. We also show how our algorithm can be used to plot stopping\nboundaries.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In dimension N=3 the cubic nonlinear Schrodinger equation has solutions which\nbecome singular, i.e. at a spatial point they blow up to infinity in finite\ntime. In 1972 Zakharov famously investigated finite time singularity formation\nin the cubic nonlinear Schrodinger equation as a model for spatial collapse of\nLangmuir waves in plasma, the most abundant form of observed matter in the\nuniverse. Zakharov assumed that (NLS) blow up of solutions is self-similar and\nradially symmetric, and that singularity formation can be modeled by a solution\nof an associated self-similar, complex ordinary differential equation~(ODE). A\nparameter a>0 appears in the ODE, and the dependent variable, Q, satisfies\n(Q(0),Q'(0))=(Q_{0},0), where Q(0)>0. A fundamentally important step towards\nputting the Zakharov model on a firm mathematical footing is to prove, when\nN=3, whether values a>0 and Q_{0}>0 exist such that Q also satisfies the\nphysically important `zero-energy' integral constraint. Since 1972 this has\nremained an open problem. Here, we resolve this issue by proving that for every\na>0 and Q(0)>0, Q satisfies the the `zero-energy' integral constraint.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We consider elliptic curves $E / \\mathbb{Q}$ for which the image of the\nadelic Galois representation $\\rho_E$ is as large as possible given a\nconstraint on the image modulo 2. For such curves, we give a characterization\nin terms of their $\\ell$-adic images, compute all examples of conductor at most\n500,000, precisely describe the image of $\\rho_E$, and offer an application to\nthe cyclicity problem. In this way, we generalize some foundational results on\nSerre curves.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  By using non-equilibrium Green's function (NEGF) method and tight-binding\n(TB) approximation, we investigated a perfect control on spin transport in a\nzigzag $\\alpha^{\\prime}$-boron nanoribbon ($\\alpha^{\\prime}$-BNR) as the must\nsemi-conducting structure of borophene. It has been found that when an\n$\\alpha^{\\prime}$-BNR is exposed to an out-of-plane exchange magnetic field,\nspin splitting occurs for both spin-up and spin-down states in specific ranges\nof energy. Therefore, the spin polarization of current could be controlled by\nadjusting the energy of incoming electrons by means of an external back-gate\nvoltage. We focus on the edge manipulation of $\\alpha^{\\prime}$-BNR by\nferro-magnetic (FM) or anti-ferromagnetic (AFM) exchange field which leads to\nthe emergence of a giant magneto resistance and a perfect spin filtering. Local\ncurrent provides the best picture of spin distribution of current in the\nnanoribbon. In order to observe the response of the system to the proximity\neffect of magnetic strips, we calculate the magnetic moment of each site. Then,\nwe show that applying a transverse or perpendicular electric field in the\npresence of the exchange magnetic field gives another controlling tool on the\nspin polarization of current in a constant energy. Finally, by simultaneous\neffect of in-plane and out-of-plane exchange magnetic field on the edges of\nnanoribbon, we reach a control on spin rotation in the scattering region. Our\ninvestigation guarantees the $\\alpha^{\\prime}$-BNR as a promising two\ndimensional (2D) structure for spintronic purposes.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  3D imaging enables a more accurate diagnosis by providing spatial information\nabout organ anatomy. However, using 3D images to train AI models is\ncomputationally challenging because they consist of tens or hundreds of times\nmore pixels than their 2D counterparts. To train with high-resolution 3D\nimages, convolutional neural networks typically resort to downsampling them or\nprojecting them to two dimensions. In this work, we propose an effective\nalternative, a novel neural network architecture that enables computationally\nefficient classification of 3D medical images in their full resolution.\nCompared to off-the-shelf convolutional neural networks, 3D-GMIC uses\n77.98%-90.05% less GPU memory and 91.23%-96.02% less computation. While our\nnetwork is trained only with image-level labels, without segmentation labels,\nit explains its classification predictions by providing pixel-level saliency\nmaps. On a dataset collected at NYU Langone Health, including 85,526 patients\nwith full-field 2D mammography (FFDM), synthetic 2D mammography, and 3D\nmammography (DBT), our model, the 3D Globally-Aware Multiple Instance\nClassifier (3D-GMIC), achieves a breast-wise AUC of 0.831 (95% CI: 0.769-0.887)\nin classifying breasts with malignant findings using DBT images. As DBT and 2D\nmammography capture different information, averaging predictions on 2D and 3D\nmammography together leads to a diverse ensemble with an improved breast-wise\nAUC of 0.841 (95% CI: 0.768-0.895). Our model generalizes well to an external\ndataset from Duke University Hospital, achieving an image-wise AUC of 0.848\n(95% CI: 0.798-0.896) in classifying DBT images with malignant findings.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  This paper mainly focuses on cones whose basis is a maximum $h$-scattered\nlinear set. We start by investigating the intersection sizes of such cones with\nthe hyperplanes. Then we analyze two constructions of point sets with few\nintersection sizes with the hyperplanes. In particular, the second one extends\nthe construction of translation KM-arcs in projective spaces, having as part at\ninfinity a cone with basis a maximum $h$-scattered linear set. As an instance\nof the second construction we obtain cylinders with a hyperoval as basis, which\nwe call hypercylinders, for which we are able to provide a stability result.\nThe main motivation for these problems is related to the connections with both\nHamming and rank distance codes. Indeed, we are able to construct codes with\nfew weights and to provide a stability result for the codes associated with\nhypercylinders.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  BCS theory has been widely successful at describing elemental bulk\nsuperconductors. Yet, as the length scales of such superconductors approach the\natomic limit, dimensionality as well as the environment of the superconductor\ncan lead to drastically different and unpredictable superconducting behavior.\nHere, we report a threefold enhancement of the superconducting critical\ntemperature and gap size in ultrathin epitaxial Al films on Si(111), when\napproaching the 2D limit, based on high-resolution scanning tunneling\nmicroscopy/spectroscopy (STM/STS) measurements. In magnetic field, the Al films\nshow type II behavior and the Meservey-Tedrow-Fulde (MTF) effect for in-plane\nmagnetic fields. Using spatially resolved spectroscopy, we characterize the\nvortex structure in the MTF regime and find strong deviations from the typical\nAbrikosov vortex. We corroborate these findings with calculations that unveil\nthe role of odd-frequency pairing and a paramagnetic Meissner effect. These\nresults illustrate two striking influences of reduced dimensionality on a BCS\nsuperconductor and present a new platform to study BCS superconductivity in\nlarge magnetic fields.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Let $W$ be a finite Coxeter group. We give an algebraic presentation of what\nwe refer to as ``the non-crossing algebra'', which is associated to the\nhyperplane complement of $W$ and to the cohomology of its Milnor fibre. This is\nused to produce simpler and more general chain (and cochain) complexes which\ncompute the integral homology and cohomology groups of the Milnor fibre $F$ of\n$W$. In the process we define a new, larger algebra $\\widetilde{A}$, which\nseems to be ``dual'' to the Fomin-Kirillov algebra, and in low ranks is\nlinearly isomorphic to it. There is also a mysterious connection between\n$\\widetilde{A}$ and the Orlik-Solomon algebra, in analogy with the fact that\nthe Fomin-Kirillov algebra contains the coinvariant algebra of $W$. This\nanalysis is applied to compute the multiplicities $\\langle \\rho,\nH^k(F,\\mathbb{C})\\rangle_W$ and $\\langle \\rho, H^k(M,\\mathbb{C})\\rangle_W$,\nwhere $M$ and $F$ are respectively the hyperplane complement and Milnor fibre\nassociated to $W$ and $\\rho$ is a representation of $W$.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We show that if $E$ is an ample vector bundle of rank at least two with some\ncurvature bound on $O_{P(E^*)}(1)$, then $E^*\\otimes \\det E$ is Kobayashi\npositive. The proof relies on comparing the curvature of $(\\det E^*)^k$ and\n$S^kE$ for large $k$ and using duality of convex Finsler metrics. Following the\nsame thread of thought, we show if $E$ is ample with similar curvature bounds\non $O_{P(E^*)}(1)$ and $O_{P(E\\otimes \\det E^*)}(1)$, then $E$ is Kobayashi\npositive. With additional assumptions, we can furthermore show that $E^*\\otimes\n\\det E$ and $E$ are Griffiths positive.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Filming atomic motion within molecules is an active pursuit of molecular\nphysics and quantum chemistry. A promising method is laser-induced Coulomb\nExplosion Imaging (CEI) where a laser pulse rapidly ionizes many electrons from\na molecule, causing the remaining ions to undergo Coulomb repulsion. The ion\nmomenta are used to reconstruct the molecular geometry which is tracked over\ntime (i.e. filmed) by ionizing at an adjustable delay with respect to the start\nof interatomic motion. Results are distorted, however, by ultrafast motion\nduring the ionizing pulse. We studied this effect in water and filmed the rapid\n\"slingshot\" motion that enhances ionization and distorts CEI results. Our\ninvestigation uncovered both the geometry and mechanism of the enhancement\nwhich may inform CEI experiments in many other polyatomic molecules.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  High resolution (HR) 3D medical image segmentation plays an important role in\nclinical diagnoses. However, HR images are difficult to be directly processed\nby mainstream graphical cards due to limited video memory. Therefore, most\nexisting 3D medical image segmentation methods use patch-based models, which\nignores global context information that is useful in accurate segmentation and\nhas low inference efficiency. To address these problems, we propose a\nsuper-resolution (SR) guided patch-free 3D medical image segmentation framework\nthat can realize HR segmentation with global information of low-resolution (LR)\ninput. The framework contains two tasks: semantic segmentation (main task) and\nsuper resolution (auxiliary task). To balance the information loss with the LR\ninput, we introduce a Self-Supervised Guidance Module (SGM), which employs a\nselective search method to crop a HR patch from the original image as\nrestoration guidance. Multi-scale convolutional layers are used to mitigate the\nscale-inconsistency between the HR guidance features and the LR features.\nMoreover, we propose a Task-Fusion Module (TFM) to exploit the inter\nconnections between segmentation and SR task. This module can also be used for\nTest Phase Fine-tuning (TPF), leading to a better model generalization ability.\nWhen predicting, only the main segmentation task is needed, while other modules\ncan be removed to accelerate the inference. The experiments results on two\ndifferent datasets show that our framework outperforms current patch-based and\npatch-free models. Our model also has a four times higher inference speed\ncompared to traditional patch-based methods. Our codes are available at:\nhttps://github.com/Dootmaan/PFSeg-Full.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  In recent years, there have been valuable efforts and contributions to make\nthe process of RDF knowledge graph creation traceable and transparent;\nextending and applying declarative mapping languages is an example. One\nchallenging step is the traceability of procedures that aim to overcome\ninteroperability issues, a.k.a. data-level integration. In most pipelines, data\nintegration is performed by ad-hoc programs, preventing traceability and\nreusability. However, formal frameworks provided by function-based declarative\nmapping languages such as FunUL and RML+FnO empower expressiveness. Data-level\nintegration can be defined as functions and integrated as part of the mappings\nperforming schema-level integration. However, combining functions with the\nmappings introduces a new source of complexity that can considerably impact the\nrequired number of resources and execution time. We tackle the problem of\nefficiently executing mappings with functions and formalize the transformation\nof them into function-free mappings. These transformations are the basis of an\noptimization process that aims to perform an eager evaluation of function-based\nmapping rules. These techniques are implemented in a framework named Dragoman.\nWe demonstrate the correctness of the transformations while ensuring that the\nfunction-free data integration processes are equivalent to the original one.\nThe effectiveness of Dragoman is empirically evaluated in 230 testbeds composed\nof various types of functions integrated with mapping rules of different\ncomplexity. The outcomes suggest that evaluating function-free mapping rules\nreduces execution time in complex knowledge graph creation pipelines composed\nof large data sources and multiple types of mapping rules. The savings can be\nup to 75%, suggesting that eagerly executing functions in mapping rules enable\nmaking these pipelines applicable and scalable in real-world settings.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We observe that computing empirical Wasserstein distance in the independence\ntest is an optimal transport (OT) problem with a special structure. This\nobservation inspires us to study a special type of OT problem and propose a\nmodified Hungarian algorithm to solve it exactly. For an OT problem between\nmarginals with $m$ and $n$ atoms ($m\\geq n$), the computational complexity of\nthe proposed algorithm is $O(m^2n)$. Computing the empirical Wasserstein\ndistance in the independence test requires solving this special type of OT\nproblem, where we have $m=n^2$. The associate computational complexity of our\nalgorithm is $O(n^5)$, while the order of applying the classic Hungarian\nalgorithm is $O(n^6)$. Numerical experiments validate our theoretical analysis.\nBroader applications of the proposed algorithm are discussed at the end.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  More and more applications that require high reliability and fault tolerance\nare realized with wireless network architectures and thus ultimately rely on\nthe wireless channels, which can be subject to impairments and blockages.\nHence, these architectures require a backup plan in the physical layer in order\nto guarantee functionality, especially when safety-relevant aspects are\ninvolved. To this end, this work proposes to utilize the reconfigurable\nintelligent surface (RIS) as a resilience mechanism to counteract outages. The\nadvantages of RISs for such a purpose derive from their inherent addition of\nalternative channel links in combination with their reconfigurability. The\nmajor benefits are investigated in a cell-free multiple-input and\nmultiple-output (MIMO) setting, in which the direct channel paths are subject\nto blockages. An optimization problem is formulated that includes rate\nallocation with beamforming and phase shift configuration and is solved with a\nresilience-aware alternating optimization approach. Numerical results show that\ndeploying even a randomly-configured RIS to a network reduces the performance\ndegradation caused by blockages. This becomes even more pronounced in the\noptimized case, in which the RIS is able to potentially counteract the\nperformance degradation entirely. Interestingly, adding more reflecting\nelements to the system brings an overall benefit for the resilience, even for\ntime-sensitive systems, due to the contribution of the RIS reflections, even\nwhen unoptimized.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Recasting phenomenological Lagrangians in terms of SM effective field theory\n(SMEFT) provides a valuable means of connecting potential BSM physics at\nmomenta well above the electroweak scale to experimental signatures at lower\nenergies. In this work we jointly fit the Wilson coefficients of SMEFT\noperators as well as the PDFs in an extension of the CT18 global analysis\nframework, obtaining self-consistent constraints to possible BSM physics\neffects. Global fits are boosted with machine-learning techniques in the form\nof neural networks to ensure efficient scans of the full PDF+SMEFT parameter\nspace. We focus on several operators relevant for top-quark pair and jet\nproduction at hadron colliders and obtain constraints on the Wilson\ncoefficients with Lagrange Multiplier scans. We find mild correlations between\nthe extracted Wilson coefficients, PDFs, and other QCD parameters, and see\nindications that these correlations may become more prominent in future\nanalyses based on data of higher precision. This work serves as a new platform\nfor joint analyses of SM and BSM physics based on the CTEQ-TEA framework.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We present a new selection of 358 blue compact dwarf galaxies (BCDs) from\n5,000 square degrees in the Dark Energy Survey (DES), and the spectroscopic\nfollow-up of a subsample of 68 objects. For the subsample of 34 objects with\ndeep spectra, we measure the metallicity via the direct T$_e$ method using the\nauroral [\\oiii]$\\lambda$ 4363 emission line. These BCDs have average oxygen\nabundance of 12+log(O/H)= 7.8, stellar masses between 10$^7$ to 10$^8$\nM$_\\odot$ and specific SFR between $\\sim$ 10$^{-9}$ to 10$^{-7}$ yr$^{-1}$. We\ncompare the position of our BCDs with the Mass-metallicity (M-Z) and\nLuminosity-metallicity (L-Z) relation derived from the Local Volume Legacy\nsample. We find the scatter around the M-Z relation is smaller than the scatter\naround the L-Z relation. We identify a correlation between the offsets from the\nM-Z and L-Z relation that we suggest is due to the contribution of metal-poor\ninflows. Finally, we explore the validity of the mass-metallicity-SFR\nfundamental plane in the mass range probed by our galaxies. We find that BCDs\nwith stellar masses smaller than $10^{8}$M$_{\\odot}$ do not follow the\nextrapolation of the fundamental plane. This result suggests that mechanisms\nother than the balance between inflows and outflows may be at play in\nregulating the position of low mass galaxies in the M-Z-SFR space.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Quantum error correction is an important ingredient for scalable quantum\ncomputing. Stabilizer codes are one of the most promising and straightforward\nways to correct quantum errors, since they do not require excessive complexity\nof physical qubits, are convenient for logical operations, and improve\nperformance with increasing the involved qubits number. Here, we propose a\nlinear scalable code of the permutative stabilizers for small distances on the\nring architecture, which takes into account the topological features of the\nsuperconducting platform. We present the way to construct the quantum circuit\nof the code and provide numerical simulation that demonstrate the exponential\nlogical error rate suppression.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We introduce the French National 3D Data Repository for Humanities designed\nfor the conservation and the publication of 3D research data in the field of\nHumanities and Social Sciences. We present the choices made for the data\norganization, metadata, standards and infrastructure towards a FAIR service.\nWith 437 references at the time of the writing, we have feedback on some\nchallenges to develop such a service and to make it widely used. This leads to\nopen questions and future developments.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Time-resolved image sensors that capture light at pico-to-nanosecond\ntimescales were once limited to niche applications but are now rapidly becoming\nmainstream in consumer devices. We propose low-cost and low-power imaging\nmodalities that capture scene information from minimal time-resolved image\nsensors with as few as one pixel. The key idea is to flood illuminate large\nscene patches (or the entire scene) with a pulsed light source and measure the\ntime-resolved reflected light by integrating over the entire illuminated area.\nThe one-dimensional measured temporal waveform, called \\emph{transient},\nencodes both distances and albedoes at all visible scene points and as such is\nan aggregate proxy for the scene's 3D geometry. We explore the viability and\nlimitations of the transient waveforms by themselves for recovering scene\ninformation, and also when combined with traditional RGB cameras. We show that\nplane estimation can be performed from a single transient and that using only a\nfew more it is possible to recover a depth map of the whole scene. We also show\ntwo proof-of-concept hardware prototypes that demonstrate the feasibility of\nour approach for compact, mobile, and budget-limited applications.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We study the volume ratio between projections of two convex bodies. Given a\nhigh-dimensional convex body $K$ we show that there is another convex body $L$\nsuch that the volume ratio between any two projections of fixed rank of the\nbodies $K$ and $L$ is large. Namely, we prove that for every $1\\leq k\\leq n$\nand for each convex body $K\\subset \\mathbb{R}^n$ there is a centrally symmetric\nbody $L \\subset \\mathbb{R}^n$ such that for any two projections $P, Q:\n\\mathbb{R}^n \\to \\mathbb{R}^n$ of rank $k$ one has $$\n  \\mbox{vr}(PK, QL) \\geq c \\, \\min\\left\\{\\frac{ k}{ \\sqrt{n}} \\,\n\\sqrt{\\frac{1}{\\log \\log \\log(\\frac{n\\log(n)}{k})}}, \\,\n  \\frac{\\sqrt{k}}{\\sqrt{\\log(\\frac{n\\log(n)}{k})}}\\right\\}, $$\n  where $c>0$ is an absolute constant. This general lower bound is sharp (up to\nlogarithmic factors) in the regime $k\\geq n^{2/3}$.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We present WALLABY pilot data release 1, the first public release of HI pilot\nsurvey data from the Wide-field ASKAP L-band Legacy All-sky Blind Survey\n(WALLABY) on the Australian Square Kilometre Array Pathfinder. Phase 1 of the\nWALLABY pilot survey targeted three $60~{\\rm deg}^2$ regions on the sky in the\ndirection of the Hydra and Norma galaxy clusters and the NGC 4636 galaxy group,\ncovering the redshift range of z < 0.08. The source catalogue, images and\nspectra of nearly 600 extragalactic HI detections and kinematic models for 109\nspatially resolved galaxies are available. As the pilot survey targeted regions\ncontaining nearby group and cluster environments, the median redshift of the\nsample of z ~ 0.014 is relatively low compared to the full WALLABY survey. The\nmedian galaxy HI mass is $2.3 \\times 10^{9}~M_{\\odot}$. The target noise level\nof 1.6 mJy per $30''$ beam and 18.5 kHz channel translates into a $5\\sigma$ HI\nmass sensitivity for point sources of about $5.2 \\times 10^{8} \\, (D_{\\rm L} /\n\\mathrm{100~Mpc})^{2} \\, M_{\\odot}$ across 50 spectral channels (~200 km/s) and\na $5\\sigma$ HI column density sensitivity of about $8.6 \\times 10^{19} \\, (1 +\nz)^{4}~\\mathrm{cm}^{-2}$ across 5 channels (~20 km/s) for emission filling the\n$30''$ beam. As expected for a pilot survey, several technical issues and\nartefacts are still affecting the data quality. Most notably, there are\nsystematic flux errors of up to several 10% caused by uncertainties about the\nexact size and shape of each of the primary beams as well as the presence of\nsidelobes due to the finite deconvolution threshold. In addition, artefacts\nsuch as residual continuum emission and bandpass ripples have affected some of\nthe data. The pilot survey has been highly successful in uncovering such\ntechnical problems, most of which are expected to be addressed and rectified\nbefore the start of the full WALLABY survey.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  Majorana zero modes have been attracting considerable attention because of\ntheir prospective applications in fault-tolerant topological quantum computing.\nIn recent years, some schemes have been proposed to detect and manipulate\nMajorana zero modes using superconducting qubits. However, manipulating and\nreading the Majorana zero modes must be kept in the time window of\nquasiparticle poisoning. In this work, we study the problem of quasiparticle\npoisoning in a split transmon qubit containing hybrid Josephson junctions\ninvolving Majorana zero modes. We show that Majorana coupling will cause parity\nmixing and 4{\\pi} Josephson effect. In addition, we obtained the expression of\nqubit parameter-dependent parity switching rate and demonstrated that\nquasiparticle poisoning can be greatly suppressed by reducing E_J/E_C via qubit\ndesign.\n\n\n###\n\n", "completion": "2022"}
{"prompt": "  We consider the following game that has been used as a way of testing claims\nof extrasensory perception (ESP). One is given a deck of $mn$ cards comprised\nof $n$ distinct types each of which appears exactly $m$ times: this deck is\nshuffled and then cards are discarded from the deck one at a time from top to\nbottom. At each step, a player (whose psychic powers are being tested) tries to\nguess the type of the card currently on top, which is then revealed to the\nplayer before being discarded. We study the expected number $S_{n,m}$ of\ncorrect predictions a player can make: one could always guess the exact same\ntype of card which shows that one can achieve $S_{n,m}>m$. We prove that the\noptimal (non-psychic) strategy is just slightly better than that and find the\nfirst order correction when $n, m$ grows at suitable rates. This is very\ndifferent from the case where $m$ is fixed and $n$ is large (He & Ottolini) and\nsimilar to the case of fixed $n$ and $m$ is large (Graham & Diaconis). The case\n$m=n$ answers a question of Diaconis.\n\n\n###\n\n", "completion": "2022"}
