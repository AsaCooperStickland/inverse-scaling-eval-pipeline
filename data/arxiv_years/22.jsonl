{"prompt": "  The recent research on the connection between gravity and thermodynamics\nsuggests that gravity could be an emergent phenomenon. Following this,\nPadmanabhan proposed a novel idea that the expansion of the universe can be\ninterpreted as equivalent to the emergence of space with the progress of cosmic\ntime. In this approach, the expansion of the universe is described by what is\nknown as the law of emergence, which states that the expansion of the universe\nis driven by the difference between the number of bulk and surface degrees of\nfreedom in a region bounded by the Hubble radius. This principle correctly\nreproduces the standard evolution of a Friedmann universe. We establish the\nconnection of the law of emergence, which is conceptually different from the\nconventional paradigm to describe cosmology, with other well-established\nresults in thermodynamics. It has been shown that the law of emergence can be\nderived from the unified first law of thermodynamics, which can then be\nconsidered as the backbone of the law. However, the law of emergence is rich in\nstructure than implied by the First law thermodynamics alone. It further\nexplains the evolution of the universe towards a state of maximum horizon\nentropy. Following this, it can be considered that the first law of\nthermodynamics, along with the additional constraints imposed by the\nmaximisation of the horizon entropy, can together lead to the law of emergence.\nIn the present article, we first make a brief review of Padmanabhan's proposal\nand then studies its connection with the thermodynamics of the horizon in the\ncontext of Einstein's, Gauss-Bonnet, and more general Lovelock gravity\ntheories.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We continue the study of graph classes in which the treewidth can only be\nlarge due to the presence of a large clique, and, more specifically, of graph\nclasses with bounded tree-independence number. In [Dallard, Milani\\v{c}, and\n\\v{S}torgel, Treewidth versus clique number. {II}. Tree-independence number,\n2022], it was shown that the Maximum Weight Independent Packing problem, which\nis a common generalization of the Independent Set and Induced Matching\nproblems, can be solved in polynomial time provided that the input graph is\ngiven along with a tree decomposition with bounded independence number. We\nprovide further examples of algorithmic problems that can be solved in\npolynomial time under this assumption. This includes, for all even positive\nintegers $d$, the problem of packing subgraphs at distance at least $d$\n(generalizing the Maximum Weight Independent Packing problem) and the problem\nof finding a large induced sparse subgraph satisfying an arbitrary but fixed\nproperty expressible in counting monadic second-order logic. As part of our\napproach, we generalize some classical results on powers of chordal graphs to\nthe context of general graphs and their tree-independence numbers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Advancements in smart vehicle design have enabled the creation of Internet of\nVehicle (IoV) technologies that can utilize the information provided by various\nsensors and wireless communication to perform complex functionality. Many of\nthese functionalities rely on high computational power and low latency. Mobile\nEdge Computing (MEC) technologies have been proposed as a way to meet these\nrequirements, as their proximity and decentralization provide unique benefits\nfor networks like real-time communication, higher throughput, and flexibility.\nDiverse challenges to the process of offloading data in a MEC-enabled IoV\nnetwork have emerged, such as offloading reliability in highly mobile\nenvironments, security for users within the same network, and energy management\nto keep users from being disincentivized to participate in the network. This\narticle surveys research studies that use AI as part of the data offloading\nprocess, categorized based on four main issues: reliability, security, energy\nmanagement, and service seller profit. Afterward, this article discusses\nchallenges and future perspectives for IoV technologies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper gives a detailed overview and a number of worked out examples\nillustrating the Kovacic \\cite{Kovacic86} algorithm for solving second order\nlinear differential equation ${A(x) y\"+ B(x) y' + C(x) y=0}$ where $A,B,C$ are\nrational functions with complex coefficients in the independent variable $x$.\nAll three cases of the algorithm were implemented in a software package based\non an object oriented design and complete source code listing given in the\nappendix with usage examples. Implementation used the Maple computer algebra\nlanguage. The complete Kovacic package in one mpl file accompany the arXiv\nversion of this paper. This package was then used to analyze the distribution\nof Kovacic algorithm cases on $3000$ differential equations\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Previous works show that Pre-trained Language Models (PLMs) can capture\nfactual knowledge. However, some analyses reveal that PLMs fail to perform it\nrobustly, e.g., being sensitive to the changes of prompts when extracting\nfactual knowledge. To mitigate this issue, we propose to let PLMs learn the\ndeterministic relationship between the remaining context and the masked\ncontent. The deterministic relationship ensures that the masked factual content\ncan be deterministically inferable based on the existing clues in the context.\nThat would provide more stable patterns for PLMs to capture factual knowledge\nthan randomly masking. Two pre-training tasks are further introduced to\nmotivate PLMs to rely on the deterministic relationship when filling masks.\nSpecifically, we use an external Knowledge Base (KB) to identify deterministic\nrelationships and continuously pre-train PLMs with the proposed methods. The\nfactual knowledge probing experiments indicate that the continuously\npre-trained PLMs achieve better robustness in factual knowledge capturing.\nFurther experiments on question-answering datasets show that trying to learn a\ndeterministic relationship with the proposed methods can also help other\nknowledge-intensive tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose a novel computer vision-based approach to aid\nReconfigurable Intelligent Surface (RIS) for dynamic beam tracking and then\nimplement the corresponding prototype verification system. A camera is attached\nat the RIS to obtain the visual information about the surrounding environment,\nwith which RIS identifies the desired reflected beam direction and then adjusts\nthe reflection coefficients according to the pre-designed codebook. Compared to\nthe conventional approaches that utilize channel estimation or beam sweeping to\nobtain the reflection coefficients, the proposed one not only saves beam\ntraining overhead but also eliminates the requirement for extra feedback links.\nWe build a 20-by-20 RIS running at 5.4 GHz and develop a high-speed control\nboard to ensure the real-time refresh of the reflection coefficients. Meanwhile\nwe implement an independent peer-to-peer communication system to simulate the\ncommunication between the base station and the user equipment. The vision-aided\nRIS prototype system is tested in two mobile scenarios: RIS works in near-field\nconditions as a passive array antenna of the base station; RIS works in\nfar-field conditions to assist the communication between the base station and\nthe user equipment. The experimental results show that RIS can quickly adjust\nthe reflection coefficients for dynamic beam tracking with the help of visual\ninformation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Mass Spectrometry Imaging (MSI), using traditional rectilinear scanning,\ntakes hours to days for high spatial resolution acquisitions. Given that most\npixels within a sample's field of view are often neither relevant to underlying\nbiological structures nor chemically informative, MSI presents as a prime\ncandidate for integration with sparse and dynamic sampling algorithms. During a\nscan, stochastic models determine which locations probabilistically contain\ninformation critical to the generation of low-error reconstructions. Decreasing\nthe number of required physical measurements thereby minimizes overall\nacquisition times. A Deep Learning Approach for Dynamic Sampling (DLADS),\nutilizing a Convolutional Neural Network (CNN) and encapsulating molecular mass\nintensity distributions within a third dimension, demonstrates a simulated 70%\nthroughput improvement for Nanospray Desorption Electrospray Ionization\n(nano-DESI) MSI tissues. Evaluations are conducted between DLADS and a\nSupervised Learning Approach for Dynamic Sampling, with Least-Squares\nregression (SLADS-LS) and a Multi-Layer Perceptron (MLP) network (SLADS-Net).\nWhen compared with SLADS-LS, limited to a single m/z channel, as well as\nmultichannel SLADS-LS and SLADS-Net, DLADS respectively improves regression\nperformance by 36.7%, 7.0%, and 6.2%, resulting in gains to reconstruction\nquality of 6.0%, 2.1%, and 3.4% for acquisition of targeted m/z.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the $Sp(4)$ gauge theory coupled to $N_f=2$ fundamental and\n$n_f=3$ antisymmetric flavours of Dirac fermions in four dimensions. This\ntheory serves as the microscopic origin for composite Higgs models with\n$SU(4)/Sp(4)$ coset, supplemented by partial top compositeness. We study\nnumerically its lattice realisation, and couple the fundamental plaquette\naction to Wilson-Dirac fermions in mixed representations, by adopting a\n(rational) hybrid Monte Carlo method, to perform non-trivial tests of the\nproperties of the resulting lattice theory.\n  We find evidence of a surface (with boundaries) of first-order bulk phase\ntransitions in the three-dimensional space of bare parameters (one coupling and\ntwo masses). Explicit evaluation of the Dirac eigenvalues confirms the expected\npatterns of global symmetry breaking. After investigating finite volume effects\nin the weak-coupling phase of the theory, for the largest available lattice we\nstudy the mass spectra of the lightest spin-0 and spin-1 flavoured mesons\ncomposed of fermions in each representation, and of the lightest half-integer\nspin composite particle made of fermions in different representations -- the\nchimera baryon. This work sets the stage for future systematical studies of the\nnon-perturbative dynamics in phenomenologically relevant regions of parameter\nspace.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A noncontextual system of random variables may become contextual if one adds\nto it a set of new variables, even if each of them is obtained by the same\ncontext-wise function of the old variables. This fact follows from the\ndefinition of contextuality, and its demonstration is trivial for\ninconsistently connected systems (i.e. systems with disturbance). However, it\nalso holds for consistently connected (and even strongly consistently\nconnected) systems, provided one acknowledges that if a given property was not\nmeasured in a given context, this information can be used in defining functions\namong the random variables.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For summarization, human preference is critical to tame outputs of the\nsummarizer in favor of human interests, as ground-truth summaries are scarce\nand ambiguous. Practical settings require dynamic exchanges between human and\nAI agent wherein feedback is provided in an online manner, a few at a time. In\nthis paper, we introduce a new framework to train summarization models with\npreference feedback interactively. By properly leveraging offline data and a\nnovel reward model, we improve the performance regarding ROUGE scores and\nsample-efficiency. Our experiments on three various datasets confirm the\nbenefit of the proposed framework in active, few-shot and online settings of\npreference learning.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A network of agents is considered whose decision processes are described by\nthe quantum decision theory previously advanced by the authors. Decision making\nis done by evaluating the utility of alternatives, their attractiveness, and\nthe available information, whose combinations form the probabilities to choose\na given alternative. As a result of the interplay between these three\ncontributions, the process of choice between several alternatives is\nmultimodal. The agents interact by exchanging information, which can take two\nforms: (i) information that an agent can directly receive from another agent\nand (ii) information collectively created by the members of the society. The\ninformation field common to all agents tends to smooth out sharp variations in\nthe temporal behaviour of the probabilities and can even remove them. For\nagents with short-term memory, the probabilities often tend to their limiting\nvalues through strong oscillations and, for a range of parameters, these\noscillations last for ever, representing an ever lasting hesitation of the\ndecision makers. Switching on the information field makes the amplitude of the\noscillations smaller and even can halt the everlasting oscillations forcing the\nprobabilities to converge to fixed limits. The dynamic disjunction effect is\ndescribed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Concerted experimental and numerical studies of step bunching on vicinal\ncrystal surfaces resulting from step-down electromigration of partially charged\nadatoms, confirmed the theoretical prediction of scaling dependence of the\nminimal bunch distance $l_{\\rm min}$ on the bunch size $N$: $l_{\\rm min} \\sim\nN^{-\\gamma}$, with $\\gamma = 2/3$. The value of the so called size-scaling\nexponent $\\gamma$ was observed in experiments on vicinal surfaces of\nsemiconducting, metallic, and dielectric materials. Careful theoretical\ninvestigations and numerical calculations predict a second value of $\\gamma =\n1/2$. However, this value is still not been reported from experiments. And we\nreport here experimental observation of step bunching in the universality class\nrelative to $\\gamma = 1/2$. This is achieved by monitoring step flow during\nsublimation of Si(111)-vicinals heated by a direct step-down current at\n~1200$^\\circ$C. In the experiment we also measure other characteristic for the\nbunching quantities, such as the mean total number of steps in the bunch $N$\nand the mean bunch width $W$. We then compare our findings with published\nexperimental and numerical data to arrive at a theoretically consistent\nframework in terms of universality classes. The ultimate benefit of our study\nis not only to advance fundamental knowledge but also to provide further\nguidance for bottom-up synthesis of vicinal nanotemplates.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We define a parametric variant of generalized Euler sums and construct\ncontour integration to give some explicit evaluations of these parametric Euler\nsums. In particular, we establish several explicit formulas of (Hurwitz) zeta\nfunctions, linear and quadratic parametric Euler sums. Furthermore, we also\ngive an explicit evaluation of alternating double zeta values\n$\\ze(\\overline{2j},2m+1)$ in terms of a combination of alternating Riemann zeta\nvalues by using the parametric Euler sums.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Automated decision systems (ADS) are increasingly used for consequential\ndecision-making. These systems often rely on sophisticated yet opaque machine\nlearning models, which do not allow for understanding how a given decision was\narrived at. In this work, we conduct a human subject study to assess people's\nperceptions of informational fairness (i.e., whether people think they are\ngiven adequate information on and explanation of the process and its outcomes)\nand trustworthiness of an underlying ADS when provided with varying types of\ninformation about the system. More specifically, we instantiate an ADS in the\narea of automated loan approval and generate different explanations that are\ncommonly used in the literature. We randomize the amount of information that\nstudy participants get to see by providing certain groups of people with the\nsame explanations as others plus additional explanations. From our quantitative\nanalyses, we observe that different amounts of information as well as people's\n(self-assessed) AI literacy significantly influence the perceived informational\nfairness, which, in turn, positively relates to perceived trustworthiness of\nthe ADS. A comprehensive analysis of qualitative feedback sheds light on\npeople's desiderata for explanations, among which are (i) consistency (both\nwith people's expectations and across different explanations), (ii) disclosure\nof monotonic relationships between features and outcome, and (iii)\nactionability of recommendations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We generalize Grover algorithm with two arbitrary phases in a density matrix\nset up. We give exact analytic expressions for the success probability after\narbitrary number of iteration of the generalized Grover operator as a function\nof number of iterations, two phase angles ({\\alpha}, \\{beta}) and parameter\n{\\xi} introduced in the off diagonal terms of the density matrix in a sense to\ncapture the coherence present in the initial quantum register. We extend Li and\nLi's idea and show for the phase matching condition {\\alpha} = -\\{beta} =\n0.35{\\pi} with two iterations and {\\xi} = 1, we can achieve success probability\n>= 0.8 only with a knowledge about the lower bound of {\\lambda} = 0.166 where\n{\\lambda} is the ratio of marked to total number states in the database.\nFinally we quantify success probability of the algorithm with decrease in\ncoherence of the initial quantum state against modest noise in this simple\nmodel.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Joint, radio-based communication, localization and sensing is a rapidly\nemerging research field with various application potentials. Greatly benefiting\nfrom these capabilities, smart city, mobility, and logistic concepts are key\ncomponents for maximizing the efficiency of modern transportation systems. In\nurban environments, both the search for parking space and freight transport are\ntime- and space-consuming and present the bottlenecks for these transportation\nchains. Providing location information for these heterogeneous requirement\nprofiles (both active and passive localization of objects), can be realized by\nusing retrofittable wireless sensor networks, which are typically only deployed\nfor active localization. An additional passive detection of objects can be\nachieved by assessing signal reflections and multipath properties of the\ntransmission channel stored within the Channel Impulse Response (CIR). In this\nwork, a proof-of-concept realization and preliminary experimental results of a\nCIR-based occupancy detection for parking lots are presented. As the time\nresolution is dependent on available bandwidth, the CIR of Ultra-wideband\ntransceivers are used. For this, the CIR is smoothed and time-variant changes\nwithin it are detected by performing a background subtraction. Finally, the\nreflecting objects are mapped to individual parking lots. The developed method\nis tested in an in-house parking garage. The work provided is a foundation for\npassive occupancy detection, whose capabilities can prospectively be enhanced\nby exploiting additional physical layers, such as 5G or even 6G.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  THz fields induce orientation in gas phase molecules via resonant\ndipole-field interaction. The degree of orientation however remains severely\nlimited due to the practical shortage of high THz-field amplitudes. In this\npaper, we experimentally demonstrate a concerted Near-IR and THz excitation\nscheme that provides significant increase in the degree of orientation at room\ntemperature gas ensembles. The experimental results are supported by\ntheoretical simulations and a detailed discussion of the multiple coherent\ntransition pathways involved in the scheme is presented.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Lexical semantics and cognitive science point to affordances (i.e. the\nactions that objects support) as critical for understanding and representing\nnouns and verbs. However, study of these semantic features has not yet been\nintegrated with the \"foundation\" models that currently dominate language\nrepresentation research. We hypothesize that predictive modeling of object\nstate over time will result in representations that encode object affordance\ninformation \"for free\". We train a neural network to predict objects'\ntrajectories in a simulated interaction and show that our network's latent\nrepresentations differentiate between both observed and unobserved affordances.\nWe find that models trained using 3D simulations from our SPATIAL dataset\noutperform conventional 2D computer vision models trained on a similar task,\nand, on initial inspection, that differences between concepts correspond to\nexpected features (e.g., roll entails rotation). Our results suggest a way in\nwhich modern deep learning approaches to grounded language learning can be\nintegrated with traditional formal semantic notions of lexical representations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Bilevel optimization have gained growing interests, with numerous\napplications found in meta learning, minimax games, reinforcement learning, and\nnested composition optimization. This paper studies the problem of distributed\nbilevel optimization over a network where agents can only communicate with\nneighbors, including examples from multi-task, multi-agent learning and\nfederated learning. In this paper, we propose a gossip-based distributed\nbilevel learning algorithm that allows networked agents to solve both the inner\nand outer optimization problems in a single timescale and share information via\nnetwork propagation. We show that our algorithm enjoys the\n$\\mathcal{O}(\\frac{1}{K \\epsilon^2})$ per-agent sample complexity for general\nnonconvex bilevel optimization and $\\mathcal{O}(\\frac{1}{K \\epsilon})$ for\nstrongly convex objective, achieving a speedup that scales linearly with the\nnetwork size. The sample complexities are optimal in both $\\epsilon$ and $K$.\nWe test our algorithm on the examples of hyperparameter tuning and\ndecentralized reinforcement learning. Simulated experiments confirmed that our\nalgorithm achieves the state-of-the-art training efficiency and test accuracy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We discover superconductivity (SC) in LaAgSb$_2$ at ambient pressure and its\nclose correlation with a charge density wave (CDW) under pressure. The\nsuperconducting transition temperature ($T_c$) exhibits a sharp peak at the CDW\ncritical pressure of 3.2 GPa. We demonstrate that the carriers inhabiting the\nSb-square net is crucial not only in the formation of CDW but also in SC for\ntheir relatively strong electron-phonon coupling (EPC). Furthermore,\ntheoretical EPC strength in pristine LaAgSb$_2$ cannot explain the observed\npeak with $T_c\\sim 1$ K, which indicates that an additional mechanism\nreinforces SC only around the CDW critical pressure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We give an explicit formulation of the weight part of Serre's conjecture for\nGL_2 using Kummer theory. This avoids any reference to p-adic Hodge theory. The\nkey inputs are a description of the reduction modulo p of crystalline\nextensions in terms of certain \"G_K-Artin-Scheier cocycles\" and a result of\nAbrashkin which describes these cocycles in terms of Kummer theory. An\nalternative explicit formulation in terms of local class field theory was\npreviously given by Dembele-Diamond-Roberts in the unramified case and by the\nsecond author in general. We show that the description of\nDembele-Diamond-Roberts can be recovered directly from ours using the explicit\nreciprocity laws of Brueckner-Shaferevich-Vostokov. These calculations\nillustrate how our use of Kummer theory eliminates certain combinatorial\ncomplications appearing in these two papers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper addresses a distributed convex optimization problem with a class\nof coupled constraints, which arise in a multi-agent system composed of\nmultiple communities modeled by cliques. First, we propose a fully distributed\ngradient-based algorithm with a novel operator inspired by the convex\nprojection, called the clique-based projection. Next, we scrutinize the\nconvergence properties for both diminishing and fixed step sizes. For\ndiminishing ones, we show the convergence to an optimal solution under the\nassumptions of the smoothness of an objective function and the compactness of\nthe constraint set. Additionally, when the objective function is strongly\nmonotone, the strict convergence to the unique solution is proved without the\nassumption of compactness. For fixed step sizes, we prove the non-ergodic\nconvergence rate of O(1/k) concerning the objective residual under the\nassumption of the smoothness of the objective function. Furthermore, we apply\nNesterov's acceleration method to the proposed algorithm and establish the\nconvergence rate of O(1/k^2). Numerical experiments illustrate the\neffectiveness of the proposed method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In mathematical psychology, decision makers are modeled using the Lindbladian\nequations from quantum mechanics to capture important human-centric features\nsuch as order effects and violation of the sure thing principle. We consider\nhuman-machine interaction involving a quantum decision maker (human) and a\ncontroller (machine). Given a sequence of human decisions over time, how can\nthe controller dynamically provide input messages to adapt these decisions so\nas to converge to a specific decision? We show via novel stochastic Lyapunov\narguments how the Lindbladian dynamics of the quantum decision maker can be\ncontrolled to converge to a specific decision asymptotically. Our methodology\nyields a useful mathematical framework for human-sensor decision making. The\nstochastic Lyapunov results are also of independent interest as they generalize\nrecent results in the literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the behaviour of Husimi, Wigner and T{\\\"o}plitz symbols of quantum\ndensity matrices when quantum statistics are tested on them, that is when on\nexchange two coordinates in one of the two variables of their integral kernel.\nWe show that to each of these actions is associated a canonical transform on\nthe cotangent bundle of the underlying classical phase space. Equivalently can\none associate a complex canonical transform on the complexification of the\nphase-space. In the off-diagonal T{\\\"o}plitz representation introduced in [P],\nthe action considered is associated to a complex aanticanonical relation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Proving linear inequalities and identities of Shannon's information measures,\npossibly with linear constraints on the information measures, is an important\nproblem in information theory. For this purpose, ITIP and other variant\nalgorithms have been developed and implemented, which are all based on solving\na linear program (LP). In particular, an identity $f = 0$ is verified by\nsolving two LPs, one for $f \\ge 0$ and one for $f \\le 0$. In this paper, we\ndevelop a set of algorithms that can be implemented by symbolic computation.\nBased on these algorithms, procedures for verifying linear information\ninequalities and identities are devised. Compared with LP-based algorithms, our\nprocedures can produce analytical proofs that are both human-verifiable and\nfree of numerical errors. Our procedures are also more efficient\ncomputationally. For constrained inequalities, by taking advantage of the\nalgebraic structure of the problem, the size of the LP that needs to be solved\ncan be significantly reduced. For identities, instead of solving two LPs, the\nidentity can be verified directly with very little computation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the present work we investigate the $\\eta_c K$, $J/\\psi K$, $\\eta_c K^*$\nand $J/\\psi K^*$ hidden-charm decay modes for the $c\\bar{c}s\\bar{u}$ four-quark\nsystem in the molecular and compact tetraquark scenarios using the\nquark-exchange model. Our theoretical results indicate that if the newly\nobserved states $Z_{cs}(3985)$ and $Z_{cs}(4000)$ are two different states,\n$Z_{cs}(4000)$ may be interpreted as the mixture\n$\\frac{1}{\\sqrt{2}}(D^0D_s^{*-}+D^{*0}D_s^{-})$ of which the $J/\\psi K$ partial\ndecay width is about $\\Gamma\\sim2.89$ MeV, while $Z_{cs}(3985)$ may be\nexplained as the mixture $\\frac{1}{\\sqrt{2}}(-D^0D_s^{*-}+D^{*0}D_s^{-})$ of\nwhich the $J/\\psi K$ partial decay width is small to zero. Moreover, if the\nstate $Z_{cs}(4000)$ can be explained as the mixed state\n$\\frac{1}{\\sqrt{2}}(D^0D_s^{*-}+D^{*0}D_s^{-})$ indeed, the partial decay width\nratio between $J/\\psi K$ and $\\eta_cK^*$ is close to unit, which indicates the\ndecay channel $\\eta_cK^*$ may be a ideal channel as well to decode the inner\nstructure of $Z_{cs}(4000)$. In addition, the partial decay width for the\ntensor molecular state $|D^{*0}D_s^{*-}\\rangle_{2^+}$ decaying into $J/\\psi\nK^*$ can reach up to a few MeV, which shows this tensor molecular state has a\ngood potential to be observed in this decay channel.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we focus on one particular instance of the shape\nreconstruction problem, in which the shape we wish to reconstruct is an\norientable smooth submanifold of the Euclidean space. Assuming we have as input\na simplicial complex K that approximates the submanifold (such as the Cech\ncomplex or the Rips complex), we recast the reconstruction problem as a L1-norm\nminimization problem in which the optimization variable is a chain of K.\nProviding that K satisfies certain reasonable conditions, we prove that the\nconsidered minimization problem has a unique solution which triangulates the\nsubmanifold and coincides with the flat Delaunay complex introduced and studied\nin a companion paper. Since the objective is a weighted L1-norm and the\ncontraints are linear, the triangulation process can thus be implemented by\nlinear programming.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop a general framework for the insertion of vertex operator on the\nstring worldsheet, in BV formalism. Such insertions correspond to deformations\nof the Master Action which breaks the gauge symmetry to a subgroup, and then\nrestoring the full gauge symmetry by integrating over a cycle in the space of\nLagrangian submanifolds. We provide the general construction, global on the\nmoduli space, which was previously conjectured in a form local on the\nworldsheet. We explain how the enhancement of the gauge symmetry in equivariant\nBV formalism can be seen as an application of the general idea of BV effective\naction. We derive an integral formula for the deformation of the contraction\noperator due to the vertex insertion.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the geometric behavior of constant mean curvature surfaces invariant\nunder screw motion in the homogeneous 3-manifolds $\\mathbb{E}(\\kappa,\\tau)$ as\nwell as the space-forms of non-negative curvature and give a complete\nclassification. We give a unified presentation of results that have appeared in\nthe literature in various forms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Microbial colonization of surfaces represents the first step towards biofilm\nformation, which is a recurring phenomenon in nature with beneficial and\ndetrimental implications in technological and medical settings. Consequently,\nthere is a current interest in elucidating the fundamental aspects of the\ninitial stages of biofilm formation of microorganisms on solid surfaces. While\nmost of the research is oriented to understand bacterial surface colonization,\nsuch observations at a fundamental level using photosynthetic microalgae are\nthus far elusive. Recent single-cell studies showed that the flagellar adhesion\nof Chlamydomonas is switched on in blue light and switched off under red light\n[Kreis et al., Nature Physics, 2018, 14, 45-49]. Here, we study this\nlight-switchable surface association of C. reinhardtii on the population level\nand measure the kinetics of adsorption and desorption of suspensions of motile\ncells on glass surfaces using bright field optical microscopy. We observe that\nboth processes exhibit a response lag relative to the time at which the blue-\nand red-light conditions are set and model this feature using time-delayed\nLangmuir-type kinetics. We find that cell adsorption occurs significantly\nfaster than desorption, which we attribute to the protein-mediated molecular\nadhesion mechanism of the cells. Adsorption experiments using phototactically\nblind Chlamydomonas mutants demonstrate that phototaxis does not affect the\ncell adsorption kinetics. Hence, this method can be used as an assay for\ncharacterizing the dynamics of the surface colonization of microbial species\nexhibiting light-regulated surface adhesion.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Convolutional neural network-based medical image classifiers have been shown\nto be especially susceptible to adversarial examples. Such instabilities are\nlikely to be unacceptable in the future of automated diagnoses. Though\nstatistical adversarial example detection methods have proven to be effective\ndefense mechanisms, additional research is necessary that investigates the\nfundamental vulnerabilities of deep-learning-based systems and how best to\nbuild models that jointly maximize traditional and robust accuracy. This paper\npresents the inclusion of attention mechanisms in CNN-based medical image\nclassifiers as a reliable and effective strategy for increasing robust accuracy\nwithout sacrifice. This method is able to increase robust accuracy by up to 16%\nin typical adversarial scenarios and up to 2700% in extreme cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present our follow-up observations with GRANDMA of transient sources\nrevealed by the Zwicky Transient Facility (ZTF). Over a period of six months,\nall ZTF triggers were examined in real time by a dedicated science module\nimplemented in the Fink broker, which will be used for the data processing of\nthe Vera C. Rubin Observatory. In this article, we present three selection\nmethods to identify kilonova candidates. Out of more than 35 million\ncandidates, a hundred sources have passed our selection criteria. Six were then\nfollowed-up by GRANDMA (by both professional and amateur astronomers). The\nmajority were finally classified either as asteroids or as supernovae events.\nWe mobilized 37 telescopes, bringing together a large sample of images, taken\nunder various conditions and quality. To complement the orphan kilonova\ncandidates (those without associated gamma-ray bursts, which were all), we\nincluded three additional supernovae alerts to conduct further observations of\nduring summer 2021. We demonstrate the importance of the amateur astronomer\ncommunity that contributed images for scientific analyzes of new sources\ndiscovered in a magnitude range r'=17-19 mag. We based our rapid kilonova\nclassification on the decay rate of the optical source that should exceed 0.3\nmag/day. GRANDMA's follow-up determined the fading rate within 1.5+/-1.2 days\npost-discovery, without waiting for further observations from ZTF. No confirmed\nkilonovae were discovered during our observing campaign. This work will be\ncontinued in the coming months in the view of preparing for kilonova searches\nin the next gravitational-wave observing run O4.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The goal of fine-grained action recognition is to successfully discriminate\nbetween action categories with subtle differences. To tackle this, we derive\ninspiration from the human visual system which contains specialized regions in\nthe brain that are dedicated towards handling specific tasks. We design a novel\nDynamic Spatio-Temporal Specialization (DSTS) module, which consists of\nspecialized neurons that are only activated for a subset of samples that are\nhighly similar. During training, the loss forces the specialized neurons to\nlearn discriminative fine-grained differences to distinguish between these\nsimilar samples, improving fine-grained recognition. Moreover, a\nspatio-temporal specialization method further optimizes the architectures of\nthe specialized neurons to capture either more spatial or temporal fine-grained\ninformation, to better tackle the large range of spatio-temporal variations in\nthe videos. Lastly, we design an Upstream-Downstream Learning algorithm to\noptimize our model's dynamic decisions during training, improving the\nperformance of our DSTS module. We obtain state-of-the-art performance on two\nwidely-used fine-grained action recognition datasets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that the lowest quantum Cram\\'{e}r-Rao bound achievable in\ninterferometry with a one-axis twisted spin coherent state is saturated by the\nasymptotic method-of-moments error of a protocol that uses one call to the\none-axis twisting, one call to time-reversed one-axis twisting, and a final\ntotal spin measurement (i.e., a twist-untwist protocol). The result is derived\nby first showing that the metrological phase diagram for one-axis twisting is\nasymptotically characterized by a single quantum Fisher information value\n$N(N+1)/2$ for all times, then constructing a twist-untwist protocol having a\nmethod-of-moments error that saturates this value. The case of finite-range\none-axis twisting is similarly analyzed, and a simple functional form for the\nmetrological phase diagram is found in both the short-range and long-range\ninteraction regimes. Numerical evidence suggests that the finite-range\nanalogues of twist-untwist protocols can exhibit a method-of-moments error that\nasymptotically saturates the lowest quantum Cram\\'{e}r-Rao bound achievable in\ninterferometry with finite-range one-axis twisted spin coherent states for all\ninteraction times.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent experiments on Ce$_2$Zr$_2$O$_7$ suggest that this material may host a\nnovel form of quantum spin ice, a three-dimensional quantum spin liquid with an\nemergent photon. The Ce$^{3+}$ local moments on the pyrochlore lattice are\ndescribed by pseudospin 1/2 degrees of freedom, whose components transform as\ndipolar and octupolar moments under symmetry operations. In principle, there\nexist four possible quantum spin ice regimes, depending on whether the Ising\ncomponent is in the dipolar/octupolar channel, and two possible flux\nconfigurations of the emergent gauge field. In this work, using exact\ndiagonalization and molecular dynamics, we investigate the equal-time and\ndynamical spin structure factors in all four quantum spin ice regimes using\nquantum and classical computations. Contrasting the distinct signatures of\nquantum and classical results for the four possible quantum spin ice regimes\nand elucidating the role of quantum fluctuations, we show that the quantum\nstructure factor computed for the $\\pi$-flux octupolar quantum spin ice regime\nis most compatible with the neutron scattering results on Ce$_2$Zr$_2$O$_7$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Computation offloading in multi-access edge computing (MEC) is an effective\nparadigm for enabling resource-intensive smart applications. However, when the\nwireless channel utilized for offloading computing activities is hostile, the\nproper advantages of MEC may not be completely realized. Intelligent reflecting\nsurface (IRS) is a new technology that has recently attracted significant\ninterest can optimize the wireless transmission environment in a programmable\nway and improving the connectivity between user equipment (UE) and base station\n(BS). In this paper, the performance of MEC architecture is analyzed\nconsidering both IRS-assisted and without IRS communication scenarios in the\ncontext of the urban micro cellular scenarios. The research obtained that the\ndeployment of IRS can reduce the spectrum and energy consumption significantly.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We define common thermodynamic concepts purely within the framework of\ngeneral Markov chains and derive Jarzynski's equality and Crooks' fluctuation\ntheorem in this setup. Regarding non-equilibrium thermodynamics, we provide a\nnew proof of Jarzynski's equality without physical reversibility assumptions.\nMoreover, we discuss an asymmetry in the definition of work that appears in the\nusual formulation of Crooks' fluctuation theorem, but only surfaces in the\nnon-continuous case. We show how this asymmetry can be cured, requiring,\nhowever, a new condition regarding the energy protocol. We notice, because of\nthe way in which one can prepare a thermodynamic system in the Boltzmann\ndistribution, the new requirement is fulfilled by all previous experimental\nsetups supporting Crooks' fluctuation theorem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  SFILES is a text-based notation for chemical process flowsheets. It was\noriginally proposed by d'Anterroches (2006) who was inspired by the text-based\nSMILES notation for molecules. The text-based format has several advantages\ncompared to flowsheet images regarding the storage format, computational\naccessibility, and eventually for data analysis and processing. However, the\noriginal SFILES version cannot describe essential flowsheet configurations\nunambiguously, such as the distinction between top and bottom products. Neither\nis it capable of describing the control structure required for the safe and\nreliable operation of chemical processes. Also, there is no publicly available\nsoftware for decoding or encoding chemical process topologies to SFILES. We\npropose the SFILES 2.0 with a complete description of the extended notation and\nnaming conventions. Additionally, we provide open-source software for the\nautomated conversion between flowsheet graphs and SFILES 2.0 strings. This way,\nwe hope to encourage researchers and engineers to publish their flowsheet\ntopologies as SFILES 2.0 strings. The ultimate goal is to set the standards for\ncreating a FAIR database of chemical process flowsheets, which would be of\ngreat value for future data analysis and processing.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Inspired by the coupled-layer construction of the X-Cube model, we introduce\nthe X-Cube Floquet code, a dynamical quantum error-correcting code where the\nnumber of encoded logical qubits grows with system size. The X-Cube Floquet\ncode is defined on a three-dimensional lattice, built from intersecting\ntwo-dimensional layers in the $xy$, $yz$, and $xz$ directions, and consists of\na periodic sequence of two-qubit measurements which couple the layers together.\nWithin a single Floquet cycle, the codespace switches between that of the\nX-Cube fracton order and layers of entangled, two-dimensional toric codes. The\nencoded logical qubits' dynamics are analyzed, and we argue that the new code\nhas a non-zero error threshold. We provide a new Hamiltonian realization of the\nX-Cube model and, more generally, explore the phase diagram related to the\nsequence of measurements that define the X-Cube Floquet code.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Extreme Classification (XC) seeks to tag data points with the most relevant\nsubset of labels from an extremely large label set. Performing deep XC with\ndense, learnt representations for data points and labels has attracted much\nattention due to its superiority over earlier XC methods that used sparse,\nhand-crafted features. Negative mining techniques have emerged as a critical\ncomponent of all deep XC methods that allow them to scale to millions of\nlabels. However, despite recent advances, training deep XC models with large\nencoder architectures such as transformers remains challenging. This paper\nidentifies that memory overheads of popular negative mining techniques often\nforce mini-batch sizes to remain small and slow training down. In response,\nthis paper introduces NGAME, a light-weight mini-batch creation technique that\noffers provably accurate in-batch negative samples. This allows training with\nlarger mini-batches offering significantly faster convergence and higher\naccuracies than existing negative sampling techniques. NGAME was found to be up\nto 16% more accurate than state-of-the-art methods on a wide array of benchmark\ndatasets for extreme classification, as well as 3% more accurate at retrieving\nsearch engine queries in response to a user webpage visit to show personalized\nads. In live A/B tests on a popular search engine, NGAME yielded up to 23%\ngains in click-through-rates.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the equivalence of some norms in Sobolev spaces on bounded\ndomains of the d-dimensional real Euclidean space and also in Sobolev spaces on\nthe boundaries of those domains.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a class of polytopes that concisely capture the structure of UV\nand IR divergences of general Feynman integrals in Schwinger parameter space,\ntreating them in a unified way as worldline segments shrinking and expanding at\ndifferent relative rates. While these polytopes conventionally arise as convex\nhulls - via Newton polytopes of Symanzik polynomials - we show that they also\nhave a remarkably simple dual description as cut out by linear inequalities\ndefining the facets. It is this dual definition that makes it possible to\ntransparently understand and efficiently compute leading UV and IR divergences\nfor any Feynman integral. In the case of the UV, this provides a transparent\ngeometric understanding of the familiar nested and overlapping divergences. In\nthe IR, the polytope exposes a new perspective on soft/collinear singularities\nand their intricate generalizations. Tropical geometry furnishes a simple\nframework for calculating the leading UV/IR divergences of any Feynman\nintegral, associating them with the volumes of certain dual cones. As concrete\napplications, we generalize Weinberg's theorem to include a characterization of\nIR divergences, and classify space-time dimensions in which general IR\ndivergences (logarithmic as well as power-law) can occur. We also compute the\nleading IR divergence of rectangular fishnet diagrams at all loop orders, which\nturn out to have a surprisingly simple combinatorial description.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Photoactive iridium complexes are of broad interest due to their applications\nranging from lighting to photocatalysis. However, the excited state property\nprediction of these complexes challenges ab initio methods such as\ntime-dependent density functional theory (TDDFT) both from an accuracy and a\ncomputational cost perspective, complicating high throughput virtual screening\n(HTVS). We instead leverage low-cost machine learning (ML) models to predict\nthe excited state properties of photoactive iridium complexes. We use\nexperimental data of 1,380 iridium complexes to train and evaluate the ML\nmodels and identify the best-performing and most transferable models to be\nthose trained on electronic structure features from low-cost density functional\ntheory tight binding calculations. Using these models, we predict the three\nexcited state properties considered, mean emission energy of phosphorescence,\nexcited state lifetime, and emission spectral integral, with accuracy\ncompetitive with or superseding TDDFT. We conduct feature importance analysis\nto identify which iridium complex attributes govern excited state properties\nand we validate these trends with explicit examples. As a demonstration of how\nour ML models can be used for HTVS and the acceleration of chemical discovery,\nwe curate a set of novel hypothetical iridium complexes and identify promising\nligands for the design of new phosphors.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a strong object detector with encoder-decoder pretraining and\nfinetuning. Our method, called Group DETR v2, is built upon a vision\ntransformer encoder ViT-Huge~\\cite{dosovitskiy2020image}, a DETR variant\nDINO~\\cite{zhang2022dino}, and an efficient DETR training method Group\nDETR~\\cite{chen2022group}. The training process consists of self-supervised\npretraining and finetuning a ViT-Huge encoder on ImageNet-1K, pretraining the\ndetector on Object365, and finally finetuning it on COCO. Group DETR v2\nachieves $\\textbf{64.5}$ mAP on COCO test-dev, and establishes a new SoTA on\nthe COCO leaderboard https://paperswithcode.com/sota/object-detection-on-coco\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Localization in aerial imagery-based maps offers many advantages, such as\nglobal consistency, geo-referenced maps, and the availability of publicly\naccessible data. However, the landmarks that can be observed from both aerial\nimagery and on-board sensors is limited. This leads to ambiguities or aliasing\nduring the data association.\n  Building upon a highly informative representation (that allows efficient data\nassociation), this paper presents a complete pipeline for resolving these\nambiguities. Its core is a robust self-tuning data association that adapts the\nsearch area depending on the entropy of the measurements. Additionally, to\nsmooth the final result, we adjust the information matrix for the associated\ndata as a function of the relative transform produced by the data association\nprocess.\n  We evaluate our method on real data from urban and rural scenarios around the\ncity of Karlsruhe in Germany. We compare state-of-the-art outlier mitigation\nmethods with our self-tuning approach, demonstrating a considerable\nimprovement, especially for outer-urban scenarios.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this article a multi-segmented planar tensegrity mechanism was presented.\nThis mechanism has a three-segment structure with each segment residing on top\nof another. The size of the segments may decrease proportionally from base to\ntop, resulting in a tapered shape from base to tip like an elephant trunk. The\nsystem was mechanically formulated as having linear springs and cables\nfunctioning as actuators. The singularities, as well as the stability of the\nparallel mechanism, were analyzed by using the principle of minimum energy.\nOptimization was also done to obtain the greatest angular deflection for a\nsegment according to a ratio between the size of the base and the moving\nplatform of the robotic system. The result of this work is a family of\nmechanisms that can generate the same workspace for different stability\nproperties.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The wave-particle duality is often considered as the modern and satisfactory\nanswer that man found in searching to know the nature of light after more than\n2000 years of questioning. It is also the answer given by quantum physics\nconcerning the nature of matter particles and any other radiations. The aim of\nthis work is to perform an analysis of this concept of wave-particle duality\nfrom a historical, philosophical and scientific point of view and to study and\ndiscuss about the relations which exist between it, the uncertainty principle\nand the concepts of phase space and microstates considered in statistical\nmechanics. These relations will be described and analyzed both from a\nphysico-mathematical and historico-philosophical perspective. It is, in\nparticular, highlighted that while the concepts of phase space and microstates\nwere already introduced in classical physics long before the discovery of the\nwave-particle duality, a correct understanding of them cannot be achieved\nwithout quantum physics. But conversely, it is also shown that the relations of\nthe wave-particle duality with uncertainty principle, phase space and\nmicrostates that are highlighted can help in a deeper understanding and more\nadequate description of this duality.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Self-supervised representation learning of Multivariate Time Series (MTS) is\na challenging task and attracts increasing research interests in recent years.\nMany previous works focus on the pretext task of self-supervised learning and\nusually neglect the complex problem of MTS encoding, leading to unpromising\nresults. In this paper, we tackle this challenge from two aspects: encoder and\npretext task, and propose a unified channel-aware self-supervised learning\nframework CaSS. Specifically, we first design a new Transformer-based encoder\nChannel-aware Transformer (CaT) to capture the complex relationships between\ndifferent time channels of MTS. Second, we combine two novel pretext tasks Next\nTrend Prediction (NTP) and Contextual Similarity (CS) for the self-supervised\nrepresentation learning with our proposed encoder. Extensive experiments are\nconducted on several commonly used benchmark datasets. The experimental results\nshow that our framework achieves new state-of-the-art comparing with previous\nself-supervised MTS representation learning methods (up to +7.70\\% improvement\non LSST dataset) and can be well applied to the downstream MTS classification.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The total solar irradiance (TSI) varies on timescales of minute to centuries.\nOn short timescales it varies due to the superposition of intensity\nfluctuations produced by turbulent convection and acoustic oscillations. On\nlonger scale times, it changes due to photospheric magnetic activity, mainly\nbecause of the facular brightenings and dimmings caused by sunspots. While\nmodern TSI variations have been monitored from space since 1970s, TSI\nvariations over much longer periods can only be estimated using either\nhistorical observations of magnetic features, possibly supported by flux\ntransport models, or from the measurements of the cosmogenic isotope (e.g.,\n\\textsuperscript{14}C or \\textsuperscript{10}Be) concentrations in tree rings\nand ice cores. The reconstruction of the TSI in the last few centuries,\nparticularly in the 17th/18th centuries during the Maunder minimum, is of\nprimary importance for studying climatic effects. To separate the temporal\ncomponents of the irradiance variations, specifically the magnetic cycle from\nsecular variability, we decomposed the signals associated with historical\nobservations of magnetic features and the solar modulation potential $\\Phi$ by\napplying an Empirical Mode Decomposition algorithm. Thus, the reconstruction is\nempirical and does not require any feature contrast or field transport model.\nThe assessed difference between the mean value during the Maunder minimum and\nthe present value is $\\simeq2.5 Wm^{-2}$. Moreover it shows, in the first half\nof the last century, a growth of $\\simeq 1.5 W m^{-2}$ which stops around the\nmiddle of the century to remain constant for the next 50 years, apart from the\nmodulation due to the solar cycle.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this article, we study the class of surfaces of revolution in the\n3-dimensional Lorentz-Minkowski space with nonvanishing Gauss curvature whose\nposition vector x satisfies the condition {\\Delta}IIIx = Ax, where A is a\nsquare matrix of order 3 and {\\Delta}III denotes the Laplace operator of the\nsecond fundamental form III of the surface. We show that such surfaces are\neither minimal or pseudospheres of a real or imaginary radius.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the complexity of computational problems arising from existence\ntheorems in extremal combinatorics. For some of these problems, a solution is\nguaranteed to exist based on an iterated application of the Pigeonhole\nPrinciple. This results in the definition of a new complexity class within\nTFNP, which we call PLC (for \"polynomial long choice\"). PLC includes all of\nPPP, as well as numerous previously unclassified total problems, including\nsearch problems related to Ramsey's theorem, the Sunflower theorem, the\nErd\\H{o}s-Ko-Rado lemma, and K\\\"onig's lemma. Whether the first two of these\nfour problems are PLC-complete is an important open question which we pursue;\nin contrast, we show that the latter two are PPP-complete. Finally, we reframe\nPPP as an optimization problem, and define a hierarchy of such problems related\nto Tur\\'an's theorem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Making music with other people is a social activity as well as an artistic\none. Music therapists take advantage of the social aspects of music to obtain\nbenefits for the patients, interacting with them musically, but this activity\nrequires an high level of expertise. We propose a serious game that helps\npeople even without musical skills interact with each other by collaboratively\ncreating a rhythm with MIDI drum pads. The gaming system analyzes the rhythm in\nreal time to add a musical feedback that enhances the aesthetical experience\nthat is a crucial part of the musical interaction. The system is evaluated\nthrough a questionnaire asking subjects who tried in couples the system if they\nperceived it as helping their interaction. Despite the early development stage\nof the game, the results of the questionnaire show positive reception.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using the formalism of order series, we provide new proofs of a couple of\nresults by Ramanujan and generalize one of his results dealing with the values\nof the zeta function. Consider the operad generated by a binary associative and\ncommutative operation and a binary associative operation, order series are one\nof the algebras over this operad. In our interpretation, Ramanujan worked with\nseries inheriting the structure of the disjoint union of posets. Using zeta\nvalues, we derive an algebra over the operad of series-parallel posets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The event camera is a bio-vision inspired camera with high dynamic range,\nhigh response speed, and low power consumption, recently attracting extensive\nattention for its use in vast vision tasks. Unlike the conventional cameras\nthat output intensity frame at a fixed time interval, event camera records the\npixel brightness change (a.k.a., event) asynchronously (in time) and sparsely\n(in space). Existing methods often aggregate events occurred in a predefined\ntemporal duration for downstream tasks, which apparently overlook varying\nbehaviors of fine-grained temporal events. This work proposes the Event\nTransformer to directly process the event sequence in its native vectorized\ntensor format. It cascades a Local Transformer (LXformer) for exploiting the\nlocal temporal correlation, a Sparse Conformer (SCformer) for embedding the\nlocal spatial similarity, and a Global Transformer (GXformer) for further\naggregating the global information in a serial means to effectively\ncharacterize the time and space correlations from input raw events for the\ngeneration of effective spatiotemporal features used for tasks. %In both\nLXformer and SCformer, Experimental studies have been extensively conducted in\ncomparison to another fourteen existing algorithms upon five different datasets\nwidely used for classification. Quantitative results report the\nstate-of-the-arts classification accuracy and the least computational resource\nrequirements, of the Event Transformer, making it practically attractive for\nevent-based vision tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This work proposes a novel method to generate realistic talking head videos\nusing audio and visual streams. We animate a source image by transferring head\nmotion from a driving video using a dense motion field generated using\nlearnable keypoints. We improve the quality of lip sync using audio as an\nadditional input, helping the network to attend to the mouth region. We use\nadditional priors using face segmentation and face mesh to improve the\nstructure of the reconstructed faces. Finally, we improve the visual quality of\nthe generations by incorporating a carefully designed identity-aware generator\nmodule. The identity-aware generator takes the source image and the warped\nmotion features as input to generate a high-quality output with fine-grained\ndetails. Our method produces state-of-the-art results and generalizes well to\nunseen faces, languages, and voices. We comprehensively evaluate our approach\nusing multiple metrics and outperforming the current techniques both\nqualitative and quantitatively. Our work opens up several applications,\nincluding enabling low bandwidth video calls. We release a demo video and\nadditional information at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/avfr.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As one of the largest e-commerce platforms in the world, Taobao's\nrecommendation systems (RSs) serve the demands of shopping for hundreds of\nmillions of customers. Click-Through Rate (CTR) prediction is a core component\nof the RS. One of the biggest characteristics in CTR prediction at Taobao is\nthat there exist multiple recommendation domains where the scales of different\ndomains vary significantly. Therefore, it is crucial to perform cross-domain\nCTR prediction to transfer knowledge from large domains to small domains to\nalleviate the data sparsity issue. However, existing cross-domain CTR\nprediction methods are proposed for static knowledge transfer, ignoring that\nall domains in real-world RSs are continually time-evolving. In light of this,\nwe present a necessary but novel task named Continual Transfer Learning (CTL),\nwhich transfers knowledge from a time-evolving source domain to a time-evolving\ntarget domain. In this work, we propose a simple and effective CTL model called\nCTNet to solve the problem of continual cross-domain CTR prediction at Taobao,\nand CTNet can be trained efficiently. Particularly, CTNet considers an\nimportant characteristic in the industry that models has been continually\nwell-trained for a very long time. So CTNet aims to fully utilize all the\nwell-trained model parameters in both source domain and target domain to avoid\nlosing historically acquired knowledge, and only needs incremental target\ndomain data for training to guarantee efficiency. Extensive offline experiments\nand online A/B testing at Taobao demonstrate the efficiency and effectiveness\nof CTNet. CTNet is now deployed online in the recommender systems of Taobao,\nserving the main traffic of hundreds of millions of active users.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Conditional Neural Processes~(CNPs) bridge neural networks with probabilistic\ninference to approximate functions of Stochastic Processes under meta-learning\nsettings. Given a batch of non-{\\it i.i.d} function instantiations, CNPs are\njointly optimized for in-instantiation observation prediction and\ncross-instantiation meta-representation adaptation within a generative\nreconstruction pipeline. There can be a challenge in tying together such two\ntargets when the distribution of function observations scales to\nhigh-dimensional and noisy spaces. Instead, noise contrastive estimation might\nbe able to provide more robust representations by learning distributional\nmatching objectives to combat such inherent limitation of generative models. In\nlight of this, we propose to equip CNPs by 1) aligning prediction with encoded\nground-truth observation, and 2) decoupling meta-representation adaptation from\ngenerative reconstruction. Specifically, two auxiliary contrastive branches are\nset up hierarchically, namely in-instantiation temporal contrastive\nlearning~({\\tt TCL}) and cross-instantiation function contrastive\nlearning~({\\tt FCL}), to facilitate local predictive alignment and global\nfunction consistency, respectively. We empirically show that {\\tt TCL} captures\nhigh-level abstraction of observations, whereas {\\tt FCL} helps identify\nunderlying functions, which in turn provides more efficient representations.\nOur model outperforms other CNPs variants when evaluating function distribution\nreconstruction and parameter identification across 1D, 2D and high-dimensional\ntime-series.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Traditional systems designed for task oriented dialog utilize knowledge\npresent only in structured knowledge sources to generate responses. However,\nrelevant information required to generate responses may also reside in\nunstructured sources, such as documents. Recent state of the art models such as\nHyKnow and SeKnow aimed at overcoming these challenges make limiting\nassumptions about the knowledge sources. For instance, these systems assume\nthat certain types of information, such as a phone number, is always present in\na structured KB while information about aspects such as entrance ticket prices\nwould always be available in documents.\n  In this paper, we create a modified version of the MutliWOZ based dataset\nprepared by SeKnow to demonstrate how current methods have significant\ndegradation in performance when strict assumptions about the source of\ninformation are removed. Then, in line with recent work exploiting pre-trained\nlanguage models, we fine-tune a BART based model using prompts for the tasks of\nquerying knowledge sources, as well as, for response generation, without making\nassumptions about the information present in each knowledge source. Through a\nseries of experiments, we demonstrate that our model is robust to perturbations\nto knowledge modality (source of information), and that it can fuse information\nfrom structured as well as unstructured knowledge to generate responses.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work, we investigate online mechanisms for trading time-sensitive\nvalued data. We adopt a continuous function $d(t)$ to represent the data value\nfluctuation over time $t$. Our objective is to design an \\emph{online}\nmechanism achieving \\emph{truthfulness} and \\emph{revenue-competitiveness}. We\nfirst prove several lower bounds on the revenue competitive ratios under\nvarious assumptions. We then propose several online truthful auction mechanisms\nfor various adversarial models, such as a randomized observe-then-select\nmechanism $\\mathcal{M}_1$ and prove that it is \\textit{truthful} and\n$\\Theta(\\log n)$-competitive under some assumptions. Then we present an\neffective truthful weighted-selection mechanism $\\mathcal{M'}_W$ by relaxing\nthe assumptions on the sizes of the discount-classes. We prove that it achieves\na competitive ratio $\\Theta(n\\log n)$ for any known non-decreasing discount\nfunction $d(t)$, and the number of buyers in each discount class $n_c \\ge 2$.\nWhen the optimum expected revenue $OPT_1$ can be estimated within a constant\nfactor, i.e. $c_0 \\cdot OPT_1 \\le Z \\le OPT_1 $ for some constant $c_0\n\\in(0,1)$, we propose a truthful online posted-price mechanism that achieves a\nconstant competitive ratio $\\frac{4}{c_0}$. Our extensive numerical evaluations\ndemonstrate that our mechanisms perform very well in most cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Parental origin effects play an important role in mammal development and\ndisorder. Case-control mother-child pair genotype data can be used to detect\nparental origin effects and is often convenient to collect in practice. Most\nexisting methods for assessing parental origin effects do not incorporate any\ncovariates, which may be required to control for confounding factors. We\npropose to model the parental origin effects through a logistic regression\nmodel, with predictors including maternal and child genotypes, parental\norigins, and covariates. The parental origins may not be fully inferred from\ngenotypes of a target genetic marker, so we propose to use genotypes of markers\ntightly linked to the target marker to increase inference efficiency. A\ncomputationally robust statistical inference procedure is developed based on a\nmodified profile likelihood in a retrospective way. A computationally feasible\nexpectation-maximization algorithm is devised to estimate all unknown\nparameters involved in the modified profile likelihood. This algorithm differs\nfrom the conventional expectation-maximization algorithm in the sense that it\nis based on a modified instead of the original profile likelihood function. The\nconvergence of the algorithm is established under some mild regularity\nconditions. This expectation-maximization algorithm also allows convenient\nhandling of missing child genotypes. Large sample properties, including weak\nconsistency, asymptotic normality, and asymptotic efficiency, are established\nfor the proposed estimator under some mild regularity conditions. Finite sample\nproperties are evaluated through extensive simulation studies and the\napplication to a real dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Finding an appropriate representation of dynamic activities in the brain is\ncrucial for many downstream applications. Due to its highly dynamic nature,\ntemporally averaged fMRI (functional magnetic resonance imaging) can only\nprovide a narrow view of underlying brain activities. Previous works lack the\nability to learn and interpret the latent dynamics in brain architectures. This\npaper builds an efficient graph neural network model that incorporates both\nregion-mapped fMRI sequences and structural connectivities obtained from DWI\n(diffusion-weighted imaging) as inputs. We find good representations of the\nlatent brain dynamics through learning sample-level adaptive adjacency matrices\nand performing a novel multi-resolution inner cluster smoothing. We also\nattribute inputs with integrated gradients, which enables us to infer (1)\nhighly involved brain connections and subnetworks for each task, (2) temporal\nkeyframes of imaging sequences that characterize tasks, and (3) subnetworks\nthat discriminate between individual subjects. This ability to identify\ncritical subnetworks that characterize signal states across heterogeneous tasks\nand individuals is of great importance to neuroscience and other scientific\ndomains. Extensive experiments and ablation studies demonstrate our proposed\nmethod's superiority and efficiency in spatial-temporal graph signal modeling\nwith insightful interpretations of brain dynamics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Model-based methods for recommender systems have been studied extensively for\nyears. Modern recommender systems usually resort to 1) representation learning\nmodels which define user-item preference as the distance between their\nembedding representations, and 2) embedding-based Approximate Nearest Neighbor\n(ANN) search to tackle the efficiency problem introduced by large-scale corpus.\nWhile providing efficient retrieval, the embedding-based retrieval pattern also\nlimits the model capacity since the form of user-item preference measure is\nrestricted to the distance between their embedding representations. However,\nfor other more precise user-item preference measures, e.g., preference scores\ndirectly derived from a deep neural network, they are computationally\nintractable because of the lack of an efficient retrieval method, and an\nexhaustive search for all user-item pairs is impractical. In this paper, we\npropose a novel method to extend ANN search to arbitrary matching functions,\ne.g., a deep neural network. Our main idea is to perform a greedy walk with a\nmatching function in a similarity graph constructed from all items. To solve\nthe problem that the similarity measures of graph construction and user-item\nmatching function are heterogeneous, we propose a pluggable adversarial\ntraining task to ensure the graph search with arbitrary matching function can\nachieve fairly high precision. Experimental results in both open source and\nindustry datasets demonstrate the effectiveness of our method. The proposed\nmethod has been fully deployed in the Taobao display advertising platform and\nbrings a considerable advertising revenue increase. We also summarize our\ndetailed experiences in deployment in this paper.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Motivated by investigating multistationarity in biochemical systems, we\naddress saddle-node bifurcations for chemical reaction networks endowed with\ngeneral kinetics. At positive equilibria, we identify structural network\nconditions that guarantee the bifurcation behavior and we develop a method to\nidentify the proper bifurcation parameters. As a relevant example, we\nexplicitly provide such bifurcation parameters for Michaelis-Menten and Hill's\nkinetics. Examples of application include reversible feedback cycles, the\ncentral carbon metabolism of E.Coli, and autocatalytic networks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study discrete allocation problems, as in the textbook notion of an\nexchange economy, but with indivisible goods. The problem is well-known to be\ndifficult. The model is rich enough to encode some of the most pathological\nbargaining configurations in game theory, like the roommate problem. Our\ncontribution is to show the existence of stable allocations (outcomes in the\nweak core, or in the bargaining set) under different sets of assumptions.\nSpecifically, we consider dichotomous preferences, categorical economies, and\ndiscrete TU markets. The paper uses varied techniques, from Scarf's balanced\ngames to a generalization of the TTC algorithm by means of Tarski fixed points.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work we introduce a unit averaging procedure to efficiently recover\nunit-specific parameters in a heterogeneous panel model. The procedure consists\nin estimating the parameter of a given unit using a weighted average of all the\nunit-specific parameter estimators in the panel. The weights of the average are\ndetermined by minimizing an MSE criterion. We analyze the properties of the\nminimum MSE unit averaging estimator in a local heterogeneity framework\ninspired by the literature on frequentist model averaging. The analysis of the\nestimator covers both the cases in which the cross-sectional dimension of the\npanel is fixed and large. In both cases, we obtain the local asymptotic\ndistribution of the minimum MSE unit averaging estimators and of the associated\nweights. A GDP nowcasting application for a panel of European countries\nshowcases the benefits of the procedure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To foster usefulness and accountability of machine learning (ML), it is\nessential to explain a model's decisions in addition to evaluating its\nperformance. Accordingly, the field of explainable artificial intelligence\n(XAI) has resurfaced as a topic of active research, offering approaches to\naddress the \"how\" and \"why\" of automated decision-making. Within this domain,\ncounterfactual explanations (CFEs) have gained considerable traction as a\npsychologically grounded approach to generate post-hoc explanations. To do so,\nCFEs highlight what changes to a model's input would have changed its\nprediction in a particular way. However, despite the introduction of numerous\nCFE approaches, their usability has yet to be thoroughly validated at the human\nlevel. Thus, to advance the field of XAI, we introduce the Alien Zoo, an\nengaging, web-based and game-inspired experimental framework. The Alien Zoo\nprovides the means to evaluate usability of CFEs for gaining new knowledge from\nan automated system, targeting novice users in a domain-general context. As a\nproof of concept, we demonstrate the practical efficacy and feasibility of this\napproach in a user study. Our results suggest that users benefit from receiving\nCFEs compared to no explanation, both in terms of objective performance in the\nproposed iterative learning task, and subjective usability. With this work, we\naim to equip research groups and practitioners with the means to easily run\ncontrolled and well-powered user studies to complement their otherwise often\nmore technology-oriented work. Thus, in the interest of reproducible research,\nwe provide the entire code, together with the underlying models and user data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present PMC-Patients, a dataset consisting of 167k patient notes with 3.1M\nrelevant article annotations and 293k similar patient annotations. The patient\nnotes are extracted by identifying certain sections from case reports in PubMed\nCentral, and those with at least CC BY-NC-SA license are re-distributed.\nPatient-article relevance and patient-patient similarity are defined by\ncitation relationships in PubMed. We also perform four tasks with PMC-Patients\nto demonstrate its utility, including Patient Note Recognition (PNR),\nPatient-Patient Similarity (PPS), Patient-Patient Retrieval (PPR), and\nPatient-Article Retrieval (PAR). In summary, PMC-Patients provides the\nlargest-scale patient notes with high quality, diverse conditions, easy access,\nand rich annotations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Majorana particles are the same as their antiparticle, and their analogues in\ncondensed matter may be a platform for quantum computing. We describe the\nsearch for these modes in semiconductor heterostructures and how disorder is a\nlimiting factor.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we prove a sharp and strong non-uniqueness for a class of weak\nsolutions to the three-dimensional magneto-hydrodynamic (MHD) system. More\nprecisely, we show that any weak solution $(v,b)\\in L^p_tL^{\\infty}_x$ is\nnon-unique in $L^p_tL^{\\infty}_x$ with $1\\le p<2$, which reveals the strong\nnon-uniqueness, and the sharpness in terms of the classical\nLadyzhenskaya-Prodi-Serrin criteria at endpoint $(2, \\infty)$. Moreover, for\nany $1\\le p<2$ and $\\epsilon>0$, we construct non-Leray-Hopf weak solutions in\n$L^p_tL^{\\infty}_x\\cap L^1_tC^{1-\\epsilon}$. The results of Navier-Stokes\nequations in \\cite{1Cheskidov} imply the sharp non-uniqueness of MHD system\nwith trivial magnetic field $b$. Our result shows the non-uniqueness for any\nweak solution $(v,b)$ including non-trivial magnetic field $b$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper proposes the MBURST, a novel multimodal solution for audio-visual\nspeech enhancements that consider the most recent neurological discoveries\nregarding pyramidal cells of the prefrontal cortex and other brain regions. The\nso-called burst propagation implements several criteria to address the credit\nassignment problem in a more biologically plausible manner: steering the sign\nand magnitude of plasticity through feedback, multiplexing the feedback and\nfeedforward information across layers through different weight connections,\napproximating feedback and feedforward connections, and linearizing the\nfeedback signals. MBURST benefits from such capabilities to learn correlations\nbetween the noisy signal and the visual stimuli, thus attributing meaning to\nthe speech by amplifying relevant information and suppressing noise.\nExperiments conducted over a Grid Corpus and CHiME3-based dataset show that\nMBURST can reproduce similar mask reconstructions to the multimodal\nbackpropagation-based baseline while demonstrating outstanding energy\nefficiency management, reducing the neuron firing rates to values up to\n\\textbf{$70\\%$} lower. Such a feature implies more sustainable implementations,\nsuitable and desirable for hearing aids or any other similar embedded systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the upper bounds for $A(n,d)$, the maximum size of codewords with\nlength $n$ and Hamming distance at least $d$. Schrijver studied the Terwilliger\nalgebra of the Hamming scheme and proposed a semidefinite program to bound\n$A(n, d)$. We derive more sophisticated matrix inequalities based on a split\nTerwilliger algebra to improve Schrijver's semidefinite programming bounds on\n$A(n, d)$. In particular, we improve the semidefinite programming bounds on\n$A(18,4)$ and $A(19, 4)$ to $6551$ and $13087$, respectively.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We report a strategy to design nanoscale cold-source field-effect transistors\n(CS-FETs) with bias-independent sub-60 mV/dec subthreshold swing (SS). By\nfirst-principles calculations and quantum-transport simulations, we reveal that\nthe energy alignment of density of states (DOS) between the drain and source\nelectrodes is critical to achieving bias-independent SS. By defining \"gate\nwindow\", we propose a device model to demonstrate how similar slopes of the\ndrain DOS falling into the gate window can stabilize the SS under different\nbias. This study underscores the significance of drain DOS engineering in the\ndesign of CS-FETs with bias-independent SS for portable electronic\napplications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present SLATE, a sequence labeling approach for extracting tasks from\nfree-form content such as digitally handwritten (or \"inked\") notes on a virtual\nwhiteboard. Our approach allows us to create a single, low-latency model to\nsimultaneously perform sentence segmentation and classification of these\nsentences into task/non-task sentences. SLATE greatly outperforms a baseline\ntwo-model (sentence segmentation followed by classification model) approach,\nachieving a task F1 score of 84.4\\%, a sentence segmentation (boundary\nsimilarity) score of 88.4% and three times lower latency compared to the\nbaseline. Furthermore, we provide insights into tackling challenges of\nperforming NLP on the inking domain. We release both our code and dataset for\nthis novel task.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Natural user interfaces are on the rise. Manufacturers for Augmented,\nVirtual, and Mixed Reality head mounted displays are increasingly integrating\nnew sensors into their consumer grade products, allowing gesture recognition\nwithout additional hardware. This offers new possibilities for bare handed\ninteraction within virtual environments. This work proposes a hand gesture\nauthoring tool for object specific grab gestures allowing virtual objects to be\ngrabbed as in the real world. The presented solution uses template matching for\ngesture recognition and requires no technical knowledge to design and create\ncustom tailored hand gestures. In a user study, the proposed approach is\ncompared with the pinch gesture and the controller for grasping virtual\nobjects. The different grasping techniques are compared in terms of accuracy,\ntask completion time, usability, and naturalness. The study showed that\ngestures created with the proposed approach are perceived by users as a more\nnatural input modality than the others.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A railway is a complex system comprising multiple infrastructure and rolling\nstock assets. To operate the system safely, reliably, and efficiently, the\ncondition many components needs to be monitored. To automate this process,\ndata-driven fault detection and diagnostics models can be employed. In\npractice, however, the performance of data-driven models can be compromised if\nthe training dataset is not representative of all possible future conditions.\nWe propose to approach this problem by learning a feature representation that\nis, on the one hand, invariant to operating or environmental factors but, on\nthe other hand, sensitive to changes in the asset's health condition. We\nevaluate how contrastive learning can be employed on supervised and\nunsupervised fault detection and diagnostics tasks given real condition\nmonitoring datasets within a railway system - one image dataset from\ninfrastructure assets and one time-series dataset from rolling stock assets.\nFirst, we evaluate the performance of supervised contrastive feature learning\non a railway sleeper defect classification task given a labeled image dataset.\nSecond, we evaluate the performance of unsupervised contrastive feature\nlearning without access to faulty samples on an anomaly detection task given a\nrailway wheel dataset. Here, we test the hypothesis of whether a feature\nencoder's sensitivity to degradation is also sensitive to novel fault patterns\nin the data. Our results demonstrate that contrastive feature learning improves\nthe performance on the supervised classification task regarding sleepers\ncompared to a state-of-the-art method. Moreover, on the anomaly detection task\nconcerning the railway wheels, the detection of shelling defects is improved\ncompared to state-of-the-art methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Current global re-localization algorithms are built on top of localization\nand mapping methods and heavily rely on scan matching and direct point cloud\nfeature extraction and therefore are vulnerable in featureless demanding\nenvironments like caves and tunnels. In this article, we propose a novel global\nre-localization framework that: a) does not require an initial guess, like most\nmethods do, while b) it has the capability to offer the top-k candidates to\nchoose from and last but not least provides an event-based re-localization\ntrigger module for enabling, and c) supporting completely autonomous robotic\nmissions. With the focus on subterranean environments with low features, we opt\nto use descriptors based on range images from 3D LiDAR scans in order to\nmaintain the depth information of the environment. In our novel approach, we\nmake use of a state-of-the-art data-driven descriptor extraction framework for\nplace recognition and orientation regression and enhance it with the addition\nof a junction detection module that also utilizes the descriptors for\nclassification purposes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Faced with changing markets and evolving consumer demands, beef industries\nare investing in grading systems to maximise value extraction throughout their\nentire supply chain. The Meat Standards Australia (MSA) system is a\ncustomer-oriented total quality management system that stands out\ninternationally by predicting quality grades of specific muscles processed by a\ndesignated cooking method. The model currently underpinning the MSA system\nrequires laborious effort to estimate and its prediction performance may be\nless accurate in the presence of unbalanced data sets where many \"muscle x\ncook\" combinations have few observations and/or few predictors of palatability\nare available. This paper proposes a novel predictive method for beef eating\nquality that bridges a spectrum of muscle x cook-specific models. At one\nextreme, each muscle x cook combination is modelled independently; at the other\nextreme a pooled predictive model is obtained across all muscle x cook\ncombinations. Via a data-driven regularization method, we cover all muscle x\ncook-specific models along this spectrum. We demonstrate that the proposed\npredictive method attains considerable accuracy improvements relative to\nindependent or pooled approaches on unique MSA data sets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This ongoing study revisits the connotations of \"digital humanists\" and\nexplores the reasons why a researcher does or does not self-identify as a\ndigital humanist. Building on semi-structured interview data collected from\nfourteen researchers and practitioners engaging in digital humanities (DH)\nprojects, this poster illustrates researchers' various understandings of\n\"digital humanist\" as a term and research identity and highlights the\ncomplexity of \"digital humanists\" as a research community. This study\ncontributes to DH scholarship with insights into the collective imaginations of\nthe digital humanist as a research community one decade after the early\nattempts. Findings of this research study also facilitate a more thorough,\ntimely, and dynamic discussion of the major workforce in digital humanities,\npotentially paving the way for future research on labor and collaboration in\nthe DH research domain.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we prove that the space $ \\mathbb{P}_p $ of all periodic\nfunction of fundamental period $ p $ is a direct sum of the space $\n\\mathbb{P}_{p/2} $ of periodic functions of fundamental period $ p/2 $ and the\nspace $ \\mathbb{AP}_{p/2} $ of antiperiodic functions of fundamental anti\nperiod $ p/2 $. The decomposition can be continued by applying the\ndecomposition process to the successively raising periodic subspaces. It is\nshown that, under certain condition, a periodic function can be written as a\nconvergent infinite series of anti periodic functions of distinct fundamental\nanti periods. In addition, we characterize the space of all periodic functions\nof period $ p \\in \\mathbb{N} $ in terms of all its periodic and antiperiodic\nsubspaces of integer periods (or anti periods). We show that the elements of a\nsubspace of such a space of periodic functions take a specific form (not\narbitrary) of linear combinations of the shifts of the elements of the given\nspace. Lastly, we introduce a lattice diagram named-periodicity diagram for a\nspace of periodic function of a fixed period $ p \\in \\mathbb{N} $. As a\nparticular example, the periodicity diagram of $ \\mathbb{P}_{12} $ is shown.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Surface Enhancement of the IceTop air-shower array will include the\naddition of radio antennas and scintillator panels, co-located with the\nexisting ice-Cherenkov tanks and covering an area of about 1 km$^2$. Together,\nthese will increase the sensitivity of the IceCube Neutrino Observatory to the\nelectromagnetic and muonic components of cosmic-ray-induced air showers at the\nSouth Pole. The inclusion of the radio technique necessitates an expanded set\nof simulation and analysis tools to explore the radio-frequency emission from\nair showers in the 70 MHz to 350 MHz band. In this paper we describe the\nsoftware modules that have been developed to work with time- and\nfrequency-domain information within IceCube's existing software framework,\nIceTray, which is used by the entire IceCube collaboration. The software\nincludes a method by which air-shower simulation, generated using CoREAS, can\nbe reused via waveform interpolation, thus overcoming a significant\ncomputational hurdle in the field.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  First order policy optimization has been widely used in reinforcement\nlearning. It guarantees to find the optimal policy for the state-feedback\nlinear quadratic regulator (LQR). However, the performance of policy\noptimization remains unclear for the linear quadratic Gaussian (LQG) control\nwhere the LQG cost has spurious suboptimal stationary points. In this paper, we\nintroduce a novel perturbed policy gradient (PGD) method to escape a large\nclass of bad stationary points (including high-order saddles). In particular,\nbased on the specific structure of LQG, we introduce a novel reparameterization\nprocedure which converts the iterate from a high-order saddle to a strict\nsaddle, from which standard random perturbations in PGD can escape efficiently.\nWe further characterize the high-order saddles that can be escaped by our\nalgorithm.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Rapid aerial grasping through robots can lead to many applications that\nutilize fast and dynamic picking and placing of objects. Rigid grippers\ntraditionally used in aerial manipulators require high precision and specific\nobject geometries for successful grasping. We propose RAPTOR, a quadcopter\nplatform combined with a custom Fin Ray gripper to enable more flexible\ngrasping of objects with different geometries, leveraging the properties of\nsoft materials to increase the contact surface between the gripper and the\nobjects. To reduce the communication latency, we present a new lightweight\nmiddleware solution based on Fast DDS (Data Distribution Service) as an\nalternative to ROS (Robot Operating System). We show that RAPTOR achieves an\naverage of 83% grasping efficacy in a real-world setting for four different\nobject geometries while moving at an average velocity of 1 m/s during grasping.\nIn a high-velocity setting, RAPTOR supports up to four times the payload\ncompared to previous works. Our results highlight the potential of aerial\ndrones in automated warehouses and other manipulation applications where speed,\nswiftness, and robustness are essential while operating in hard-to-reach\nplaces.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Photonic neuromorphic computing has emerged as a promising avenue toward\nbuilding a low-latency and energy-efficient non-von-Neuman computing system.\nPhotonic spiking neural network (PSNN) exploits brain-like spatiotemporal\nprocessing to realize high-performance neuromorphic computing. However, the\nnonlinear computation of PSNN remains a significant challenging. Here, we\nproposed and fabricated a photonic spiking neuron chip based on an integrated\nFabry-P\\'erot laser with a saturable absorber (FP-SA) for the first time. The\nnonlinear neuron-like dynamics including temporal integration, threshold and\nspike generation, refractory period, and cascadability were experimentally\ndemonstrated, which offers an indispensable fundamental building block to\nconstruct the PSNN hardware. Furthermore, we proposed time-multiplexed spike\nencoding to realize functional PSNN far beyond the hardware integration scale\nlimit. PSNNs with single/cascaded photonic spiking neurons were experimentally\ndemonstrated to realize hardware-algorithm collaborative computing, showing\ncapability in performing classification tasks with supervised learning\nalgorithm, which paves the way for multi-layer PSNN for solving complex tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The interactions of $\\bar{D}^{(*)} \\Lambda_c - \\bar{D}^{(*)}\\Sigma_c^{(*)}$\nare studied within the framework of a dynamical coupled-channel approach. A\nseries of bound states and resonances with different spin and parity are\ndynamically generated in the hidden charm sector. Four $S$-wave bound states\nare found in the mass range of 4.3 to 4.5 GeV, close to the pentaquark states\nobserved by LHCb. Two of the states have a spin parity of $J^P= 1/2^-$ and the\nother two have $J^P=3/2^-$. In addition, several resonances with different spin\nand parity in higher partial waves are predicted.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we have explored the field equations of f(T, B) gravity as an\nextension of teleparallel gravity in an isotropic and homogeneous space time.\nIn the basic formalism developed, the dynamical parameters are derived by\nincorporating the power law and exponential scale factor function. The models\nare showing accelerating behaviour and approaches to $\\Lambda$CDM at late\ntime.The present value of the equation of state parameter for both the cases\nare obtained to be in accordance with the range provided by cosmological\nobservations. The geometrical parameters and the scalar field reconstruction\nare performed to assess the viability of a late time accelerating Universe.\nFurther the stability of both the models are presented. It has been observed\nthat both the models are parameters dependent. Since most of the geometrically\nmodified theories of gravity are favouring the violation of strong energy\ncondition, we have derived the energy conditions both for the power law and\nexponential model. In both the models, the violation of strong energy condition\nestablished.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Chern insulator displays a quantized Hall effect without Landau levels.\nIn a landmark paper in 1988, Haldane showed that a Chern insulator could be\nrealized through complex next-nearest-neighbor hopping in a honeycomb lattice.\nDespite its profound impact on the field of topological physics and recent\nimplementation in cold-atom experiments, the Haldane model has remained elusive\nin solid-state materials. Here, we report the experimental realization of a\nHaldane Chern insulator in AB-stacked MoTe2/WSe2 moir\\'e bilayers, which form a\nhoneycomb moir\\'e lattice with two sublattices residing in different layers. We\nshow that the moir\\'e bilayer filled with two charge particles per unit cell is\na quantum spin Hall (QSH) insulator with a tunable charge gap. Under a small\nout-of-plane magnetic field, it becomes a Chern insulator with Chern number c=1\nfrom magneto-transport studies. The results are qualitatively captured by a\ngeneralized Kane-Mele tight-binding Hamiltonian. The Zeeman field splits the\nQSH insulator into two halves of opposite valley--one with a positive and the\nother a negative moir\\'e band gap. Our study highlights the unique potential of\nsemiconductor moir\\'e materials in engineering topological lattice\nHamiltonians.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multi-label Text Classification (MLTC) is the task of categorizing documents\ninto one or more topics. Considering the large volumes of data and varying\ndomains of such tasks, fully supervised learning requires manually fully\nannotated datasets which is costly and time-consuming. In this paper, we\npropose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text\nClassification (WSMLTC) model that reduces the need for full supervision. This\nnew model (1) produces BERT sentence embeddings and calibrates them using a\nflow model, (2) generates an initial topic-document matrix by averaging results\nof a seeded sparse topic model and a textual entailment model which only\nrequire surface name of topics and 4-6 seed words per topic, and (3) adopts a\nVAE framework to reconstruct the embeddings under the guidance of the\ntopic-document matrix. Finally, (4) it uses the means produced by the encoder\nmodel in the VAE architecture as predictions for MLTC. Experimental results on\n6 multi-label datasets show that BFV can substantially outperform other\nbaseline WSMLTC models in key metrics and achieve approximately 84% performance\nof a fully-supervised model.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  High-amplitude free stream turbulence and surface roughness elements can\nexcite a laminar boundary layer flow sufficiently to cause streamwise oriented\nvortices to develop. These vortices resemble elongated streaks having alternate\nspanwise variations of the streamwise velocity. Downstream, the vortices\n`wobble' through an inviscid secondary instability mechanism and, ultimately,\ntransition to turbulence. We formulate an optimal control algorithm to suppress\nthe growth rate of the streamwise vortex system. Considering a high Reynolds\nnumber asymptotic framework, we reduce the full compressible Navier-Stokes\nequations to the nonlinear compressible boundary region equations (NCBRE). We\nthen implement the method of Lagrange multipliers via an appropriate\ntransformation of the original constrained optimization problem into an\nunconstrained form to obtain the disturbance equations in the form of the\nadjoint compressible boundary region equations (ACBRE) and corresponding\noptimality conditions. Numerical solutions of the ACBRE approach for\nhigh-supersonic and hypersonic flows reveal a significant reduction in the\nkinetic energy and wall shear stress for all considered configurations. We\npresent contour plots to demonstrate the qualitative effect of increased\ncontrol iterations. Our results indicate that the primary vortex instabilities\ngradually flatten in the spanwise direction thanks to the ACBRE algorithm.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In settings from fact-checking to question answering, we frequently want to\nknow whether a collection of evidence (premises) entails a hypothesis. Existing\nmethods primarily focus on the end-to-end discriminative version of this task,\nbut less work has treated the generative version in which a model searches over\nthe space of statements entailed by the premises to constructively derive the\nhypothesis. We propose a system for doing this kind of deductive reasoning in\nnatural language by decomposing the task into separate steps coordinated by a\nsearch procedure, producing a tree of intermediate conclusions that faithfully\nreflects the system's reasoning process. Our experiments on the EntailmentBank\ndataset (Dalvi et al., 2021) demonstrate that the proposed system can\nsuccessfully prove true statements while rejecting false ones. Moreover, it\nproduces natural language explanations with a 17% absolute higher step validity\nthan those produced by an end-to-end T5 model.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Natural language interfaces (NLIs) provide users with a convenient way to\ninteractively analyze data through natural language queries. Nevertheless,\ninteractive data analysis is a demanding process, especially for novice data\nanalysts. When exploring large and complex SQL databases from different\ndomains, data analysts do not necessarily have sufficient knowledge about\ndifferent data tables and application domains. It makes them unable to\nsystematically elicit a series of topically-related and meaningful queries for\ninsight discovery in target domains. We develop a NLI with a step-wise query\nrecommendation module to assist users in choosing appropriate next-step\nexploration actions. The system adopts a data-driven approach to suggest\nsemantically relevant and context-aware queries for application domains of\nusers' interest based on their query logs. Also, the system helps users\norganize query histories and results into a dashboard to communicate the\ndiscovered data insights. With a comparative user study, we show that our\nsystem can facilitate a more effective and systematic data analysis process\nthan a baseline without the recommendation module.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A diagram of groupoid correspondences is a homomorphism to the bicategory of\n\\'etale groupoid correspondences. We study examples of such diagrams, including\ncomplexes of groups and self-similar higher-rank graphs. We encode the diagram\nin a single groupoid, which we call its groupoid model. The groupoid model is\ndefined so that there is a natural bijection between its actions on a space and\nsuitably defined actions of the diagram. We describe the groupoid model in\nseveral cases, including a complex of groups or a self-similar group. We show\nthat the groupoid model is a bilimit in the bicategory of groupoid\ncorrespondences.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We construct a concrete isomorphism from the permutohedral variety to the\nregular semisimple Hessenberg variety associated to the Hessenberg function\n$h_+(i)=i+1$, $1\\le i\\le n-1$. In the process of defining the isomorphism, we\nintroduce a sequence of varieties which we call the prepermutohedral varieties.\nWe first determine the toric structure of these varieties and compute the Euler\ncharacteristics and the Betti numbers using the theory of toric varieties.\nThen, we describe the cohomology of these varieties. We also find a natural way\nto encode the one-dimensional components of the cohomology using the codes\ndefined by Stembridge. Applying the isomorphisms we constructed, we are also\nable to describe the geometric structure of regular semisimple Hessenberg\nvarieties associated to the Hessenberg function represented by $h_k= (2,3,\n\\cdots, k+1, n,\\cdots,n)$, $1\\le k\\le n-3$. In particular, we are able to write\ndown the cohomology ring of the variety. Finally, we determine the dot\nrepresentation of the permutation group ${\\frak S}_n$ on these Hessenberg\nvarieties.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deflectometry as a technical approach to assessing reflective surfaces has\nnow existed for almost 40 years. Different aspects and variations of the method\nhave been studied in multiple theses and research articles, and reviews are\nalso becoming available for certain subtopics. Still a field of active\ndevelopment with many unsolved problems, deflectometry now encompasses a large\nvariety of application domains, hardware setup types, and processing workflows\ndesigned for different purposes, and spans a range from qualitative defect\ninspection of large vehicles to precision measurements of microscopic optics.\nOver these years, many exciting developments have accumulated in the underlying\ntheory, in the systems design, and in the implementation specifics. This\ndiversity of topics is difficult to grasp for experts and non-experts alike and\nmay present an obstacle to a wider acceptance of deflectometry as a useful tool\nin other research fields and in the industry.\n  This paper presents an attempt to summarize the status of deflectometry, and\nto map relations between its notable \"spin-off\" branches. The intention of the\npaper is to provide a common communication basis for practitioners and at the\nsame time to offer a convenient entry point for those interested in learning\nand using the method. The list of references is extensive but definitely not\nexhaustive, introducing some prominent trends and established research groups\nin order to facilitate further self-directed exploration by the reader.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recasting phenomenological Lagrangians in terms of SM effective field theory\n(SMEFT) provides a valuable means of connecting potential BSM physics at\nmomenta well above the electroweak scale to experimental signatures at lower\nenergies. In this work we jointly fit the Wilson coefficients of SMEFT\noperators as well as the PDFs in an extension of the CT18 global analysis\nframework, obtaining self-consistent constraints to possible BSM physics\neffects. Global fits are boosted with machine-learning techniques in the form\nof neural networks to ensure efficient scans of the full PDF+SMEFT parameter\nspace. We focus on several operators relevant for top-quark pair and jet\nproduction at hadron colliders and obtain constraints on the Wilson\ncoefficients with Lagrange Multiplier scans. We find mild correlations between\nthe extracted Wilson coefficients, PDFs, and other QCD parameters, and see\nindications that these correlations may become more prominent in future\nanalyses based on data of higher precision. This work serves as a new platform\nfor joint analyses of SM and BSM physics based on the CTEQ-TEA framework.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Information-directed sampling (IDS) has revealed its potential as a\ndata-efficient algorithm for reinforcement learning (RL). However, theoretical\nunderstanding of IDS for Markov Decision Processes (MDPs) is still limited. We\ndevelop novel information-theoretic tools to bound the information ratio and\ncumulative information gain about the learning target. Our theoretical results\nshed light on the importance of choosing the learning target such that the\npractitioners can balance the computation and regret bounds. As a consequence,\nwe derive prior-free Bayesian regret bounds for vanilla-IDS which learns the\nwhole environment under tabular finite-horizon MDPs. In addition, we propose a\ncomputationally-efficient regularized-IDS that maximizes an additive form\nrather than the ratio form and show that it enjoys the same regret bound as\nvanilla-IDS. With the aid of rate-distortion theory, we improve the regret\nbound by learning a surrogate, less informative environment. Furthermore, we\nextend our analysis to linear MDPs and prove similar regret bounds for Thompson\nsampling as a by-product.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Avoided crossings of level pairs with opposite slopes can form potential\nenergy curves for the external degree of freedom of quantum particles. We\ninvestigate nonadiabatic decay of metastable states on such avoided crossings\n(MSACs) using diabatic and adiabatic representations. The system is described\nby a single scaled adiabaticity parameter, $V$. The time-independent\ntwo-component Schr\\\"odinger equation is solved in both representations, and the\nnonadiabatic lifetimes of MSACs are determined from a wave-function flux\ncalculation and from the Breit-Wigner formula, leading to four lifetime values\nfor each MSAC. We also solve the time-dependent Schr\\\"odinger equation in both\npictures and derive the MSAC lifetimes from wave-function decay. The sets of\nsix non-perturbative values for the MSAC lifetimes agree well, validating the\napproaches. As the adiabaticity parameter $V$ is increased by about a factor of\nten, the MSAC character transitions from marginally to highly stable, with the\nlifetimes increasing by about ten orders of magnitude. The $\\nu$-dependence of\nthe lifetimes in several regimes is discussed. Time-dependent perturbation\ntheory is found to yield approximate lifetimes that deviate by $\\lesssim 30\\%$\nfrom the non-perturbative results, while predictions based on the\nsemi-classical Landau-Zener tunneling equation are found to be up to a factor\nof twenty off, over the ranges of $V$ and $\\nu$ studied. The results are\nrelevant to numerous atomic and molecular systems with quantum states on\nintersecting, coupled potential energy curves.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Autonomous drones can operate in remote and unstructured environments,\nenabling various real-world applications. However, the lack of effective\nvision-based algorithms has been a stumbling block to achieving this goal.\nExisting systems often require hand-engineered components for state estimation,\nplanning, and control. Such a sequential design involves laborious tuning,\nhuman heuristics, and compounding delays and errors. This paper tackles the\nvision-based autonomous-drone-racing problem by learning deep sensorimotor\npolicies. We use contrastive learning to extract robust feature representations\nfrom the input images and leverage a two-stage learning-by-cheating framework\nfor training a neural network policy. The resulting policy directly infers\ncontrol commands with feature representations learned from raw images, forgoing\nthe need for globally-consistent state estimation, trajectory planning, and\nhandcrafted control design. Our experimental results indicate that our\nvision-based policy can achieve the same level of racing performance as the\nstate-based policy while being robust against different visual disturbances and\ndistractors. We believe this work serves as a stepping-stone toward developing\nintelligent vision-based autonomous systems that control the drone purely from\nimage inputs, like human pilots.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Pomset logic and BV are both logics that extend multiplicative linear logic\n(with Mix) with a third connective that is self-dual and non-commutative.\nWhereas pomset logic originates from the study of coherence spaces and proof\nnets, BV originates from the study of series-parallel orders, cographs, and\nproof systems. Both logics enjoy a cut-admissibility result, but for neither\nlogic can this be done in the sequent calculus. Provability in pomset logic can\nbe checked via a proof net correctness criterion and in BV via a deep inference\nproof system. It has long been conjectured that these two logics are the same.\n  In this paper we show that this conjecture is false. We also investigate the\ncomplexity of the two logics, exhibiting a huge gap between the two. Whereas\nprovability in BV is NP-complete, provability in pomset logic is\n$\\Sigma_2^p$-complete. We also make some observations with respect to possible\nsequent systems for the two logics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the paper, we propose two models of Artificial Intelligence (AI) patents\nin European Union (EU) countries addressing spatial and temporal behaviour. In\nparticular, the models can quantitatively describe the interaction between\ncountries or explain the rapidly growing trends in AI patents. For spatial\nanalysis Poisson regression is used to explain collaboration between a pair of\ncountries measured by the number of common patents. Through Bayesian inference,\nwe estimated the strengths of interactions between countries in the EU and the\nrest of the world. In particular, a significant lack of cooperation has been\nidentified for some pairs of countries.\n  Alternatively, an inhomogeneous Poisson process combined with the logistic\ncurve growth accurately models the temporal behaviour by an accurate trend\nline. Bayesian analysis in the time domain revealed an upcoming slowdown in\npatenting intensity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $\\Bbbk$ be a field of characteristic $p>0$, $V$ a finite-dimensional\n$\\Bbbk$-vector-space, and $G$ a finite $p$-group acting $\\Bbbk$-linearly on\n$V$. Let $S = \\Sym V^*$. We show that $S^G$ is a polynomial ring if and only if\nthe dimension of its singular locus is less than $\\rank_\\Bbbk V^G$. Confirming\na conjecture of Shank-Wehlau-Broer, we show that if $S^G$ is a direct summand\nof $S$, then $S^G$ is a polynomial ring, in the following cases:\n\\begin{enumerate}\n  \\item $\\Bbbk = \\bbF_p$ and $\\rank_\\Bbbk V^G = 4$; or\n  \\item $|G| = p^3$. \\end{enumerate} In order to prove the above result, we\nalso show that if $\\rank_\\Bbbk V^G \\geq \\rank_\\Bbbk V - 2$, then the Hilbert\nideal $\\hilbertIdeal_{G,S}$ is a complete intersection.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Graph clustering is the task of partitioning a collection of observed\nnetworks into groups of similar networks. Here similarity means networks have a\nsimilar structure or graph topology. To this end, a model-based approach is\ndeveloped, where the networks are modelled by a finite mixture model of\nstochastic block models. Moreover, a computationally efficient clustering\nalgorithm is developed. The procedure is an agglomerative hierarchical\nalgorithm that maximizes the so-called integrated classification likelihood\ncriterion. The bottom-up algorithm consists of successive merges of clusters of\nnetworks. Those merges require a means to match block labels of two stochastic\nblock models to overcome the label-switching problem. This problem is addressed\nwith a new distance measure for the comparison of stochastic block models based\non their graphons. The algorithm provides a cluster hierarchy in form of a\ndendrogram and valuable estimates of all model parameters.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the first Bayesian method for tomographic decomposition of the\nplane-of-sky orientation of the magnetic field with the use of stellar\npolarimetry and distance. This standalone tomographic inversion method presents\nan important step forward in reconstructing the magnetized interstellar medium\n(ISM) in 3D within dusty regions. We develop a model in which the polarization\nsignal from the magnetized and dusty ISM is described by thin layers at various\ndistances. Our modeling makes it possible to infer the mean polarization\n(amplitude and orientation) induced by individual dusty clouds and to account\nfor the turbulence-induced scatter in a generic way. We present a likelihood\nfunction that explicitly accounts for uncertainties in polarization and\nparallax. We develop a framework for reconstructing the magnetized ISM through\nthe maximization of the log-likelihood using a nested sampling method. We test\nour Bayesian inversion method on mock data taking into account realistic\nuncertainties from $Gaia$ and as expected for the optical polarization survey\nPASIPHAE according to the currently planned observing strategy. We demonstrate\nthat our method is effective in recovering the cloud properties as soon as the\npolarization induced by a cloud to its background stars is higher than $\\sim\n0.1\\%$, for the adopted survey exposure time and level of systematic\nuncertainty. Our method makes it possible to recover not only the mean\npolarization properties but also to characterize the intrinsic scatter, thus\nopening ways to characterize ISM turbulence and the magnetic field strength.\nFinally, we apply our method to an existing dataset of starlight polarization\nwith known line-of-sight decomposition, demonstrating agreement with previous\nresults and an improved quantification of uncertainties in cloud properties.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Speech coding facilitates the transmission of speech over low-bandwidth\nnetworks with minimal distortion. Neural-network based speech codecs have\nrecently demonstrated significant improvements in quality over traditional\napproaches. While this new generation of codecs is capable of synthesizing\nhigh-fidelity speech, their use of recurrent or convolutional layers often\nrestricts their effective receptive fields, which prevents them from\ncompressing speech efficiently. We propose to further reduce the bitrate of\nneural speech codecs through the use of pretrained Transformers, capable of\nexploiting long-range dependencies in the input signal due to their inductive\nbias. As such, we use a pretrained Transformer in tandem with a convolutional\nencoder, which is trained end-to-end with a quantizer and a generative\nadversarial net decoder. Our numerical experiments show that supplementing the\nconvolutional encoder of a neural speech codec with Transformer speech\nembeddings yields a speech codec with a bitrate of $600\\,\\mathrm{bps}$ that\noutperforms the original neural speech codec in synthesized speech quality when\ntrained at the same bitrate. Subjective human evaluations suggest that the\nquality of the resulting codec is comparable or better than that of\nconventional codecs operating at three to four times the rate.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  FLRW equations are analyzed in a universe with a cosmic scalar background\nthat is spatially uniform but time-varying. Some solvable scalar potentials to\nthe combined dynamics in such a universe are presented. They are consistent\nwith the scalar dynamics as a consequence of energy momentum conservation.\nCertain potentials are found to provide very good fits to type Ia supernovae\ndata, with the kinetic and potential energies of the scalar providing the\nsource for dark matter and dark energy. The scalar rolls down the potential as\nthe universe expands, with the potential playing the role of a time-varying\ncosmological constant, modeling a scenario recently discussed in the\nliterature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Channel pruning is widely used to reduce the complexity of deep network\nmodels. Recent pruning methods usually identify which parts of the network to\ndiscard by proposing a channel importance criterion. However, recent studies\nhave shown that these criteria do not work well in all conditions. In this\npaper, we propose a novel Feature Shift Minimization (FSM) method to compress\nCNN models, which evaluates the feature shift by converging the information of\nboth features and filters. Specifically, we first investigate the compression\nefficiency with some prevalent methods in different layer-depths and then\npropose the feature shift concept. Then, we introduce an approximation method\nto estimate the magnitude of the feature shift, since it is difficult to\ncompute it directly. Besides, we present a distribution-optimization algorithm\nto compensate for the accuracy loss and improve the network compression\nefficiency. The proposed method yields state-of-the-art performance on various\nbenchmark networks and datasets, verified by extensive experiments. Our codes\nare available at: https://github.com/lscgx/FSM.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This letter deals with the optimization problems of stochastic switching\nbuffer networks, where the switching law is governed by Markov process. The\ndynamical buffer network is introduced, and its application in modeling the\ncar-sharing network is also presented. To address the nonconvexity for getting\na solution as close-to-the-global-optimal as possible of the optimization\nproblem, we adopt a succinct but effective nonconvex optimization method called\n\\emph{ DC (difference of convex functions) programming}. By resorting to the\nlog-log convexity of a class of nonlinear functions called posynomials, the\noptimization problems can be reduced to DC programming problems. Finally, we\nverify the effectiveness of our results by simulation experiments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hexahedral (hex) meshing is a long studied topic in geometry processing with\nmany fascinating and challenging associated problems. Hex meshes vary in\ncomplexity from structured to unstructured depending on application or domain\nof interest. Fully structured meshes require that all interior mesh edges are\nadjacent to exactly four hexes. Edges not satisfying this criteria are\nconsidered singular and indicate an unstructured hex mesh. Singular edges join\ntogether into singular curves that either form closed cycles, end on the mesh\nboundary, or end at a singular node, a complex junction of more than two\nsingular curves. While all hex meshes with singularities are unstructured,\nthose with more complex singular nodes tend to have more distorted elements and\nsmaller scaled Jacobian values. In this work, we study the topology of singular\nnodes. We show that all eight of the most common singular nodes are\ndecomposable into just singular curves. We further show that all singular\nnodes, regardless of edge valence, are locally decomposable. Finally we\ndemonstrate these decompositions on hex meshes, thereby decreasing their\ndistortion and converting all singular nodes into singular curves. With this\ndecomposition, the enigmatic complexity of 3D singular nodes becomes\neffectively 2D.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the effect of acoustic phonons on the quantum phase transition in\nthe O($N$) model. We develop a renormalization group analysis near (3+1)\nspace-time dimensions and derive the RG equations using an\n$\\epsilon$-expansion. Our results indicate that when the number of flavors of\nthe underlying O($N$) model exceeds a critical number $N_c=4$, the quantum\ntransition remains second-order of the Wilson-Fisher type while, for $N\\le 4$,\nit is a weakly first-order transition. We characterize this weakly first-order\ntransition by a length-scale $\\xi^*$, below which the behavior appears to be\ncritical. At finite temperatures for $N\\le 4$, a tricritical point separates\nthe weakly first-order and second-order transitions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many speech coders are based on linear prediction coding (LPC), nevertheless\nwith LPC is not possible to model the nonlinearities present in the speech\nsignal. Because of this there is a growing interest for nonlinear techniques.\nIn this paper we discuss ADPCM schemes with a nonlinear predictor based on\nneural nets, which yields an increase of 1-2.5dB in the SEGSNR over classical\nmethods. This paper will discuss the block-adaptive and sample-adaptive\npredictions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recently, Minkowski Tensors (MT) have gained popularity for morphological\nanalysis tasks. As opposed to the scalar Minkowski functionals (MF; in 2D given\nby area, perimeter and Euler characteristic), MT can characterize symmetry and\norientation of a body. This has been used for a variety of tasks, e.g. to\ndetect interstellar bubbles by tracing back the origins of filaments in\nHII-regions, or to search for alignment of structures in the CMB. I present a\nmarching-square-based method for calculating MT and MF on the sphere for maps\nin the Healpix format. MT are calculated for a local neighborhood and can then\nbe summed up/averaged over a larger region, using their additivity property.\nThis provides the possibility of localized analyses looking for CMB\nanisotropies and non-Gaussianities at varying scales.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The stripe illumination lies between the illumination in the\nspatial-frequency domain and the point illumination. Although the stripe\nillumination has a periodic structure as the illumination in the\nspatial-frequency domain, light from the stripe illumination can reach deep\nregions in biological tissue since it can be regarded as an array of point\nilluminations. For a pair of a source and a detector, the shape of light paths\nwhich connect the source and detector is called the banana shape. First, we\ninvestigate the depth of the banana. In the case of the zero boundary\ncondition, we found that the depth of the center of the banana is $d_{\\rm\nSD}/(2\\sqrt{2})$, where $d_{\\rm SD}$ is the distance between the source and\ndetector on the boundary. In general, the depth is about $0.3d_{\\rm SD}$ when\n$d_{\\rm SD}\\approx2\\,{\\rm cm}$ and the ratio of refractive indices on the\nboundary is about $1.37$. Next, we perform diffuse optical tomography for the\nstripe illumination against forward data taken by Monte Carlo simulation. We\nconsider an impulse illumination of the shape of a stripe. This time-resolved\nmeasurement is more informative than conventional continuous-wave measurements\nin the spatial-frequency domain.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To use heterogeneous hardware, programmers must have sufficient technical\nskills to utilize OpenMP, CUDA, and OpenCL. On the basis of this, I have\nproposed environment-adaptive software that enables automatic conversion,\nconfiguration, and high performance operation of once written code, in\naccordance with the hardware. However, although it has been considered to\nconvert the code according to the offload devices, there has been no study\nwhere to place the offloaded applications to satisfy users' requirements of\nprice and response time. In this paper, as a new element of environment-adapted\nsoftware, I examine a method to calculate appropriate locations using linear\nprogramming method. I confirm that applications can be arranged appropriately\nthrough simulations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We construct families of embedded, singly periodic Karcher--Scherk saddle\ntowers of any genus $g$ in the quotient with any even number $2n>2$ of almost\nparallel Scherk ends. A surface in such a family looks like $n$ parallel planes\nconnected by $n-1+g$ small catenoid necks. In the limit, the family converges\nto an $n$-sheeted vertical plane with $n-1+g$ singular points termed nodes in\nthe quotient. For the nodes to open up into catenoid necks, their locations\nmust satisfy a set of balance equations whose solutions are given by the roots\nof Stieltjes polynomials. In a subsequent paper, we will construct minimal\nsurfaces by gluing saddle towers with catenoid limits of saddle towers along\ntheir wings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent arguments that machine learning (ML) is facing a reproducibility and\nreplication crisis suggest that some published claims in ML research cannot be\ntaken at face value. These concerns inspire analogies to the replication crisis\naffecting the social and medical sciences. They also inspire calls for the\nintegration of statistical approaches to causal inference and predictive\nmodeling. A deeper understanding of what reproducibility concerns in supervised\nML research have in common with the replication crisis in experimental science\nputs the new concerns in perspective, and helps researchers avoid \"the worst of\nboth worlds,\" where ML researchers begin borrowing methodologies from\nexplanatory modeling without understanding their limitations and vice versa. We\ncontribute a comparative analysis of concerns about inductive learning that\narise in causal attribution as exemplified in psychology versus predictive\nmodeling as exemplified in ML. We identify themes that re-occur in reform\ndiscussions, like overreliance on asymptotic theory and non-credible beliefs\nabout real-world data generating processes. We argue that in both fields,\nclaims from learning are implied to generalize outside the specific environment\nstudied (e.g., the input dataset or subject sample, modeling implementation,\netc.) but are often impossible to refute due to undisclosed sources of variance\nin the learning pipeline. In particular, errors being acknowledged in ML expose\ncracks in long-held beliefs that optimizing predictive accuracy using huge\ndatasets absolves one from having to consider a true data generating process or\nformally represent uncertainty in performance claims. We conclude by discussing\nrisks that arise when sources of errors are misdiagnosed and the need to\nacknowledge the role of human inductive biases in learning and reform.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Very young (t $\\lesssim$ 10 Myrs) stars possess strong magnetic fields that\nchannel ionized gas from the interiors of their circumstellar discs to the\nsurface of the star. Upon impacting the stellar surface, the shocked gas\nrecombines and emits hydrogen spectral lines. To characterize the density and\ntemperature of the gas within these accretion streams, we measure equivalent\nwidths of Brackett (Br) 11-20 emission lines detected in 1101 APOGEE spectra of\n326 likely pre-main sequence accretors. For sources with multiple observations,\nwe measure median epoch-to-epoch line strength variations of 10% in Br11 and\n20% in Br20. We also fit the measured line ratios to predictions of radiative\ntransfer models by Kwan & Fischer. We find characteristic best-fit electron\ndensities of $n_e$ = 10$^{11} - 10^{12}$ cm$^{-3}$, and excitation temperatures\nthat are inversely correlated with electron density (from T$\\sim$5000 K for\n$n_e \\sim 10^{12}$ cm$^{-3}$, to T$\\sim$12500 K at $n_e \\sim 10^{11}$\ncm$^{-3}$). These physical parameters are in good agreement with predictions\nfrom modelling of accretion streams that account for the hydrodynamics and\nradiative transfer within the accretion stream. We also present a supplementary\ncatalog of line measurements from 9733 spectra of 4255 Brackett emission line\nsources in the APOGEE DR17 dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A nonlinear transmisson problem for an elastic full von Karman beam is\nconsidered here. We prove that the system possesses a compact global attractor.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the first meta-analysis of co-evolutionary learning networks for\ndigital disease surveillance research over last 10 years. In doing so, we show\nthe co-evolution and dynamical changes that occurred in academic research\nrelated to digital disease surveillance for improving accuracy, approach and\nresults. Using dynamic network analysis, we are able to show the incorporation\nof social media-based analytics and algorithms which have been proposed and\nlater improved by other researchers as co-evolutionary learning networks. This\nessentially demonstrates how we improve our research and increase accuracy\nthrough feedback loop for correcting the behaviour of an open system and\nperhaps infer learning patterns, reliability and validity using 10 years\nscientific research in digital disease surveillance.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Large transformer-based pre-trained language models have achieved impressive\nperformance on a variety of knowledge-intensive tasks and can capture factual\nknowledge in their parameters. We argue that storing large amounts of knowledge\nin the model parameters is sub-optimal given the ever-growing amounts of\nknowledge and resource requirements. We posit that a more efficient alternative\nis to provide explicit access to contextually relevant structured knowledge to\nthe model and train it to use that knowledge. We present LM-CORE -- a general\nframework to achieve this -- that allows \\textit{decoupling} of the language\nmodel training from the external knowledge source and allows the latter to be\nupdated without affecting the already trained model. Experimental results show\nthat LM-CORE, having access to external knowledge, achieves significant and\nrobust outperformance over state-of-the-art knowledge-enhanced language models\non knowledge probing tasks; can effectively handle knowledge updates; and\nperforms well on two downstream tasks. We also present a thorough error\nanalysis highlighting the successes and failures of LM-CORE.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  3D point cloud segmentation has made tremendous progress in recent years.\nMost current methods focus on aggregating local features, but fail to directly\nmodel long-range dependencies. In this paper, we propose Stratified Transformer\nthat is able to capture long-range contexts and demonstrates strong\ngeneralization ability and high performance. Specifically, we first put forward\na novel key sampling strategy. For each query point, we sample nearby points\ndensely and distant points sparsely as its keys in a stratified way, which\nenables the model to enlarge the effective receptive field and enjoy long-range\ncontexts at a low computational cost. Also, to combat the challenges posed by\nirregular point arrangements, we propose first-layer point embedding to\naggregate local information, which facilitates convergence and boosts\nperformance. Besides, we adopt contextual relative position encoding to\nadaptively capture position information. Finally, a memory-efficient\nimplementation is introduced to overcome the issue of varying point numbers in\neach window. Extensive experiments demonstrate the effectiveness and\nsuperiority of our method on S3DIS, ScanNetv2 and ShapeNetPart datasets. Code\nis available at https://github.com/dvlab-research/Stratified-Transformer.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we address the numerical solution of the quadratic optimal\ntransport problem in its dynamical form, the so-called Benamou-Brenier\nformulation. When solved using interior point methods, the main computational\nbottleneck is the solution of large saddle point linear systems arising from\nthe associated Newton-Raphson scheme. The main purpose of this paper is to\ndesign efficient preconditioners to solve these linear systems via iterative\nmethods. Among the proposed preconditioners, we introduce one based on the\npartial commutation of the operators that compose the dual Schur complement of\nthese saddle point linear systems, which we refer as BB-preconditioner. A\nseries of numerical tests show that the BB-preconditioner is the most efficient\namong those presented, with a CPU-time scaling only slightly more than linearly\nwith respect to the number of unknowns used to discretize the problem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study a query model of computation in which an n-vertex k-hypergraph can\nbe accessed only via its independence oracle or via its colourful independence\noracle, and each oracle query may incur a cost depending on the size of the\nquery. In each of these models, we obtain oracle algorithms to approximately\ncount the hypergraph's edges, and we unconditionally prove that no oracle\nalgorithm for this problem can have significantly smaller worst-case oracle\ncost than our algorithms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This report presents the results of the Models and Effective Field Theories\nSubgroup of the Off-Shell Interpretations Task Force in the LHC Higgs Working\nGroup. The main goal of the subgroup was to discuss and advance the potential\nimpact of off-shell Higgs measurements on searches for BSM physics carried out\nin the EFT framework or as benchmark model studies. In the first contribution,\nthe off-shell potential to resolve flat directions in parameter space for\non-shell measurements is studied. Furthermore, the sensitivity of off-shell\nmeasurements to SMEFT dimension-6 operators for the gg $\\to$ ZZ process is\ndiscussed, and studies of explicit models that are testable in off-shell\nproduction are reviewed. In the second contribution, the SMEFT effects in the\noff-shell gluon fusion and electroweak processes are discussed. Subsequently,\nthe computation of integrated and differential effects using SMEFT@NLO and\nMG5_aMC@NLO, or JHUGen and MCFM, is demonstrated. On that basis, a study of the\nprospects of obtaining additional SMEFT constraints - beyond those from\nexisting global fits - by utilising the off-shell process is presented. For\nclarification, a revised introduction, definition and discussion of the Higgs\nbasis parametrisation of the SMEFT is given in the third contribution. In short\nnotes on the SMEFT, the Higgs basis with an additional constraint is discussed\nand relations between the Higgs and Warsaw bases are presented. Lastly, an\noverview of EFT calculations and tools is given.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the first part of this paper we develop some theorems in linear algebra\napplicable to information theory when all random variables involved are linear\nfunctions of the individual bits of a source of independent bits.\n  We say that a collection of subspaces of a vector space are \"coordinated\" if\nthe vector space has a basis such that each subspace is spanned by its\nintersection with the basis. We measure the failure of a collection of\nsubspaces to be coordinated by an invariant that we call the \"discoordination\"\nof the family. We develop some foundational results regarding discoordination.\nIn particular, these results give a number of new formulas involving three\nsubspaces of a vector space.\n  We then apply a number of our results, along with a method of Tian to obtain\nsome new lower bounds in a special case of the basic coded caching problem. In\nterms of the usual notation for these problems, we show that for $N=3$\ndocuments and $K=3$ caches, we have $6M+5R\\ge 11$ for a scheme that achieves\nthe memory-rate pair $(M,R)$, assuming the scheme is linear. We also give a new\ncaching scheme for $N=K=3$ that achieves the pair $(M,R) = (1/2,5/3)$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The problem of estimating a linear functional based on observational data is\ncanonical in both the causal inference and bandit literatures. We analyze a\nbroad class of two-stage procedures that first estimate the treatment effect\nfunction, and then use this quantity to estimate the linear functional. We\nprove non-asymptotic upper bounds on the mean-squared error of such procedures:\nthese bounds reveal that in order to obtain non-asymptotically optimal\nprocedures, the error in estimating the treatment effect should be minimized in\na certain weighted $L^2$-norm. We analyze a two-stage procedure based on\nconstrained regression in this weighted norm, and establish its\ninstance-dependent optimality in finite samples via matching non-asymptotic\nlocal minimax lower bounds. These results show that the optimal non-asymptotic\nrisk, in addition to depending on the asymptotically efficient variance,\ndepends on the weighted norm distance between the true outcome function and its\napproximation by the richest function class supported by the sample size.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Alaskan landscape has undergone substantial changes in recent decades,\nmost notably the expansion of shrubs and trees across the Arctic. We developed\na dynamic statistical model to quantify the impact of climate change on the\nstructural transformation of ecosystems using remotely sensed imagery. We used\nlatent trajectory processes in a hierarchical framework to model dynamic state\nprobabilities that evolve annually, from which we derived transition\nprobabilities between ecotypes. Our latent trajectory model accommodates\ntemporal irregularity in survey intervals and uses spatio-temporally\nheterogeneous climate drivers to infer rates of land cover transitions. We\ncharacterized multi-scale spatial correlation induced by plot and subplot\narrangement in our study system. We also developed a Polya-Gamma sampling\nstrategy to improve computation. Our model facilitates inference on the\nresponse of ecosystems to shifts in the climate and can be used to predict\nfuture land cover transitions under various climate scenarios.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Post-starburst (PSB) galaxies are defined as having experienced a recent\nburst of star formation, followed by a prompt truncation in further activity.\nIdentifying the mechanism(s) causing a galaxy to experience a post-starburst\nphase therefore provides integral insight into the causes of rapid quenching.\nGalaxy mergers have long been proposed as a possible post-starburst trigger.\nEffectively testing this hypothesis requires a large spectroscopic galaxy\nsurvey to identify the rare PSBs as well as high quality imaging and robust\nmorphology metrics to identify mergers. We bring together these critical\nelements by selecting PSBs from the overlap of the Sloan Digital Sky Survey and\nthe Canada-France Imaging Survey and applying a suite of classification\nmethods: non-parametric morphology metrics such as asymmetry and Gini-M20, a\nconvolutional neural network trained to identify post-merger galaxies, and\nvisual classification. This work is therefore the largest and most\ncomprehensive assessment of the merger fraction of PSBs to date. We find that\nthe merger fraction of PSBs ranges from 19% to 42% depending on the merger\nidentification method and details of the PSB sample selection. These merger\nfractions represent an excess of 3-46x relative to non-PSB control samples. Our\nresults demonstrate that mergers play a significant role in generating PSBs,\nbut that other mechanisms are also required. However, applying our merger\nidentification metrics to known post-mergers in the IllustrisTNG simulation\nshows that ~70% of recent post-mergers (<200 Myr) would not be detected. Thus,\nwe cannot exclude the possibility that nearly all post-starburst galaxies have\nundergone a merger in their recent past.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We calculate the production of $\\chi_{c}$ and $\\eta_{c}$ by the semi-coherent\nand coherent two-photon interaction in ultra-peripheral heavy ion collisions at\nRelativistic Heavy Ion Collider (RHIC) and Large Hadron Collider (LHC)\nenergies. The differential cross section of transverse momentum distribution\nand rapidity distribution for $AB\\stackrel{\\gamma\\gamma}{\\longrightarrow}AHB$\n(H=$\\chi_{c}$ and $\\eta_{c}$), are estimated by using the equivalent photon\napproximation in ultra-peripheral nucleus-nucleus collisions. The numerical\nresults demonstrate that the experimental study of $\\chi_{c}$ and $\\eta_{c}$ in\nultra-peripheral nucleus-nucleus collisions is feasible at RHIC and LHC\nenergies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we present for the first time comprehensive and detailed\nresults on the correspondence between the extrapolated deep inelastic structure\nfunction $g_1$ of both the proton and the neutron with the same quantity\nmeasured in the nucleon resonance region. We use a QCD parameterization of the\nworld data on DIS spin structure functions, extrapolated into the nucleon\nresonance region and averaged over various intervals in the scaling variable\n$x$. We compare the results with the large data set collected in the\nquark-hadron transition region by the CLAS collaboration, averaged over the\nsame intervals. We present this comparison as a function of the momentum\ntransfer $Q^2$. We find that, depending on the averaging interval and the\nminimum momentum transfer chosen, a clear transition to quark-hadron duality\ncan be observed in both nucleon species. Furthermore, we show, for the first\ntime, the scaling behavior of $g_1$ measured in the resonance region at\nsufficiently high momentum transfer. Our results can be used to quantify the\ndeviations from the applicability of pQCD for data taken at moderate energies,\nand help with extraction of quark distribution functions from such data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose Deep Lossless Image Coding (DLIC), a full resolution learned\nlossless image compression algorithm. Our algorithm is based on a neural\nnetwork combined with an entropy encoder. The neural network performs a density\nestimation on each pixel of the source image. The density estimation is then\nused to code the target pixel, beating FLIF in terms of compression rate.\nSimilar approaches have been attempted. However, long run times make them\nunfeasible for real world applications. We introduce a parallelized GPU based\nimplementation, allowing for encoding and decoding of grayscale, 8-bit images\nin less than one second. Because DLIC uses a neural network to estimate the\nprobabilities used for the entropy coder, DLIC can be trained on domain\nspecific image data. We demonstrate this capability by adapting and training\nDLIC with Magnet Resonance Imaging (MRI) images.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In 2018, we reported our discovery of a dozen quiescent X-ray binaries in the\ncentral parsec (pc) of the Galaxy (Hailey et al. 2018). In a recent follow-up\npaper (Mori et al. 2021), we published an extended analysis of these sources\nand other X-ray binaries (XRBs) in the central pc and beyond, showing that most\nif not all of the 12 non-thermal sources are likely black hole low-mass X-ray\nbinary (BH-LMXB) candidates. In response, Maccarone et al. 2022 (TM22\nhereafter) argued, primarily on the claim that neutron star low-mass X-ray\nbinaries (NS-LMXBs) often do not have short outburst recurrence times (<~ 10\nyr), that they cannot be excluded as a designation for the 12 quiescent X-ray\nbinary sources. TM22 cites three main factors in their study: (1) X-ray\noutburst data of NS transients detected by RXTE and MAXI, (2) the Galactic\npopulation of NS-LMXBs, and (3) (persistently) quiescent NS-LMXBs in globular\nclusters. We address these arguments of TM22 and correct their\nmisunderstandings of our work and the literature, even though most of these\npoints have already been thoroughly addressed by Mori et al. 2021. We also\ncorrect TM22's assertion that our arguments are based solely on NS transients'\nrecurrence times.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The increasing complexity of interrelated systems has made the use of\nmultiplex networks an important tool for explaining the nature of relations\nbetween elements in the system. In this paper, we aim at investigating various\naspects of countries' behaviour during the coronavirus pandemic period. By\nmeans of a multiplex network we consider simultaneously stringency index\nvalues, COVID-19 infections and international trade data, in order to detect\nclusters of countries that showed a similar reaction to the pandemic. We\npropose a new methodological approach based on the Estrada communicability for\nidentifying communities on a multiplex network, based on a two-step\noptimization. At first, we determine the optimal inter-layer intensity between\nlevels by minimizing a distance function. Hence, the optimal inter-layer\nintensity is used to detect communities on each layer. Our findings show that\nthe community detection on this multiplex network has greater information power\nthan classical methods for single-layer networks. Our approach better reveals\nclusters on each layer with respect to the application of the same approach on\neach single-layer. Moreover, detected groups in the multiplex case benefit of a\nhigher cohesion, leading to identifying on each layer a lower number of\ncommunities with respect to the ones obtained in the single-layer cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Forest formulas that generalize Zimmermann's forest formula in quantum field\ntheory have been obtained for the computation of the antipode in the dual of\nenveloping algebras of pre-Lie algebras. In this work, largely motivated by\nMurua's analysis of the Baker-Campbell-Hausdorff formula, we show that the same\nideas and techniques generalize and provide effective tools to handle\ncomputations in these algebras, which are of utmost importance in numerical\nanalysis and related areas. We illustrate our results by studying the action of\nthe pre-Lie exponential and the Magnus operator in the free pre-Lie algebra and\nin a pre-Lie algebra of words originating in free probability. The latter\nexample provides combinatorial formulas relating the different brands of\ncumulants in non-commutative probability.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we carry out a semi-analytic general relativistic study of a\nGamma-Ray Bursts (GRB) jet that is breaking out of a cocoon or stellar\nenvelope. We solve hydrodynamic equations with the relativistic equation of\nstate that takes care of fluid composition. In short GRBs, a general\nrelativistic approach is required to account for curved spacetime in strong\ngravity. The piercing of the jet through the cocoon resembles a de Laval nozzle\nand the jet may go through recollimation shock transitions. We show that the\npossibility of shock transition and the shock properties are sensitive to the\nmatter composition and the cocoon strength. Obtained Lorentz factors in\nthermally driven jets comfortably reach a few $\\times$10.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many important phenomena in quantum devices are dynamic, meaning that they\ncannot be studied using time-averaged measurements alone. Experiments that\nmeasure such transient effects are collectively known as fast readout. One of\nthe most useful techniques in fast electrical readout is radio-frequency\nreflectometry, which can measure changes in impedance (both resistive and\nreactive) even when their duration is extremely short, down to a microsecond or\nless. Examples of reflectometry experiments, some of which have been realised\nand others so far only proposed, include projective measurements of qubits and\nMajorana devices for quantum computing, real-time measurements of mechanical\nmotion and detection of non-equilibrium temperature fluctuations. However, all\nof these experiments must overcome the central challenge of fast readout: the\nlarge mismatch between the typical impedance of quantum devices (set by the\nresistance quantum) and of transmission lines (set by the impedance of free\nspace). Here, we review the physical principles of radio-frequency\nreflectometry and its close cousins, measurements of radio-frequency\ntransmission and emission. We explain how to optimise the speed and sensitivity\nof a radio-frequency measurement, and how to incorporate new tools such as\nsuperconducting circuit elements and quantum-limited amplifiers into advanced\nradio-frequency experiments. Our aim is three-fold: to introduce the readers to\nthe technique, to review the advances to date and to motivate new experiments\nin fast quantum device dynamics. Our intended audience includes\nexperimentalists in the field of quantum electronics who want to implement\nradio-frequency experiments or improve them, together with physicists in\nrelated fields who want to understand how the most important radio-frequency\nmeasurements work.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In a convolution neural network, a composition of linear scalar product,\nnon-linear activation function and maximum pooling computations are intensively\ninvoked. As such, to design and implement privacy-preserving, high efficiency\nmachine learning mechanisms, one highly demands a practical crypto tool for\nsecure arithmetic computations. SPDZ, an interesting framework of secure\nmulti-party computations is a promising technique deployed for industry-scale\nmachine learning development if one is able to generate Beaver (multiplication)\ntriple offline efficiently. This paper studies secure yet efficient Beaver\ntriple generators leveraging privacy-preserving scalar product protocols which\nin turn can be constructed from additive-only homomorphic encryptions(AHEs).\nDifferent from the state-of-the-art solutions, where a party first splits her\nprivate input into a shared vector and then invokes an AHE to compute scalar\nproduct of the shared vectors managed by individual MPC server, we formalize\nBeaver triple generators in the context of 2-party shared scalar product\nprotocol and then dispense the generated shares to MPC servers. As such, the\nprotocol presented in this paper can be viewed as a dual construction of the\nstate-of-the-art AHE based solutions. Furthermore, instead of applying the\nPaillier encryption as a basis of our previous constructions or inheriting from\nsomewhat homomorphic encryptions, we propose an alternative construction of AHE\nfrom polynomial ring learning with error (RLWE) which results in an efficient\nimplementation of Beaver triple generators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Previous gait phase detection as convolutional neural network (CNN) based\nclassification task requires cumbersome manual setting of time delay or heavy\noverlapped sliding windows to accurately classify each phase under different\ntest cases, which is not suitable for streaming Inertial-Measurement-Unit (IMU)\nsensor data and fails to adapt to different scenarios. This paper presents a\nsegmentation based gait phase detection with only a single six-axis IMU sensor,\nwhich can easily adapt to both walking and running at various speeds. The\nproposed segmentation uses CNN with gait phase aware receptive field setting\nand IMU oriented processing order, which can fit to high sampling rate of IMU\nup to 1000Hz for high accuracy and low sampling rate down to 20Hz for real time\ncalculation. The proposed model on the 20Hz sampling rate data can achieve\naverage error of 8.86 ms in swing time, 9.12 ms in stance time and 96.44\\%\naccuracy of gait phase detection and 99.97\\% accuracy of stride detection. Its\nreal-time implementation on mobile phone only takes 36 ms for 1 second length\nof sensor data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The era of fast radio bursts (FRBs) was open in 2007, when a very bright\nradio pulse of unknown origin was discovered occasionally in the archival data\nof Parkes Telescope. Over the past fifteen years, this mysterious phenomenon\nhave caught substantial attention among the scientific community and become one\nof the hottest topic in high-energy astrophysics. The total number of events\nhas a dramatic increase to a few hundred recently, benefiting from new\ndedicated surveys and improved observational techniques. Our understanding of\nthese bursts has been undergoing a revolutionary growth with observational\nbreakthroughs announced consistently. In this chapter, we will give a\ncomprehensive introduction of FRBs, including the latest progress. Starting\nfrom the basics, we will go through population study, inherent physical\nmechanism, and all the way to the application in cosmology. Plenty of open\nquestions exist right now and there is more surprise to come in this active\nyoung field.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly\nprogressing to offer unprecedented spatial resolution of a diverse array of\nchemical species. In particular, they are employed to characterize on-surface\nchemical reactions by directly examining precursors and products. Chiral\neffects and self-assembled structures can also be investigated. This open\nsource, modular, python based scheme automates the categorization of a variety\nof molecules present in medium sized (10$\\times$10 to 100$\\times$100 nm)\nscanned probe images.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Blockchain systems based on a reusable resource, such as proof-of-stake\n(PoS), provide weaker security guarantees than those based on proof-of-work.\nSpecifically, they are vulnerable to long-range attacks, where an adversary can\ncorrupt prior participants in order to rewrite the full history of the chain.\nTo prevent this attack on a PoS chain, we propose a protocol that checkpoints\nthe state of the PoS chain to a proof-of-work blockchain such as Bitcoin. Our\ncheckpointing protocol hence does not rely on any central authority. Our work\nuses Schnorr signatures and leverages Bitcoin recent Taproot upgrade, allowing\nus to create a checkpointing transaction of constant size. We argue for the\nsecurity of our protocol and present an open-source implementation that was\ntested on the Bitcoin testnet.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Vision-based autonomous navigation systems rely on fast and accurate object\ndetection algorithms to avoid obstacles. Algorithms and sensors designed for\nsuch systems need to be computationally efficient, due to the limited energy of\nthe hardware used for deployment. Biologically inspired event cameras are a\ngood candidate as a vision sensor for such systems due to their speed, energy\nefficiency, and robustness to varying lighting conditions. However, traditional\ncomputer vision algorithms fail to work on event-based outputs, as they lack\nphotometric features such as light intensity and texture. In this work, we\npropose a novel technique that utilizes the temporal information inherently\npresent in the events to efficiently detect moving objects. Our technique\nconsists of a lightweight spiking neural architecture that is able to separate\nevents based on the speed of the corresponding objects. These separated events\nare then further grouped spatially to determine object boundaries. This method\nof object detection is both asynchronous and robust to camera noise. In\naddition, it shows good performance in scenarios with events generated by\nstatic objects in the background, where existing event-based algorithms fail.\nWe show that by utilizing our architecture, autonomous navigation systems can\nhave minimal latency and energy overheads for performing object detection.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  New experiments that target the B-mode polarization signals in the Cosmic\nMicrowave Background require more sensitivity, more detectors, and thus\nlarger-aperture millimeter-wavelength telescopes, than previous experiments.\nThese larger apertures require ever larger vacuum windows to house cryogenic\noptics. Scaling up conventional vacuum windows, such as those made of High\nDensity Polyethylene (HDPE), require a corresponding increase in the thickness\nof the window material to handle the extra force from the atmospheric pressure.\nThicker windows cause more transmission loss at ambient temperatures,\nincreasing optical loading and decreasing sensitivity. We have developed the\nuse of woven High Modulus Polyethylene (HMPE), a material 100 times stronger\nthan HDPE, to manufacture stronger, thinner windows using a pressurized hot\nlamination process. We discuss the development of a specialty autoclave for\ngenerating thin laminate vacuum windows and the optical and mechanical\ncharacterization of full scale science grade windows, with the goal of\ndeveloping a new window suitable for BICEP Array cryostats and for future CMB\napplications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hybrid stochastic differential equations are a useful tool to model\ncontinuously varying stochastic systems which are modulated by a random\nenvironment that may depend on the system state itself. In this paper, we\nestablish the pathwise convergence of the solutions to hybrid stochastic\ndifferential equations via space-grid discretizations. While time-grid\ndiscretizations are a classical approach for simulation purposes, our\nspace-grid discretization provides a link with multi-regime Markov modulated\nBrownian motions, leading to computational tractability. We exploit our\nconvergence result to obtain efficient approximations to first passage\nprobabilities and expected occupation times of the solutions hybrid stochastic\ndifferential equations, results which are the first of their kind for such a\nrobust framework. We finally illustrate the performance of the resulting\napproximations in numerical examples.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Accurate estimation of Intrinsic Dimensionality (ID) is of crucial importance\nin many data mining and machine learning tasks, including dimensionality\nreduction, outlier detection, similarity search and subspace clustering.\nHowever, since their convergence generally requires sample sizes (that is,\nneighborhood sizes) on the order of hundreds of points, existing ID estimation\nmethods may have only limited usefulness for applications in which the data\nconsists of many natural groups of small size. In this paper, we propose a\nlocal ID estimation strategy stable even for `tight' localities consisting of\nas few as 20 sample points. The estimator applies MLE techniques over all\navailable pairwise distances among the members of the sample, based on a recent\nextreme-value-theoretic model of intrinsic dimensionality, the Local Intrinsic\nDimension (LID). Our experimental results show that our proposed estimation\ntechnique can achieve notably smaller variance, while maintaining comparable\nlevels of bias, at much smaller sample sizes than state-of-the-art estimators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A new two-dimensional procedure for extrapolation of the values of matter,\nneutron, and proton radii obtained in no-core shell model (NCSM) calculations\nto infinite size of its basis is proposed. A relationship between the radii is\nused as an additional test. Together with the JISP16 potential, which is\nfrequently used in NCSM calculations of the radii, the Daejeon16 potential is\napplied for these purposes for the first time. Halo nucleus 6He is the object\nof studies. The small spread of radii values, successful testing using\nrelationship between the values of these three radii, and reasonable agreement\nbetween the obtained results and experimental data as well as the results of\nother advanced ab initio calculations demonstrate the high efficiency of the\ndeveloped approach and, therefore, good prospects for its application. The\nperformed investigations and analysis of the results of other studies indicate\nthat the halo of 6He has a large size 0.7 - 0.8 fm.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The dichromatic number of a digraph is the minimum size of a partition of its\nvertices into acyclic induced subgraphs. Given a class of digraphs $\\mathcal\nC$, a digraph $H$ is a hero in $\\mc C$ if $H$-free digraphs of $\\mathcal C$\nhave bounded dichromatic number. In a seminal paper, Berger at al. give a\nsimple characterization of all heroes in tournaments. In this paper, we give a\nsimple proof that heroes in quasi-transitive oriented graphs are the same as\nheroes in tournaments. We also prove that it is not the case in the class of\noriented multipartite graphs, disproving a conjecture of Aboulker, Charbit and\nNaserasr. We also give a full characterisation of heroes in oriented complete\nmultipartite graphs up to the status of a single tournament on $6$ vertices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multi-player multi-armed bandits (MMAB) study how decentralized players\ncooperatively play the same multi-armed bandit so as to maximize their total\ncumulative rewards. Existing MMAB models mostly assume when more than one\nplayer pulls the same arm, they either have a collision and obtain zero\nrewards, or have no collision and gain independent rewards, both of which are\nusually too restrictive in practical scenarios. In this paper, we propose an\nMMAB with shareable resources as an extension to the collision and\nnon-collision settings. Each shareable arm has finite shareable resources and a\n\"per-load\" reward random variable, both of which are unknown to players. The\nreward from a shareable arm is equal to the \"per-load\" reward multiplied by the\nminimum between the number of players pulling the arm and the arm's maximal\nshareable resources. We consider two types of feedback: sharing demand\ninformation (SDI) and sharing demand awareness (SDA), each of which provides\ndifferent signals of resource sharing. We design the DPE-SDI and SIC-SDA\nalgorithms to address the shareable arm problem under these two cases of\nfeedback respectively and prove that both algorithms have logarithmic regrets\nthat are tight in the number of rounds. We conduct simulations to validate both\nalgorithms' performance and show their utilities in wireless networking and\nedge computing.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we present our submission to Shared Metrics Task: RoBLEURT\n(Robustly Optimizing the training of BLEURT). After investigating the recent\nadvances of trainable metrics, we conclude several aspects of vital importance\nto obtain a well-performed metric model by: 1) jointly leveraging the\nadvantages of source-included model and reference-only model, 2) continuously\npre-training the model with massive synthetic data pairs, and 3) fine-tuning\nthe model with data denoising strategy. Experimental results show that our\nmodel reaching state-of-the-art correlations with the WMT2020 human annotations\nupon 8 out of 10 to-English language pairs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Technological advances in holography, robotics, and 3D printing are starting\nto realize the vision of a holodeck. These immersive 3D displays must address\nuser safety from the start to be viable. A holodeck's safety challenges are\nnovel because its applications will involve explicit physical interactions\nbetween humans and synthesized 3D objects and experiences in real-time. This\npioneering paper first proposes research directions for modeling safety in\nfuture holodeck applications from traditional physical human-robot interaction\nmodeling. Subsequently, we propose a test-bed to enable safety validation of\nphysical human-robot interaction based on existing augmented reality and\nvirtual simulation technology.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  News articles both shape and reflect public opinion across the political\nspectrum. Analyzing them for social bias can thus provide valuable insights,\nsuch as prevailing stereotypes in society and the media, which are often\nadopted by NLP models trained on respective data. Recent work has relied on\nword embedding bias measures, such as WEAT. However, several representation\nissues of embeddings can harm the measures' accuracy, including low-resource\nsettings and token frequency differences. In this work, we study what kind of\nembedding algorithm serves best to accurately measure types of social bias\nknown to exist in US online news articles. To cover the whole spectrum of\npolitical bias in the US, we collect 500k articles and review psychology\nliterature with respect to expected social bias. We then quantify social bias\nusing WEAT along with embedding algorithms that account for the aforementioned\nissues. We compare how models trained with the algorithms on news articles\nrepresent the expected social bias. Our results suggest that the standard way\nto quantify bias does not align well with knowledge from psychology. While the\nproposed algorithms reduce the~gap, they still do not fully match the\nliterature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Most pregnancies and births result in a good outcome, but complications are\nnot uncommon and when they do occur, they can be associated with serious\nimplications for mothers and babies. Predictive modeling has the potential to\nimprove outcomes through better understanding of risk factors, heightened\nsurveillance, and more timely and appropriate interventions, thereby helping\nobstetricians deliver better care. For three types of complications we identify\nand study the most important risk factors using Explainable Boosting Machine\n(EBM), a glass box model, in order to gain intelligibility: (i) Severe Maternal\nMorbidity (SMM), (ii) shoulder dystocia, and (iii) preterm preeclampsia. While\nusing the interpretability of EBM's to reveal surprising insights into the\nfeatures contributing to risk, our experiments show EBMs match the accuracy of\nother black-box ML methods such as deep neural nets and random forests.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Vascular segmentation extracts blood vessels from images and serves as the\nbasis for diagnosing various diseases, like ophthalmic diseases.\nOphthalmologists often require high-resolution segmentation results for\nanalysis, which leads to super-computational load by most existing methods. If\nbased on low-resolution input, they easily ignore tiny vessels or cause\ndiscontinuity of segmented vessels. To solve these problems, the paper proposes\nan algorithm named SuperVessel, which gives out high-resolution and accurate\nvessel segmentation using low-resolution images as input. We first take\nsuper-resolution as our auxiliary branch to provide potential high-resolution\ndetail features, which can be deleted in the test phase. Secondly, we propose\ntwo modules to enhance the features of the interested segmentation region,\nincluding an upsampling with feature decomposition (UFD) module and a feature\ninteraction module (FIM) with a constraining loss to focus on the interested\nfeatures. Extensive experiments on three publicly available datasets\ndemonstrate that our proposed SuperVessel can segment more tiny vessels with\nhigher segmentation accuracy IoU over 6%, compared with other state-of-the-art\nalgorithms. Besides, the stability of SuperVessel is also stronger than other\nalgorithms. We will release the code after the paper is published.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Mathematical modeling of fluid flow in a porous medium is usually described\nby a continuity equation and a chosen constitutive law. The latter, depending\non the problem at hand, may be a nonlinear relation between the fluid's\npressure gradient and velocity. The actual shape of this relation is normally\nchosen at the outset of the problem, even though, in practice, the fluid may\nexperience velocities outside of its range of applicability. We propose here an\nadaptive model, so that the most appropriate law is locally selected depending\non the computed velocity. From the analytical point of view, we show\nwell-posedness of the problem when the law is monotone in velocity and show\nexistence in one space dimension otherwise. From the computational point of\nview, we present a new approach based on regularizing via mollification the\nunderlying dissipation, i.e., the power lost by the fluid to the porous medium\nthrough drag. The resulting regularization is shown to converge to the original\nproblem using $\\Gamma$-convergence on the dissipation in the monotone case.\nThis approach gives rise to a variational numerical scheme which applies to\nvery general problems and which we validate on three test cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Language model fusion helps smart assistants recognize words which are rare\nin acoustic data but abundant in text-only corpora (typed search logs).\nHowever, such corpora have properties that hinder downstream performance,\nincluding being (1) too large, (2) beset with domain-mismatched content, and\n(3) heavy-headed rather than heavy-tailed (excessively many duplicate search\nqueries such as \"weather\"). We show that three simple strategies for selecting\nlanguage modeling data can dramatically improve rare-word recognition without\nharming overall performance. First, to address the heavy-headedness, we\ndownsample the data according to a soft log function, which tunably reduces\nhigh frequency (head) sentences. Second, to encourage rare-word exposure, we\nexplicitly filter for words rare in the acoustic data. Finally, we tackle\ndomain-mismatch via perplexity-based contrastive selection, filtering for\nexamples matched to the target domain. We down-select a large corpus of web\nsearch queries by a factor of 53x and achieve better LM perplexities than\nwithout down-selection. When shallow-fused with a state-of-the-art, production\nspeech engine, our LM achieves WER reductions of up to 24% relative on\nrare-word sentences (without changing overall WER) compared to a baseline LM\ntrained on the raw corpus. These gains are further validated through favorable\nside-by-side evaluations on live voice search traffic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The advent of Cloud Computing enabled the proliferation of IoT applications\nfor smart environments. However, the distance of these resources makes them\nunsuitable for delay-sensitive applications. Hence, Fog Computing has emerged\nto provide such capabilities in proximity to end devices through distributed\nresources. These limited resources can collaborate to serve distributed IoT\napplication workflows using the concept of stateless micro Fog service\nreplicas, which provides resiliency and maintains service availability in the\nface of failures. Load balancing supports this collaboration by optimally\nassigning workloads to appropriate services, i.e., distributing the load among\nFog nodes to fairly utilize compute and network resources and minimize\nexecution delays. In this paper, we propose using ELECTRE, a Multi-Criteria\nDecision Analysis (MCDA) approach, to efficiently balance the load in Fog\nenvironments. We considered multiple objectives to make service selection\ndecisions, including compute and network load information. We evaluate our\napproach in a realistic unbalanced topological setup with heterogeneous\nworkload requirements. To the best of our knowledge, this is the first time\nELECTRE-based methods are used to balance the load in Fog environments. Through\nsimulations, we compared the performance of our proposed approach with\ntraditional baseline methods that are commonly used in practice, namely random,\nRound-Robin, nearest node, and fastest service selection algorithms. In terms\nof the overall system performance, our approach outperforms these methods with\nup to 67% improvement.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper presents a new algorithm for the parallel in time (PiT) numerical\nsimulation of time dependent partial/ordinary differential equations. We\npropose a reliable alternative to the well know parareal in time algorithm, by\nformulating the parallel in time problem algebraically and solve it using an\nadapted Bi-Conjugate gradient stabilized method. The proposed Parallel in time\nStable Bi-Conjugate algorithm (PiTSBiCG) has a great potential in stabilizing\nthe parallel resolution for a variety of problems. In this work, we describe\nthe mathematical approach to the new algorithm and provide numerical evidences\nthat show its superiority to the standard parareal method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The access to activity of subcortical structures offers unique opportunity\nfor building intention dependent brain-computer interfaces, renders abundant\noptions for exploring a broad range of cognitive phenomena in the realm of\naffective neuroscience including complex decision making processes and the\neternal free-will dilemma and facilitates diagnostics of a range of\nneurological deceases. So far this was possible only using bulky, expensive and\nimmobile fMRI equipment. Here we present an interpretable domain grounded\nsolution to recover the activity of several subcortical regions from the\nmultichannel EEG data and demonstrate up to 60% correlation between the actual\nsubcortical blood oxygenation level dependent sBOLD signal and its EEG-derived\ntwin. Then, using the novel and theoretically justified weight interpretation\nmethodology we recover individual spatial and time-frequency patterns of scalp\nEEG predictive of the hemodynamic signal in the subcortical nuclei. The\ndescribed results not only pave the road towards wearable subcortical activity\nscanners but also showcase an automatic knowledge discovery process facilitated\nby deep learning technology in combination with an interpretable domain\nconstrained architecture and the appropriate downstream task.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Despite having the potential to provide significant insights into tactical\npreparations for future matches, very few studies have considered the spatial\ntrends of team attacking possessions in rugby league. Those which have\nconsidered these trends have used grid based aggregation methods, which provide\na discrete understanding of rugby league match play but may fail to provide a\ncomplete understanding of the spatial trends of attacking possessions due to\nthe dynamic nature of the sport. In this study, we use Kernel Density\nEstimation (KDE) to provide a continuous understanding of the spatial trends of\nattacking possessions in rugby league on a team by team basis. We use the\nWasserstein distance to understand the differences between teams (i.e. using\nall of each team's data) and within teams (i.e. using a single team's data\nagainst different opponents). Our results show that KDEs are able to provide\ninteresting tactical insights at the between team level. Furthermore, at the\nwithin team level, the results are able to show patterns of spatial trends for\nattacking teams, which are present against some opponents but not others. The\nresults could help sports practitioners to understand opposition teams'\nprevious performances and prepare tactical strategies for matches against them.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the Cauchy problem for the Schr\\\"odinger-improved Boussinesq system\nin a two dimensional domain. Under natural assumptions on the data without\nsmallness, we prove the existence and uniqueness of global strong solutions.\nMoreover, we consider the vanishing \"improvement\" limit of global solutions as\nthe coefficient of the linear term of the highest order in the equation of ion\nsound waves tends to zero. Under the same smallness assumption on the data as\nin the Zakharov case, solutions in the vanishing \"improvement\" limit are shown\nto satisfy the Zakharov system.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Good research data management is essential in modern-day lab work. Various\nsolutions exist that are either highly specific or need a significant effort to\nbe customized appropriately. This paper presents an integrated solution for\nindividuals and small groups of researchers in data-driven deductive research.\nOur electronic lab book generates itself out of notes and files, which are\ngenerated by one or several experiments. The generated electronic lab book is\nthen presented on a Django-based website. The automated gathering of metadata\nsignificantly reduces the documentation effort for the lab worker and prevents\nhuman error in the repetitive task of manually entering basic meta-data. The\nskilled user can quickly adapt the electronic lab book software to his needs\nbecause the software employs widely used open-source software libraries with\nactive communities and excellent documentation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum Chaos has originally emerged as the field which studies how the\nproperties of classical chaotic systems arise in their quantum counterparts.\nThe growing interest in quantum many-body systems, with no obvious classical\nmeaning has led to consider time-dependent quantities that can help to\ncharacterize and redefine Quantum Chaos. This article reviews the prominent\nrole that the out of time ordered correlator (OTOC) plays to achieve such goal.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this contribution we have collected some facts about Killing and\nKilling-Yano tensors that we feel are of general interest for researchers\nworking on problems that rely on differential geometry. We also include some of\nour recent studies pertaining to currents, charges and (super)invariants for\nparticles and tensionless strings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Dilatons (and moduli) couple to the masses and coupling constants of ordinary\nmatter, and these quantities are fixed by the local value of the dilaton field.\nIf, in addition, the dilaton with mass $m_\\phi$ contributes to the cosmic dark\nmatter density, then such quantities oscillate in time at the dilaton Compton\nfrequency. We show how these oscillations lead to broadening and shifting of\nthe Voigt profile of the Ly$\\alpha$ forest, in a manner that is correlated with\nthe local dark matter density. We further show how tomographic methods allow\nthe effect to be reconstructed by observing the Ly$\\alpha$ forest spectrum of\ndistant quasars. We then simulate a large number of quasar lines of sight using\nthe lognormal density field, and forecast the ability of future astronomical\nsurveys to measure this effect. We find that in the ultra low mass range\n$10^{-32}\\text{ eV}\\leq m_\\phi\\leq 10^{-28}\\text{ eV}$ upcoming observations\ncan improve over existing limits to the dilaton electron mass and fine\nstructure constant couplings set by fifth force searches by up to five orders\nof magnitude. Our projected limits apply assuming that the ultralight dilaton\nmakes up a few percent of the dark matter density, consistent with upper limits\nset by the cosmic microwave background anisotropies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We prove the following formula for the ground state energy density of a\ndilute Bose gas with density $\\rho$ in $2$ dimensions in the thermodynamic\nlimit \\begin{align*} e^{\\rm{2D}}(\\rho) = 4\\pi \\rho^2 Y\\left(1 - Y \\vert \\log Y\n\\vert + \\left( 2\\Gamma + \\frac{1}{2} + \\log(\\pi) \\right) Y \\right) + o(\\rho^2\nY^{2}). \\end{align*} Here $Y= |\\log(\\rho a^2)|^{-1}$ and $a$ is the scattering\nlength of the two-body potential. This result in $2$ dimensions corresponds to\nthe famous Lee-Huang-Yang formula in $3$ dimensions. The proof is valid for\nessentially all positive potentials with finite scattering length, in\nparticular it covers the crucial case of the hard core potential.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, based on some prior estimates, we show that the essential\nspectrum $\\lambda=0$ is a bifurcation point for an superlinear elliptic\nequation with only local conditions, which generalizes a series of earlier\nresults on an open problem proposed by C. A. Stuart in 1983 [Lecture Notes in\nMathematics, 1017].\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent theoretical work has highlighted that quantizing a superconducting\ncircuit in the presence of time-dependent flux $\\Phi(t)$ generally produces\nHamiltonian terms proportional to $d\\Phi/dt$ unless a special allocation of the\nflux across inductive terms is chosen. Here, we present an experiment probing\nthe effects of a fast flux ramp applied to a heavy-fluxonium circuit. The\nexperiment confirms that na\\\"ive omission of the $d\\Phi/dt$ term leads to\ntheoretical predictions inconsistent with experimental data. Experimental data\nare fully consistent with recent theory that includes the derivative term or\nequivalently uses \"irrotational variables\" that uniquely allocate the flux to\nproperly eliminate the $d\\Phi/dt$ term.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Circadian rhythms are a process of the sleep-wake cycle that regulates the\nphysical, mental and behavioural changes in all living beings with a period of\nroughly 24 h. Wearable accelerometers are typically used in livestock\napplications to record animal movement from which we can estimate the activity\ntype. Here, we use the overall movement recorded by accelerometers worn on the\nnecks of newborn calves for a period of 8 weeks. From the movement data, we\ncalculate 24 h periodicity intensities corresponding to circadian rhythms, from\na 7-day window that slides through up to 8-weeks of data logging. The strength\nor intensity of the 24 h periodicity is computed at intervals as the calves\nbecome older, which is an indicator of individual calf welfare. We observe that\nthe intensities of these 24 h periodicities for individual calves, derived from\nmovement data, increase and decrease synchronously in a herd of 19 calves. Our\nresults show that external factors affecting the welfare of the herd can be\nobserved by processing and visualising movement data in this way and our method\nreveals insights that are not observable from movement data alone.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Expectation-Maximization (EM) algorithm has been predominantly used to\napproximate the maximum likelihood estimation of the location-scale Gaussian\nmixtures. However, when the models are over-specified, namely, the chosen\nnumber of components to fit the data is larger than the unknown true number of\ncomponents, EM needs a polynomial number of iterations in terms of the sample\nsize to reach the final statistical radius; this is computationally expensive\nin practice. The slow convergence of EM is due to the missing of the locally\nstrong convexity with respect to the location parameter on the negative\npopulation log-likelihood function, i.e., the limit of the negative sample\nlog-likelihood function when the sample size goes to infinity. To efficiently\nexplore the curvature of the negative log-likelihood functions, by specifically\nconsidering two-component location-scale Gaussian mixtures, we develop the\nExponential Location Update (ELU) algorithm. The idea of the ELU algorithm is\nthat we first obtain the exact optimal solution for the scale parameter and\nthen perform an exponential step-size gradient descent for the location\nparameter. We demonstrate theoretically and empirically that the ELU iterates\nconverge to the final statistical radius of the models after a logarithmic\nnumber of iterations. To the best of our knowledge, it resolves the\nlong-standing open question in the literature about developing an optimization\nalgorithm that has optimal statistical and computational complexities for\nsolving parameter estimation even under some specific settings of the\nover-specified Gaussian mixture models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Mixed Sample Regularization (MSR), such as MixUp or CutMix, is a powerful\ndata augmentation strategy to generalize convolutional neural networks.\nPrevious empirical analysis has illustrated an orthogonal performance gain\nbetween MSR and the conventional offline Knowledge Distillation (KD). To be\nmore specific, student networks can be enhanced with the involvement of MSR in\nthe training stage of the sequential distillation. Yet, the interplay between\nMSR and online knowledge distillation, a stronger distillation paradigm, where\nan ensemble of peer students learn mutually from each other, remains\nunexplored. To bridge the gap, we make the first attempt at incorporating\nCutMix into online distillation, where we empirically observe a significant\nimprovement. Encouraged by this fact, we propose an even stronger MSR\nspecifically for online distillation, named as Cut^nMix. Furthermore, a novel\nonline distillation framework is designed upon Cut^nMix, to enhance the\ndistillation with feature level mutual learning and a self-ensemble teacher.\nComprehensive evaluations on CIFAR10 and CIFAR100 with six network\narchitectures show that our approach can consistently outperform\nstate-of-the-art distillation methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Full waveform inversion (FWI) is one of a family of methods that allows the\nreconstruction of earth subsurface parameters from measurements of waves at or\nnear the surface. This is a numerical optimization problem that uses the whole\nwaveform information of all arrivals to update the subsurface parameters that\ngovern seismic wave propagation. We apply FWI in the multi-scale approach on\ntwo well-known benchmarks: Marmousi and 2004 BP velocity model. For the forward\nmodeling, we use an RBF-FD solver on hexagonal grids and quasi-optimal shape\nparameters.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Sinkhorn operator has recently experienced a surge of popularity in\ncomputer vision and related fields. One major reason is its ease of integration\ninto deep learning frameworks. To allow for an efficient training of respective\nneural networks, we propose an algorithm that obtains analytical gradients of a\nSinkhorn layer via implicit differentiation. In comparison to prior work, our\nframework is based on the most general formulation of the Sinkhorn operator. It\nallows for any type of loss function, while both the target capacities and cost\nmatrices are differentiated jointly. We further construct error bounds of the\nresulting algorithm for approximate inputs. Finally, we demonstrate that for a\nnumber of applications, simply replacing automatic differentiation with our\nalgorithm directly improves the stability and accuracy of the obtained\ngradients. Moreover, we show that it is computationally more efficient,\nparticularly when resources like GPU memory are scarce.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A well known conjecture of Burr and Erdos asserts that the Ramsey number\n$r(Q_n)$ of the hypercube $Q_n$ on $2^n$ vertices is of the order $O(2^n)$. In\nthis paper, we show that $r(Q_n)=O(2^{2n-c n})$ for a universal constant $c>0$,\nimproving upon the previous best known bound $r(Q_n)=O(2^{2n})$, due to Conlon,\nFox and Sudakov.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a CO(3-2) study of four systems composed of six (ultra) luminous\ninfrared galaxies (U/LIRGs), located at 0.28 < z < 0.44, that straddle the\ntransition region between regular star forming galaxies and starbursts. These\ngalaxies benefit from previous multi-wavelength analysis allowing in depth\nexploration of an understudied population of U/LIRGs at a time when the\nuniverse is experiencing a rapid decline in star formation rate density. We\ndetect CO(3-2) emission in four targets and these galaxies fall between the\nloci of regular star forming galaxies and starbursts on the Kennicutt-Schmidtt\nrelation. Compared to low luminosity LIRGs and high luminosity ULIRGs at\nsimilar redshifts, we find they all have similar molecular gas budgets with the\ndifference in their star formation rates (SFR) driven by the star formation\nefficiency (SFE). This suggests that at these redshifts large molecular gas\nreservoirs must coincide with an increased SFE to transition a galaxy into the\nstarburst regime. We studied the structure and kinematics and found our four\ndetections are either interacting or have disturbed morphology which may be\ndriving the SFE. One of the CO(3-2) non-detections has a strong continuum\ndetection, and has been previously observed in H$\\alpha$, suggesting an unusual\ninterstellar medium for a ULIRG. We conclude that our sample of transitioning\nU/LIRGs fill the gap between regular star forming galaxies and starbursts,\nsuggest a continuous change in SFE between these two populations and the\nincreased SFE may be driven by morphology and differing stages of interaction.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Random walks simulate the randomness of objects, and are key instruments in\nvarious fields such as computer science, biology and physics. The counter part\nof classical random walks in quantum mechanics are the quantum walks. Quantum\nwalk algorithms provide an exponential speedup over classical algorithms.\nClassical and quantum random walks can be applied in social network analysis,\nand can be used to define specific centrality metrics in terms of node\noccupation on single-layer and multilayer networks. In this paper, we applied\nthese new centrality measures to three real criminal networks derived from an\nanti-mafia operation named Montagna and a multilayer network derived from them.\nOur aim is to (i) identify leaders in our criminal networks, (ii) study the\ndependence between these centralities and the degree, (iii) compare the results\nobtained for the real multilayer criminal network with those of a synthetic\nmultilayer network which replicates its structure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a family of models, which we name matrix models associated with\nchildren's drawings -- the so-called dessin d'enfant. Dessins d'enfant are\ngraphs of a special kind drawn on a closed connected orientable surface (in the\nsky). The vertices of such a graph are small disks that we call stars. We\nattach random matrices to the edges of the graph and get multimatrix models.\nAdditionally, to the stars we attach source matrices. They play the role of\nfree parameters or model coupling constants. The answers for our integrals are\nexpressed through quantities that we call the \"spectrum of stars.\" The answers\nmay also include some combinatorial numbers, such as Hurwitz numbers or\ncharacters from group representation theory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This article concerns a question asked by M. V. Nori on homotopy of sections\nof Projective modules defined on the polynomial algebra over a smooth affine\ndomain $R$. While this question has an affirmative answer, it is known that the\nassertion does not hold if: (1) $\\dim(R)=2$; or (2) $d\\geq 3$ but $R$ is not\nsmooth. We first prove that an affirmative answer can be given for $\\dim(R)=2$\nwhen $R$ is an $\\bar{\\mathbb{F}}_p$-algebra. Next, for $d\\geq 3$ we find the\nprecise obstruction for the failure in the singular case. Further, we improve a\nresult of Mandal (related to Nori's question) in the case when the ring $A$ is\nan affine $\\bar{\\mathbb{F}}_p$-algebra of dimension $d$. We apply this\nimprovement to define the $n$-th Euler class group $E^n(A)$, where $2n\\ge d+2.$\nMoreover, if $A$ is smooth, we associate to a unimodular row $v$ of length\n$n+1$ its Euler class $e(v)\\in E^n(A)$ and show that the corresponding stably\nfree module, say, $P(v)$ has a unimodular element if and only if $e(v)$\nvanishes in $E^n(A)$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Nonzero mean value of phonon angular momentum (PAM) in chiral materials can\nbe generated when a temperature gradient is applied. We find that both diagonal\nand off-diagonal terms of PAM contribute to mean PAM by using the Kubo formula\nwhere both diagonal and off-diagonal elements of the heat current operator are\nconsidered. The calculation results show that the off-diagonal term is dominant\nwhen the phonon scattering is strong enough. This finding reveals that the\nquantum transition between different phonon modes induced by temperature\ngradient strongly affects the local atomic rotation. Our discovery provides an\nexplanation of the recently observed chiral phonon activated spin Seebeck\neffect.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Solid body tides provide key information on the interior structure,\nevolution, and origin of the planetary bodies. Our Solar system harbours a very\ndiverse population of planetary bodies, including those composed of rock, ice,\ngas, or a mixture of all. While a rich arsenal of geophysical methods has been\ndeveloped over several years to infer knowledge about the interior of the\nEarth, the inventory of tools to investigate the interiors of other\nSolar-system bodies remains limited. With seismic data only available for the\nEarth, the Moon, and Mars, geodetic measurements, including the observation of\nthe tidal response, have become especially valuable and therefore, has played\nan important role in understanding the interior and history of several Solar\nsystem bodies. To use tidal response measurements as a means to obtain\nconstraints on the interior structure of planetary bodies, appropriate\nunderstanding of the viscoelastic reaction of the materials from which the\nplanets are formed is needed. Here, we review the fundamental aspects of the\ntidal modeling and the information on the present-day interior properties and\nevolution of several planets and moons based on studying their tidal response.\nWe begin with an outline of the theory of viscoelasticity and tidal response.\nNext, we proceed by discussing the information on the tidal response and the\ninferred structure of Mercury, Venus, Mars and its moons, the Moon, and the\nlargest satellites of giant planets, obtained from the analysis of the data\nthat has been provided by space missions. We also summarise the upcoming\npossibilities offered by the currently planned missions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This article is a continuation of our investigations in the function space\n$C(X)$ with respect to the topology $\\tau^s_\\mathfrak{B}$ of strong uniform\nconvergence on $\\mathfrak{B}$ in line of (Chandra et al. 2020 \\cite{dcpdsd} and\nDas et al. 2022 \\cite{pddcsd-3}) using the idea of strong uniform convergence\n(Beer and Levi, 2009 \\cite{bl}) on a bornology. First we focus on the notion of\ntightness property of $(C(X),\\tau^s_\\mathfrak{B})$ and some of its variations\nsuch as supertightness, Id-fan tightness and $T$-tightness. Certain situations\nare discussed when $C(X)$ is a {\\rm k}-space with respect to the topology\n$\\tau^s_\\mathfrak{B}$. Next the notions of strong $\\mathfrak{B}$-open game and\n$\\gamma_{\\mathfrak{B}^s}$-open game on $X$ are introduced and some of its\nconsequences are investigated. Finally, we consider discretely selective\nproperty and related games. On $(C(X),\\tau^s_\\mathfrak{B})$ several\ninteractions between topological games related to discretely selective\nproperty, the Gruenhage game on $(C(X),\\tau^s_\\mathfrak{B})$ and certain games\non $X$ are presented.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Ivlev's pioneering work started in the 1970's showed a new and promissory way\nin the study of modal logic from the perspective of many-valued logics.\nContinuing our previous work on Ivlev-like non-normal modal logics with\nnon-deterministic semantics, we present in this paper tableau systems for Tm,\nS4m and S5m, the non-normal versions of T, S4 and S5, respectively, as well as\nfor their corresponding first-order extensions Tm*, S4m* and S5m*.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Few-shot learning (FSL), purposing to resolve the problem of data-scarce, has\nattracted considerable attention in recent years. A popular FSL framework\ncontains two phases: (i) the pre-train phase employs the base data to train a\nCNN-based feature extractor. (ii) the meta-test phase applies the frozen\nfeature extractor to novel data (novel data has different categories from base\ndata) and designs a classifier for recognition. To correct few-shot data\ndistribution, researchers propose Semi-Supervised Few-Shot Learning (SSFSL) by\nintroducing unlabeled data. Although SSFSL has been proved to achieve\noutstanding performances in the FSL community, there still exists a fundamental\nproblem: the pre-trained feature extractor can not adapt to the novel data\nflawlessly due to the cross-category setting. Usually, large amounts of noises\nare introduced to the novel feature. We dub it as Feature-Extractor-Maladaptive\n(FEM) problem. To tackle FEM, we make two efforts in this paper. First, we\npropose a novel label prediction method, Isolated Graph Learning (IGL). IGL\nintroduces the Laplacian operator to encode the raw data to graph space, which\nhelps reduce the dependence on features when classifying, and then project\ngraph representation to label space for prediction. The key point is that: IGL\ncan weaken the negative influence of noise from the feature representation\nperspective, and is also flexible to independently complete training and\ntesting procedures, which is suitable for SSFSL. Second, we propose Graph\nCo-Training (GCT) to tackle this challenge from a multi-modal fusion\nperspective by extending the proposed IGL to the co-training framework. GCT is\na semi-supervised method that exploits the unlabeled samples with two modal\nfeatures to crossly strengthen the IGL classifier.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We formulate a phenomenological model to study the power applied by a cyclist\non a velodrome -- for individual timetrials -- taking into account the\nstraights, circular arcs, connecting transition curves, and banking. The\ndissipative forces we consider are air resistance, rolling resistance, lateral\nfriction and drivetrain resistance. Also, in general, the power is used to\nincrease the kinetic and potential energy. However, to model a steady ride --\nas expected for individual timetrials -- we assume a constant centre-of-mass\nspeed and allow the cadence and power to vary during a lap. Hence, the only\nmechanical energy to consider is the increase of potential energy due to\nraising the centre of mass upon exiting each curve. Following derivations and\njustifications of expressions that constitute this mathematical model, we\npresent a numerical example. We show that, as expected, the cadence and power\nvary only slightly during a steady ride. Also, we examine changes in the\nrequired average power per lap due to modifications of various quantities, such\nas air density at a velodrome, laptime and several others, as well as the model\nsensitivity to input errors. Such an examination is of immediate use in\nstrategizing the performance for individual pursuits and the Hour Record, as\nwell as in evaluating the reliability of model predictions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Rare genetic disorders affect more than 6% of the global population. Reaching\na diagnosis is challenging because rare disorders are very diverse. Many\ndisorders have recognizable facial features that are hints for clinicians to\ndiagnose patients. Previous work, such as GestaltMatcher, utilized\nrepresentation vectors produced by a DCNN similar to AlexNet to match patients\nin high-dimensional feature space to support \"unseen\" ultra-rare disorders.\nHowever, the architecture and dataset used for transfer learning in\nGestaltMatcher have become outdated. Moreover, a way to train the model for\ngenerating better representation vectors for unseen ultra-rare disorders has\nnot yet been studied. Because of the overall scarcity of patients with\nultra-rare disorders, it is infeasible to directly train a model on them.\nTherefore, we first analyzed the influence of replacing GestaltMatcher DCNN\nwith a state-of-the-art face recognition approach, iResNet with ArcFace.\nAdditionally, we experimented with different face recognition datasets for\ntransfer learning. Furthermore, we proposed test-time augmentation, and model\nensembles that mix general face verification models and models specific for\nverifying disorders to improve the disorder verification accuracy of unseen\nultra-rare disorders. Our proposed ensemble model achieves state-of-the-art\nperformance on both seen and unseen disorders.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Most computer algebra systems (CAS) support symbolic integration as core\nfunctionality. The majority of the integration packages use a combination of\nheuristic algebraic and rule-based (integration table) methods. In this paper,\nwe present a hybrid (symbolic-numeric) methodology to calculate the indefinite\nintegrals of univariate expressions. The primary motivation for this work is to\nadd symbolic integration functionality to a modern CAS (the symbolic\nmanipulation packages of SciML, the Scientific Machine Learning ecosystem of\nthe Julia programming language), which is mainly designed toward numerical and\nmachine learning applications and has a different set of features than\ntraditional CAS. The symbolic part of our method is based on the combination of\ncandidate terms generation (borrowed from the Homotopy operators theory) with\nrule-based expression transformations provided by the underlying CAS. The\nnumeric part is based on sparse-regression, a component of Sparse\nIdentification of Nonlinear Dynamics (SINDy) technique. We show that this\nsystem can solve a large variety of common integration problems using only a\nfew dozen basic integration rules.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Neutron captures and delayed decays of reaction products are common sources\nof backgrounds in ultra-rare event searches. In this work, we studied\n$^{13}$C($\\alpha,n)^{16}$O reactions induced by $\\alpha$-particles emitted\nwithin the calibration sources of the \\textsc{Majorana Demonstrator}. These\nsources are thorium-based calibration standards enclosed in carbon-rich\nmaterials. The reaction rate was estimated by using the 6129-keV $\\gamma$-rays\nemitted from the excited $^{16}$O states that are populated when the incoming\n$\\alpha$-particles exceed the reaction Q-value. Thanks to the excellent energy\nperformance of the \\textsc{Demonstrator}'s germanium detectors, these\ncharacteristic photons can be clearly observed in the calibration data.\nFacilitated by \\textsc{Geant4} simulations, a comparison between the observed\n6129-keV photon rates and predictions by a TALYS-based software was performed.\nThe measurements and predictions were found to be consistent, albeit with large\nstatistical uncertainties. This agreement provides support for background\nprojections from ($\\alpha,n$)-reactions in future double-beta decay search\nefforts.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a brief introduction to the Dyson-Schwinger equations (DSEs)\napproach to hadron and high-energy physics. In particular, how this formalism\nis applied to calculate the electromagnetic form factors $\\gamma^* \\gamma^* \\to\n\\textbf{P}^0$ and $\\gamma^* \\textbf{P}^\\pm \\to \\textbf{P}^\\pm$ (with\n$\\textbf{P}^\\pm$ and $\\textbf{P}^0$ charged and neutral ground-state\npseudoscalar mesons, respectively) is discussed. Subsequently, the\ncorresponding contributions of those form factors to the muon anomalous\nmagnetic moment ($g-2$) are estimated. We look forward to promoting the DSE\napproach to address theoretical aspects of the muon $g-2$, highlighting some\ncalculations that could be carried out in the future.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose a novel end-to-end user-defined keyword spotting\nmethod that utilizes linguistically corresponding patterns between speech and\ntext sequences. Unlike previous approaches requiring speech keyword enrollment,\nour method compares input queries with an enrolled text keyword sequence. To\nplace the audio and text representations within a common latent space, we adopt\nan attention-based cross-modal matching approach that is trained in an\nend-to-end manner with monotonic matching loss and keyword classification loss.\nWe also utilize a de-noising loss for the acoustic embedding network to improve\nrobustness in noisy environments. Additionally, we introduce the LibriPhrase\ndataset, a new short-phrase dataset based on LibriSpeech for efficiently\ntraining keyword spotting models. Our proposed method achieves competitive\nresults on various evaluation sets compared to other single-modal and\ncross-modal baselines.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Doped Mott insulators exhibit some of the most intriguing quantum phases of\nmatter, including quantum spin-liquids, unconventional superconductors, and\nnon-Fermi liquid metals. Such phases often arise when itinerant electrons are\nclose to a Mott insulating state, and thus experience strong spatial\ncorrelations. Proximity between different layers of van der Waals\nheterostructures naturally realizes a platform for experimentally studying the\nrelationship between localized, correlated electrons and itinerant electrons.\nHere, we explore this relationship by studying the magnetic landscape of\n4Hb-TaS2, which realizes an alternate stack of a candidate spin liquid and a\nsuperconductor. We report on a spontaneous vortex phase whose vortex density\ncan be trained in the normal state. We show that time reversal symmetry is\nbroken above Tc, indicating the presence of a magnetic phase independent of the\nsuperconductor. Strikingly, this phase does not generate detectable magnetic\nsignals. We use scanning superconducting quantum interference device (SQUID)\nmicroscopy to show that it is incompatible with ferromagnetic ordering. The\ndiscovery of this new form of hidden magnetism illustrates how combining\nsuperconductivity with a strongly correlated system can lead to new, unexpected\nphysics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Partial maximum distance separable (PMDS) codes are a kind of erasure codes\nwhere the nodes are divided into multiple groups with each forming an MDS code\nwith a smaller code length, thus they allow repairing a failed node with only a\nfew helper nodes and can correct all erasure patterns that are\ninformation-theoretically correctable. However, the repair of a failed node of\nPMDS codes still requires a large amount of communication if the group size is\nlarge. Recently, PMDS array codes with each local code being an MSR code were\nintroduced to reduce the repair bandwidth further. However, they require\nextensive rebuilding access and unavoidably a significant sub packetization\nlevel. In this paper, we first propose two constructions of PMDS array codes\nwith two global parities that have smaller sub-packetization levels and much\nsmaller finite fields than the existing one. One construction can support an\narbitrary number of local parities and has $(1+\\epsilon)$-optimal repair\nbandwidth (i.e., $(1+\\epsilon)$ times the optimal repair bandwidth), while the\nother one is limited to two local parities but has significantly smaller\nrebuilding access and its sub packetization level is only $2$. In addition, we\npresent a construction of PMDS array code with three global parities, which has\na smaller sub-packetization level as well as $(1+\\epsilon)$-optimal repair\nbandwidth, the required finite field is significantly smaller than existing\nones.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hybrid excitations of light and matter, namely, polaritons, in the\nultrastrong coupling regime have been intensively investigated to explore novel\nmaterial functions and realize coherent control of material properties by\noptical means. However, realization of ultrastrong coupling in a-few-electron\nsystems has been challenging, because the electronic dipole moment decreases\nwith decreasing electron numbers in the system. Here, we fabricate a\ngate-defined quantum dot (QD) in the vicinity of a gap of a terahertz (THz)\nsplit-ring resonator (SRR). By illuminating the system with external THz\nradiation, the QD shows a current change whose spectrum exhibits anti-crossing\nbehavior between the resonant excitation of the quantized electronic states and\nthe resonance mode of the SRR. Our result indicates that, owing to the field\nenhancement by the THz SRR, the system enters the ultrastrong coupling regime\neven when only a few electrons reside in the QD.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Wireless communication offers many benefits for control such as substantially\nreduced deployment costs, higher flexibility, as well as easier data access. It\nis thus not surprising that smart and wireless sensors and actuators are\nincreasingly used in industry. With these enhanced possibilities, exciting new\ntechnologies such as Control-as-a-Service arise, where (for example) controller\ndesign or tuning based on input-output-data can be outsourced to a cloud or\nmobile device. This implies, however, that sensitive plant information may\nbecome available to service providers or, possibly, attackers.\n  Against this background, we focus on privacy-preserving optimal PID tuning\nas-a-Service here. In particular, we combine homomorphic encryption with\nextremum seeking in order to provide a purely data-driven and confidential\ntuning algorithm. The encrypted realization requires several adaptions of\nestablished extremum seekers. These encompass relative parameter updates,\nstochastic gradient approximations, and a normalized objective function. As a\nresult, and as illustrated by various numerical examples, the proposed\nencrypted extremum seeker is able to tune PID controllers for a wide variety of\nplants without being too conservative.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We solve the Wheeler-DeWitt equation for the planar AdS-Schwarzschild\ninterior in a minisuperspace approximation involving the volume and spatial\nanisotropy of the interior. A Gaussian wavepacket is constructed that is peaked\non the classical interior solution. Simple observables are computed using this\nwavepacket, demonstrating the freedom to a choose a relational notion of\n`clock' in the interior and characterizing the approach to the spacelike\nsingularity. The Wheeler-DeWitt equation may be extended out through the\nhorizon, where it describes the holographic renormalization group flow of the\nblack hole exterior. This amounts to the Hamilton-Jacobi evolution of the\nmetric component $g_{tt}$ from positive interior values to negative exterior\nvalues. The interior Gaussian wavepacket is shown to evolve into the Lorentizan\npartition function of the boundary conformal field theory over a microcanonical\nenergy window.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present our current understanding of the formation and early evolution of\nprotostars, protoplanetary disks, and the driving of outflows as dictated by\nthe interplay of magnetic fields and partially ionized gas in molecular cloud\ncores. In recent years, the field has witnessed enormous development through\nsub-millimeter observations which in turn have constrained models of protostar\nformation. As a result of these observations % that the observations provided,\nthe state-of-the-art theoretical understanding of the formation and evolution\nof young stellar objects is described. In particular, we emphasize the\nimportance of the coupling, decoupling, and re-coupling between weakly ionized\ngas and the magnetic field on appropriate scales. This highlights the complex\nand intimate relationship between gravitational collapse and magnetic fields in\nyoung protostars.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider a variant of the prize collecting Steiner tree problem in which\nwe are given a \\emph{directed graph} $D=(V,A)$, a monotone submodular prize\nfunction $p:2^V \\rightarrow \\mathbb{R}^+ \\cup \\{0\\}$, a cost function $c:V\n\\rightarrow \\mathbb{Z}^{+}$, a root vertex $r \\in V$, and a budget $B$. The aim\nis to find an out-subtree $T$ of $D$ rooted at $r$ that costs at most $B$ and\nmaximizes the prize function. We call this problem \\emph{Directed Rooted\nSubmodular Tree} (\\textbf{DRSO}).\n  Very recently, Ghuge and Nagarajan [SODA\\ 2020] gave an optimal\nquasi-polynomial-time $O\\left(\\frac{\\log n'}{\\log \\log\nn'}\\right)$-approximation algorithm, where $n'$ is the number of vertices in an\noptimal solution, for the case in which the costs are associated to the edges.\n  In this paper, we give a polynomial-time algorithm for \\textbf{DRSO} that\nguarantees an approximation factor of $O(\\sqrt{B}/\\epsilon^3)$ at the cost of a\nbudget violation of a factor $1+\\epsilon$, for any $\\epsilon \\in (0,1]$. The\nsame result holds for the edge-cost case, to the best of our knowledge this is\nthe first polynomial-time approximation algorithm for this case. We further\nshow that the unrooted version of \\textbf{DRSO} can be approximated to a factor\nof $O(\\sqrt{B})$ without budget violation, which is an improvement over the\nfactor $O(\\Delta \\sqrt{B})$ given in~[Kuo et al.\\ IEEE/ACM\\ Trans.\\ Netw.\\\n2015] for the undirected and unrooted case, where $\\Delta$ is the maximum\ndegree of the graph. Finally, we provide some new/improved approximation bounds\nfor several related problems, including the additive-prize version of\n\\textbf{DRSO}, the maximum budgeted connected set cover problem, and the\nbudgeted sensor cover problem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  HDSDP is a numerical software solving the semidefinite programming problems.\nThe main framework of HDSDP resembles the dual-scaling interior point solver\nDSDP[2] and several new features, especially a dual method based on the\nsimplified homogeneous self-dual embedding, have been implemented. The\nembedding enhances stability of dual method and several new heuristics and\ncomputational techniques are designed to accelerate its convergence. HDSDP aims\nto show how dual-scaling algorithms benefit from the self-dual embedding and it\nis developed in parallel to DSDP5.8. Numerical experiments over several\nclassical benchmark datasets exhibit its robustness and efficiency, and\nparticularly its advantages on SDP instances featuring low-rank structure and\nsparsity. The pre-built binary of HDSDP is currently freely available at\nhttps://github.com/COPT-Public/HDSDP.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper addresses the following question: given a topological quantum\nfield theory on $\\mathbb R^n$ built from an action functional, when is it\npossible to globalize the theory so that it makes sense on an arbitrary smooth\noriented $n$-manifold? We study a broad class of topological field theories --\nthose of AKSZ type -- and obtain an explicit condition for the vanishing of the\nframing anomaly, i.e., the obstruction to performing this globalization\nprocedure. We also interpret our results in terms of identifying the\nobservables as an algebra over the framed little $n$-disks operad. Our analysis\nuses the BV formalism for perturbative field theory and the notion of\nfactorization homology.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Persistent currents in annular geometries have played an important role in\ndisclosing the quantum phase coherence of superconductors and mesoscopic\nelectronic systems. Ultracold atomic gases in multiply connected traps also\nexhibit long-lived supercurrents, and have attracted much interest both for\nfundamental studies of superfluid dynamics and as prototypes for atomtronic\ncircuits. Here, we report on the realization of supercurrents in homogeneous,\ntunable fermionic rings. We gain exquisite, rapid control over quantized\npersistent currents in all regimes of the BCS-BEC crossover through a universal\nphase-imprinting technique, attaining on-demand circulations $w$ as high as 8.\nHigh-fidelity read-out of the superfluid circulation state is achieved by\nexploiting an interferometric protocol, which also yields local information\nabout the superfluid phase around the ring. In the absence of externally\nintroduced perturbations, we find the induced metastable supercurrents to be as\nlong-lived as the atomic sample. Conversely, we trigger and inspect the\nsupercurrent decay by inserting a single small obstacle within the ring. For\ncirculations higher than a critical value, the quantized current is observed to\ndissipate via the emission of vortices, i.e., quantized phase slips, which we\ndirectly image, in good agreement with numerical simulations. The critical\ncirculation at which the superflow becomes unstable is found to depend starkly\non the interaction strength, taking its maximum value for the unitary Fermi\ngas. Our results demonstrate fast and accurate control of quantized collective\nexcitations in a macroscopic quantum system, and establish strongly interacting\nfermionic superfluids as excellent candidates for atomtronic applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Interesting discrepancies in cosmological parameters are challenging the\nsuccess of the $\\Lambda$CDM model. Direct measurements of the Hubble constant\n$H_0$ using Cepheid variables and supernovae turn out to be higher than\ninferred from the Cosmic Microwave Background (CMB). Weak galaxy lensing\nsurveys consistently report values of the strength of matter clustering\n$\\sigma_8$ lower than values derived from the CMB in the context of\n$\\Lambda$CDM. In this paper we address these discrepancies in cosmological\nparameters by considering Dark Energy (DE) as a fluid with evolving equation of\nstate $w_{\\mathrm{de}}(z)$, constant sound speed squared\n$\\hat{c}_{\\mathrm{s}}^{2}$, and vanishing anisotropic stress $\\sigma$. Our\n$w_{\\mathrm{de}}(z)$ is derived from the Holographic Principle and can\nconsecutively exhibit radiation-like, matter-like, and DE-like behaviour, thus\naffecting the sound horizon and the comoving angular diameter distance, hence\n$H_0$. Here we show DE sound speed plays a part in the matter clustering\nbehaviour through its effect on the evolution of the gravitational potential.\nWe compute cosmological constraints using several data set combinations\nincluding primary CMB, CMB lensing, redshift-space-distortions, local\ndistance-ladder, supernovae, and baryon acoustic oscillations. In our analysis\nwe marginalise over $\\hat{c}_{\\mathrm{s}}^{2}$ and find\n$\\hat{c}_{\\mathrm{s}}^{2}=1$ is excluded at $\\gtrsim 3\\sigma$. For our baseline\nresult including the whole data set we found $H_0$ and $\\sigma_8$ in good\nagreement (within $\\approx 2\\sigma$) with low redshift probes. Our constraint\nfor the baryon energy density $\\omega_{\\rm{b}}$ is however in $\\approx 3\\sigma$\ntension with BBN constraints. We conclude evolving DE also having non-standard\nclustering properties [e.g., $\\hat{c}_{\\mathrm{s}}^{2}(z,k)$] might be relevant\nfor the solution of current discrepancies in cosmological parameters.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we study admissible extensions of several theories T of reverse\nmathematics. The idea is that in such an extension the structure M = (N,S,\\in)\nof the natural numbers N and collection of sets of natural numbers S has to\nobey the axioms of T while simultaneously one also has a set-theoretic world\nwith transfinite levels erected on top of M governed by the axioms of\nKripke-Platek set theory, KP.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  I present an overview of recent developments in the microscopic description\nof the quark-gluon plasma. I will concentrate on medium-induced emission and\ntransverse momentum broadening. These are two key ingredients of the theory of\njet modifications in the QCD medium and of the kinetic theory used for\ntransport and thermalisation. The main focus is on progress towards a better\nunderstanding of theory and of its uncertainties.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Stochastic Gradient Descent (SGD) has been the method of choice for learning\nlarge-scale non-convex models. While a general analysis of when SGD works has\nbeen elusive, there has been a lot of recent progress in understanding the\nconvergence of Gradient Flow (GF) on the population loss, partly due to the\nsimplicity that a continuous-time analysis buys us. An overarching theme of our\npaper is providing general conditions under which SGD converges, assuming that\nGF on the population loss converges. Our main tool to establish this connection\nis a general converse Lyapunov like theorem, which implies the existence of a\nLyapunov potential under mild assumptions on the rates of convergence of GF. In\nfact, using these potentials, we show a one-to-one correspondence between rates\nof convergence of GF and geometrical properties of the underlying objective.\nWhen these potentials further satisfy certain self-bounding properties, we show\nthat they can be used to provide a convergence guarantee for Gradient Descent\n(GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It\nturns out that these self-bounding assumptions are in a sense also necessary\nfor GD/SGD to work. Using our framework, we provide a unified analysis for\nGD/SGD not only for classical settings like convex losses, or objectives that\nsatisfy PL / KL properties, but also for more complex problems including Phase\nRetrieval and Matrix sq-root, and extending the results in the recent work of\nChatterjee 2022.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Code-mixed machine translation has become an important task in multilingual\ncommunities and extending the task of machine translation to code mixed data\nhas become a common task for these languages. In the shared tasks of WMT 2022,\nwe try to tackle the same for both English + Hindi to Hinglish and Hinglish to\nEnglish. The first task dealt with both Roman and Devanagari script as we had\nmonolingual data in both English and Hindi whereas the second task only had\ndata in Roman script. To our knowledge, we achieved one of the top ROUGE-L and\nWER scores for the first task of Monolingual to Code-Mixed machine translation.\nIn this paper, we discuss the use of mBART with some special pre-processing and\npost-processing (transliteration from Devanagari to Roman) for the first task\nin detail and the experiments that we performed for the second task of\ntranslating code-mixed Hinglish to monolingual English.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Semantic image editing utilizes local semantic label maps to generate the\ndesired content in the edited region. A recent work borrows SPADE block to\nachieve semantic image editing. However, it cannot produce pleasing results due\nto style discrepancy between the edited region and surrounding pixels. We\nattribute this to the fact that SPADE only uses an image-independent local\nsemantic layout but ignores the image-specific styles included in the known\npixels. To address this issue, we propose a style-preserved modulation (SPM)\ncomprising two modulations processes: The first modulation incorporates the\ncontextual style and semantic layout, and then generates two fused modulation\nparameters. The second modulation employs the fused parameters to modulate\nfeature maps. By using such two modulations, SPM can inject the given semantic\nlayout while preserving the image-specific context style. Moreover, we design a\nprogressive architecture for generating the edited content in a coarse-to-fine\nmanner. The proposed method can obtain context-consistent results and\nsignificantly alleviate the unpleasant boundary between the generated regions\nand the known pixels.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Terrestrial free-space quantum key distribution is ideally suited for\ndeployment in dense urban environments. The transition from laboratory to\ncommercial deployment, however, raises a number of important engineering and\ndeployment issues. Here, we investigate these issues for efficient BB84 using a\nweak coherent pulse-decoy state protocol. We calculate expected key lengths for\ndifferent environmental conditions and when the scope for optimisation of\nprotocol parameters is restricted due to practical considerations. In\nparticular, we find that for a fixed receiver basis choice probability, it can\nbe advantageous to allow the transmitter to have a different basis choice\nprobability depending on varying channel loss and background light levels.\nFinally, we examine the effects of pulse intensity uncertainty finding that\nthey can dramatically reduce the key length. These results can be used to\ndetermine the loss budget for the free-space optics of a QKD systems and assist\nin their design.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we investigate the parameterized complexity for NP-hard graph\nproblems parameterized by a structural parameter modular-width. We develop a\nrecipe that is able to simplify the process of obtaining polynomial Turing\ncompressions for a class of graph problems parameterized by modular-width. By\nusing the recipe, we demonstrate that several problems, which include Chromatic\nNumber, Independent Det, Hamiltonian Cycle, etc. have polynomial Turing\ncompressions parameterized by modular-width. In addition, under the assumption\nthat P $\\neq$ NP, we provide tight kernels for a few problems such as Steiner\nTree parameterized by modular-width. As a byproduct of the result of the tight\nkernels, new parameterized algorithms for these problems are obtained.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the problem of counting all cycles or self-avoiding walks (SAWs) on\ntriangulated planar graphs. We present a subexponential $2^{O(\\sqrt{n})}$ time\nalgorithm for this counting problem. Among the technical ingredients used in\nthis algorithm are the planar separator theorem and a delicate analysis using\npairs of Motzkin paths and Motzkin numbers. We can then adapt this algorithm to\nuniformly sample SAWs, in subexponential time. Our work is motivated by the\nproblem of gerrymandered districting maps.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the visible spectrum vortex beams have found various applications, ranging\nfrom optical tweezers to super-resolution imaging. Recently, these beams have\nbeen demonstrated using X-rays and electron beams. However, so far, no in-depth\ndiscussion has been carried out on the vortex quality, which could become\nessential for a variety of vortex applications. Here, we investigate the mode\nconversion efficiency (MCE), vortex structure and stability (in terms of vortex\nsplitting) of the vortex fields generated by spiral zone plates (SZP). We have\ndesigned and fabricated SZPs with varying topological charge of both binary and\nkinoform profile. Kinoforms are known for their 100 % diffraction efficiency in\nthe ideal case. In this work, both types are contrasted with regard to the\nvortex quality. Utilizing ptychographic coherent diffraction imaging and by\ncomparing to simulations the wavefront of the generated fields is\ncharacterized. It was found, that the MCE and vortex structure exhibit the same\ndependencies on material and ZP properties as the diffraction efficiency (DE)\nand that the kinoform profile in this sense also improves the vortex quality.\nWith growing SZP charge the MCE decreases. The results link the parameters of\noptics to the properties of the vortices and help to maximize the performance\nof ZP based vortex generators for future applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The rotation symmetric bosonic code (RSBC) is a unified framework of\npractical bosonic codes that have rotation symmetries, such as cat codes and\nbinomial codes. While cat codes achieve the break-even point in which the\ncoherence time of the encoded qubits exceeds that of unencoded qubits, with\nbinomial codes nearly approaching that point, the state preparation fidelity\nneeds to be still improved for practical quantum computing. Concerning this\nproblem, we investigate the framework of symmetry expansion, a class of quantum\nerror mitigation that virtually projects the state onto the noise-free\nsymmetric subspace by exploiting the system's intrinsic symmetries and\npost-processing of measurement outcomes. Although symmetry expansion has been\nlimited to error mitigation of quantum states immediately before measurement,\nwe successfully generalize symmetry expansion for state preparation. To\nimplement our method, we use an ancilla qubit and only two controlled-rotation\ngates via dispersive interactions between the bosonic code states and the\nancilla qubit. Interestingly, this method also allows us to virtually prepare\nthe RSBC states only from easy-to-prepare states, e.g., coherent states. We\nalso discuss that the conventional symmetry expansion protocol can be applied\nto improve the computation fidelity when the symmetries of rotation bosonic\ncodes are unavailable due to low measurement fidelity. By giving comprehensive\nanalytical and numerical arguments regarding the trace distance between the\nerror-mitigated state and the ideal state and the sampling cost of quantum\nerror mitigation, we show that symmetry expansion dramatically suppresses the\neffect of photon loss. Our novel error mitigation method will significantly\nenhance computation accuracy in the near-term bosonic quantum computing\nparadigm.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using the embedded gradient vector field method (see P. Birtea, D. Comanescu,\nHessian operators on constraint manifolds, J. Nonlinear Science 25, 2015), we\npresent a general formula for the Laplace-Beltrami operator defined on a\nconstraint manifold, written in the ambient coordinates. Regarding the\northogonal group as a constraint submanifold of the Euclidean space of $n\\times\nn$ matrices, we give an explicit formula for the Laplace-Beltrami operator on\nthe orthogonal group using the ambient Euclidean coordinates. We apply this new\nformula for some relevant functions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given $r\\geq2$ and an $r$-uniform hypergraph $F$, the $F$-bootstrap process\nstarts with an $r$-uniform hypergraph $H$ and, in each time step, every\nhyperedge which \"completes\" a copy of $F$ is added to $H$. The maximum running\ntime of this process has been recently studied in the case that $r=2$ and $F$\nis a complete graph by Bollob\\'as, Przykucki, Riordan and Sahasrabudhe\n[Electron. J. Combin. 24(2) (2017), Paper No. 2.16], Matzke\n[arXiv:1510.06156v2] and Balogh, Kronenberg, Pokrovskiy and Szab\\'o\n[arXiv:1907.04559v1]. We consider the case that $r\\geq3$ and $F$ is the\ncomplete $r$-uniform hypergraph on $k$ vertices. Our main results are that the\nmaximum running time is $\\Theta\\left(n^r\\right)$ if $k\\geq r+2$ and\n$\\Omega\\left(n^{r-1}\\right)$ if $k=r+1$. For the case $k=r+1$, we conjecture\nthat our lower bound is optimal up to a constant factor when $r=3$, but suspect\nthat it can be improved by more than a constant factor for large $r$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop a density-matrix renormalization group (DMRG) algorithm for the\nsimulation of quantum circuits. This algorithm can be seen as the extension of\ntime-dependent DMRG from the usual situation of hermitian Hamiltonian matrices\nto quantum circuits defined by unitary matrices. For small circuit depths, the\ntechnique is exact and equivalent to other matrix product state (MPS) based\ntechniques. For larger depths, it becomes approximate in exchange for an\nexponential speed up in computational time. Like an actual quantum computer,\nthe quality of the DMRG results is characterized by a finite fidelity. However,\nunlike a quantum computer, the fidelity depends strongly on the quantum circuit\nconsidered. For the most difficult possible circuit for this technique, the\nso-called \"quantum supremacy\" benchmark of Google Inc. , we find that the DMRG\nalgorithm can generate bit strings of the same quality as the seminal Google\nexperiment on a single computing core. For a more structured circuit used for\ncombinatorial optimization (Quantum Approximate Optimization Algorithm or\nQAOA), we find a drastic improvement of the DMRG results with error rates\ndropping by a factor of 100 compared with random quantum circuits. Our results\nsuggest that the current bottleneck of quantum computers is their fidelities\nrather than the number of qubits.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This study investigates the density anomalies observed in silica glass. The\ndensity of silica glass does not monotonically increase during cooling; it\ninstead shows a maximum and minimum. Despite tremendous efforts, the structural\norigin of these density anomalies is not clearly understood. Our technique that\ncombines topological data analysis and machine learning reveals that the\ndensity anomalies are characterized by a change in the one-dimensional topology\nof the -Si-Si- network rather than that of the entire -Si-O- network. The\none-dimensional topology of the -Si-Si- network changes at the temperatures at\nwhich the maximum and minimum densities are observed. In contrast, the\none-dimensional topologies of the -O-O- and -Si-O- networks change at lower\ntemperatures. Our work demonstrates the value of topological techniques in\nunderstanding the transitions in glassy materials and sheds light on the\ncharacterization of glass-liquid transitions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The production of Higgs bosons in weak boson fusion has the second largest\ncross section among Higgs-production processes at the LHC. As such, this\nprocess plays an important role in detailed studies of Higgs interactions with\nvector bosons. In this paper we extend the available description of Higgs boson\nproduction in weak boson fusion by considering anomalous $HVV$ interactions and\nNNLO QCD radiative corrections at the same time. We find that, while leading\norder QCD predictions are too uncertain to allow for detailed studies of the\nanomalous couplings, NLO QCD results are sufficiently precise, most of the\ntime. The NNLO QCD corrections alter the NLO QCD predictions only marginally,\nbut their availability enhances the credibility of conclusions based on NLO QCD\ncomputations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The whole Al-Li phase diagram is predicted from first principles calculations\nand statistical mechanics including the effect of configurational and\nvibrational entropy. The formation enthalpy of different configurations at\ndifferent temperatures was accurately predicted by means of cluster expansions\nthat were fitted from first principles calculations. The vibrational entropic\ncontribution of each configuration was determined from the bond length vs. bond\nstiffness relationships for each type of bond and the Gibbs free energy of the\ndifferent phases was obtained as a function of temperature from Monte Carlo\nsimulations. The predicted phase diagram was in excellent agreement with the\ncurrently accepted experimental one in terms of the stable (AlLi, Al2Li3,\nAlLi2, Al4Li9) and metastable (Al3Li) phases, of the phase boundaries between\nthem and of the maximum stability temperature of line compounds. In addition,\nit provided accurate information about the gap between Al3Li and AlLi solvus\nlines. Finally, the influence of the vibrational entropy on the correct\nprediction of the phase diagram is discussed. Overall, the methodology shows\nthat accurate phase diagrams of alloys of technological interest can be\npredicted from first principles calculations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Rouquier blocks, also known as the RoCK blocks, are important blocks of\nthe symmetric groups algebras and the Hecke algebras of type A, with the\npartitions labelling the Specht modules that belong to these blocks having a\nparticular abacus configuration. We generalise the definition of Rouquier\nblocks to the Ariki-Koike algebras, where the Specht modules are indexed by\nmultipartitions, and explore the properties of these blocks\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Theoretical works have shown that off-plane motions of bars can heat stars in\nthe vertical direction during buckling but is not clear how do they affect the\nrest of components of the Stellar Velocity Ellipsoid (SVE). We study the 2D\nspatial distribution of the vertical, $\\sigma_{z}$, azimuthal, $\\sigma_{\\phi}$\nand radial, $\\sigma_{r}$ velocity dispersions in the inner regions of Auriga\ngalaxies, a set of high-resolution magneto-hydrodynamical cosmological zoom-in\nsimulations, to unveil the influence of the bar on the stellar kinematics.\n$\\sigma_{z}$ and $\\sigma_{\\phi}$ maps exhibit non-axisymmetric features that\nclosely match the bar light distribution with low $\\sigma$ regions along the\nbar major axis and high values in the perpendicular direction. On the other\nhand, $\\sigma_{r}$ velocity dispersion maps present more axisymmetric\ndistributions. We show that isophotal profile differences best capture the\nimpact of the bar on the three SVE components providing strong correlations\nwith bar morphology proxies although there is no relation with individual\n$\\sigma$. Time evolution analysis shows that these differences are a\nconsequence of the bar formation and that they tightly coevolve with the\nstrength of the bar. We discuss the presence of different behaviours of\n$\\sigma_{z}$ and its connection with observations. This work helps us\nunderstand the intrinsic $\\sigma$ distribution and motivates the use of\nisophotal profiles as a mean to quantify the effect of bars.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Communication efficiency plays an important role in accelerating the\ndistributed training of Deep Neural Networks (DNN). All-reduce is the key\ncommunication primitive to reduce model parameters in distributed DNN training.\nMost existing all-reduce algorithms are designed for traditional electrical\ninterconnect systems, which cannot meet the communication requirements for\ndistributed training of large DNNs. One of the promising alternatives for\nelectrical interconnect is optical interconnect, which can provide high\nbandwidth, low transmission delay, and low power cost. We propose an efficient\nscheme called WRHT (Wavelength Reused Hierarchical Tree) for implementing\nall-reduce operation in optical interconnect system, which can take advantage\nof WDM (Wavelength Division Multiplexing) to reduce the communication time of\ndistributed data-parallel DNN training. We further derive the minimum number of\ncommunication steps and communication time to realize the all-reduce using\nWRHT. Simulation results show that the communication time of WRHT is reduced by\n75.59%, 49.25%, and 70.1% respectively compared with three traditional\nall-reduce algorithms simulated in optical interconnect system. Simulation\nresults also show that WRHT can reduce the communication time for all-reduce\noperation by 86.69% and 84.71% in comparison with two existing all-reduce\nalgorithms in electrical interconnect system.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose an effective model for an exponentially expanding universe in the\nbrane-world scenario. The setup consists of a 5D black hole living on a closed\nbrane. In case the brane acquires a specific configuration, which we deduce\nfrom stability arguments, the induced metric outside the black hole horizon on\nthe brane becomes de Sitter in static coordinates. Solving for a homogeneous\nand isotropic fluid in the corresponding FLRW coordinates we find the\nperturbative Friedmann equation and derive the 4D gravitational constant. We\nfind that the bulk fluid density inside the brane, which has the same equation\nof state as the fluid on the brane, contributes to the energy density in the\nFriedmann equation and therefore in late time may be attributed to dark matter.\nStudying the stability of the setup we observe that the brane becomes\nstabilized, in the presence of matter on the brane, with a de Sitter length\nthat is qualitatively of the order of Schwarzschild radius of the universe. We\nstudy the semi-classical evaporation of de Sitter space-time in this framework.\nWe also discuss effects other than black hole evaporation that can alter de\nSitter lifetime. In particular, this model can provide a lifetime compatible\nwith Trans-Planckian Censorship conjecture for the current de Sitter phase.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work, we investigate the effects of first-order phase transitions on\nthe singlet fermionic dark matter in the scotogenic model. It is known that\nthis dark matter candidate tends to conflict with the relevant constraints such\nas the neutrino oscillation data and charged lepton flavor violating processes\nif its thermal production mechanism is assumed. We find that the dark matter\nproduction mechanisms are modified by first-order phase transitions at some\nspecific parameter regions, where the phase transitions can be one-step or\ntwo-step depending on the parameters. If the phase transition is one-step, a\nsufficiently low nucleation temperature is required to reproduce the observed\nrelic abundance of dark matter. If the phase transition is two-step, the dark\nmatter should never be thermalized, otherwise the abundance would remain too\nmuch and overclose the universe. This is because the nucleation temperature\ncannot be low as in the one-step case. Therefore we require another way of dark\nmatter production, the freeze-in mechanism for the two-step case. We show that\nthe freeze-in mechanism is modified by the temporary vacuum expectation value\nof the inert scalar field. In both cases, the first-order phase transitions\ncould produce observable gravitational wave spectra. In particular for the\none-step phase transition, the generated gravitational waves with sizable\nenergy density are intrinsically correlated with the dark matter production\nmechanism, and can be detectable by future space-based interferometers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the freedom of communication provided in online social media, hate\nspeech has increasingly generated. This leads to cyber conflicts affecting\nsocial life at the individual and national levels. As a result, hateful content\nclassification is becoming increasingly demanded for filtering hate content\nbefore being sent to the social networks. This paper focuses on classifying\nhate speech in social media using multiple deep models that are implemented by\nintegrating recent transformer-based language models such as BERT, and neural\nnetworks. To improve the classification performances, we evaluated with several\nensemble techniques, including soft voting, maximum value, hard voting and\nstacking. We used three publicly available Twitter datasets (Davidson,\nHatEval2019, OLID) that are generated to identify offensive languages. We fused\nall these datasets to generate a single dataset (DHO dataset), which is more\nbalanced across different labels, to perform multi-label classification. Our\nexperiments have been held on Davidson dataset and the DHO corpora. The later\ngave the best overall results, especially F1 macro score, even it required more\nresources (time execution and memory). The experiments have shown good results\nespecially the ensemble models, where stacking gave F1 score of 97% on Davidson\ndataset and aggregating ensembles 77% on the DHO dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A physics package for a compact cold atomic clock is hereby presented. The\nuniqueness of this package is its small dimensions that enable, for the first\ntime, implementation of a primary cold atomic clock in a standard package of 3U\nheight (=133mm). These dimensions are made possible by using an in-vacuum\nMicrowave (MW) Loop Gap Resonator (LGR) whose length and diameter can be\nreduced from those of a typical resonator. Following our presentation of the\ndesign, we analyze the homogeneity of the MW field amplitude and phase. We find\nthat the expected clock instability due to non-uniformity of the MW field is\n$\\sim1.5\\cdot10^{-14}$/day. Adding other clock errors, we estimate the total\nuncertainty of a cold atomic clock of this design will be around\n$\\sim2\\cdot10^{-14}$/day which results in a time drift of a few\nnanoseconds/day. Such a clock can serve as a primary-grade frequency reference,\nand may replace the cesium beam atomic clock and the GPS-disciplined rubidium\natomic clocks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Topological vacua are a family of degenerate ground states of Yang-Mills\nfields with zero field strength but nontrivial topological structures. They\nplay a fundamental role in particle physics and quantum field theory, but have\nnot yet been experimentally observed. Here we report the first theoretical\nproposal and experimental realization of synthetic topological vacua with a\ncloud of atomic Bose-Einstein condensates. Our setup provides a promising\nplatform to demonstrate the fundamental concept that a vacuum, rather than\nbeing empty, has rich spatial structures. The Hamiltonian for the vacuum of\ntopological number n = 1 is synthesized and the related Hopf index is measured.\nThe vacuum of topological number n = 2 is also realized, and we find that vacua\nwith different topological numbers have distinctive spin textures and Hopf\nlinks. Our work opens up opportunities for exploring topological vacua and\nrelated long-sought-after instantons in tabletop experiments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Data-driven speech processing models usually perform well with a large amount\nof text supervision, but collecting transcribed speech data is costly.\nTherefore, we propose SpeechCLIP, a novel framework bridging speech and text\nthrough images to enhance speech models without transcriptions. We leverage\nstate-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images\nand spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior\nstate-of-the-art on image-speech retrieval and performs zero-shot speech-text\nretrieval without direct supervision from transcriptions. Moreover, SpeechCLIP\ncan directly retrieve semantically related keywords from speech.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Anomaly detection is essential for preventing hazardous outcomes for\nsafety-critical applications like autonomous driving. Given their\nsafety-criticality, these applications benefit from provable bounds on various\nerrors in anomaly detection. To achieve this goal in the semi-supervised\nsetting, we propose to provide Probably Approximately Correct (PAC) guarantees\non the false negative and false positive detection rates for anomaly detection\nalgorithms. Our method (PAC-Wrap) can wrap around virtually any existing\nsemi-supervised and unsupervised anomaly detection method, endowing it with\nrigorous guarantees. Our experiments with various anomaly detectors and\ndatasets indicate that PAC-Wrap is broadly effective.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Anomalous pattern detection aims to identify instances where deviation from\nnormalcy is evident, and is widely applicable across domains. Multiple\nanomalous detection techniques have been proposed in the state of the art.\nHowever, there is a common lack of a principled and scalable feature selection\nmethod for efficient discovery. Existing feature selection techniques are often\nconducted by optimizing the performance of prediction outcomes rather than its\nsystemic deviations from the expected. In this paper, we proposed a\nsparsity-based automated feature selection (SAFS) framework, which encodes\nsystemic outcome deviations via the sparsity of feature-driven odds ratios.\nSAFS is a model-agnostic approach with usability across different discovery\ntechniques. SAFS achieves more than $3\\times$ reduction in computation time\nwhile maintaining detection performance when validated on publicly available\ncritical care dataset. SAFS also results in a superior performance when\ncompared against multiple baselines for feature selection.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we define and solve the Inverse Stochastic Optimal Control\n(ISOC) problem of the linear-quadratic Gaussian (LQG) and the linear-quadratic\nsensorimotor (LQS) control model. These Stochastic Optimal Control (SOC) models\nare state-of-the-art approaches describing human movements. The LQG ISOC\nproblem consists of finding the unknown weighting matrices of the quadratic\ncost function and the covariance matrices of the additive Gaussian noise\nprocesses based on ground truth trajectories observed from the human in\npractice. The LQS ISOC problem aims at additionally finding the covariance\nmatrices of the signal-dependent noise processes characteristic for the LQS\nmodel. We propose a solution to both ISOC problems which iteratively estimates\ncost function and covariance matrices via two bi-level optimizations.\nSimulation examples show the effectiveness of our developed algorithm. It finds\nparameters that yield trajectories matching mean and variance of the ground\ntruth data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This study examines the effect of dark matter halos on the orbital and escape\ndynamics of stars in the central region of barred galaxies. A three-dimensional\ngravitational model with a central bulge, bar, disc, and dark matter halo (or\nsimply dark halo) has been set up and analyzed from the viewpoint of escape in\nopen Hamiltonian systems for this purpose. Additionally, this model has been\nexamined separately for the dark halo profiles: oblate and NFW. In both\ncircumstances, an escape mechanism has been identified near the saddle points\nof the phase space, which correspond to the bar ends. The escaping motion of\nstars is seen using orbital maps and Poincar\\'e surface section maps generated\nin various phase planes. Finally, the relationship between chaos and dark halo\nparameters such as mass, size, circular velocity, and nature has been studied.\nOur findings suggest that oblate dark halos are preferred over NFW dark halos\nfor justifying the formation of full-fledged spiral arms and extended\ndistribution of dark halos in giant spiral galaxies with supermassive black\nholes (SMBHs) at their centers. Again, the oblate dark halos well justify the\nemergence of less prominent or poor spiral arms and the core-dominated\ndistribution of dark halos in dwarf and LSB galaxies in the absence of central\nSMBHs. On the other hand, extreme central baryonic feedback is required for the\nNFW halos to generate spiral patterns, and such dark halos should be preferred\nfor galaxies with extremely energetic centers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Modeling effective representations using multiple views that positively\ninfluence each other is challenging, and the existing methods perform poorly on\nElectroencephalogram (EEG) signals for sleep-staging tasks. In this paper, we\npropose a novel multi-view self-supervised method (mulEEG) for unsupervised EEG\nrepresentation learning. Our method attempts to effectively utilize the\ncomplementary information available in multiple views to learn better\nrepresentations. We introduce diverse loss that further encourages\ncomplementary information across multiple views. Our method with no access to\nlabels beats the supervised training while outperforming multi-view baseline\nmethods on transfer learning experiments carried out on sleep-staging tasks. We\nposit that our method was able to learn better representations by using\ncomplementary multi-views.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Simultaneous analysis of the C_2 and CN molecular bands in the 5100-5200 and\n7930-8100A spectral regions is a promising alternative for the accurate\ndetermination of the carbon (C) and nitrogen (N) abundance in the atmospheres\nof the solar-like stars. Practical implementation of this new method became\npossible after recent improvements of the molecular constants for both\nmolecules. The new molecular data predicted the correct line strength and line\npositions; therefore, they were included in the Vienna Atomic Line Database\n(VALD), which is widely used by astronomers and spectroscopists. In this paper,\nwe demonstrate that the molecular data analysis provides C and, in particular,\nN abundances consistent with those derived from the atomic lines. We illustrate\nthis by performing the analysis for three stars. Our results provide strong\narguments for using the combination of C_2 and CN molecular lines for accurate\nnitrogen abundance determination keeping in mind the difficulties of using the\nN1 lines in the observed spectra of the solar-like stars.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  MAXI J1421-613 is an X-ray burster discovered by Monitor of All-sky X-ray\nImage (MAXI) on 9 January 2014 and is considered to be a low-mass X-ray binary.\nA previous study analyzing follow-up observation data obtained by Suzaku on 31\nJanuary to 3 February 2014 reported that an annular emission of ~3'-9' radius\nwas found around the transient source. The most plausible origin of the annular\nemission is a dust scattering echo by the outburst of MAXI J1421-613. In this\npaper, we confirm the annular emission by analyzing the data of the Swift\nfollow-up observation which was conducted by the photon counting mode on 18\nJanuary 2014. In a radial profile, we found an annular emission at ~2'.5-4'.5.\nIts spectrum was well explained by an absorbed power law, and the photon index\nwas higher than that of MAXI J1421-613 itself by delta Gamma~2. The flux and\nradius of the annular emission observed by Swift are explained by dust\nscattering of the same outburst as is responsible for the annular emission\nobserved by Suzaku. Assuming that the dust layer causing the annular emission\nfound by Swift is located at the same position as the CO cloud in front of MAXI\nJ1421-613, the distance to the transient source was estimated to be ~3 kpc,\nwhich is consistent with the value estimated by the previous study of Suzaku.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  $\\mathsf{S5}$-subordination algebras were recently introduced as a\ngeneralization of de Vries algebras, and it was proved that the category\n$\\mathsf{SubS5^S}$ of $\\mathsf{S5}$-subordination algebras and compatible\nsubordination relations between them is equivalent to the category of compact\nHausdorff spaces and closed relations. We generalize MacNeille completions of\nboolean algebras to the setting of $\\mathsf{S5}$-subordination algebras, and\nutilize the relational nature of the morphisms in $\\mathsf{SubS5^S}$ to prove\nthat the MacNeille completion functor establishes an equivalence between\n$\\mathsf{SubS5^S}$ and its full subcategory consisting of de Vries algebras. We\nalso generalize ideal completions of boolean algebras to the setting of\n$\\mathsf{S5}$-subordination algebras and prove that the ideal completion\nfunctor establishes a dual equivalence between $\\mathsf{SubS5^S}$ and the\ncategory of compact regular frames and preframe homomorphisms. Our results are\nchoice-free and provide further insight into Stone-like dualities for compact\nHausdorff spaces with various morphisms between them. In particular, we show\nhow they restrict to the wide subcategories of $\\mathsf{SubS5^S}$ corresponding\nto continuous relations and continuous functions between compact Hausdorff\nspaces.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent experiments on multilayer graphene materials have discovered a\nplethora of correlated phases, including ferromagnetism and superconductivity,\nin the absence of a moir\\'{e} potential. These findings pose an intriguing\nquestion of whether an underlying moir\\'{e} potential plays a key role in\ndetermining the phases realizable in tunable two-dimensional quantum materials,\nor whether it merely acts as a weak periodic potential that perturbs an\nunderlying correlated many body state. In this work, employing a Hartree-Fock\nmean field analysis, we examine this question theoretically by quantitatively\nstudying the effects of an hexagonal Boron Nitride (h-BN) substrate on\nABC-stacked trilayer graphene (ABC-TLG). For the topologically trivial regime,\nwe find that the moir\\'{e} potential leads to a strong suppression of the\nferromagnetism of the underlying metal. Further, band insulators appear solely\nat full filling of the moir\\'{e} unit cell, with a moir\\'{e} potential stronger\nthan is conventionally assumed. Thus the observed correlated insulating phases\nin ABC-TLG aligned with h-BN cannot be understood through band folding of the\nferromagnetic metal found without the moir\\'{e} potential. For the\ntopologically non-trivial regime, we discover the appearance of prominent\nincompressible states when fractional hole fillings (of the moir\\'{e} unit\ncell) coincide with the occurrence of fractional-metallic states in the\nmoir\\'{e}-less setting, as well as a slight weakening of the ferromagnetic\nnature of the phases; however this once again requires a moir\\'{e} potential\nstronger than is conventionally assumed. Our findings highlight the importance\nof interactions in renormalizing the electronic bandstructure, and emphasizes\nthe key role played by the moir\\'{e} potential in determining the strong\ncorrelation physics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  During the COVID-19 pandemic, computed tomography (CT) is a good way to\ndiagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a\nform of computed tomography that uses advanced methods to improve image\nresolution. Publicly accessible COVID-19 CT image datasets are very difficult\nto come by due to privacy concerns, which impedes the study and development of\nAI-powered COVID-19 diagnostic algorithms based on CT images. To address this\nproblem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution\nchest CT Scan image dataset that includes not only COVID-19 cases of Ground\nGlass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT\nimages of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which\nincludes slice-level, and patient-level labels, has the potential to aid\nCOVID-19 research, especially for diagnosis and differentiation using\nartificial intelligence algorithms, machine learning and deep learning methods.\nThis dataset is accessible through web at: http://databiox.com and includes\n181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy\nPaving, Air Space Consolidation and Negative.\n  Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging,\nChest Image.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a scheme that uses Ramsey interferometry to directly probe the\nWigner function of a neutral atom confined in an optical trap. The proposed\nscheme relies on the well-established fact that the Wigner function at a given\npoint $(x,p)$ in phase space is proportional to the expectation value of the\nparity operator relative to that point. In this work, we show that parity-even\nand parity-odd motional states can be mapped to two distinct internal states of\nthe atom by using state-dependent trapping potentials. The Wigner function can\nthus be measured point-by-point in phase space with a single, direct\nmeasurement of the internal state population. Numerical simulations show that\nthe scheme is robust in that it applies not only to deep, harmonic potentials\nbut also to shallower, anharmonic traps.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum resources may provide advantage over its classical counterparts.\nTheoretically, in certain tasks, this advantage can be very high. In this work,\nwe construct such a task based on a tripartite game played by Referee, Alice,\nand Bob. Referee sends Alice a value of a random variable. At the same time,\nReferee also sends Bob some partial information regarding that value. Alice is\nnot allowed to know what information is sent to Bob by the Referee. Again, Bob\ndoes not know which value of the random variable is sent to Alice. Now, the\ngame can be won if and only if Bob can unambiguously identify the value of the\nvariable with some nonzero probability, no matter what information Bob receives\nor which value is sent to Alice. However, to help Bob, Alice sends some limited\namount of information to him, based on a strategy which is fixed by Alice and\nBob before the game begins. We show that if Alice sends limited amount of\nclassical information then the game cannot be won while the quantum analog of\nthe `limited amount of classical information' is sufficient for winning the\ngame. Thus, it establishes quantum advantage. We further analyze several\nvariants of the game and provide certain bounds on the success probabilities.\nMoreover, we establish connections between trine ensemble, mutually unbiased\nbases, and the encoding-decoding strategies of those variants. We also discuss\nthe role of quantum coherence in the present context.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop a new method to infer the temporal, geometric, and energetic\nproperties of galaxy outflows, by combining stellar spectral modeling to infer\nstarburst ages, and absorption lines to measure velocities. If winds are\naccelerated with time during a starburst event, then these two measurements\nenable us to solve for the wind radius, similarly to length scales and the\nHubble parameter in Big Bang cosmology. This wind radius is the vital, but\nhard-to-constrain parameter in wind physics. We demonstrate the method using\nspectra of 87 starburst galaxies at z=0.05-0.44, finding that winds accelerate\nthroughout the starburst phase and grow to typical radii of ~1 kpc in ~10 Myr.\nMass flow rates increase rapidly with time, and the mass-loading factor exceeds\nunity at about 10 Myr - while still being accelerated, the gas will likely\nunbind from the local potential and enrich the circumgalactic medium. We model\nthe mechanical energy available from stellar winds and supernovae, and estimate\nthat a negligible amount is accounted for in the cool outflow at early times.\nHowever, the energy deposition increases rapidly and ~10% of the budget is\naccounted for in the cool flow at 10 Myr, similar to some recent hydrodynamical\nsimulations. We discuss how this model can be developed, especially for\nhigh-redshift galaxies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We use filtrations of the tangent bundle of a manifold starting with an\nintegrable subbundle to define transverse symbols to the corresponding\nfoliation, define a condition of transversally Rockland and prove that\ntransversally Rockland operators yield a K-homology class. We construct an\nequivariant KK-class for transversally Rockland transverse symbols and show a\nPoincare duality type result linking the class of an operator and its symbol.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Music mixing traditionally involves recording instruments in the form of\nclean, individual tracks and blending them into a final mixture using audio\neffects and expert knowledge (e.g., a mixing engineer). The automation of music\nproduction tasks has become an emerging field in recent years, where rule-based\nmethods and machine learning approaches have been explored. Nevertheless, the\nlack of dry or clean instrument recordings limits the performance of such\nmodels, which is still far from professional human-made mixes. We explore\nwhether we can use out-of-domain data such as wet or processed multitrack music\nrecordings and repurpose it to train supervised deep learning models that can\nbridge the current gap in automatic mixing quality. To achieve this we propose\na novel data preprocessing method that allows the models to perform automatic\nmusic mixing. We also redesigned a listening test method for evaluating music\nmixing systems. We validate our results through such subjective tests using\nhighly experienced mixing engineers as participants.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A Head Related Transfer Function (HRTF) characterizes how a human ear\nreceives sounds from a point in space, and depends on the shapes of one's head,\npinna, and torso. Accurate estimations of HRTFs for human subjects are crucial\nin enabling binaural acoustic applications such as sound localization and 3D\nsound spatialization. Unfortunately, conventional approaches for HRTF\nestimation rely on specialized devices or lengthy measurement processes. This\nwork proposes a novel lightweight method for HRTF individualization that can be\nimplemented using commercial-off-the-shelf components and performed by average\nusers in home settings. The proposed method has two key components: a\ngenerative neural network model that can be individualized to predict HRTFs of\nnew subjects from sparse measurements, and a lightweight measurement procedure\nthat collects HRTF data from spatial locations. Extensive experiments using a\npublic dataset and in house measurement data from 10 subjects of different ages\nand genders, show that the individualized models significantly outperform a\nbaseline model in the accuracy of predicted HRTFs. To further demonstrate the\nadvantages of individualized HRTFs, we implement two prototype applications for\nbinaural localization and acoustic spatialization. We find that the performance\nof a localization model is improved by 15 degree after trained with\nindividualized HRTFs. Furthermore, in hearing tests, the success rate of\ncorrectly identifying the azimuth direction of incoming sounds increases by\n183% after individualization.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a quantum algorithm based on the analytically-optimal selection of\na single-qubit gate in parameterized quantum circuits (PQCs). Our algorithm\noptimizes the PQC structure by sequentially replacing a single-qubit gate in\nthe PQC with the optimal one minimizing the objective function in the\nvariational quantum algorithm. To directly find local optima, our method uses\nmatrix factorization whose matrix elements consist of slightly-modified circuit\nevaluations on the objective function, which is in contrast to conventional\nsequential optimizers that utilize sinusoidal properties. Optimal selection\nover single-qubit gates based on this matrix factorization leads to more\nefficient optimization of PQCs. Moreover, we show that the framework of matrix\nfactorization utilized in our method unifies and extends the existing\nsequential methods. We perform numerical experiments demonstrating the efficacy\nof the framework.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The $R^2$ inflation which is an extension of general relativity (GR) by\nquadratic scalar curvature introduces a quasi-de Sitter expansion of the early\nUniverse governed by Ricci scalar being an eigenmode of d'Alembertian operator.\nIn this paper, we derive a most general theory of gravity admitting $R^2$\ninflationary solution which turned out to be higher curvature non-local\nextension of GR. We study in detail inflationary perturbations in this theory\nand analyse the structure of form factors that leads to a massive scalar\n(scalaron) and massless tensor degrees of freedom. We argue that the theory\ncontains only finite number of free parameters which can be fixed by\ncosmological observations. We derive predictions of our generalized non-local\n$R^2$-like inflation and obtain the scalar spectral index $n_s\\approx\n1-\\frac{2}{N}$ and any value of the tensor-to-scalar ratio $r<0.036$. In this\ntheory, tensor spectral index can be either positive or negative $n_t\\lessgtr\n0$ and the well-known consistency relation $r = -8n_t$ is violated in a\nnon-trivial way. We also compute running of the tensor spectral index and\ndiscuss observational implications to distinguish this model from several\nclasses of scalar field models of inflation. These predictions allow us to\nprobe the nature of quantum gravity in the scope of future CMB and\ngravitational wave observations. Finally we comment on how the features of\ngeneralized non-local $R^2$-like inflation cannot be captured by established\nnotions of the so-called effective field theory of single field inflation and\nhow we must redefine the way we pursue inflationary cosmology.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We prove that the Kuramoto model on a graph can contain infinitely many\nnon-equivalent stable equilibria. More precisely, we prove that for every\npositive integer d there is a connected graph such that the set of stable\nequilibria contains a manifold of dimension d. In particular, we solve a\nconjecture of R. Delabays, T. Coletta and P. Jacquod about the number of\nequilibria on planar graphs. Our results are based on the analysis of balanced\nconfigurations, which correspond to equilateral polygon linkages in topology.\nIn order to analyze the stability of manifolds of equilibria we apply\ntopological bifurcation theory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The light cone formalism of a massive scalar field has been shown by Dirac to\nhave many advantages. But it is not manifestly Lorentz invariant. We will show\nthat this is a feature not a bug: Lorentz invariance is indeed a symmetry, but\nin a different sense defined by Drinfel'd. The key idea is that the mass shell\n(mass hyperboloid) is a Poisson-Lie group: there is a non-abelian group\nmultiplication and non-zero Poisson brackets between components of\nfour-momentum. Rotations form the dual group of the hyperboloid in the sense of\nDrinfel'd. Infinitesimal Lorentz transformations form a Lie bi-algebra.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Axion-gluon interaction induces quadratic couplings between the axion and the\nmatter fields. We find that, if the axion is an ultralight dark matter field,\nit induces small oscillations of the mass of the hadrons as well as other\nnuclear quantities. As a result, atomic energy levels oscillate. We use\ncurrently available atomic spectroscopy data to constrain such axion-gluon\ncoupling. We also project the sensitivities of future experiments, such as ones\nusing molecular and nuclear clock transitions. We show that current and\nnear-future experiments constrain a finely-tuned parameter space of axion\nmodels. These can compete or dominate the already-existing constraints from\noscillating neutron electric dipole moment and supernova bound, in addition to\nthose expected from near future magnetometer-based experiments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we derive a new method for determining shared features of\ndatasets by employing joint non-negative matrix factorization and analyzing the\nresulting factorizations. Our approach uses the joint factorization of two\ndataset matrices $X_1,X_2$ into non-negative matrices $X_1 = AS_1, X_2 = AS_2$\nto derive a similarity measure that determines how well a shared basis for\n$X_1, X_2$ approximates each dataset. We also propose a dataset distance\nmeasure built upon this method and the learned factorization. Our method is\nable to successfully identity differences in structure in both image and text\ndatasets. Potential applications include classification, detecting plagiarism\nor other manipulation, and learning relationships between data sets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  All children enjoy blowing soap bubbles that also show up in our bath and\nwhen we wash dishes. We analyze the thinning and breaking of soap bubble neck\nwhen it is stretched. To contrast with the more widely studied film whose\nboundaries are open, we concentrate on the bubble with a conserved air volume\nV. Like film (F), non-equilibrium state can be divided into four regimes for\nbubble (B): (1) roll-off, (2) cusp approach, (3) pinch-off and (4) breakup. We\nestablish the existence of self-similarity in F-1, B-1 and B-3, and universal\nproperty in F-1 and B-1 for the profile of soap membrane. The former means that\nthe profile at successive times can be mapped to a master curve after being\nrescaled by the countdown time \\tau. Whiles, the latter further requires this\nmaster curve to be identical for different ring sizes R for film and different\nV and R for bubble while keeping V/R^3 fixed. The exhibition of universal\nproperty indicates that the process of memory erasing starts earlier than\nregime 3. We also found that the minimum radius scales as h_{min}~\\tau ^{1/2},\nindependent of V and pulling speed. Note that the validity of our discussion is\nlimited by the duration of roll-off regime from 10^{-2}~10^{-3} s.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the increasing emphasis on the safe autonomy for robots, model-based\nsafe control approaches such as Control Barrier Functions have been extensively\nstudied to ensure guaranteed safety during inter-robot interactions. In this\npaper, we introduce the Parametric Control Barrier Function (Parametric-CBF), a\nnovel variant of the traditional Control Barrier Function to extend its\nexpressivity in describing different safe behaviors among heterogeneous robots.\nInstead of assuming cooperative and homogeneous robots using the same safe\ncontrollers, the ego robot is able to model the neighboring robots' underlying\nsafe controllers through different Parametric-CBFs with observed data. Given\nlearned parametric-CBF and proved forward invariance, it provides greater\nflexibility for the ego robot to better coordinate with other heterogeneous\nrobots with improved efficiency while enjoying formally provable safety\nguarantees. We demonstrate the usage of Parametric-CBF in behavior prediction\nand adaptive safe control in the ramp merging scenario from the applications of\nautonomous driving. Compared to traditional CBF, Parametric-CBF has the\nadvantage of capturing varying drivers' characteristics given richer\ndescription of robot behavior in the context of safe control. Numerical\nsimulations are given to validate the effectiveness of the proposed method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study a general class of multiplicative functions by relating \"short\naverages\" to its \"long average\". More precisely, we estimate asymptotically the\nvariance of such a class of functions in short intervals using Fourier analysis\nand counting rational points on certain binary forms.\n  Our result is applicable to the interesting multiplicative functions\n$\\mu_k(n),\\frac{\\phi(n)}{n}, \\frac{n}{\\phi(n)},$ $\\mu^2(n)\\frac{\\phi(n)}{n}$,\n$\\sigma_{\\alpha}(n)$, $(-1)^{\\#\\{p\\,: \\, p^k|n\\}}$ and many others that\nestablish various new results and improvements in short intervals to the\nliterature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We perform a quantitative analysis of Monte Carlo simulations results of\nphase separation in ternary blends upon evaporation of one component.\nSpecifically, we calculate the average domain size and plot it as a function of\nsimulation time to compute the exponent of the obtained power-law. We compare\nand discuss results obtained by two different methods, for three different\nmodels: 2D binary-state model (Ising model), 2D ternary-state model with and\nwithout evaporation. For the ternary-state models, we study additionally the\ndependence of the domain growth on concentration, temperature and initial\ncomposition. We reproduce the expected 1/3 exponent for the Ising model, while\nfor the ternary-state model without evaporation and for the one with\nevaporation we obtain lower values of the exponent. It turns out that phase\nseparation patterns that can form in this type of systems are complex. The\nobtained quantitative results give valuable insights towards devising\ncomputable theoretical estimations of size effects on morphologies as they\noccur in the context of organic solar cells.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It has recently been shown that the Large Magellanic Cloud (LMC) has a\nsubstantial effect on the Milky Way's stellar halo and stellar streams. Here,\nwe explore how deformations of the Milky Way and LMC's dark matter haloes\naffect stellar streams, and whether these effects are observable. In\nparticular, we focus on the Orphan-Chenab (OC) stream which passes particularly\nclose to the LMC, and spans a large portion of the Milky Way's halo. We\nrepresent the Milky Way--LMC system using basis function expansions that\ncapture their evolution in an $N$-body simulation. We present the properties of\nthis system, such as the evolution of the densities and force fields of each\ngalaxy. The OC stream is evolved in this time-dependent, deforming potential,\nand we investigate the effects of the various moments of the Milky Way and the\nLMC. We find that the simulated OC stream is strongly influenced by the\ndeformations of both the Milky Way and the LMC, and that this effect is much\nlarger than current observational errors. In particular, the Milky Way dipole\nhas the biggest impact on the stream, followed by the evolution of the LMC's\nmonopole, and the LMC's quadrupole. Detecting these effects would confirm a key\nprediction of collisionless, cold dark matter, and would be a powerful test of\nalternative dark matter and alternative gravity models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hyperloop is a sonic-speed train transporting passengers and freights in a\nvacuum tube without friction or air resistance. Two essential communications in\nsuch vehicles are central control connection and real-time dispatching, also an\noptional data connection for passengers is welcome. The high mobility of\nHyperloop imposes a severe impact on the performance of wireless communication\nlinks. Therefore, designing a new wireless communication system is necessary to\ncope with the challenges. Motivated by the importance of doppler spreading,\nthis paper focuses on the characterizations of the wireless channel in\nHyperloop and then analyzes the performance degradation of the radio link.\nAfterward, a comprehensive overview of the present railway communication\ntechnologies as potential solutions for the Hyperloop project and a detailed\ndiscussion of their cons and pros are provided. It is shown that current\ncommunication technologies are not satisfactory for this scenario. Finally,\nthis paper concludes the key points that need to be considered in future\nrailway communications systems in order to overcome the Hyperloop communication\nchallenges. So, we open up a new issue that Hyperloop communications require\ndesigning a novel method or improving the existing technologies or a\ncombination of different techniques (some of these techniques are mentioned).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Half-integral weight modular forms are naturally viewed as automorphic forms\non the so-called metaplectic covering of\n$\\operatorname{GL}_2(\\mathbf{A}_{\\mathbf{Q}})$ -- a central extension by the\nroots of unity $\\mu_2$ in $\\mathbf{Q}$. For an odd prime number $p$, we give a\ncomplete classification of the smooth irreducible genuine mod-$p$\nrepresentations of the corresponding covering of\n$\\operatorname{GL}_2(\\mathbf{Q}_p)$ by showing that the functor of taking\npro-$p$-Iwahori-invariants and its left adjoint define a bijection onto the set\nof simple right modules of the pro-$p$ Iwahori Hecke algebra. As an application\nof our investigation of the irreducible subquotients of the universal module\nover the spherical Hecke algebra depending on some weight, we prove that being\nof finite length is equivalent to being finitely generated and admissible.\nFinally, we explain a relation to locally algebraic irreducible unramified\ngenuine principal series representations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate the intrinsic anomalous thermal Hall effect as an effective\nprobe of the order parameters of non-unitary pairing states of the putative\nspin-triplet superconductor UTe$_2$. We consider all symmetry-allowed\nnon-unitary states under magnetic fields comprehensively, and calculate the\nanomalous Hall conductivity arising from time-reversal symmetry-breaking Weyl\npoint nodes of the gap functions, assuming three types of Fermi surfaces;\nellipsoid-shaped, cylinder-shaped, and ring-shaped ones, which are predicted by\nprevious band calculations. We find the nonzero anomalous thermal Hall\nconductivity in the certain ratio of the two irreducible representations. The\nresults may be utilized for future exploration of pairing states of UTe$_2$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Large, curated datasets are required to leverage speech-based tools in\nhealthcare. These are costly to produce, resulting in increased interest in\ndata sharing. As speech can potentially identify speakers (i.e., voiceprints),\nsharing recordings raises privacy concerns. We examine the re-identification\nrisk for speech recordings, without reference to demographic or metadata, using\na state-of-the-art speaker recognition system. We demonstrate that the risk is\ninversely related to the number of comparisons an adversary must consider,\ni.e., the search space. Risk is high for a small search space but drops as the\nsearch space grows ($precision >0.85$ for $<1*10^{6}$ comparisons, $precision\n<0.5$ for $>3*10^{6}$ comparisons). Next, we show that the nature of a speech\nrecording influences re-identification risk, with non-connected speech (e.g.,\nvowel prolongation) being harder to identify. Our findings suggest that speaker\nrecognition systems can be used to re-identify participants in specific\ncircumstances, but in practice, the re-identification risk appears low.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that the process of non-instantaneous reheating during the\npost-inflationary period can have a sizable impact on the charged lepton\nequilibration temperature in the early Universe. This suggests relooking the\neffects of lepton flavors in the leptogenesis scenario where the production and\ndecay of right-handed neutrinos take place within this prolonged era of\nreheating. We find this observation has the potential to shift the flavor\nregime(s) of leptogenesis compared to the standard radiation-dominated era.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Organisations use issue tracking systems (ITSs) to track and document their\nprojects' work in units called issues. This style of documentation encourages\nevolutionary refinement, as each issue can be independently improved, commented\non, linked to other issues, and progressed through the organisational workflow.\nCommonly studied ITSs so far include GitHub, GitLab, and Bugzilla, while Jira,\none of the most popular ITS in practice with a wealth of additional\ninformation, has yet to receive similar attention. Unfortunately, diverse\npublic Jira datasets are rare, likely due to the difficulty in finding and\naccessing these repositories. With this paper, we release a dataset of 16\npublic Jiras with 1822 projects, spanning 2.7 million issues with a combined\ntotal of 32 million changes, 9 million comments, and 1 million issue links. We\nbelieve this Jira dataset will lead to many fruitful research projects\ninvestigating issue evolution, issue linking, cross-project analysis, as well\nas cross-tool analysis when combined with existing well-studied ITS datasets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Cluster-level inference procedures are widely used for brain mapping. These\nmethods compare the size of clusters obtained by thresholding brain maps to an\nupper bound under the global null hypothesis, computed using Random Field\nTheory or permutations. However, the guarantees obtained by this type of\ninference - i.e. at least one voxel is truly activated in the cluster - are not\ninformative with regards to the strength of the signal therein. There is thus a\nneed for methods to assess the amount of signal within clusters; yet such\nmethods have to take into account that clusters are defined based on the data,\nwhich creates circularity in the inference scheme. This has motivated the use\nof post hoc estimates that allow statistically valid estimation of the\nproportion of activated voxels in clusters. In the context of fMRI data, the\nAll-Resolutions Inference framework introduced in [25] provides post hoc\nestimates of the proportion of activated voxels. However, this method relies on\nparametric threshold families, which results in conservative inference. In this\npaper, we leverage randomization methods to adapt to data characteristics and\nobtain tighter false discovery control. We obtain Notip, for Non-parametric\nTrue Discovery Proportion control: a powerful, non-parametric method that\nyields statistically valid guarantees on the proportion of activated voxels in\ndata-derived clusters. Numerical experiments demonstrate substantial gains in\nnumber of detections compared with state-of-the-art methods on 36 fMRI\ndatasets. The conditions under which the proposed method brings benefits are\nalso discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider a logistics planning problem of prepositioning relief items in\npreparation for an impending hurricane landfall. This problem is modeled as a\nmultiperiod network flow problem where the objective is to minimize the\nlogistics cost of operating the network and the penalty for unsatisfied demand.\nWe assume that the demand for relief items can be derived from the hurricane's\npredicted intensity and landfall location, which evolves according to a Markov\nchain. We consider this problem in two different settings, depending on whether\nthe time of landfall is assumed to be deterministic (and known a priori) or\nrandom. For the former case, we introduce a fully adaptive multi-stage\nstochastic programming (MSP) model that allows the decision-maker to adjust the\nprepositioning decisions, sequentially, over multiple stages, as the\nhurricane's characteristics become clearer. For the latter case, we extend the\nMSP model with a random number of stages introduced in Guigues (2021), to the\ncase where the underlying stochastic process is assumed to be stage-wise\ndependent. We benchmark the performance of the MSP models with other\napproximation policies such as the static and rolling-horizon two-stage\nstochastic programming approaches. Our numerical results provide key insight\ninto the value of MSP, in disaster relief logistics planning.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  I am a second-year cognitive science major, and as a student who has\ncompleted my physical science distributive requirements, I will likely never\nagain come across Gauss's law. So why do I feel that the time and effort I\ndevoted to solving Gauss's law problems was worth it? Partly, I inherently\nenjoy the learning process and the new perspective on the physical world I have\nacquired by understanding electromagnetism. But I was also inspired by the ways\nin which physics problems train the mind in effective problem-solving\nstrategies. (Of course I was -- I am a cognitive science major!) Two themes\nemerged as I reflected on this realization. First, physics problems serve as\nuseful toy models for more complex problems outside of physics, training us in\nbroadly transferable problem-solving skills. Second, the physics\nproblem-solving process invites us to reflect on our unique cognitive and\naffective processes. These themes are interconnected and complimentary. An\nimproved metacognitive understanding of our minds facilitates solving\nprogressively more complex problems, and the act of solving increasingly\ndifficult problems provides further insight into our minds. In what follows,\nProfessor Zosia Krusberg and I consider nine general lessons offered by the\nphysics problem-solving process.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spontaneous material shape changes, such as swelling, growth or thermal\nexpansion, can be used to trigger dramatic elastic instabilities in thin\nshells. These instabilities originate in geometric incompatibility between the\npreferred extrinsic and intrinsic curvature of the shell, which may be modified\nby active deformations through the thickness and in plane respectively. Here,\nwe solve the simplest possible model of such instabilities, which assumes the\nshells are shallow, thin enough to bend but not stretch, and subject to\nhomogeneous preferred curvatures. We consider separately the cases of zero,\npositive and negative Gaussian curvature. We identify two types of\nsuper-critical symmetry breaking instability, in which the shell's principal\ncurvature spontaneously breaks discrete up-down symmetry and continuous planar\nisotropy respectively. These are then augmented by inversion instabilities, in\nwhich the shell jumps sub-critically between up/down broken symmetry states,\nand rotation instabilities, in which the curvatures rotate by 90 degrees\nbetween states of broken isotropy without release of energy. Each instability\nhas a thickness independent threshold value for the preferred extrinsic\ncurvature proportional to the square-root of Gauss curvature. Finally, we show\nthat the threshold for the isotropy-breaking instability is the same for deep\nspherical caps, in good agreement with recently published data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Third-party libraries (TPLs) are frequently reused in software to reduce\ndevelopment cost and the time to market. However, external library dependencies\nmay introduce vulnerabilities into host applications. The issue of library\ndependency has received considerable critical attention. Many package managers,\nsuch as Maven, Pip, and NPM, are proposed to manage TPLs. Moreover, a\nsignificant amount of effort has been put into studying dependencies in\nlanguage ecosystems like Java, Python, and JavaScript except C/C++. Due to the\nlack of a unified package manager for C/C++, existing research has only few\nunderstanding of TPL dependencies in the C/C++ ecosystem, especially at large\nscale.\n  Towards understanding TPL dependencies in the C/C++ecosystem, we collect\nexisting TPL databases, package management tools, and dependency detection\ntools, summarize the dependency patterns of C/C++ projects, and construct a\ncomprehensive and precise C/C++ dependency detector. Using our detector, we\nextract dependencies from a large-scale database containing 24K C/C++\nrepositories from GitHub. Based on the extracted dependencies, we provide the\nresults and findings of an empirical study, which aims at understanding the\ncharacteristics of the TPL dependencies. We further discuss the implications to\nmanage dependency for C/C++ and the future research directions for software\nengineering researchers and developers in fields of library development,\nsoftware composition analysis, and C/C++package manager.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Motivated by their role as the direct or indirect source of many of the\nelements in the Universe, numerical modeling of core collapse supernovae began\nmore than five decades ago. Progress toward ascertaining the explosion\nmechanism(s) has been realized through increasingly sophisticated models, as\nphysics and dimensionality have been added, as physics and numerical modeling\nhave improved, and as the leading computational resources available to modelers\nhave become far more capable. The past five to ten years have witnessed the\nemergence of a consensus across the core collapse supernova modeling community\nthat had not existed in the four decades prior. For the majority of progenitors\n- i.e., slowly rotating progenitors - the efficacy of the delayed shock\nmechanism, where the stalled supernova shock wave is revived by neutrino\nheating by neutrinos emanating from the proto-neutron star, has been\ndemonstrated by all core collapse supernova modeling groups, across progenitor\nmass and metallicity. With this momentum, and now with a far deeper\nunderstanding of the dynamics of these events, the path forward is clear. While\nmuch progress has been made, much work remains to be done, but at this time we\nhave every reason to be optimistic we are on track to answer one of the most\nimportant outstanding questions in astrophysics: How do massive stars end their\nlives?\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep learning has gained huge empirical successes in large-scale\nclassification problems. In contrast, there is a lack of statistical\nunderstanding about deep learning methods, particularly in the minimax\noptimality perspective. For instance, in the classical smooth decision boundary\nsetting, existing deep neural network (DNN) approaches are rate-suboptimal, and\nit remains elusive how to construct minimax optimal DNN classifiers. Moreover,\nit is interesting to explore whether DNN classifiers can circumvent the curse\nof dimensionality in handling high-dimensional data. The contributions of this\npaper are two-fold. First, based on a localized margin framework, we discover\nthe source of suboptimality of existing DNN approaches. Motivated by this, we\npropose a new deep learning classifier using a divide-and-conquer technique:\nDNN classifiers are constructed on each local region and then aggregated to a\nglobal one. We further propose a localized version of the classical Tsybakov's\nnoise condition, under which statistical optimality of our new classifier is\nestablished. Second, we show that DNN classifiers can adapt to low-dimensional\ndata structures and circumvent the curse of dimensionality in the sense that\nthe minimax rate only depends on the effective dimension, potentially much\nsmaller than the actual data dimension. Numerical experiments are conducted on\nsimulated data to corroborate our theoretical results.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Paging is a prototypical problem in the area of online algorithms. It has\nalso played a central role in the development of learning-augmented algorithms\n-- a recent line of research that aims to ameliorate the shortcomings of\nclassical worst-case analysis by giving algorithms access to predictions. Such\npredictions can typically be generated using a machine learning approach, but\nthey are inherently imperfect. Previous work on learning-augmented paging has\ninvestigated predictions on (i) when the current page will be requested again\n(reoccurrence predictions), (ii) the current state of the cache in an optimal\nalgorithm (state predictions), (iii) all requests until the current page gets\nrequested again, and (iv) the relative order in which pages are requested.\n  We study learning-augmented paging from the new perspective of requiring the\nleast possible amount of predicted information. More specifically, the\npredictions obtained alongside each page request are limited to one bit only.\nWe consider two natural such setups: (i) discard predictions, in which the\npredicted bit denotes whether or not it is ``safe'' to evict this page, and\n(ii) phase predictions, where the bit denotes whether the current page will be\nrequested in the next phase (for an appropriate partitioning of the input into\nphases). We develop algorithms for each of the two setups that satisfy all\nthree desirable properties of learning-augmented algorithms -- that is, they\nare consistent, robust and smooth -- despite being limited to a one-bit\nprediction per request. We also present lower bounds establishing that our\nalgorithms are essentially best possible.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study speech-to-speech translation (S2ST) that translates speech from one\nlanguage into another language and focuses on building systems to support\nlanguages without standard text writing systems. We use English-Taiwanese\nHokkien as a case study, and present an end-to-end solution from training data\ncollection, modeling choices to benchmark dataset release. First, we present\nefforts on creating human annotated data, automatically mining data from large\nunlabeled speech datasets, and adopting pseudo-labeling to produce weakly\nsupervised data. On the modeling, we take advantage of recent advances in\napplying self-supervised discrete representations as target for prediction in\nS2ST and show the effectiveness of leveraging additional text supervision from\nMandarin, a language similar to Hokkien, in model training. Finally, we release\nan S2ST benchmark set to facilitate future research in this field. The demo can\nbe found at https://huggingface.co/spaces/facebook/Hokkien_Translation .\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In Bayesian Network Structure Learning (BNSL), one is given a variable set\nand parent scores for each variable and aims to compute a DAG, called Bayesian\nnetwork, that maximizes the sum of parent scores, possibly under some\nstructural constraints. Even very restricted special cases of BNSL are\ncomputationally hard, and, thus, in practice heuristics such as local search\nare used. A natural approach for a local search algorithm is a hill climbing\nstrategy, where one replaces a given BNSL solution by a better solution within\nsome pre-defined neighborhood as long as this is possible. We study\nordering-based local search, where a solution is described via a topological\nordering of the variables. We show that given such a topological ordering, one\ncan compute an optimal DAG whose ordering is within inversion distance $r$ in\nsubexponential FPT time; the parameter $r$ allows to balance between solution\nquality and running time of the local search algorithm. This running time bound\ncan be achieved for BNSL without structural constraints and for all structural\nconstraints that can be expressed via a sum of weights that are associated with\neach parent set. We also introduce a related distance called `window inversions\ndistance' and show that the corresponding local search problem can also be\nsolved in subexponential FPT time for the parameter $r$. For two further\nnatural modification operations on the variable orderings, we show that\nalgorithms with an FPT time for $r$ are unlikely. We also outline the limits of\nordering-based local search by showing that it cannot be used for common\nstructural constraints on the moralized graph of the network.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This contribution deals with an extension to our developed novel cubature\nmethods of degrees 5 on Wiener space. In our previous studies, we have shown\nthat the cubature formula is exact for all multiple Stratonovich integrals up\nto dimension equal to the degree. In fact, cubature method reduces solving a\nstochastic differential equation to solving a finite set of ordinary\ndifferential equations. Now, we apply the above methods to construct trinomial\nmodels and to price different financial derivatives. We will compare our\nnumerical solutions with the Black's and Black--Scholes models' analytical\nsolutions. The constructed model has practical usage in pricing American-style\nderivatives and can be extended to more sophisticated stochastic market models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  3D object detection from monocular image(s) is a challenging and\nlong-standing problem of computer vision. To combine information from different\nperspectives without troublesome 2D instance tracking, recent methods tend to\naggregate multiview feature by sampling regular 3D grid densely in space, which\nis inefficient. In this paper, we attempt to improve multi-view feature\naggregation by proposing a learnable keypoints sampling method, which scatters\npseudo surface points in 3D space, in order to keep data sparsity. The\nscattered points augmented by multi-view geometric constraints and visual\nfeatures are then employed to infer objects location and shape in the scene. To\nmake up the limitations of single frame and model multi-view geometry\nexplicitly, we further propose a surface filter module for noise suppression.\nExperimental results show that our method achieves significantly better\nperformance than previous works in terms of 3D detection (more than 0.1 AP\nimprovement on some categories of ScanNet). The code will be publicly\navailable.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a family of new solutions to the tetrahedron equation of the form\n$RLLL=LLLR$, where $L$ operator may be regarded as a quantized six-vertex model\nwhose Boltzmann weights are specific representations of the $q$-oscillator or\n$q$-Weyl algebras. When the three $L$'s are associated with the $q$-oscillator\nalgebra, $R$ coincides with the known intertwiner of the quantized coordinate\nring $A_q(sl_3)$. On the other hand, $L$'s based on the $q$-Weyl algebra lead\nto new $R$'s whose elements are either factorized or expressed as a terminating\n$q$-hypergeometric type series.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding the rheology of granular assemblies is important for natural\nand engineering systems, but the relationship between inter-particle friction\n(or microscopic friction) and macroscopic friction is still not well\nunderstood. In this study, using the the discrete element method (DEM) with\nspherical particles and realistic contact laws, we investigate the mechanics of\ngranular systems with a wide range of inter-particle frictional coefficients\nand aim to establish a friction-dependent rheology for dry granular flows. The\ncorresponding results show that increasing inter-particle friction dramatically\nincreases the effective frictional coefficient, $\\mu_{\\textrm{eff}}$, while\ndecreasing the solid fraction of the system and increasing the transitional\ninertial number that marks the division of quasi-static regimes and\nintermediate flow regimes. We further propose a new dimensionless number,\n$\\mathcal{M}$, as a ratio between the inertial effect and frictional effect,\nwhich is similar to the effective aspect ratio in granular column collapses,\nand unifies the influence of inter-particle friction with the inertial number.\nWe then establish a relationship between $\\mathcal{M}$ and the dimensionless\ngranular temperature, $\\Theta$, to further universalize the influence of\ninter-particle frictions. Such study can broaden the application of the\n$\\mu(I)$ rheology in natural and engineering systems and help establish a more\ngeneral constitutive model for complex granular systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The cold dark matter (CDM) scenario predicts that galactic halos should host\na huge amount of subhalos possibly lighter than planets, depending on the\nnature of dark matter. Predicting their abundance and distribution has\nimportant implications for dark matter searches and searches for subhalos\nthemselves, as they could provide a decisive test of the CDM paradigm. A major\ndifficulty in subhalo population model building is to account for the\ngravitational stripping induced by baryons, which strongly impact on the\noverall dynamics inside galaxies. In this paper, we focus on these \"baryonic\"\ntides from analytical perspectives, summarizing previous work on galactic disk\nshocking, and thoroughly revisiting the impact of individual encounters with\nstars. For the latter, we go beyond the reference calculation of Gerhard and\nFall (1983) to deal with penetrative encounters, and provide new analytical\nresults. Based upon a full statistical analysis of subhalo energy change during\nmultiple stellar encounters possibly occurring during disk crossing, we show\nthat subhalos lighter than $\\sim 1$~M$_\\odot$ are very efficiently pruned by\nstellar encounters. This modifies their mass function in a stellar environment.\nIn contrast, disk shocking is more efficient at pruning massive subhalos. In\nshort, if reasonably resilient, subhalos surviving disk crossing have lost all\ntheir mass but an inner cuspy part, with a tidal mass function strongly\ndeparting from the cosmological one. If fragile, stellar encounters make their\nnumber density drop by an additional order of magnitude with respect to\ndisk-shocking effects only (e.g., at the solar position in the Milky Way). Our\nresults can be incorporated to any analytical or numerical subhalo population\nmodel, as we show for illustration. This study complements those based on\ncosmological simulations, which cannot resolve dark matter subhalos on such\nsmall scales.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Control of structured light is of great importance to explore fundamental\nphysical effects and extend practical scientific applications, which has been\nadvanced by accepting methods of quantum optics - many classical analogies of\nexotic quantum states were designed using structured modes. However, the\nprevailing quantum-like structured modes are limited by discrete states where\nthe mode index is analog to the photon number state. Yet, beyond discrete\nstates, there is a broad range of quantum states to be explored in the field of\nstructured light -- continuous-variable (CV) states. As a typical example of CV\nstates, squeezed state plays a prominent role in high-sensitivity\ninterferometry and gravitational wave detection. In this work, we bring\ntogether two seemingly disparate branches of physics, namely, classical\nstructured light and quantum squeezed state. We propose the structured light\nanalogy of squeezed state (SLASS), which can break the spatial limit following\nthe process of surpassing the standard quantum limit (SQL) with quantum\nsqueezed states. This work paves the way for adopting methods from CV quantum\nstates into structured light, opening new research directions of CV\nentanglement, teleportation, classical and quantum informatics of structured\nlight in the future.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The MIllimeter Sardinia radio Telescope Receiver based on Array of Lumped\nelements kids, MISTRAL, is a millimetric ($\\simeq 90GHz$) multipixel camera\nbeing built for the Sardinia Radio Telescope. It is going to be a facility\ninstrument and will sample the sky with 12 arcsec angular resolution, 4 arcmin\nfield of view, through 408 Kinetic Inductance Detectors (KIDs). The\nconstruction and the beginning of commissioning is planned to be in 2022.\nMISTRAL will allow the scientific community to propose a wide variety of\nscientific cases including protoplanetary discs study, star forming regions,\ngalaxies radial profiles, and high angular resolution measurements of the\nSunyaev Zel'dovich (SZ) effect with the investigation of the morphology of\ngalaxy cluster and the search for the Cosmic Web.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a novel Transformer-based network architecture for instance-aware\nimage-to-image translation, dubbed InstaFormer, to effectively integrate\nglobal- and instance-level information. By considering extracted content\nfeatures from an image as tokens, our networks discover global consensus of\ncontent features by considering context information through a self-attention\nmodule in Transformers. By augmenting such tokens with an instance-level\nfeature extracted from the content feature with respect to bounding box\ninformation, our framework is capable of learning an interaction between object\ninstances and the global image, thus boosting the instance-awareness. We\nreplace layer normalization (LayerNorm) in standard Transformers with adaptive\ninstance normalization (AdaIN) to enable a multi-modal translation with style\ncodes. In addition, to improve the instance-awareness and translation quality\nat object regions, we present an instance-level content contrastive loss\ndefined between input and translated image. We conduct experiments to\ndemonstrate the effectiveness of our InstaFormer over the latest methods and\nprovide extensive ablation studies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We explain how to adapt the methods of Abouzaid-McLean-Smith to the setting\nof Hamiltonian Floer theory. We develop a language around equivariant\n``$\\langle k \\rangle$-manifolds'', which are a type of manifold-with-corners\nthat suffices to capture the combinatorics of Floer-theoretic constructions. We\ndescribe some geometry which allows us to straightforwardly adapt Lashofs's\nstable equivariant smoothing theory and Bau-Xu's theory of FOP-perturbations to\n$\\langle k \\rangle$-manifolds. This allows us to compatibly smooth global\nKuranishi charts on all Hamiltonian Floer trajectories at once, in order to\nextract a Floer complex and prove the Arnol'd conjecture over the integers. We\nalso make first steps towards a further development of the theory, outlining\nthe analog of bifurcation analysis in this setting, which can give short\nindependence proofs of the independence of Floer-theoretic invariants of all\nchoices involved in their construction.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider multiwinner elections in Euclidean space using the minimax\nChamberlin-Courant rule. In this setting, voters and candidates are embedded in\na $d$-dimensional Euclidean space, and the goal is to choose a committee of $k$\ncandidates so that the rank of any voter's most preferred candidate in the\ncommittee is minimized. (The problem is also equivalent to the ordinal version\nof the classical $k$-center problem.) We show that the problem is NP-hard in\nany dimension $d \\geq 2$, and also provably hard to approximate. Our main\nresults are three polynomial-time approximation schemes, each of which finds a\ncommittee with provably good minimax score. In all cases, we show that our\napproximation bounds are tight or close to tight. We mainly focus on the\n$1$-Borda rule but some of our results also hold for the more general\n$r$-Borda.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The present paper introduces the analysis of the eigenvalue problem for the\nelasticity equations when the so called Navier-Lam\\'e system is considered.\nSuch a system introduces the displacement, rotation and pressure of some linear\nand elastic structure. The analysis of the spectral problem is based in the\ncompact operators theory. A finite element method based in polynomials of\ndegree $k\\geq 1$ are considered in order to approximate the eigenfrequencies\nand eigenfunctions of the system. Convergence and error estimate are presented.\nAn a posteriori error analysis is performed, where the reliability and\nefficiency of the proposed estimator is proved. We end this contribution\nreporting a series of numerical tests in order to assess the performance of the\nproposed numerical method, for the a priori and a posteriori estimates.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we prove a Robinson consistency theorem for a class of\nmany-sorted hybrid logics as a consequence of an Omitting Types Theorem. An\nimportant corollary of this result is an interpolation theorem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Implicit integration of the viscous term can significantly improve\nperformance in computational fluid dynamics for highly viscous fluids such as\nlava. We show improvements over our previous proposal for semi-implicit viscous\nintegration in Smoothed Particle Hydrodynamics, extending it to support a wider\nrange of boundary models. Due to the resulting loss of matrix symmetry, a key\nadvancement is a more robust version of the biconjugate gradient stabilized\nmethod to solve the linear systems, that is also better suited for\nparallelization in both shared-memory and distributed-memory systems. The\nadvantages of the new solver are demostrated in applications with both\nNewtonian and non-Newtonian fluids, covering both the numerical aspect\n(improved convergence thanks to the possibility to use more accurate boundary\nmodel) and the computing aspect (with excellent strong scaling and satisfactory\nweak scaling).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We report the results of six years (2013-2018) of measurements of $^{222}$Rn\nair concentration, relative humidity, atmospheric pressure and temperature in\nthe halls A, B and C of the Canfranc Underground Laboratory (LSC). We have\ncalculated all the Pearson correlation coefficients among these parameters and\nwe have found a positive correlation between the $^{222}$Rn concentration and\nthe relative humidity. Both correlated variables show a seasonal periodicity.\nThe joint analysis of laboratory data and four years (2015-2018) of the\nmeteorological variables outside the laboratory shows the correlation between\nthe $^{222}$Rn concentration and the outside temperature. The collected\ninformation stresses the relevance of designing good Rn-mitigation strategies\nin current and future experiments at LSC; in particular, we have checked for\ntwo years (2017-2018) the good performance of the mitigation procedure of the\nANAIS--112 experiment. Finally, in another measurement (2019-2021) for two\nyears of live time, we report an upper limit to the residual $^{222}$Rn content\nof the radon-free air provided by the radon abatement system installed in the\nlaboratory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Single-reference coupled-cluster theory is an accurate and affordable\ncomputational method for the nuclear many-body problem. For open-shell nuclei,\nthe reference state typically breaks rotational invariance and angular momentum\nmust be restored as a good quantum number. We perform angular-momentum\nprojection after variation and employ the disentangled coupled-cluster\nformalism and a Hermitian approach. We compare our results with benchmarks for\n$^8$Be and $^{20}$Ne using a two-nucleon interaction from chiral effective\nfield theory and for $pf$-shell nuclei within the traditional shell model. We\ncompute the rotational band in the exotic nucleus $^{34}$Mg and find agreement\nwith data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the challenge of finding a deterministic policy for a Markov\ndecision process that uniformly (in all states) maximizes one reward subject to\na probabilistic constraint over a different reward. Existing solutions do not\nfully address our precise problem definition, which nevertheless arises\nnaturally in the context of safety-critical robotic systems. This class of\nproblem is known to be hard, but the combined requirements of determinism and\nuniform optimality can create learning instability. In this work, after\ndescribing and motivating our problem with a simple example, we present a\nsuitable constrained reinforcement learning algorithm that prevents learning\ninstability, using recursive constraints. Our proposed approach admits an\napproximative form that improves efficiency and is conservative w.r.t. the\nconstraint.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The production, shipping, usage, and disposal of consumer goods have a\nsubstantial impact on greenhouse gas emissions and the depletion of resources.\nModern retail platforms rely heavily on Machine Learning (ML) for their search\nand recommender systems. Thus, ML can potentially support efforts towards more\nsustainable consumption patterns, for example, by accounting for sustainability\naspects in product search or recommendations. However, leveraging ML potential\nfor reaching sustainability goals requires data on sustainability.\nUnfortunately, no open and publicly available database integrates\nsustainability information on a product-by-product basis. In this work, we\npresent the GreenDB, which fills this gap. Based on search logs of millions of\nusers, we prioritize which products users care about most. The GreenDB schema\nextends the well-known schema.org Product definition and can be readily\nintegrated into existing product catalogs to improve sustainability information\navailable for search and recommendation experiences. We present our proof of\nconcept implementation of a scraping system that creates the GreenDB dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Although the asymptotic properties of the parameter estimator have been\nderived in the $p_{0}$ model for directed graphs with the differentially\nprivate bi-degree sequence, asymptotic theory in general models is still\nlacking. In this paper, we release the bi-degree sequence of directed graphs\nvia the discrete Laplace mechanism, which satisfies differential privacy. We\nuse the moment method to estimate the unknown model parameter. We establish a\nunified asymptotic result, in which consistency and asymptotic normality of the\ndifferentially private estimator holds. We apply the unified theoretical result\nto the Probit model. Simulations and a real data demonstrate our theoretical\nfindings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We make a detailed analysis of three key algorithms (Serial Dictatorship and\nthe naive and adaptive variants of the Boston algorithm) for the housing\nallocation problem, under the assumption that agent preferences are chosen iid\nuniformly from linear orders on the items. We compute limiting distributions\n(with respect to some common utility functions) as $n\\to \\infty$ of both the\nutilitarian welfare and the order bias. To do this, we compute limiting\ndistributions of the outcomes for an arbitrary agent whose initial relative\nposition in the tiebreak order is $\\theta\\in[0,1]$, as a function of $\\theta$.\nThe results for the Boston algorithms are all new, and we expect that these\nfundamental results on the stochastic processes underlying these algorithms\nwill have wider applicability in future. Overall our results show that the\ndifferences in utilitarian welfare performance of the three algorithms are\nfairly small but still important. However, the differences in order bias are\nmuch greater. Also, Naive Boston beats Adaptive Boston, which beats Serial\nDictatorship, on both utilitarian welfare and order bias.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we give a classification of Codazzi hypersurfaces in a Lie\ngroup $(Nil^{4},\\widetilde g)$. We also give a characterization of a class of\nminimal hypersurfaces in $(Nil^{4},\\widetilde g)$ with an example, non trivial,\nof a minimal surface in this class.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum discord has been shown to be a resource for quantum advantage in\naddition to quantum entanglement. While many experiments have demonstrated\nclassical analogies of entanglement, none have done so for discord. We present\na proof-of-concept demonstration for creating a classical analogue of quantum\ndiscord using classical light that takes advantage of the analogy between the\nstate of two qubits and the spatial modes of a Laguerre-Gauss beam. We\ndemonstrate the validity of this approach by comparing the intensity profiles\nof theoretical simulations to experimental results for different values of\ndiscord. Such a classical analogue of quantum discord may provide further\ninsight in understanding and development of quantum information technologies\nthat make use of discord.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Ultracold collisions of neutral atoms and molecules have been of great\ninterest since experimental advances enabled the cooling and trapping of such\nspecies. This study is a theoretical investigation of a low-energy collision\nbetween an alkali atom and a diatomic molecule, accompanied by absorption of a\nphoton from an external electromagnetic field. The long-range interaction\nbetween the two species is treated, including the atomic spin-orbit\ninteraction. The long-range potential energy curves for the triatomic complex\nare calculated in realistic detail, while the short-range behavior is mimicked\nby applying different boundary conditions at the van der Waals length. The\nphotoassociation (PA) rate of an atom colliding with a dimer is calculated for\ndifferent alkali atoms, namely Na and Cs. The model developed in this study is\nalso tested against known results for the formation rate of the Cs$_3$ complex\nvia PA, namely to compare with the work done by Rios et al., PRL 115, 073201\n(2015), and the results are in generally good agreement.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A homomorphism from a graph $G$ to a graph $H$ is an edge-preserving mapping\nfrom $V(G)$ to $V(H)$. Let $H$ be a fixed graph with possible loops. In the\nlist homomorphism problem, denoted by \\textsc{LHom}($H$), the instance is a\ngraph $G$, whose every vertex is equipped with a subset of $V(H)$, called list.\nWe ask whether there exists a homomorphism from $G$ to $H$, such that every\nvertex from $G$ is mapped to a vertex from its list.\n  We study the complexity of the \\textsc{LHom}($H$) problem in intersection\ngraphs of various geometric objects. In particular, we are interested in\nanswering the question for what graphs $H$ and for what types of geometric\nobjects, the \\textsc{LHom}($H$) problem can be solved in time subexponential in\nthe number of vertices of the instance.\n  We fully resolve this question for string graphs, i.e., intersection graphs\nof continuous curves in the plane. Quite surprisingly, it turns out that the\ndichotomy exactly coincides with the analogous dichotomy for graphs excluding a\nfixed path as an induced subgraph [Okrasa, Rz\\k{a}\\.zewski, STACS 2021].\n  Then we turn our attention to subclasses of string graphs, defined as\nintersections of fat objects. We observe that the (non)existence of\nsubexponential-time algorithms in such classes is closely related to the size\n$\\mathrm{mrc}(H)$ of a maximum reflexive clique in $H$, i.e., maximum number of\npairwise adjacent vertices, each of which has a loop. We study the maximum\nvalue of $\\mathrm{mrc}(H)$ that guarantees the existence of a\nsubexponential-time algorithm for \\textsc{LHom}($H$) in intersection graphs of\n(i) convex fat objects, (ii) fat similarly-sized objects, and (iii) disks. In\nthe first two cases we obtain optimal results, by giving matching algorithms\nand lower bounds.\n  Finally, we discuss possible extensions of our results to weighted\ngeneralizations of \\textsc{LHom}($H$).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Communication overhead is one of the major obstacles to train large deep\nlearning models at scale. Gradient sparsification is a promising technique to\nreduce the communication volume. However, it is very challenging to obtain real\nperformance improvement because of (1) the difficulty of achieving an scalable\nand efficient sparse allreduce algorithm and (2) the sparsification overhead.\nThis paper proposes O$k$-Top$k$, a scheme for distributed training with sparse\ngradients. O$k$-Top$k$ integrates a novel sparse allreduce algorithm (less than\n6$k$ communication volume which is asymptotically optimal) with the\ndecentralized parallel Stochastic Gradient Descent (SGD) optimizer, and its\nconvergence is proved. To reduce the sparsification overhead, O$k$-Top$k$\nefficiently selects the top-$k$ gradient values according to an estimated\nthreshold. Evaluations are conducted on the Piz Daint supercomputer with neural\nnetwork models from different deep learning domains. Empirical results show\nthat O$k$-Top$k$ achieves similar model accuracy to dense allreduce. Compared\nwith the optimized dense and the state-of-the-art sparse allreduces,\nO$k$-Top$k$ is more scalable and significantly improves training throughput\n(e.g., 3.29x-12.95x improvement for BERT on 256 GPUs).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Omni-resonance refers to the broadening of the spectral transmission through\na planar cavity, not by changing the cavity structure, but by judiciously\npreconditioning the incident optical field. As such, broadband imaging can be\nperformed through such a cavity with all the wavelengths simultaneously\nresonating. We examine here the spatial resolution of omni-resonant imaging and\nfind that the spectral linewidth of the cavity resonance determines the spatial\nresolution. Surprisingly, the spatial resolution improves at longer wavelengths\nbecause of the negative angular dispersion intrinsic to Fabry-Perot resonances,\nin contrast to conventional diffraction-limited optical imaging systems where\nthe spatial resolution improves at shorter wavelengths. These results are\nimportant for applications ranging from transparent solar windows to nonlinear\nresonant image processing.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We discuss examples of two dimensional metallic states with charge\nfractionalization, and we will demonstrate that the mechanism of charge\nfractionalization leads to exotic metallic behaviors at low and intermediate\ntemperature. The simplest example of such state is constructed by fermionic\npartons at finite density coupled to a $Z_N$ gauge field, whose properties can\nbe studied through rudimentary methods. This simple state has the following\nexotic features: (1) at low temperature this state is a \"bad metal\" whose\nresistivity can exceed the Mott-Ioffe-Regel limit; (2) while increasing\ntemperature $T$ the resistivity $\\rho(T)$ is a nonmonotonic function, and it\ncrosses over from a bad metal at low $T$ to a good metal at relatively high\n$T$; (3) the optical conductivity $\\sigma(\\omega)$ has a small Drude weight at\nlow $T$, and a larger Drude weight at intermediate $T$; (4) at low temperature\nthe metallic state has a large Lorenz number, which strongly violates the\nWiedemann-Franz law. A more complex example with fermionic partons at finite\ndensity coupled to a SU(N) gauge field will also be constructed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Which factors influence the human assessment of creativity exhibited by a\ncomputational system is a core question of computational creativity (CC)\nresearch. Recently, the system's embodiment has been put forward as such a\nfactor, but empirical studies of its effect are lacking. To this end, we\npropose an experimental framework which isolates the effect of embodiment on\nthe perception of creativity from its effect on creativity per se. We not only\nmanipulate the system's embodiment, but also the perceptual evidence as the\nbasis for the human creativity assessment. We motivate the core framework with\nembodiment and perceptual evidence as independent and the creative process as\ncontrolled variable, and we provide recommendations on measuring the assessment\nof creativity as dependent variable. We hope the framework will inspire others\nto study the human perception of embodied CC in a principled manner.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The objective is to find a Cellular Automata rule that can form a 2D point\npattern with a maximum number of points (1-cells). Points are not allowed to\ntouch each other, they have to be separated by 0-cells, and every 0-cell can\nfind at least one point in its Moore-neighborhood. Probabilistic rules are\ndesigned that can solve this task with asynchronous updating and cyclic\nboundary condition. The task is considered as a tiling problem, where point\ntiles are used to cover the space with overlaps. A point tile consists of a\ncenter pixel (the kernel with value 1) and 8 surrounding pixels forming the\nhull with value 0. The term pixel is used to distinguish the cells of a tile\nfrom the cells of a cellular automaton. For each of the 9 tile pixels a\nso-called template is defined by a shift of the point tile. In the rule\napplication, the 9 templates are tested at the actual cell position. If all\ntemplate pixels (except the central reference pixel) of a template match with\nthe corresponding neighbors of the actual cell under consideration, the cell's\nstate is adjusted to the reference pixel's value. Otherwise the cell is set to\nthe random value 0 or 1 with a certain probability. The hull pixels are allowed\nto overlap. In order to evolve a maximum of points, the overlap between tiles\nhas to be maximized. To do that, the number of template hits is counted.\nDepending on the hit-number, additional noise is injected with certain\nprobabilities. Thereby optimal patterns with the maximum number of points can\nbe evolved. The behavior and performance of the designed rules is evaluated for\ndifferent parameter settings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We demonstrate a genuine quantum feature of heat: the power emitted by a\nqubit (quantum two-level system) into a reservoir under continuous driving\nshows well-defined peaks as a function of frequency $f$. These resonant\nfeatures appear due to the accumulation of the dynamical phase during the\ndriving. The position of the $n$th maximum is given by $f=f_{\\rm M}/n$, where\n$f_{\\rm M}$ is the mean frequency of the qubit in the cycle, and their\nappearance is independent of the form of the drive and the number of heat baths\nattached, and even the presence or absence of spectral filtering. We propose\nthat this non-trivial quantum heat can be detected by observing the\nsteady-state power absorbed by a resistor acting as a bolometer attached to a\ndriven superconducting qubit. This quantum heat is expected to play a crucial\nrole in the performance of driven thermal devices such as quantum heat engines\nand refrigerators. We also show that by optimizing the cycle protocol, we\nrecover the favorable classical limit in fast driven systems without the use of\ncounter-diabatic drive protocols.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we introduce DA$^2$, the first large-scale dual-arm\ndexterity-aware dataset for the generation of optimal bimanual grasping pairs\nfor arbitrary large objects. The dataset contains about 9M pairs of\nparallel-jaw grasps, generated from more than 6000 objects and each labeled\nwith various grasp dexterity measures. In addition, we propose an end-to-end\ndual-arm grasp evaluation model trained on the rendered scenes from this\ndataset. We utilize the evaluation model as our baseline to show the value of\nthis novel and nontrivial dataset by both online analysis and real robot\nexperiments. All data and related code will be open-sourced at\nhttps://sites.google.com/view/da2dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Existence of symmetric solutions to the Gaussian Minkowski problem was\nestablished by Huang, Xi and Zhao. In this paper, we show the existence of\nnon-symmetric solutions to this problem by studying the related\nMonge-Amp\\`{e}re type equation on the sphere.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Knowledge distillation is an approach to transfer information on\nrepresentations from a teacher to a student by reducing their difference. A\nchallenge of this approach is to reduce the flexibility of the student's\nrepresentations inducing inaccurate learning of the teacher's knowledge. To\nresolve it in BERT transferring, we investigate distillation of structures of\nrepresentations specified to three types: intra-feature, local inter-feature,\nglobal inter-feature structures. To transfer them, we introduce \\textit{feature\nstructure distillation} methods based on the Centered Kernel Alignment, which\nassigns a consistent value to similar features structures and reveals more\ninformative relations. In particular, a memory-augmented transfer method with\nclustering is implemented for the global structures. In the experiments on the\nnine tasks for language understanding of the GLUE dataset, the proposed methods\neffectively transfer the three types of structures and improve performance\ncompared to state-of-the-art distillation methods. Indeed, the code for the\nmethods is available in https://github.com/maroo-sky/FSD\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The development of miniaturized nuclear power (NP) units and the improvement\nof the carbon trading market provide a new way to realize the low-carbon\noperation of integrated energy systems (IES). In this study, NP units and\ncarbon trading mechanisms are introduced into the IES to build a new low-carbon\nscheduling model. In view of the decrease in system operation flexibility\ncaused by the introduction of NP unit, on the one hand, the heating renovation\nof the NP unit is carried out to make it a cogeneration unit, to expand its\noperating range and improve its operating flexibility; on the other hand,\nauxiliary equipment such as electricity storage system, heat storage system and\npower to gas unit, which can carry out energy time translation or energy form\nconversion, are introduced into IES to jointly improve the flexibility of\nsystem operation. In the model-solving stage, the chance-constrained\nprogramming (CCP) model considering the uncertainty of the renewable energy\n(RE) output is converted into an equivalent mixed-integer linear programming\n(MILP) model using discretized step transformation. The test system built based\non real data of an IES in North China shows that the proposed method has good\neconomic and low-carbon environmental protection benefits.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Unpaired image translation algorithms can be used for sim2real tasks, but\nmany fail to generate temporally consistent results. We present a new approach\nthat combines differentiable rendering with image translation to achieve\ntemporal consistency over indefinite timescales, using surface consistency\nlosses and \\emph{neural neural textures}. We call this algorithm TRITON\n(Texture Recovering Image Translation Network): an unsupervised, end-to-end,\nstateless sim2real algorithm that leverages the underlying 3D geometry of input\nscenes by generating realistic-looking learnable neural textures. By settling\non a particular texture for the objects in a scene, we ensure consistency\nbetween frames statelessly. Unlike previous algorithms, TRITON is not limited\nto camera movements -- it can handle the movement of objects as well, making it\nuseful for downstream tasks such as robotic manipulation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The FCC structure of Pd$\\rm_{1-x}$Ag$\\rm_{x}$ ($\\rm{x}=$ 0.25, 0.50, 0.75)\nalloys is considered as a fuel cell component in this study. We have looked\ninto its qualities as a component of a fuel cell to see whether it could be\npotentially used as an alternative replacement of the Pt catalyst. We used\nDensity Functional Theory (DFT) to study H and CO interaction with the surface,\nand Kinetic Monte Carlo~(KMC) to study H and CO desorption from the surface.\nThe bulk modulus and equilibrium crystal structures of Pd$\\rm_{1-x}$Ag$\\rm_{x}$\nalloys were computed using the GPAW code within plane wave basis set $\\&$ a PBE\nexchange correlation functional treatment. The best values of a lattice\nconstant for the system are obtained by total energy calculations versus\nlattice cell volumes as fitted to the stabilized jellium model. Surface\nenergies, cohesive energies, and\\ binding energy of Pd$\\rm_{1-x}$Ag$\\rm_{x}$\nalloys were computed to analyze the stability properties of structures. Band\nstructure calculations reveal the electronic and optical properties of these\nalloys. The density of states~(DOS) and projected density of states~(PDOS) show\nthe availability of the eigenstates for occupation. The desorption process is\nstudied within the Arrhenius type desorption rate $\\&$ a temperature\nprogramming. The effects of lateral interactions between adsorbed molecules on\nfirst order desorption (molecular adsorption) $\\&$ second order desorption were\ntaken into account. Adsorption energies of H and CO on Pd$\\rm_3$Ag~(111) as\ncalculated using DFT is used in the process. The outcomes show good qualitative\nagreement with literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We offer a new perspective on risk aggregation with FGM copulas. Along the\nway, we discover new results and revisit existing ones, providing simpler\nformulas than one can find in the existing literature. This paper builds on two\nnovel representations of FGM copulas based on symmetric multivariate Bernoulli\ndistributions and order statistics. First, we detail families of multivariate\ndistributions with closed-form solutions for the cumulative distribution\nfunction or moments of the aggregate random variables. We order aggregate\nrandom variables under the convex order and provide methods to compute the\ncumulative distribution function of aggregate rvs when the marginals are\ndiscrete. Finally, we discuss risk-sharing and capital allocation, providing\nnumerical examples for each.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Decaying dark matter models provide a physically motivated way of channeling\nenergy between the matter and radiation sectors. In principle, this could\naffect the predicted value of the Hubble constant in such a way as to\naccommodate the discrepancies between CMB inferences and local measurements of\nthe same. Here, we revisit the model of warm dark matter decaying\nnon-relativistically to invisible radiation. In particular, we rederive the\nbackground and perturbation equations starting from a decaying neutrino model\nand describe a new, computationally efficient method of computing the decay\nproduct perturbations up to large multipoles. We conduct MCMC analyses to\nconstrain all three model parameters, for the first time including the mass of\nthe decaying species, and assess the ability of the model to alleviate the\nHubble and $\\sigma_8$ tensions, the latter being the discrepancy between the\nCMB and weak gravitational lensing constraints on the amplitude of matter\nfluctuations on an $8 h^{-1}$ Mpc$^{-1}$ scale. We find that the model reduces\nthe $H_0$ tension from $\\sim 4 \\sigma$ to $\\sim 3 \\sigma$ and neither\nalleviates nor worsens the $S_8 \\equiv \\sigma_8 (\\Omega_m/0.3)^{0.5}$ tension,\nultimately showing only mild improvements with respect to $\\Lambda$CDM.\nHowever, the values of the model-specific parameters favoured by data is found\nto be well within the regime of relativistic decays where inverse processes are\nimportant, rendering a conclusive evaluation of the decaying warm dark matter\nmodel open to future work.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spectrally constrained sequences (SCSs) play an important role in modern\ncommunication and radar systems operating over non-contiguous spectrum. Despite\nnumerous research attempts over the past years, very few works are known on the\nconstructions of optimal SCSs with low cross-correlations. In this paper, we\naddress such a major problem by introducing a unifying framework to construct\nunimodular SCS families using circular Florentine rectangles (CFRs) and\ninterleaving techniques. By leveraging the uniform power allocation in the\nfrequency domain for all the admissible carriers (a necessary condition for\nbeating the existing periodic correlation lower bound of SCSs), we present a\ntighter correlation lower bound and show that it is achievable by our proposed\nSCS families including multiple SCS sets with zero correlation zone properties.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In functional materials like thermoelectric materials and superconductors,\nthe interplay between functionality, electronic structure, and phonon\ncharacteristics is one of the key factors to improve functionality and to\nunderstand the mechanisms. In the first part of this article, we briefly review\ninvestigations on lattice anharmonicity in functional materials by Gr\\\"uneisen\nparameter ({\\gamma}G). One can find that the {\\gamma}G can be a good scale for\nlarge lattice anharmonicity and for detecting a change in anharmonicity\namplitude in functional materials. Then, we show original results on estimation\nof {\\gamma}G for recently-developed high-entropy-alloy-type (HEA-type)\nfunctional materials with a layered structure and a NaCl-type structure. As a\ncommon trend between those two systems with two- and three-dimensional\nstructures, we find that {\\gamma}G increases by a slight increase in\nconfigurational entropy of mixing ({\\Delta}Smix), and then {\\gamma}G decreases\nwith increasing {\\Delta}Smix in high-entropy region.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The review considers various groups of Heusler compounds, which can have the\nproperties of a semiconductor, a half-metallic ferromagnet, a spin gapless\nsemiconductor, a topological semimetal, and a noncollinear antiferromagnet. In\nthese Heusler compounds, \"conventional\" from the point of view of the crystal\nstructure, unusual kinetic and magnetic properties can be observed, which are\ncaused by the features of their electronic structure (e.g., presence of an\nenergy gap for one spin projection) and magnetic state (e.g., strong\nferromagnetism, compensated ferrimagnetism, etc.). Their magnetic and kinetic\ncharacteristics are very sensitive to external influences. Depending on the\nalloy composition and external parameters, transitions between the considered\nstates can be realized. All this opens up further prospects for controlling the\nelectronic and magnetic characteristics of such compounds and their practical\napplication.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In nuclear theory, the generator coordinate method (GCM), a type of\nconfiguration mixing method, is often used for the microscopic description of\ncollective motions. However, the GCM has a problem that a structure of the\ncollective subspace, which is the Hilbert space spanned by the configurations,\nis not generally understood. In this paper, I investigate the structure of the\ncollective subspace in the dynamical GCM (DGCM), an improved version of the\nGCM. I then show that it is restricted to a specific form that combines tensor\nproducts and direct sums under reasonable conditions. By imposing additional\nspecific conditions that are feasible in actual numerical calculations, it is\npossible to write the collective subspace as a simple tensor product of the\ncollective part and the others. These discussions are not dependent on the\ndetails of the function space used for generating the configurations and can be\napplied to various methods, including the mean-field theory. Moreover, this\nanalytical technique can also be applied to a variation after projection method\n(VAP), then which reveals that under a specific condition, the function space\nof the VAP has an untwisted structure. These consequences can provide powerful\ntools for discussing the collective motions with the DGCM or the GCM.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For over three decades, the planning community has explored countless methods\nfor data-driven model acquisition. These range in sophistication (e.g., simple\nset operations to full-blown reformulations), methodology (e.g., logic-based\nvs. planing-based), and assumptions (e.g., fully vs. partially observable).\nWith no fewer than 43 publications in the space, it can be overwhelming to\nunderstand what approach could or should be applied in a new setting. We\npresent a holistic characterization of the action model acquisition space and\nfurther introduce a unifying framework for automated action model acquisition.\nWe have re-implemented some of the landmark approaches in the area, and our\ncharacterization of all the techniques offers deep insight into the research\nopportunities that remain; i.e., those settings where no technique is capable\nof solving.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Digital twins have emerged as a key technology for optimizing the performance\nof engineering products and systems. High-fidelity numerical simulations\nconstitute the backbone of engineering design, providing an accurate insight\ninto the performance of complex systems. However, large-scale, dynamic,\nnon-linear models require significant computational resources and are\nprohibitive for real-time digital twin applications. To this end, reduced order\nmodels (ROMs) are employed, to approximate the high-fidelity solutions while\naccurately capturing the dominant aspects of the physical behavior. The present\nwork proposes a new machine learning (ML) platform for the development of ROMs,\nto handle large-scale numerical problems dealing with transient nonlinear\npartial differential equations. Our framework, mentioned as\n$\\textit{FastSVD-ML-ROM}$, utilizes $\\textit{(i)}$ a singular value\ndecomposition (SVD) update methodology, to compute a linear subspace of the\nmulti-fidelity solutions during the simulation process, $\\textit{(ii)}$\nconvolutional autoencoders for nonlinear dimensionality reduction,\n$\\textit{(iii)}$ feed-forward neural networks to map the input parameters to\nthe latent spaces, and $\\textit{(iv)}$ long short-term memory networks to\npredict and forecast the dynamics of parametric solutions. The efficiency of\nthe $\\textit{FastSVD-ML-ROM}$ framework is demonstrated for a 2D linear\nconvection-diffusion equation, the problem of fluid around a cylinder, and the\n3D blood flow inside an arterial segment. The accuracy of the reconstructed\nresults demonstrates the robustness and assesses the efficiency of the proposed\napproach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the problem of finding a maximum popular matching in a\nmany-to-many matching setting with two-sided preferences and matroid\nconstraints. This problem was proposed by Kamiyama [TCS 2020] and solved in the\nspecial case where matroids are base orderable. Utilizing a recently shown\nmatroid exchange property, we show that the problem is tractable for arbitrary\nmatroids.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recently, style transfer is a research area that attracts a lot of attention,\nwhich transfers the style of an image onto a content target. Extensive research\non style transfer has aimed at speeding up processing or generating\nhigh-quality stylized images. Most approaches only produce an output from a\ncontent and style image pair, while a few others use complex architectures and\ncan only produce a certain number of outputs. In this paper, we propose a\nsimple method for representing style features in many ways called Deep Feature\nRotation (DFR), while not only producing diverse outputs but also still\nachieving effective stylization compared to more complex methods. Our approach\nis representative of the many ways of augmentation for intermediate feature\nembedding without consuming too much computational expense. We also analyze our\nmethod by visualizing output in different rotation weights. Our code is\navailable at https://github.com/sonnguyen129/deep-feature-rotation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many complex systems are composed of interacting parts, and the underlying\nlaws are usually simple and universal. While graph neural networks provide a\nuseful relational inductive bias for modeling such systems, generalization to\nnew system instances of the same type is less studied. In this work we trained\ngraph neural networks to fit time series from an example nonlinear dynamical\nsystem, the belief propagation algorithm. We found simple interpretations of\nthe learned representation and model components, and they are consistent with\ncore properties of the probabilistic inference algorithm. We successfully\nidentified a `graph translator' between the statistical interactions in belief\npropagation and parameters of the corresponding trained network, and showed\nthat it enables two types of novel generalization: to recover the underlying\nstructure of a new system instance based solely on time series observations, or\nto construct a new network from this structure directly. Our results\ndemonstrated a path towards understanding both dynamics and structure of a\ncomplex system and how such understanding can be used for generalization.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The electric power grid has evolved significantly over the past two decades\nin response to climate change. Increased levels of renewable energy generation,\nas a prominent feature of this evolution, have led to new congestion patterns\nin the transmission network. The transmission system is originally designed for\nconventional energy sources, with predictable flow patterns. Insufficient\ntransfer capability in congested transmission systems results in commitment of\nmore expensive power plants and higher levels of renewable energy curtailment.\nOne way to mitigate congestion is adoption of power flow control through\nvariable-impedance flexible ac transmission system (FACTS) devices. In this\npaper the impacts of power flow control on generation cost, carbon emissions\nand renewable energy curtailment are studied under a wide range of scenarios,\nincluding generation mix from major US regional transmission organizations, and\ndifferent load curves, representing seasonal variations. A two-stage stochastic\nunit commitment, including FACTS adjustment, is used to evaluate the impacts of\nFACTS devices on various types and penetration levels of renewable energy. The\nresults show that FACTS installation effectively reduces generation cost,\ncarbon emissions, and renewable energy curtailment. Location of renewable\nenergy resources, peak-hour demand and the system's generation mix are among\nthe influential factors.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper presents the arrangement of an in-pipe climbing robot that works\nusing a clever differential part to explore complex associations of lines.\nStandard wheeled/continued in-pipe climbing robots are leaned to slip and take\nwhile exploring in pipe turns. The mechanism helps in achieving the first\neventual outcome of clearing out slip and drag in the robot tracks during\ndevelopment. The proposed differential comprehends the down to earth limits of\nthe standard two-yield differential, which is cultivated the underlying time\nfor a differential with three outcomes. The mechanism definitively changes the\ntrack paces of the robot considering the powers applied on each track inside\nthe line association, by clearing out the prerequisite for any unique control.\nThe entertainment of the robot crossing in the line network in different\nbearings and in pipe-turns without slip shows the proposed arrangement's\nampleness.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In [8](arXiv:2111.06159) we introduced the notion of a\nk-almost-quasifibration. In this article we update this definition and call it\na k-c-quasifibration. This will help us to relate it to quasifibrations. We\nstudy some basic properties of k-c-quasifibrations. We also generalize a series\nof results on quasifibrations ([1]) to k-c-quasifibrations giving criteria for\na map to be a k-c-quasifibration.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper applies a discontinuous Galerkin finite element method to the\nKelvin-Voigt viscoelastic fluid motion equations when the forcing function is\nin $L^\\infty({\\bf L}^2)$-space. Optimal a priori error estimates in\n$L^\\infty({\\bf L}^2)$-norm for the velocity and in $L^\\infty(L^2)$-norm for the\npressure approximations for the semi-discrete discontinuous Galerkin method are\nderived here. The main ingredients for establishing the error estimates are the\nstandard elliptic duality argument and a modified version of the Sobolev-Stokes\noperator defined on appropriate broken Sobolev spaces. Further, under the\nsmallness assumption on the data, it has been proved that these estimates are\nvalid uniformly in time. Then, a first-order accurate backward Euler method is\nemployed to discretize the semi-discrete discontinuous Galerkin Kelvin-Voigt\nformulation completely. The fully discrete optimal error estimates for the\nvelocity and pressure are established. Finally, using the numerical\nexperiments, theoretical results are verified. It is worth highlighting here\nthat the error results in this article for the discontinuous Galerkin method\napplied to the Kelvin-Voigt model using finite element analysis are the first\nattempt in this direction.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The two-body gravitational lens equation underlying planetary microlensing is\nusually transformed into a quintic polynomial that can only be solved\nnumerically. Here, I present methods to acquire approximate analytic and exact\nsemi-analytic solutions. First, I propose the pure-shear approximation, which\nallows one to acquire closed-form magnification solutions that are accurate\napart from a small region near the primary star. While previous works on the\nperturbative picture suggest that the uniform-shear Chang-Refsdal lens only\ndescribes the vicinity of planetary caustics and breaks down in the resonant\nregime, the pure-shear lens formalism allows for all three caustic topologies.\nI show that the recently proposed offset degeneracy is a direct consequence of\nthe pure-shear approximation. Second, the sole recognition that there always\nexists one image that is largely unaffected by the presence of the planet\nallows one to easily factor out the corresponding root from the quintic\npolynomial, reducing it to an analytically solvable quartic polynomial. This\nallows one to acquire semi-analytic solutions that are exact. The two analytic\nsimplifications proposed here not only can allow for substantially faster\nforward models, but also facilitates the use of gradient-based inference\nalgorithms that provide additional factors of acceleration for the analysis of\nobserved events.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Traffic safety is important in reducing death and building a harmonious\nsociety. In addition to studies of accident incidences, the perception of\ndriving risk is significant in guiding the implementation of appropriate\ndriving countermeasures. Risk assessment can be conducted in real-time for\ntraffic safety due to the rapid development of communication technology and\ncomputing capabilities. This paper aims at the problems of difficult\ncalibration and inconsistent thresholds in the existing risk assessment\nmethods. It proposes a risk assessment model based on the potential field to\nquantify the driving risk of vehicles. Firstly, virtual energy is proposed as\nan attribute considering vehicle sizes and velocity. Secondly, the driving risk\nsurrogate(DRS) is proposed based on potential field theory to describe the risk\ndegree of vehicles. Risk factors are quantified by establishing submodels,\nincluding an interactive vehicle risk surrogate, a restrictions risk surrogate,\nand a speed risk surrogate. To unify the risk threshold, acceleration for\nimplementation guidance is derived from the risk field strength. Finally, a\nnaturalistic driving dataset in Nanjing, China, is selected, and 3063 pairs of\nfollowing naturalistic trajectories are screened out. Based on that, the\nproposed model and other models use for comparisons are calibrated through the\nimproved particle optimization algorithm. Simulations prove that the proposed\nmodel performs better than other algorithms in risk perception and response,\ncar-following trajectory, and velocity estimation. In addition, the proposed\nmodel exhibits better car-following ability than existing car-following models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The distribution of the muon content of highly inclined Monte Carlo cosmic\nray showers is affected by the influence of Earth's geomagnetic field. It is\nfound that the shapes of the positive and negative muon distributions get\naffected/modified by the influence of the Earth's geomagnetic field. Such a\ncorrelation between the earth's geomagnetic activity and the cosmic ray (CR)\nair shower muons is found sensitive to the primary cosmic ray mass composition.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Public clouds provide scalable and cost-efficient computing through resource\nsharing. However, moving from traditional on-premises service management to\nclouds introduces new challenges; failure to correctly provision, maintain, or\ndecommission elastic services can lead to functional failure and vulnerability\nto attack. In this paper, we explore a broad class of attacks on clouds which\nwe refer to as cloud squatting. In a cloud squatting attack, an adversary\nallocates resources in the cloud (e.g., IP addresses) and thereafter leverages\nlatent configuration to exploit prior tenants. To measure and categorize cloud\nsquatting we deployed a custom Internet telescope within the Amazon Web\nServices us-east-1 region. Using this apparatus, we deployed over 3 million\nservers receiving 1.5 million unique IP addresses (56% of the available pool)\nover 101 days beginning in March of 2021. We identified 4 classes of cloud\nservices, 7 classes of third-party services, and DNS as sources of exploitable\nlatent configurations. We discovered that exploitable configurations were both\ncommon and in many cases extremely dangerous; we received over 5 million cloud\nmessages, many containing sensitive data such as financial transactions, GPS\nlocation, and PII. Within the 7 classes of third-party services, we identified\ndozens of exploitable software systems spanning hundreds of servers (e.g.,\ndatabases, caches, mobile applications, and web services). Lastly, we\nidentified 5446 exploitable domains spanning 231 eTLDs-including 105 in the top\n10,000 and 23 in the top 1000 popular domains. Through tenant disclosures we\nhave identified several root causes, including (a) a lack of organizational\ncontrols, (b) poor service hygiene, and (c) failure to follow best practices.\nWe conclude with a discussion of the space of possible mitigations and describe\nthe mitigations to be deployed by Amazon in response to this study.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Exemplar-based image translation establishes dense correspondences between a\nconditional input and an exemplar (from two different domains) for leveraging\ndetailed exemplar styles to achieve realistic image translation. Existing work\nbuilds the cross-domain correspondences implicitly by minimizing feature-wise\ndistances across the two domains. Without explicit exploitation of\ndomain-invariant features, this approach may not reduce the domain gap\neffectively which often leads to sub-optimal correspondences and image\ntranslation. We design a Marginal Contrastive Learning Network (MCL-Net) that\nexplores contrastive learning to learn domain-invariant features for realistic\nexemplar-based image translation. Specifically, we design an innovative\nmarginal contrastive loss that guides to establish dense correspondences\nexplicitly. Nevertheless, building correspondence with domain-invariant\nsemantics alone may impair the texture patterns and lead to degraded texture\ngeneration. We thus design a Self-Correlation Map (SCM) that incorporates scene\nstructures as auxiliary information which improves the built correspondences\nsubstantially. Quantitative and qualitative experiments on multifarious image\ntranslation tasks show that the proposed method outperforms the\nstate-of-the-art consistently.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As research in automatically detecting bugs grows and produces new\ntechniques, having suitable collections of programs with known bugs becomes\ncrucial to reliably and meaningfully compare the effectiveness of these\ntechniques. Most of the existing approaches rely on benchmarks collecting\nmanually curated real-world bugs, or synthetic bugs seeded into real-world\nprograms. Using real-world programs entails that extending the existing\nbenchmarks or creating new ones remains a complex time-consuming task.\n  In this paper, we propose a complementary approach that automatically\ngenerates programs with seeded bugs. Our technique, called HyperPUT, builds C\nprograms from a \"seed\" bug by incrementally applying program transformations\n(introducing programming constructs such as conditionals, loops, etc.) until a\nprogram of the desired size is generated. In our experimental evaluation, we\ndemonstrate how HyperPUT can generate buggy programs that can challenge in\ndifferent ways the capabilities of modern bug-finding tools, and some of whose\ncharacteristics are comparable to those of bugs in existing benchmarks. These\nresults suggest that HyperPUT can be a useful tool to support further research\nin bug-finding techniques -- in particular their empirical evaluations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper proposes a generalised propulsion energy consumption model (PECM)\nfor rotary-wing ummanned aerial vehicles (UAVs) under the consideration of the\npractical thrust-to-weight ratio (TWR) with respect to the velocity,\nacceleration and direction change of the UAVs. To verify the effectiveness of\nthe proposed PECM, we consider a UAV-enabled communication system, where a\nrotary-wing UAV serves multiple ground users as an aerial base station. We aim\nto maximize the energy efficiency (EE) of the UAV by jointly optimizing the\nuser scheduling and UAV trajectory variables. However, the formulated problem\nis a non-convex fractional integer programming problem, which is challenging to\nobtain its optimal solution. To tackle this, we propose an efficient iterative\nalgorithm by decomposing the original problem into two sub-problems to obtain a\nsuboptimal solution based on the successive convex approximation technique.\nSimulation results show that the optimized UAV trajectory by applying the\nproposed PECM are smoother and the corresponding EE has significant improvement\nas compared to other benchmark schemes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Developing data-driven machine-learning interatomic potentials for materials\ncontaining many elements becomes increasingly challenging due to the vast\nconfiguration space that must be sampled by the training data. We study the\nlearning rates and achievable accuracy of machine-learning interatomic\npotentials for many-element alloys with different combinations of descriptors\nfor the local atomic environments. We show that for a five-element alloy\nsystem, potentials using simple low-dimensional descriptors can reach\nmeV/atom-accuracy with modestly sized training datasets, significantly\noutperforming the high-dimensional SOAP descriptor in data efficiency,\naccuracy, and speed. In particular, we develop a computationally fast\nmachine-learned and tabulated Gaussian approximation potential (tabGAP) for\nMo-Nb-Ta-V-W alloys with a combination of two-body, three-body, and a new\nsimple scalar many-body density descriptor based on the embedded atom method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A gauge-invariant Wigner quantum mechanical theory is obtained by applying\nthe Weyl-Stratonovich transform to the von Neumann equation for the density\nmatrix. The transform reduces to the Weyl transform in the electrostatic limit,\nwhen the vector potential and thus the magnetic field are zero. Both cases\ninvolve a center-of-mass transform followed by a Fourier integral on the\nrelative coordinate introducing the momentum variable. The latter is continuous\nif the limits of the integral are infinite, or, equivalently, the coherence\nlength is infinite. However, the quantum theory involves Fourier transforms of\nthe electromagnetic field components, which imposes conditions on their\nbehavior at infinity. Conversely, quantum systems are bounded and often very\nsmall, as is, for instance, the case in modern nanoelectronics. This implies a\nfinite coherence length, which avoids the need to regularize non-converging\nFourier integrals. Accordingly, the momentum space becomes discrete, giving\nrise to momentum quantization and to a semi-discrete gauge-invariant Wigner\nequation. To gain insights into the peculiarities of this theory one needs to\nanalyze the equation for specific electromagnetic conditions. We derive the\nevolution equation for the linear electromagnetic case and show that it\nsignificantly simplifies for a limit dictated by the long coherence length\nbehavior, which involves momentum derivatives: In the discrete momentum picture\nthese derivatives are presented by finite difference quantities which, together\nwith further approximations, allow to develop a computationally feasible model\nwhich offers physical insights into the involved quantum processes. In\nparticular, a Fredholm integral equation of the second kind is obtained, where\nthe 'power' of the kernel components, measuring their rate of modification of\nthe quantum evolution, can be evaluated.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A nonparametric method is proposed for estimating the quantile spectra and\ncross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency\nand quantile level. The method is based on the quantile discrete Fourier\ntransform (QDFT) defined by trigonometric quantile regression and the quantile\nseries (QSER) defined by the inverse Fourier transform of the QDFT. A\nnonparametric spectral estimator is constructed from the autocovariance and\ncross-covariance functions of the QSER using the lag-window (LW) approach.\nVarious quantile smoothing techniques are employed further to reduce the\nstatistical variability of the estimator across quantiles, among which is a new\ntechnique called spline quantile regression (SQR). The performance of the\nproposed estimation method is evaluated through a simulation study.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We construct a complete system of primitive orthogonal idempotents and give\nan explicit quiver presentation of the monoid algebra of the stylic monoid\nintroduced by Abram and Reutenauer [arXiv:2106.06556].\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As the outbreak of COVID-19 enters its third year, we have now enough data to\nanalyse the behavior of the pandemic with mathematical models over a long\nperiod of time. The pandemic alternates periods of high and low infections, in\na way that sheds a light on the nature of mathematical model that can be used\nfor reliable predictions. The main hypothesis of the model presented here is\nthat the oscillatory behavior is a structural feature of the outbreak, even\nwithout postulating a time-dependence of the coefficients. As such, it should\nbe reflected by the presence of limit cycles as asymptotic solutions. This\nstems from the introduction of (i) a non-linear waning immunity based on the\nconcept of immunity booster (already used for other pathologies); (ii) a fine\ndescription of the compartments with a discrimination between individuals\ninfected/vaccinated for the first time, and individuals already\ninfected/vaccinated, undergoing to new infections/doses. We provide a\nproof-of-concept that our novel model is capable of reproducing long-term\noscillatory behavior of many infectious diseases, and, in particular, the\nperiodic nature of the waves of infection. Periodic solutions are inherent to\nthe model, and achieved without changing parameter values in time. This may\nrepresent an important step in the long-term modeling of COVID-19 and similar\ndiseases, as the natural, unforced behavior of the solution shows the\nqualitative characteristics observed during the COVID-19 pandemic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study rigidity questions for pairs of Lie algebras\n$(\\mathfrak{g},\\mathfrak{n})$ admitting a post-Lie algebra structure. We show\nthat if $\\mathfrak{g}$ is semisimple and $\\mathfrak{n}$ is arbitrary, then we\nhave rigidity in the sense that $\\mathfrak{g}$ and $\\mathfrak{n}$ must be\nisomorphic. The proof uses a result on the decomposition of a Lie algebra\n$\\mathfrak{g}=\\mathfrak{s}_1\\dotplus \\mathfrak{s}_2$ as the direct vector space\nsum of two semisimple subalgebras. We show that $\\mathfrak{g}$ must be\nsemisimple and hence isomorphic to the direct Lie algebra sum\n$\\mathfrak{g}\\cong \\mathfrak{s}_1\\oplus \\mathfrak{s}_2$. This solves some open\nexistence questions for post-Lie algebra structures on pairs of Lie algebras\n$(\\mathfrak{g},\\mathfrak{n})$. We prove additional existence results for pairs\n$(\\mathfrak{g},\\mathfrak{n})$, where $\\mathfrak{g}$ is complete, and for pairs,\nwhere $\\mathfrak{g}$ is reductive with $1$-dimensional center and\n$\\mathfrak{n}$ is solvable or nilpotent.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Traditional finite-temperature Kohn-Sham density functional theory (KSDFT)\nhas an unfavorable scaling with respect to the electron number or at high\ntemperatures. The evaluation of the ground-state density in KSDFT can be\nreplaced by the Chebyshev trace (CT) method. In addition, the use of stochastic\norbitals within the CT method leads to the stochastic density functional theory\n[Phys. Rev. Lett. 111, 106402 (2013)] (SDFT) and its improved theory, mixed\nstochastic-deterministic density functional theory [Phys. Rev. Lett. 125,\n055002 (2020)] (MDFT). We have implemented the above four methods within the\nfirst-principles package ABACUS. All of the four methods are based on the\nplane-wave basis set with the use of norm-conserving pseudopotentials and the\nperiodic boundary conditions with the use of $k$-point sampling in the\nBrillouin zone. By using the KSDFT calculation results as benchmarks, we\nsystematically evaluate the accuracy and efficiency of the CT, SDFT, and MDFT\nmethods via examining a series of physical properties, which include the\nelectron density, the free energy, the atomic forces, stress, and density of\nstates for a few condensed phase systems. The results suggest that our\nimplementations of CT, SDFT, and MDFT not only reproduce the KSDFT results with\na high accuracy, but also exhibit several advantages over the tradition KSDFT\nmethod. We expect these methods can be of great help in studying\nhigh-temperature and large-size extended systems such as warm dense matter and\ndense plasma.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  District heating is an important component in the EU strategy to reach the\nset emission goals, since it allows an efficient supply of heat while using the\nadvantages of sector coupling between different energy carriers such as power,\nheat, gas and biomass. Most district heating systems use several different\ntypes of units to produce heat for hundreds or thousands of households. The\ntechnologies reach from natural gas-fired and electric boilers to biomass-fired\nunits as well as waste heat from industrial processes and solar thermal units.\nFurthermore, combined heat and power units (CHP) units are often included to\nuse the synergy effects of excess heat from electricity production.\n  We propose a generic mathematical formulation for the operational production\noptimization in district heating systems. The generality of the model allows it\nto be used for most district heating systems although they might use different\ncombinations of technologies in different system layouts. The mathematical\nformulation is based on stochastic programming to account for the uncertainty\nof production from non-dispatchable units such as waste heat and solar heat.\nFurthermore, the model is easily adaptable to different application cases in\ndistrict heating such as operational planning, bidding to electricity markets\nand long-term evaluation. We present results from three real cases in Denmark\nwith different requirements.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Wannier functions have become a powerful tool in the electronic structure\ncalculations of extended systems. The generalized Pipek-Mezey Wannier functions\nexhibit appealing characteristics (e.g., reaching an optimal localization and\nthe separation of the $\\sigma$-$\\pi$ orbitals) when compared with other\nschemes. However, when applied to giant nanoscale systems, the orbital\nlocalization suffers from a large computational cost overhead when one is\ninterested in localized states in a small fragment of the system. Herein we\npresent a swift, efficient, and robust approach for obtaining regionally\nlocalized orbitals of a subsystem within the generalized Pipek-Mezey scheme.\nThe proposed algorithm introduces a reduced workspace and sequentially exhausts\nthe entire orbital space until the convergence of the localization functional.\nIt tackles systems with $\\sim$10000 electrons within 0.5 hours with no loss in\nlocalization quality compared to the traditional approach. Regionally localized\norbitals with a higher extent of localization are obtained via judiciously\nextending the subsystem's size. Exemplifying on large bulk and a 4-nm wide slab\nof diamond with NV$^-$ center, we demonstrate the methodology and discuss how\nthe choice of the localization region affects the excitation energy of the\ndefect. Furthermore, we show how the sequential algorithm is easily extended to\nstochastic methodologies that do not provide individual single-particle\neigenstates. It is thus a promising tool to obtain regionally localized states\nfor solving the electronic structure problems of a subsystem embedded in giant\ncondensed systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the Grothendieck monoid (a monoid version of the Grothendieck group)\nof an extriangulated category, and give some results which are new even for\nabelian categories. First, we classify Serre subcategories and dense 2-out-of-3\nsubcategories using the Grothendieck monoid. Second, in good situations, we\nshow that the Grothendieck monoid of the localization of an extriangulated\ncategory is isomorphic to the natural quotient monoid of the original\nGrothendieck monoid. This includes the cases of the Serre quotient of an\nabelian category and the Verdier quotient of a triangulated category. As a\nconcrete example, we introduce an intermediate subcategory of the derived\ncategory of an abelian category, which lies between the abelian category and\nits one shift. We show that intermediate subcategories bijectively correspond\nto torsionfree classes in the abelian category, and then compute the\nGrothendieck monoid of an intermediate subcategory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It's well known that every planar graph is $4$-colorable. A toroidal graph is\na graph that can be embedded on a torus. It's proved that every toroidal graph\nis $7$-colorable. A proper coloring of a graph is called \\emph{odd} if every\nnon-isolated vertex has at least one color that appears an odd number of times\nin its neighborhood. The smallest number of colors that admits an odd coloring\nof a graph $ G $ is denoted by $\\chi_{o}(G)$. In this paper, we prove that if\n$G$ is tortoidal, then $\\chi_{o}\\left({G}\\right)\\le9$; Note that $K_7$ is a\ntoroidal graph, the upper bound is no less than $7$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We tailor the quantum statistics of a bosonic field to deterministically\ndrive a quantum system into a target state. Experimentally accessible states of\nthe field achieve good control of multi-level or -qubit systems, notably also\nat coupling strengths beyond the rotating-wave approximation. This extends\noptimal control theory to the realm of fully quantized, strongly coupled\ncontrol and target degrees of freedom.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a universal classifier for binary Neyman-Pearson classification\nwhere null distribution is known while only a training sequence is available\nfor the alternative distribution. The proposed classifier interpolates between\nHoeffding's classifier and the likelihood ratio test and attains the same error\nprobability prefactor as the likelihood ratio test, i.e., the same prefactor as\nif both distributions were known. In addition, like Hoeffding's universal\nhypothesis test, the proposed classifier is shown to attain the optimal error\nexponent tradeoff attained by the likelihood ratio test whenever the ratio of\ntraining to observation samples exceeds a certain value. We propose a lower\nbound and an upper bound to the training to observation ratio. In addition, we\npropose a sequential classifier that attains the optimal error exponent\ntradeoff.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As reproducibility becomes a greater concern, conferences have largely\nconverged to a strategy of asking reviewers to indicate whether code was\nattached to a submission. This is part of a larger trend of taking action based\non assumed ideals, without studying if those actions will yield the desired\noutcome. Our argument is that this focus on code for replication is misguided\nif we want to improve the state of reproducible research. This focus can be\nharmful -- we should not force code to be submitted. There is a lack of\nevidence for effective actions taken by conferences to encourage and reward\nreproducibility. We argue that venues must take more action to advance\nreproducible machine learning research today.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the evolution equations for the bulk viscous pressure, diffusion\ncurrent and shear tensor derived within second-order relativistic dissipative\nhydrodynamics from kinetic theory. By matching the higher order moments\ndirectly to the dissipative quantities, all terms which are of second order in\nthe Knudsen number Kn vanish, leaving only terms of order\n$\\mathcal{O}(\\textrm{Re}^{-1} \\textrm{Kn})$ and $\\mathcal{O}(\\textrm{Re}^{-2})$\nin the relaxation equations, where $\\textrm{Re}^{-1}$ is the inverse Reynolds\nnumber. We therefore refer to this scheme as the Inverse-Reynolds-Dominance\n(IReD) approach. The remaining (non-vanishing) transport coefficients can be\nobtained exclusively in terms of the inverse of the collision matrix. This\nprocedure fixes unambiguously the relaxation times of the dissipative\nquantities, which are no longer related to the eigenvalues of the inverse of\nthe collision matrix. In particular, we find that the relaxation times\ncorresponding to higher-order moments grow as their order increases, thereby\ncontradicting the \\textit{separation of scales} paradigm. The formal (up to\nsecond order) equivalence with the standard DNMR approach is proven and the\nconnection between the IReD transport coefficients and the usual DNMR ones is\nestablished.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In an epidemic, how should an organization with limited testing resources\nsafely return to in-person activities after a period of lockdown? We study this\nquestion in a setting where the population at hand is heterogeneous in both\nutility for in-person activities and probability of infection. In such a period\nof re-integration, tests can be used as a certificate of non-infection, whereby\nthose in negative tests are permitted to return to in-person activities for a\ndesignated amount of time. Under the assumption that samples can be pooled, the\nquestion of how to allocate the limited testing budget to the population in\nsuch a way as to maximize the utility of individuals in negative tests (who\nsubsequently return to in-person activities) is non-trivial, with a large space\nof potential testing allocations. We show that non-overlapping testing\nallocations, which are both conceptually and (crucially) logistically more\nsimple to implement, are approximately optimal, and we design an efficient\ngreedy algorithm for finding non-overlapping testing allocations with\napproximately optimal welfare (overall utility). We also provide empirical\nevidence in support of the efficacy of our approach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Gravitational lensing of fast radio bursts (FRBs) offers an exciting avenue\nfor several cosmological applications. However, it is not yet clear how many\nsuch events future surveys will detect nor how to optimally find them. We use\nthe known properties of FRBs to forecast detection rates of gravitational\nlensing on delay timescales from microseconds to years, corresponding to lens\nmasses spanning fifteen orders of magnitude. We highlight the role of the FRB\nredshift distribution on our ability to observe gravitational lensing. We\nconsider cosmological lensing of FRBs by stars in foreground galaxies and show\nthat strong stellar lensing will dominate on microsecond timescales. Upcoming\nsurveys such as DSA-2000 and CHORD will constrain the fraction of dark matter\nin compact objects (e.g. primordial black holes) and may detect millilensing\nevents from intermediate mass black holes (IMBHs) or small dark matter halos.\nCoherent all-sky monitors will be able to detect longer-duration lensing events\nfrom massive galaxies, in addition to short time-scale lensing. Finally, we\npropose a new application of FRB gravitational lensing that will measure\ndirectly the circumgalactic medium of intervening galaxies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recently, malevolent user hacking has become a huge problem for real-world\ncompanies. In order to learn predictive models for recommender systems,\nfactorization techniques have been developed to deal with user-item ratings. In\nthis paper, we suggest a broad architecture of a factorization model with\nadversarial training to get over these issues. The effectiveness of our systems\nis demonstrated by experimental findings on real-world datasets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We assessed several agreement coefficients applied in 2x2 contingency tables,\nwhich are commonly applied in research due to dicotomization by the conditions\nof the subjects (e.g., male or female) or by conveniency of the classification\n(e.g., traditional thresholds leading to separations in healthy or diseased,\nexposed or non-exposed, etc.). More extreme table configurations (e.g., high\nagreement between raters) are also usual, but some of the coefficients have\nproblems with imbalanced tables. Here, we not only studied some especific\nestimators, but also developed a general method to the study for any estimator\ncandidate to be an agreement measurement. This method was developed in open\nsource R codes and it is avaliable to the researchers. Here, we tested this\nmethod by verifying the performance of several traditional estimators over all\n1,028,789 tables with size ranging from 1 to 68. Cohen's kappa showed\nhandicapped behavior similar to Pearson's r, Yule's Q, and Yule's Y. Scott's pi\nhas ambiguity to assess situations of agreement between raters. Shankar and\nBangdiwala's B was mistaken in all situations of neutrality and when there is\ngreater disagreement between raters. Dice's F1 and McNemar's chi-squared\nincompletely assess the information of the contingency table, showing the\npoorest performance among all. We concluded that Holley and Guilford's G is the\nbest agreement estimator, closely followed by Gwet's AC1 and they should be\nconsidered as the first choices for agreement measurement in contingency 2x2\ntables. All procedures and data were implemented in R and are available to\ndownload from https://sourceforge.net/projects/tables2x2.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The use of mid-circuit measurement and qubit reset within quantum programs\nhas been introduced recently and several applications demonstrated that perform\nconditional branching based on these measurements. In this work, we go a step\nfurther and describe a next-generation implementation of classical computation\nembedded within quantum programs that enables the real-time calculation and\nadjustment of program variables based on the mid-circuit state of measured\nqubits. A full-featured Quantum Intermediate Representation (QIR) model is used\nto describe the quantum circuit including its embedded classical computation.\nThis integrated approach eliminates the need to evaluate and store a\npotentially prohibitive volume of classical data within the quantum program in\norder to explore multiple solution paths. It enables a new type of quantum\nalgorithm that requires fewer round-trips between an external classical driver\nprogram and the execution of the quantum program, significantly reducing\ncomputational latency, as much of the classical computation can be performed\nduring the coherence time of quantum program execution. We review practical\nchallenges to implementing this approach along with developments underway to\naddress these challenges. An implementation of this novel and powerful quantum\nprogramming pattern, a random walk phase estimation algorithm, is demonstrated\non a physical quantum computer with an analysis of its benefits and feasibility\nas compared to existing quantum computing methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the universal scaling limit of random partitions obeying the Schur\nmeasure. Extending our previous analysis [arXiv:2012.06424], we obtain the\nhigher-order Pearcey kernel describing the multi-critical behavior in the cusp\nscaling limit. We explore the gap probability associated with the higher\nPearcey kernel, and derive the coupled nonlinear differential equation and the\nasymptotic behavior in the large gap limit.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose a new feature descriptor Cross-Centroid Ripple\nPattern (CRIP) for facial expression recognition. CRIP encodes the transitional\npattern of a facial expression by incorporating cross-centroid relationship\nbetween two ripples located at radius r1 and r2 respectively. These ripples are\ngenerated by dividing the local neighborhood region into subregions. Thus, CRIP\nhas ability to preserve macro and micro structural variations in an extensive\nregion, which enables it to deal with side views and spontaneous expressions.\nFurthermore, gradient information between cross centroid ripples provides\nstrenght to captures prominent edge features in active patches: eyes, nose and\nmouth, that define the disparities between different facial expressions. Cross\ncentroid information also provides robustness to irregular illumination.\nMoreover, CRIP utilizes the averaging behavior of pixels at subregions that\nyields robustness to deal with noisy conditions. The performance of proposed\ndescriptor is evaluated on seven comprehensive expression datasets consisting\nof challenging conditions such as age, pose, ethnicity and illumination\nvariations. The experimental results show that our descriptor consistently\nachieved better accuracy rate as compared to existing state-of-art approaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Today's smart-grids have seen a clear rise in new ways of energy generation,\ntransmission, and storage. This has not only introduced a huge degree of\nvariability, but also a continual shift away from traditionally centralized\ngeneration and storage to distributed energy resources (DERs). In addition, the\ndistributed sensors, energy generators and storage devices, and networking have\nled to a huge increase in attack vectors that make the grid vulnerable to a\nvariety of attacks. The interconnection between computational and physical\ncomponents through a largely open, IP-based communication network enables an\nattacker to cause physical damage through remote cyber-attacks or attack on\nsoftware-controlled grid operations via physical- or cyber-attacks. Transactive\nEnergy (TE) is an emerging approach for managing increasing DERs in the\nsmart-grids through economic and control techniques. Transactive Smart-Grids\nuse the TE approach to improve grid reliability and efficiency. However,\nskepticism remains in their full-scale viability for ensuring grid reliability.\nIn addition, different TE approaches, in specific situations, can lead to very\ndifferent outcomes in grid operations. In this paper, we present a\ncomprehensive web-based platform for evaluating resilience of smart-grids\nagainst a variety of cyber- and physical-attacks and evaluating impact of\nvarious TE approaches on grid performance. We also provide several case-studies\ndemonstrating evaluation of TE approaches as well as grid resilience against\ncyber and physical attacks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The creation of well understood structures using spectral hole burning is an\nimportant task in the use of technologies based on rare earth ion doped\ncrystals. We apply a series of different techniques to model and improve the\nfrequency dependent population change in the atomic level structure of Thulium\nYttrium Gallium Garnet (Tm:YGG). In particular we demonstrate that at zero\napplied magnetic field, numerical solutions to frequency dependent three-level\nrate equations show good agreement with spectral hole burning results. This\nallows predicting spectral structures given a specific hole burning sequence,\nthe underpinning spectroscopic material properties, and the relevant laser\nparameters. This enables us to largely eliminate power dependent hole\nbroadening through the use of adiabatic hole-burning pulses. Though this system\nof rate equations shows good agreement at zero field, the addition of a\nmagnetic field results in unexpected spectral diffusion proportional to the\ninduced Tm ion magnetic dipole moment and average magnetic field strength,\nwhich, through the quadratic Zeeman effect, dominates the optical spectrum over\nlong time scales. Our results allow optimization of the preparation process for\nspectral structures in a large variety of rare earth ion doped materials for\nquantum memories and other applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent advances in programmable quantum devices brought to the fore the\nintriguing possibility of using them to realise and investigate topological\nquantum spin liquid phases. This new and exciting direction brings about\nimportant research questions on how to probe and determine the presence of such\nexotic, highly entangled phases. One of the most promising tools is\ninvestigating the behaviour of the topological excitations, and in particular\ntheir fractional statistics. In this work we put forward a generic route to\nachieve this, and we illustrate it in the specific case of $\\mathbb{Z}_2$\ntopological spin liquids implemented with the aid of combinatorial gauge\nsymmetry. We design a convenient architecture to study signatures of fractional\nstatistics via quasiparticle interferometry, and we assess its robustness to\ndiagonal and off-diagonal disorder, as well as to dephasing -- effects that are\ngenerally pervasive in noisy quantum programmable devices. A useful counterpart\nof our scheme is that it provides a clear test of the `quantumness' of these\ndevices, since the signatures that we are looking for crucially hinge on\nquantum coherence and quantum interference effects in the system.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Single-cell technologies are revolutionizing the entire field of biology. The\nlarge volumes of data generated by single-cell technologies are\nhigh-dimensional, sparse, heterogeneous, and have complicated dependency\nstructures, making analyses using conventional machine learning approaches\nchallenging and impractical. In tackling these challenges, deep learning often\ndemonstrates superior performance compared to traditional machine learning\nmethods. In this work, we give a comprehensive survey on deep learning in\nsingle-cell analysis. We first introduce background on single-cell technologies\nand their development, as well as fundamental concepts of deep learning\nincluding the most popular deep architectures. We present an overview of the\nsingle-cell analytic pipeline pursued in research applications while noting\ndivergences due to data sources or specific applications. We then review seven\npopular tasks spanning through different stages of the single-cell analysis\npipeline, including multimodal integration, imputation, clustering, spatial\ndomain identification, cell-type deconvolution, cell segmentation, and\ncell-type annotation. Under each task, we describe the most recent developments\nin classical and deep learning methods and discuss their advantages and\ndisadvantages. Deep learning tools and benchmark datasets are also summarized\nfor each task. Finally, we discuss the future directions and the most recent\nchallenges. This survey will serve as a reference for biologists and computer\nscientists, encouraging collaborations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Electron paramagnetic resonance (EPR) can provide unique insight into the\nchemical structure and magnetic properties of dopants in oxide and\nsemiconducting materials that are of interest for applications in electronics,\ncatalysis, and quantum sensing. Here, we demonstrate that EPR in combination\nwith scanning tunneling microscopy (STM) allows for probing the bonding and\ncharge state of alkali metal atoms on an ultrathin magnesium oxide layer on a\nAg substrate. We observe a magnetic moment of $1\\mu_\\mathrm{B}$ for Li$_2$,\nLiNa, and Na$_2$ dimers corresponding to spin radicals with a charge state of\n$+1e$. Single alkali atoms have the same charge state and no magnetic moment.\nThe ionization of the adsorbates is attributed to charge transfer through the\noxide to the metal substrate. Our work highlights the potential of EPR-STM to\nprovide insight into dopant atoms that are relevant for the control of the\nelectrical properties of surfaces and nanodevices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given labeled data in a source domain, unsupervised domain adaptation has\nbeen widely adopted to generalize models for unlabeled data in a target domain,\nwhose data distributions are different. However, existing works are\ninapplicable to face recognition under privacy constraints because they require\nsharing of sensitive face images between domains. To address this problem, we\npropose federated unsupervised domain adaptation for face recognition, FedFR.\nFedFR jointly optimizes clustering-based domain adaptation and federated\nlearning to elevate performance on the target domain. Specifically, for\nunlabeled data in the target domain, we enhance a clustering algorithm with\ndistance constrain to improve the quality of predicted pseudo labels. Besides,\nwe propose a new domain constraint loss (DCL) to regularize source domain\ntraining in federated learning. Extensive experiments on a newly constructed\nbenchmark demonstrate that FedFR outperforms the baseline and classic methods\non the target domain by 3% to 14% on different evaluation metrics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Self-adaptive systems are expected to mitigate disruptions by continually\nadjusting their configuration and behaviour. This mitigation is often reactive.\nTypically, environmental or internal changes trigger a system response only\nafter a violation of the system requirements. Despite a broad agreement that\nprevention is better than cure in self-adaptation, proactive adaptation methods\nare underrepresented within the repertoire of solutions available to the\ndevelopers of self-adaptive systems. To address this gap, we present a\nwork-in-progress approach for the pre diction of system-level disruptions\n(PRESTO) through parametric model checking. Intended for use in the analysis\nstep of the MAPE-K (Monitor-Analyse-Plan-Execute over a shared Knowledge)\nfeedback control loop of self-adaptive systems, PRESTO comprises two stages.\nFirst, time-series analysis is applied to monitoring data in order to identify\ntrends in the values of individual system and/or environment parameters. Next,\nfuture non-functional requirement violations are predicted by using parametric\nmodel checking, in order to establish the potential impact of these trends on\nthe reliability and performance of the system. We illustrate the application of\nPRESTO in a case study from the autonomous farming domain.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Chemically active droplets provide simple models for cell-like systems that\ncan grow and divide. Such active droplet systems are driven away from\nthermodynamic equilibrium and turn over chemically, which corresponds to a\nsimple metabolism. We consider two scenarios of non-equilibrium driving. First,\ndroplets are driven via the system boundaries by external reservoirs that\nsupply nutrient and remove waste (boundary-driven). Second, droplets are driven\nby a chemical energy provided by a fuel in the bulk (bulk-driven). For both\nscenarios, we discuss the conservation of energy and matter as well as the\nbalance of entropy. We use conserved and non-conserved fields to analyze the\nnon-equilibrium steady states of active droplets. Using an effective droplet\nmodel, we explore droplet stability and instabilities leading to droplet\ndivision. Our work reveals that droplet division occurs quite generally in\nactive droplet systems. Our results suggest that life-like processes such as\nmetabolism and division can emerge in simple non-equilibrium systems that\ncombine the physics of phase separation and chemical reactions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Neural Network Quantum States (NQS) represent quantum wavefunctions with\nsmall stochastic neural networks. We study the wavefunction access provided by\nNQS and relate it to results from distribution testing. This first leads to\nimproved distribution testing algorithms for NQS and also motivates definition\nof a variant of sample and query wavefunction access model, the amplitude ratio\naccess. We study it independently of NQS, argue that it is strictly weaker than\nthe usual sample and query access model, but also that it retains many of its\nsimulation capabilities. Secondly, we give an NQS with just three nodes that\ndoes not encode a valid wavefunction and cannot be sampled from.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We give a general review on the application of Ergodic theory to the\ninvestigation of dynamics of the Yang-Mills gauge fields and of the\ngravitational systems, as well as its application in the Monte Carlo method and\nfluid dynamics. In ergodic theory the maximally chaotic dynamical systems\n(MCDS) can be defined as dynamical systems that have nonzero Kolmogorov\nentropy. The hyperbolic dynamical systems that fulfil the Anosov C-condition\nbelong to the MCDS insofar as they have exponential instability of their phase\ntrajectories and positive Kolmogorov entropy. It follows that the C-condition\ndefines a rich class of MCDS that span over an open set in the space of all\ndynamical systems. The large class of Anosov-Kolmogorov MCDS is realised on\nRiemannian manifolds of negative sectional curvatures and on high-dimensional\ntori. The interest in MCDS is rooted in the attempts to understand the\nrelaxation phenomena, the foundations of the statistical mechanics, the\nappearance of turbulence in fluid dynamics, the non-linear dynamics of\nYang-Mills field and gravitating N-body systems as well as black hole\nthermodynamics. Our aim is to investigate classical- and quantum-mechanical\nproperties of MCDS and their role in the theory of fundamental interactions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Exact Circular Pattern Matching (ECPM) problem consists of reporting\nevery occurrence of a rotation of a pattern $P$ in a text $T$. In many\nreal-world applications, specifically in computational biology, circular\nrotations are of interest because of their prominence in virus DNA. Thus, given\nno restrictions on pre-processing time, how quickly all such circular rotation\noccurrences is of interest to many areas of study. We highlight, to the best of\nour knowledge, a novel approach to the ECPM problem and present four data\nstructures that accompany this approach, each with their own time-space\ntrade-offs, in addition to experimental results to determine the most\ncomputationally feasible data structure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Driving a quantum system across quantum critical points leads to\nnon-adiabatic excitations in the system. This in turn may adversely affect the\nfunctioning of a quantum machine which uses a quantum critical substance as its\nworking medium. Here we propose a bath-engineered quantum engine (BEQE), in\nwhich we use the Kibble--Zurek mechanism and critical scaling laws to formulate\na protocol for enhancing the performance of finite-time quantum engines\noperating close to quantum phase transitions. In the case of free fermionic\nsystems, BEQE enables finite-time engines to outperform engines operating in\nthe presence of shortcuts to adiabaticity, and even infinite-time engines under\nsuitable conditions, thus showing the remarkable advantages offered by this\ntechnique. Open questions remain regarding the use of BEQE based on\nnon-integrable models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recently CNN-based RGB-D salient object detection (SOD) has obtained\nsignificant improvement on detection accuracy. However, existing models often\nfail to perform well in terms of efficiency and accuracy simultaneously. This\nhinders their potential applications on mobile devices as well as many\nreal-world problems. To bridge the accuracy gap between lightweight and large\nmodels for RGB-D SOD, in this paper, an efficient module that can greatly\nimprove the accuracy but adds little computation is proposed. Inspired by the\nfact that depth quality is a key factor influencing the accuracy, we propose an\nefficient depth quality-inspired feature manipulation (DQFM) process, which can\ndynamically filter depth features according to depth quality. The proposed DQFM\nresorts to the alignment of low-level RGB and depth features, as well as\nholistic attention of the depth stream to explicitly control and enhance\ncross-modal fusion. We embed DQFM to obtain an efficient lightweight RGB-D SOD\nmodel called DFM-Net, where we in addition design a tailored depth backbone and\na two-stage decoder as basic parts. Extensive experimental results on nine\nRGB-D datasets demonstrate that our DFM-Net outperforms recent efficient\nmodels, running at about 20 FPS on CPU with only 8.5Mb model size, and\nmeanwhile being 2.9/2.4 times faster and 6.7/3.1 times smaller than the latest\nbest models A2dele and MobileSal. It also maintains state-of-the-art accuracy\nwhen even compared to non-efficient models. Interestingly, further statistics\nand analyses verify the ability of DQFM in distinguishing depth maps of various\nqualities without any quality labels. Last but not least, we further apply\nDFM-Net to deal with video SOD (VSOD), achieving comparable performance against\nrecent efficient models while being 3/2.3 times faster/smaller than the prior\nbest in this field. Our code is available at https://github.com/zwbx/DFM-Net.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Under some assumptions on the hierarchy of relevant energy scales, we compute\nthe nonrelativistic QCD (NRQCD) long-distance matrix elements (LDMEs) for\ninclusive production of $J/\\psi$, $\\psi(2S)$, and $\\Upsilon$ states based on\nthe potential NRQCD (pNRQCD) effective field theory. Based on the pNRQCD\nformalism, we obtain expressions for the LDMEs in terms of the quarkonium\nwavefunctions at the origin and universal gluonic correlators, which do not\ndepend on the heavy quark flavor or the radial excitation. This greatly reduces\nthe number of nonperturbative unknowns and substantially enhances the\npredictive power of the nonrelativistic effective field theory formalism. We\nobtain improved determinations of the LDMEs for $J/\\psi$, $\\psi(2S)$, and\n$\\Upsilon$ states thanks to the universality of the gluonic correlators, and\nobtain phenomenological results for cross sections and polarizations at large\ntransverse momentum that agree well with measurements at the LHC.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the aim to support London's safer recovery from the pandemic by improving\nroad safety intelligently, this study investigated the spatiotemporal patterns\nof age-involved car crashes and affecting factors, upon answering two main\nresearch questions: (1)\"What are the spatial and temporal patterns of car\ncrashes as well as their changes in two typical years, 2019 and 2020, in\nLondon, and how the influential factors work?\"; (2)\"What are the spatiotemporal\npatterns of casualty by age groups, and how people's daily activities affect\nthe patterns pre- and para- the pandemic\"? Three approaches, i.e., spatial\nanalysis (network Kernel Density Estimation, NetKDE), factor analysis, and\nspatiotemporal data mining (tensor decomposition), had been implemented to\nidentify the temporal patterns of car crashes on weekly and daily basis\nrespectively, detect the crashes' hot spots, and to gain better understanding\nthe effect from citizens' daily activity on crashes' patterns pre- and para-\nthe pandemic. It had been found from the study that car crashes mainly\nclustered in the central part of London, especially busier areas around denser\nhubs of point-of-interest (POIs); the POIs, as a reflector for citizens' daily\nactivities and travel behaviours, can be of help to gain a better understanding\nof the crashes' patterns, upon further assessment on interactions through the\ngeographical detector; the crashes' casualty patterns varied by age group, with\ndistinctive relationships between POIs and crashes' pattern for corresponding\nage group categorised. In all, the paper provided an in-depth exploratory\nanalysis of car crashes and their casualty patterns in London to facilitate\ndeployment policies towards post-pandemic safer recovery upon COVID-19.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To each finitely generated group $G$, we associate a quasi-isometric\ninvariant called the isoperimetric spectrum of $G$. If $G$ is finitely\npresented, our invariant is closely related to the Dehn function of $G$. The\nmain goal of this paper is to initiate the study of isoperimetric spectra of\nfinitely generated (but not necessarily finitely presented) groups. In\nparticular, we compute the isoperimetric spectrum of small cancellation groups,\ncertain wreath products, and free Burnside groups of sufficiently large odd\nexponent. We also address some natural questions on the structure of the poset\nof isoperimetric spectra. As an application, we show that there exist\n$2^{\\aleph_0}$ pairwise non-quasi-isometric finitely generated groups of finite\nexponent.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Computed medical imaging systems require a computational reconstruction\nprocedure for image formation. In order to recover a useful estimate of the\nobject to-be-imaged when the recorded measurements are incomplete, prior\nknowledge about the nature of object must be utilized. In order to improve the\nconditioning of an ill-posed imaging inverse problem, deep learning approaches\nare being actively investigated for better representing object priors and\nconstraints. This work proposes to use a style-based generative adversarial\nnetwork (StyleGAN) to constrain an image reconstruction problem in the case\nwhere additional information in the form of a prior image of the sought-after\nobject is available. An optimization problem is formulated in the intermediate\nlatent-space of a StyleGAN, that is disentangled with respect to meaningful\nimage attributes or \"styles\", such as the contrast used in magnetic resonance\nimaging (MRI). Discrepancy between the sought-after and prior images is\nmeasured in the disentangled latent-space, and is used to regularize the\ninverse problem in the form of constraints on specific styles of the\ndisentangled latent-space. A stylized numerical study inspired by MR imaging is\ndesigned, where the sought-after and the prior image are structurally similar,\nbut belong to different contrast mechanisms. The presented numerical studies\ndemonstrate the superiority of the proposed approach as compared to classical\napproaches in the form of traditional metrics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We prove an arithmetic path integral formula for the inverse p-adic absolute\nvalues of the Kubota-Leopoldt p-adic L-functions at roots of unity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Automated methods have been widely used to identify and analyze mental health\nconditions (e.g., depression) from various sources of information, including\nsocial media. Yet, deployment of such models in real-world healthcare\napplications faces challenges including poor out-of-domain generalization and\nlack of trust in black box models. In this work, we propose approaches for\ndepression detection that are constrained to different degrees by the presence\nof symptoms described in PHQ9, a questionnaire used by clinicians in the\ndepression screening process. In dataset-transfer experiments on three social\nmedia datasets, we find that grounding the model in PHQ9's symptoms\nsubstantially improves its ability to generalize to out-of-distribution data\ncompared to a standard BERT-based approach. Furthermore, this approach can\nstill perform competitively on in-domain data. These results and our\nqualitative analyses suggest that grounding model predictions in\nclinically-relevant symptoms can improve generalizability while producing a\nmodel that is easier to inspect.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL\nsystem predicts how well a given sentence will be translated, without recourse\nto the actual translation, thus eschewing unnecessary resource allocation when\ntranslation quality is bound to be low. PreQuEL can be defined relative to a\ngiven MT system (e.g., some industry service) or generally relative to the\nstate-of-the-art. From a theoretical perspective, PreQuEL places the focus on\nthe source text, tracing properties, possibly linguistic features, that make a\nsentence harder to machine translate.\n  We develop a baseline model for the task and analyze its performance. We also\ndevelop a data augmentation method (from parallel corpora), that improves\nresults substantially. We show that this augmentation method can improve the\nperformance of the Quality-Estimation task as well. We investigate the\nproperties of the input text that our model is sensitive to, by testing it on\nchallenge sets and different languages. We conclude that it is aware of\nsyntactic and semantic distinctions, and correlates and even over-emphasizes\nthe importance of standard NLP features.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Radiology report summarization is a growing area of research. Given the\nFindings and/or Background sections of a radiology report, the goal is to\ngenerate a summary (called an Impression section) that highlights the key\nobservations and conclusions of the radiology study. Recent efforts have\nreleased systems that achieve promising performance as measured by widely used\nsummarization metrics such as BLEU and ROUGE. However, the research area of\nradiology report summarization currently faces important limitations. First,\nmost of the results are reported on private datasets. This limitation prevents\nthe ability to reproduce results and fairly compare different systems and\nsolutions. Secondly, to the best of our knowledge, most research is carried out\non chest X-rays. Sometimes, studies even omit to mention the concerned modality\nand anatomy in the radiology reports used for their experiments. To palliate\nthese limitations, we propose a new dataset of six different modalities and\nanatomies based on the MIMIC-III database. We further release our results and\nthe data splits used to carry out our experiments. Finally, we propose a simple\nreport summarization system that outperforms the previous replicable research\non the existing dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Thanks to their universal approximation properties and new efficient training\nstrategies, Deep Neural Networks are becoming a valuable tool for the\napproximation of mathematical operators. In the present work, we introduce\nMesh-Informed Neural Networks (MINNs), a class of architectures specifically\ntailored to handle mesh based functional data, and thus of particular interest\nfor reduced order modeling of parametrized Partial Differential Equations\n(PDEs). The driving idea behind MINNs is to embed hidden layers into discrete\nfunctional spaces of increasing complexity, obtained through a sequence of\nmeshes defined over the underlying spatial domain. The approach leads to a\nnatural pruning strategy which enables the design of sparse architectures that\nare able to learn general nonlinear operators. We assess this strategy through\nan extensive set of numerical experiments, ranging from nonlocal operators to\nnonlinear diffusion PDEs, where MINNs are compared to classical fully connected\nDeep Neural Networks. Our results show that MINNs can handle functional data\ndefined on general domains of any shape, while ensuring reduced training times,\nlower computational costs, and better generalization capabilities, thus making\nMINNs very well-suited for demanding applications such as Reduced Order\nModeling and Uncertainty Quantification for PDEs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a robust method for averaging numbers contaminated by a large\nproportion of outliers. Our method, dubbed RODIAN, is inspired by the key idea\nof MINPRAN [1]: We assume that the outliers are uniformly distributed within\nthe range of the data and we search for the region that is least likely to\ncontain outliers only. The median of the data within this region is then taken\nas RODIAN. Our approach can accurately estimate the true mean of data with more\nthan 50% outliers and runs in time $O(n\\log n)$. Unlike other robust\ntechniques, it is completely deterministic and does not rely on a known inlier\nerror bound. Our extensive evaluation shows that RODIAN is much more robust\nthan the median and the least-median-of-squares. This result also holds in the\ncase of non-uniform outlier distributions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Classical flux compactifications contribute to a well-controlled corner of\nthe string landscape, therefore providing an important testing ground for a\nvariety of conjectures. We focus here on type II supergravity compactifications\non 6d group manifolds towards 4d maximally symmetric spacetimes. We develop a\ncode where the truncation to left-invariant scalars and the dimensional\nreduction to a 4d theory are automated, for any possible configuration of\nOp-planes and Dp-branes. We then prove that any such truncation is consistent.\nWe further compute the mass spectrum and analyse the stability of many de\nSitter, Minkowski or anti-de Sitter solutions, as well as their consistency\nwith swampland conjectures.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Audio driven talking head synthesis is a challenging task that attracts\nincreasing attention in recent years. Although existing methods based on 2D\nlandmarks or 3D face models can synthesize accurate lip synchronization and\nrhythmic head pose for arbitrary identity, they still have limitations, such as\nthe cut feeling in the mouth mapping and the lack of skin highlights. The\nmorphed region is blurry compared to the surrounding face. A Keypoint Based\nEnhancement (KPBE) method is proposed for audio driven free view talking head\nsynthesis to improve the naturalness of the generated video. Firstly, existing\nmethods were used as the backend to synthesize intermediate results. Then we\nused keypoint decomposition to extract video synthesis controlling parameters\nfrom the backend output and the source image. After that, the controlling\nparameters were composited to the source keypoints and the driving keypoints. A\nmotion field based method was used to generate the final image from the\nkeypoint representation. With keypoint representation, we overcame the cut\nfeeling in the mouth mapping and the lack of skin highlights. Experiments show\nthat our proposed enhancement method improved the quality of talking-head\nvideos in terms of mean opinion score.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Photon pairs generated by spontaneous parametric down-conversion are\nessential for optical quantum information processing, in which the quality of\nbiphoton states is crucial for the performance. To engineer the biphoton\nwavefunction (BWF) on-chip, the pump envelope function and the phase matching\nfunction are commonly adjusted, while the modal field overlap has been\nconsidered as a constant in the frequency range of interest. In this work, by\nutilizing modal coupling in a system of coupled waveguides, we explore the\nmodal field overlap as a new degree of freedom for biphoton engineering. We\nprovide design examples for on-chip generations of polarization entangled\nphotons and heralded single photons, respectively. This strategy can be applied\nto waveguides of different materials and structures, offering new possibilities\nfor photonic quantum state engineering.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that coherent topoi are right Kan injective with respect to flat\nembeddings of topoi. We recover the ultrastructure on their category of points\nas a consequence of this result. We speculate on possible notions of\nultracategory in various arenas of formal model theory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Efficient and automated design of optimizers plays a crucial role in\nfull-stack AutoML systems. However, prior methods in optimizer search are often\nlimited by their scalability, generability, or sample efficiency. With the goal\nof democratizing research and application of optimizer search, we present the\nfirst efficient, scalable and generalizable framework that can directly search\non the tasks of interest. We first observe that optimizer updates are\nfundamentally mathematical expressions applied to the gradient. Inspired by the\ninnate tree structure of the underlying math expressions, we re-arrange the\nspace of optimizers into a super-tree, where each path encodes an optimizer.\nThis way, optimizer search can be naturally formulated as a path-finding\nproblem, allowing a variety of well-established tree traversal methods to be\nused as the search algorithm. We adopt an adaptation of the Monte Carlo method\nto tree search, equipped with rejection sampling and equivalent-form detection\nthat leverage the characteristics of optimizer update rules to further boost\nthe sample efficiency. We provide a diverse set of tasks to benchmark our\nalgorithm and demonstrate that, with only 128 evaluations, the proposed\nframework can discover optimizers that surpass both human-designed counterparts\nand prior optimizer search methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum computing represents a paradigm shift for computation requiring an\nentirely new computer architecture. However, there is much that can be learned\nfrom traditional classical computer engineering. In this paper, we describe the\nParallel Research Kernels (PRK), a tool that was very useful for designing\nclassical parallel computing systems. The PRK are simple kernels written to\nexpose bottlenecks that limit classical parallel computing performance. We\nhypothesize that an analogous tool for quantum computing, Quantum Research\nKernels (QRK), may similarly aid the co-design of software and hardware for\nquantum computing systems, and we give a few examples of representative QRKs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Logistic regression training over encrypted data has been an attractive idea\nto security concerns for years. In this paper, we propose a faster gradient\nvariant called $\\texttt{quadratic gradient}$ to implement logistic regression\ntraining in a homomorphic encryption domain, the core of which can be seen as\nan extension of the simplified fixed Hessian.\n  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient\nAlgorithm (Adagrad) respectively with this gradient variant and evaluate the\nenhanced algorithms on several datasets. Experimental results show that the\nenhanced methods have a state-of-the-art performance in convergence speed\ncompared to the naive first-order gradient methods. We then adopt the enhanced\nNAG method to implement homomorphic logistic regression training and obtain a\ncomparable result by only $3$ iterations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A fundamental question in designing lossy data compression schemes is how\nwell one can do in comparison with the rate-distortion function, which\ndescribes the known theoretical limits of lossy compression. Motivated by the\nempirical success of deep neural network (DNN) compressors on large, real-world\ndata, we investigate methods to estimate the rate-distortion function on such\ndata, which would allow comparison of DNN compressors with optimality. While\none could use the empirical distribution of the data and apply the\nBlahut-Arimoto algorithm, this approach presents several computational\nchallenges and inaccuracies when the datasets are large and high-dimensional,\nsuch as the case of modern image datasets. Instead, we re-formulate the\nrate-distortion objective, and solve the resulting functional optimization\nproblem using neural networks. We apply the resulting rate-distortion\nestimator, called NERD, on popular image datasets, and provide evidence that\nNERD can accurately estimate the rate-distortion function. Using our estimate,\nwe show that the rate-distortion achievable by DNN compressors are within\nseveral bits of the rate-distortion function for real-world datasets.\nAdditionally, NERD provides access to the rate-distortion achieving channel, as\nwell as samples from its output marginal. Therefore, using recent results in\nreverse channel coding, we describe how NERD can be used to construct an\noperational one-shot lossy compression scheme with guarantees on the achievable\nrate and distortion. Experimental results demonstrate competitive performance\nwith DNN compressors.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Characterizing large noisy multiparty quantum states using genuine multiparty\nentanglement is a challenging task. In this paper, we calculate lower bounds of\ngenuine multiparty entanglement localized over a chosen multiparty subsystem of\nmulti-qubit stabilizer states in the noiseless and noisy scenario. In the\nabsence of noise, adopting a graph-based technique, we perform the calculation\nfor arbitrary graph states as representatives of the stabilizer states, and\nshow that the graph operations required for the calculation has a polynomial\nscaling with the system size. As demonstrations, we compute the localized\ngenuine multiparty entanglement over subsystems of large graphs having linear,\nladder, and square structures. We also extend the calculation for graph states\nsubjected to single-qubit Markovian or non-Markovian Pauli noise on all qubits,\nand demonstrate, for a specific lower bound of the localizable genuine\nmultiparty entanglement corresponding to a specific Pauli measurement setup,\nthe existence of a critical noise strength beyond which all of the post\nmeasured states are biseparable. The calculation is also useful for arbitrary\nlarge stabilizer states under noise due to the local unitary connection between\nstabilizer states and graph states. We demonstrate this by considering a toric\ncode defined on a square lattice, and computing a lower bound of localizable\ngenuine multiparty entanglement over a non-trivial loop of the code. Similar to\nthe graph states, we show the existence of the critical noise strength in this\ncase also, and discuss its interesting features.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A generalized approach for the implementation of memristive two-terminal\ncircuits with piesewise-smooth characteristics is proposed on the example of a\nmultifunctional circuit based on a transistor switch. Two versions of the\ncircuit are taken into consideration: an experimental model of the\npiecewise-smooth memristor (Chua's memristor) and a piecewise-smooth memristive\ncapacitor. Physical experiments are combined with numerical modelling of the\ndiscussed circuit models. Thus, it is demonstrated that the considered circuit\nis a flexible solution for synthesis of a wide range of memristive systems with\ntuneable characteristics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  An emergent quantized field enriches quantum many-body systems. We propose an\nantiparticle analog of the exciton in semimetals as an emergent collective mode\nin interacting electron systems. We show that inter-band excitations in\nsemimetals are generally comprised of both excitons and antiparticles of\nexcitons. These two stand for two distinct inter-band collective modes in\nsemimetals, having different energies and opposite conserved charges. The\nconserved charge here is a quantity conjugate to a joint U(1) symmetry of two\nelectron's bands associated with the inter-band excitations. The opposite\ncharges foster fertile scattering processes among the inter-band collective\nmodes. In spin-polarized systems, they also suggest possible experimental\ndetections of the antiparticles. We clarify that the effective theory of the\ninter-band excitations is given by a generalized Klein-Gordon theory. Our\ntheory provides a comprehensive understanding of excitonic spectra in generic\nsemimetals, bringing a new insight into electronic collective phenomena in\nsolids.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The effects of external forcing on a turbulent, liquid-fuelled,\nswirl-stabilized gas turbine combustor operating at a pressure of approximately\n1 MPa are explored experimentally. In particular, the dynamics and coupling\nbetween the hydrodynamics, heat release rate and acoustics are compared for\nvarious forcing amplitudes at a fixed forcing frequency $f_f$. The\nhydrodynamics were characterized via laser Mie scattering from droplets in the\nfuel spray, while the heat release rate was qualitatively measured using\nchemiluminescence (CL) emissions in the 312 $\\pm$ 12.5 nm wavelength range,\nboth at 10 kHz. The dynamics at the frequencies of interest were extracted\nusing spectral proper orthogonal decomposition (SPOD). In the unforced case,\nthe spray and CL oscillations exhibited similar dynamics, dominated by\noscillations at frequency $f_0$, whereas the pressure fluctuations were\npredominantly at $f_P$. As the forcing amplitude was increased from zero, the\nspray and CL exhibited changes in their power spectra characteristic of the\nsuppression route to synchronization. The pressure fluctuations, however, were\nobserved to follow the phase-locking route to synchronization. In contrast with\nexpectations from synchronization theory, the amplitude of the pressure\nfluctuations increased significantly not only after lock-on, but also as the\nfrequency detuning with $f_f$ decreased. It is shown that this increase in\namplitude is not due to intermittency in the frequency of the pressure\noscillations. The simultaneous occurrence of phase-locking and suppression\nillustrates the rich variety of dynamics that can occur in practical combustor\nsystems. In addition, the amplification of the pressure oscillations based on\nthe frequency detuning with the forcing suggests that classical reasoning based\non the Rayleigh Index may not be sufficient to understand the high amplitude\nbehaviour of multimodal systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Object-mediated joint action is believed to be enabled by implicit\ninformation exchange between interacting individuals using subtle haptic\nsignals within their interaction forces. The characteristics of these haptic\nsignals have, however, remained unclear. Here we analyzed the interaction\nforces during an empirical dyadic interaction task using Granger-Geweke\ncausality analysis, which allowed us to quantify the causal influence of each\nindividual's forces on their partner's. We observed that the inter-partner\ninfluence was not the same at every frequency. Specifically, in the frequency\nband of [2.15-7] Hz, we observed inter-partner differences of causal influence\nthat were invariant of the movement frequencies in the task and present only\nwhen information exchange was indispensable for task performance. Moreover, the\ninterpartner difference in this frequency band was observed to be correlated\nwith the task performance by the dyad. Our results suggest that forces in the\n[2.15-7] Hz band constitute task related information exchange between\nindividuals during physical interactions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The sea level observations from satellite altimetry are characterised by a\nsparse spatial and temporal coverage. For this reason, along-track data are\nroutinely interpolated into daily grids. The latter are strongly smoothed in\ntime and space and are generated using an optimal interpolation routine\nrequiring several pre-processing steps and covariance characterisation. In this\nstudy, we assess the potential of Random Forest Regression to estimate daily\nsea level anomalies. Along-track sea level data from 2004 are used to build a\ntraining dataset whose predictors are the neighbouring observations. The\nvalidation is based on the comparison against daily averages from tide gauges.\nThe generated dataset is on average 10% more correlated to the tide gauge\nrecords than the commonly used product from Copernicus. While the latter is\nmore optimised for the detection of spatial mesoscales, we show how the\nmethodology of this study has the potential to improve the characterisation of\nsea level variability.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent research has revealed that reducing the temporal and spatial\nredundancy are both effective approaches towards efficient video recognition,\ne.g., allocating the majority of computation to a task-relevant subset of\nframes or the most valuable image regions of each frame. However, in most\nexisting works, either type of redundancy is typically modeled with another\nabsent. This paper explores the unified formulation of spatial-temporal dynamic\ncomputation on top of the recently proposed AdaFocusV2 algorithm, contributing\nto an improved AdaFocusV3 framework. Our method reduces the computational cost\nby activating the expensive high-capacity network only on some small but\ninformative 3D video cubes. These cubes are cropped from the space formed by\nframe height, width, and video duration, while their locations are adaptively\ndetermined with a light-weighted policy network on a per-sample basis. At test\ntime, the number of the cubes corresponding to each video is dynamically\nconfigured, i.e., video cubes are processed sequentially until a sufficiently\nreliable prediction is produced. Notably, AdaFocusV3 can be effectively trained\nby approximating the non-differentiable cropping operation with the\ninterpolation of deep features. Extensive empirical results on six benchmark\ndatasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2\nand Diving48) demonstrate that our model is considerably more efficient than\ncompetitive baselines.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Unsupervised speech recognition has shown great potential to make Automatic\nSpeech Recognition (ASR) systems accessible to every language. However,\nexisting methods still heavily rely on hand-crafted pre-processing. Similar to\nthe trend of making supervised speech recognition end-to-end, we introduce\nwav2vec-U 2.0 which does away with all audio-side pre-processing and improves\naccuracy through better architecture. In addition, we introduce an auxiliary\nself-supervised objective that ties model predictions back to the input.\nExperiments show that wav2vec-U 2.0 improves unsupervised recognition results\nacross different languages while being conceptually simpler.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent developments in the aerospace industry have led to a dramatic\nreduction in the manufacturing and launch costs of low Earth orbit satellites.\nThe new trend enables the paradigm shift of satellite-terrestrial integrated\nnetworks with global coverage. In particular, the integration of 5G\ncommunication systems and satellites has the potential to restructure\nnext-generation mobile networks. By leveraging the network function\nvirtualization and network slicing, the orbital 5G core networks will\nfacilitate the coordination and management of network functions in\nsatellite-terrestrial integrated networks. We are the first to deploy a\nlightweight 5G core network on a real-world satellite to investigate its\nfeasibility. We conducted experiments to validate the onboard 5G core network\nfunctions. The validated procedures include registration and session setup\nprocedures. The results show that the 5G core network can function normally and\ngenerate correct signaling.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider a multi-user two-ray ground reflection scenario with unknown\ndistances between transmitter and receivers. By using two frequencies per user\nin parallel, we can mitigate possible destructive interference and ensure\nultra-reliability with only very limited knowledge at the transmitter. In this\nwork, we consider the problem of assigning two frequencies to each receiver in\na multi-user communication system such that the average minimum receive power\nis maximized. In order to solve this problem, we introduce a generalization of\nthe quadratic multiple knapsack problem to include heterogeneous profits and\ndevelop an algorithm to solve it. Compared to random frequency assignment, we\nreport a gain of around 6 dB in numerical simulations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Owing to the plentiful information released by the commodity devices, WiFi\nsignals have been widely studied for various wireless sensing applications. In\nmany works, both received signal strength indicator (RSSI) and the channel\nstate information (CSI) are utilized as the key factors for precise sensing.\nHowever, the calculation and relationship between RSSI and CSI is not explained\nin detail. Furthermore, there are few works focusing on the measurement\nvariation of the WiFi signal which impacts the sensing results. In this paper,\nthe relationship between RSSI and CSI is studied in detail and the measurement\nvariation of amplitude and phase information is investigated by extensive\nexperiments. In the experiments, transmitter and receiver are directly\nconnected by power divider and RF cables and the signal transmission is\nquantitatively controlled by RF attenuators. By changing the intensity of\nattenuation, the measurement of RSSI and CSI is carried out under different\nconditions. From the results, it is found that in order to get a reliable\nmeasurement of the signal amplitude and phase by commodity WiFi, the\nattenuation of the channels should not exceed 60 dB. Meanwhile, the difference\nbetween two channels should be lower than 10 dB. An active control mechanism is\nsuggested to ensure the measurement stability. The findings and criteria of\nthis work is promising to facilitate more precise sensing technologies with\nWiFi signal.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A body $\\Theta$ containing two phases, which may form a periodic composite\nwith microstructure much smaller that the body, or which may have structure on\na length scale comparable to the body, is subjected to slowly time varying\nboundary conditions that would produce an approximate uniform field in $\\Theta$\nwere it filled with homogeneous material. Here slowly time varying means that\nthe wavelengths and attenuation lengths of waves at the frequencies associated\nwith the time variation are much larger than the size of $\\Theta$, so that we\ncan make a quasistatic approximation. At least one of the two phase does not\nhave an instantaneous response but rather depends on fields at prior times. The\nfields may be those associated with electricity, magnetism, fluid flow in\nporous media, or antiplane elasticity. We find, subject to these\napproximations, that the time variation of the boundary conditions can be\ndesigned so boundary measurements at a specific time $t=t_0$ exactly yield the\nvolume fractions of the phases, independent of the detailed geometric\nconfiguration of the phases. Moreover, for specially tailored time variations,\nthe volume fraction can be exactly determined frommeasurements at any time $t$,\nnot just at the specific time $t=t_0$. We also show how time varying boundary\nconditions, not oscillating at the single frequency $\\omega_0$, can be designed\nto exactly retrieve the response at $\\omega_0$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Switchbacks are sudden, large radial deflections of the solar wind magnetic\nfield, widely revealed in interplanetary space by the Parker Solar Probe. The\nswitchbacks' formation mechanism and sources are still unresolved, although\ncandidate mechanisms include Alfv\\'enic turbulence, shear-driven\nKelvin-Helmholtz instabilities, interchange reconnection, and geometrical\neffects related to the Parker spiral. This Letter presents observations from\nthe Metis coronagraph onboard Solar Orbiter of a single large propagating\nS-shaped vortex, interpreted as first evidence of a switchback in the solar\ncorona. It originated above an active region with the related loop system\nbounded by open-field regions to the East and West. Observations, modeling, and\ntheory provide strong arguments in favor of the interchange reconnection origin\nof switchbacks. Metis measurements suggest that the initiation of the\nswitchback may also be an indicator of the origin of slow solar wind.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Minimizing risk with fairness constraints is one of the popular approaches to\nlearning a fair classifier. Recent works showed that this approach yields an\nunfair classifier if the training set is corrupted. In this work, we study the\nminimum amount of data corruption required for a successful flipping attack.\nFirst, we find lower/upper bounds on this quantity and show that these bounds\nare tight when the target model is the unique unconstrained risk minimizer.\nSecond, we propose a computationally efficient data poisoning attack algorithm\nthat can compromise the performance of fair learning algorithms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Accurate modeling of diffusive transport of nanoparticles across nanopores is\na particularly challenging problem. The reason is that for such narrow pores\nthe large surface-to-volume ratio amplifies the relevance of the nanoscopic\ndetails and of the effective interactions at the interface with pore walls.\nClose to the pore wall, there is no clear separation between the length scales\nassociated with molecular interactions, layering of the solvent at the\ninterface with the pore and the particle size. Therefore, the standard\nhydrodynamic arguments may not apply and alternative solutions to determining\naverage transport coefficients need to be developed. We here address this\nproblem by offering a multiscale ansatz that uses effective potentials\ndetermined from molecular dynamics simulations to parametrise a four state\nstochastic model for the positional configuration of the particle in the pore.\nThis is in turn combined with diffusivities in the centre of the pore and at\nthe pore wall to calculate the average diffusion constant. We apply this model\nto the diffusion of fullerenes in a toluene filled slit nanopore and calculate\nthe mean diffusion coefficient as a function of the pore size. We show that the\naccuracy of our model is affected by the partial slip of the toluene on the\npore wall.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Watermarking has become a plausible candidate for ownership verification and\nintellectual property protection of deep neural networks. Regarding image\nclassification neural networks, current watermarking schemes uniformly resort\nto backdoor triggers. However, injecting a backdoor into a neural network\nrequires knowledge of the training dataset, which is usually unavailable in the\nreal-world commercialization. Meanwhile, established watermarking schemes\noversight the potential damage of exposed evidence during ownership\nverification and the watermarking algorithms themselves. Those concerns decline\ncurrent watermarking schemes from industrial applications. To confront these\nchallenges, we propose a knowledge-free black-box watermarking scheme for image\nclassification neural networks. The image generator obtained from a data-free\ndistillation process is leveraged to stabilize the network's performance during\nthe backdoor injection. A delicate encoding and verification protocol is\ndesigned to ensure the scheme's security against knowledgable adversaries. We\nalso give a pioneering analysis of the capacity of the watermarking scheme.\nExperiment results proved the functionality-preserving capability and security\nof the proposed watermarking scheme.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This is an initial manuscript that presents the basic idea of \"slightly\naltruistic Nash equilibrium\", \"bi-layer game topology\", \"rolling horizon target\nselection\". This manuscript is just used for peer discussion and joint Ph.D.\napplication affairs rather than submission to any journal. Thus some references\nare not all provided. The complete paper for submission will be provided in the\nnext version.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the paper we study a measure version of the evolutionary nonlinear\nBoltzmann-type equation in which we admit a random number of collisions of\nparticles. We consider first a stationary model and use two methods to find its\nfixed points: the first based on Zolotarev seminorm and the second on\nKantorovich-Rubinstein maximum principle. Then a dynamic version of\nBoltzmann-type equation is considered and its asymptotical stability is shown.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given a multi-agent linear system, we formalize and solve a trajectory\noptimization problem that encapsulates trajectory tracking, distance-based\nformation control and input energy minimization. To this end, a numerical\nprojection operator Newton's method is developed to find a solution by the\nminimization of a cost functional able to capture all these different tasks. To\nstabilize the formation, a particular potential function has been designed,\nallowing to obtain specified geometrical configurations while the barycenter\nposition and velocity of the system follows a desired trajectory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We prove Griffiths inequalities for the $O(N)$-spin model with inhomogeneous\ncoupling constants and external magnetic field for any $N\\geq 2$. This is\nachieved by using a representation of $O(N)$-spins in terms of random paths\nthat reduces to the random current representation of the Ising model for $N=1$\nand an identity that is analogous to the switching lemma for random currents.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Boltzmann equation describes the evolution of the phase-space probability\ndistribution of classical particles under binary collisions. Approximations to\nit underlie the basis for several scholarly fields, including aerodynamics and\nplasma physics. While these approximations are appropriate in their respective\ndomains, they can be violated in niche but diverse applications which require\ndirect numerical solution of the original nonlinear Boltzmann equation. An\nexpanded implementation of the Galerkin-Petrov conservative spectral algorithm\nis employed to study a wide variety of physical problems. Enabled by\ndistributed precomputation, solutions of the spatially homogeneous Boltzmann\nequation can be achieved in seconds on modern personal hardware, while\nspatially-inhomogeneous problems are solvable in minutes. Several benchmarks\nagainst both analytic theoretical predictions and comparisons to other\nBoltzmann solvers are presented in the context of several domains including\nweakly ionized plasma, gaseous fluids, and atomic-plasma interaction.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A relatively hyperbolic group $G$ is said to be QCERF if all finitely\ngenerated relatively quasiconvex subgroups are closed in the profinite topology\non $G$. Assume that $G$ is a QCERF relatively hyperbolic group with double\ncoset separable (e.g., virtually polycyclic) peripheral subgroups. Given any\ntwo finitely generated relatively quasiconvex subgroups $Q,R \\leqslant G$ we\nprove the existence of finite index subgroups $Q'\\leqslant_f Q$ and $R'\n\\leqslant_f R$ such that the join $\\langle Q',R'\\rangle$ is again relatively\nquasiconvex in $G$. We then show that, under the minimal necessary hypotheses\non the peripheral subgroups, products of finitely generated relatively\nquasiconvex subgroups are closed in the profinite topology on $G$. From this we\nobtain the separability of products of finitely generated subgroups for several\nclasses of groups, including limit groups, Kleinian groups and balanced\nfundamental groups of finite graphs of free groups with cyclic edge groups.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work, we propose a technique to transfer speech recognition\ncapabilities from audio speech recognition systems to visual speech\nrecognizers, where our goal is to utilize audio data during lipreading model\ntraining. Impressive progress in the domain of speech recognition has been\nexhibited by audio and audio-visual systems. Nevertheless, there is still much\nto be explored with regards to visual speech recognition systems due to the\nvisual ambiguity of some phonemes. To this end, the development of visual\nspeech recognition models is crucial given the instability of audio models. The\nmain contributions of this work are i) building on recent state-of-the-art\nword-based lipreading models by integrating sequence-level and frame-level\nKnowledge Distillation (KD) to their systems; ii) leveraging audio data during\ntraining visual models, a feat which has not been utilized in prior word-based\nwork; iii) proposing the Gaussian-shaped averaging in frame-level KD, as an\nefficient technique that aids the model in distilling knowledge at the sequence\nmodel encoder. This work proposes a novel and competitive architecture for\nlip-reading, as we demonstrate a noticeable improvement in performance, setting\na new benchmark equals to 88.64% on the LRW dataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that solutions to the Ablowitz--Ladik system converge to solutions of\nthe cubic nonlinear Schr\\\"odinger equation for merely $L^2$ initial data.\nFurthermore, we consider initial data for this lattice model that excites\nFourier modes near both critical points of the discrete dispersion relation and\ndemonstrate convergence to a decoupled system of nonlinear Schr\\\"odinger\nequations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The interplay of topology and non-Hermiticity has led to diverse, exciting\nmanifestations in a plethora of systems. In this work, we systematically\ninvestigate the role of non-Hermiticity in the Chern insulating Haldane model\non a dice lattice. Due to the presence of a non-dispersive flat band, the\ndice-Haldane model hosts a topologically rich phase diagram with the\nnon-trivial phases accommodating Chern numbers $\\pm 2$. We introduce\nnon-Hermiticity into this model in two ways -- through balanced non-Hermitian\ngain and loss, and by non-reciprocal hopping in one direction. Both these types\nof non-Hermiticity induce higher-order exceptional points of order three. We\nsubstantiate the presence and the order of these higher-order exceptional\npoints using the phase rigidity and its scaling. Further, we construct a phase\ndiagram to identify and locate the occurrence of these exceptional points in\nthe parameter space. Non-Hermiticity has yet more interesting consequences on a\nfinite-sized lattice. Unlike for balanced gain and loss, in the case of\nnon-reciprocal hopping, the nearest-neighbour dice lattice system under\nperiodic boundary conditions accommodates a finite, non-zero spectral area in\nthe complex plane. This manifests as the non-Hermitian skin effect when open\nboundary conditions are invoked. In the more general case of the dice-Haldane\nlattice model, the non-Hermitian skin effect can be caused by both gain and\nloss or non-reciprocity. Fascinatingly, the direction of localization of the\neigenstates depends on the nature and strength of the non-Hermiticity. We\nestablish the occurrence of the skin effect using the local density of states,\ninverse participation ratio and the edge probability, and demonstrate its\nrobustness to disorder. Our results place the dice-Haldane model as an exciting\nplatform to explore non-Hermitian physics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a low-cost ultraviolet to infrared absolute quantum efficiency\ndetector characterization system developed using commercial off-the-shelf\ncomponents. The key components of the experiment include a light source,a\nregulated power supply, a monochromator, an integrating sphere, and a\ncalibrated photodiode. We provide a step-by-step procedure to construct the\nphoton and quantum efficiency transfer curves of imaging sensors. We present\nresults for the GSENSE 2020 BSI CMOS sensor and the Sony IMX 455 BSI CMOS\nsensor. As a reference for similar characterizations, we provide a list of\nparts and associated costs along with images of our setup.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A hybrid (i.e., physics-guided data-driven) feedforward tracking controller\nis proposed for systems with unmodeled linear or nonlinear dynamics. The\ncontroller is based on the filtered basis function (FBF) approach, hence it is\ncalled a hybrid FBF controller. It formulates the feedforward control input to\na system as a linear combination of a set of basis functions whose coefficients\nare selected to minimize tracking errors. The basis functions are filtered\nusing a combination of two linear models to predict and minimize the tracking\nerrors. The first model is physics-based and remains unaltered during the\nexecution of the controller, while the second is data-driven and is\ncontinuously updated during the execution of the controller. To ensure its\npracticality and safe learning, the proposed hybrid FBF controller is equipped\nwith the ability to handle delays in data acquisition and to detect impending\ninstability due to its inherent data-driven feedback loop. Its effectiveness is\ndemonstrated via application to vibration compensation of a 3D printer with\nunmodeled linear and nonlinear dynamics. Thanks to the proposed hybrid FBF\ncontroller, the tracking accuracy of the 3D printer is significantly improved\nin experiments involving high-speed printing, compared to a standard FBF\ncontroller that does not incorporate a data-driven model. Furthermore, the\nability of the hybrid FBF controller to detect and, hence, potentially avoid\nimpending instability is demonstrated offline using data collected online from\nexperiments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Dynamical decoupling is a key method to mitigate errors in a quantum\nmechanical system, and we studied it in a series of papers dealing in\nparticular with the problems arising from unbounded Hamiltonians. The standard\nbangbang model of dynamical decoupling, which we also used in those papers,\nrequires decoupling operations with infinite amplitude, which is strictly\nspeaking unrealistic from a physical point of view. In this paper we look at\ndecoupling operations of finite amplitude, discuss under what assumptions\ndynamical decoupling works with such finite amplitude operations, and show how\nthe bangbang description arises as a limit, hence justifying it as a reasonable\napproximation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding and controlling the interaction between the excitonic states of\na quantum emitter and the plasmonic modes of a nanocavity is one of the most\nrelevant current scientific challenges, key for the development of many\napplications, from quantum information processing devices to polaritonic\ncatalysts. In this paper we demonstrate that the tunnel electroluminescence of\nC60 nanocrystals enclosed in the plasmonic nanocavity between a metallic\nsurface and the tip of a Scanning Tunnelling Microscope, and isolated from the\nmetal surface by a thin NaCl film, can be switched from a broad emission\nspectrum, revealing the plasmonic modes of the cavity, to a narrow band\nemission, displaying only the excitonic states of the C60 molecules by changing\nthe bias voltage applied to the junction. Plasmonic emission is found in the\nsame voltage region in which the rate of inelastic tunnel transitions is large\nand, thus, vanishes for large voltages. Excitonic emission, on the other hand,\ndominates the spectra in the high-voltage region in which the inelastic rate is\nlow, demonstrating that the excitons cannot be created by an inelastic tunnel\nprocess. These results point towards new possible mechanisms to explain the\ntunnel electroluminescence of quantum emitters and offer new avenues to develop\nelectrically tuneable nanoscale light sources.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Reproducibility is an increasing concern in Artificial Intelligence (AI),\nparticularly in the area of Deep Learning (DL). Being able to reproduce DL\nmodels is crucial for AI-based systems, as it is closely tied to various tasks\nlike training, testing, debugging, and auditing. However, DL models are\nchallenging to be reproduced due to issues like randomness in the software\n(e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There\nare various practices to mitigate some of the aforementioned issues. However,\nmany of them are either too intrusive or can only work for a specific usage\ncontext. In this paper, we propose a systematic approach to training\nreproducible DL models. Our approach includes three main parts: (1) a set of\ngeneral criteria to thoroughly evaluate the reproducibility of DL models for\ntwo different domains, (2) a unified framework which leverages a\nrecord-and-replay technique to mitigate software-related randomness and a\nprofile-and-patch technique to control hardware-related non-determinism, and\n(3) a reproducibility guideline which explains the rationales and the\nmitigation strategies on conducting a reproducible training process for DL\nmodels. Case study results show our approach can successfully reproduce six\nopen source and one commercial DL models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Federated Learning (FL) is a distributed machine learning paradigm that\nenables learning models from decentralized private datasets, where the labeling\neffort is entrusted to the clients. While most existing FL approaches assume\nhigh-quality labels are readily available on users' devices; in reality, label\nnoise can naturally occur in FL and follows a non-i.i.d. distribution among\nclients. Due to the non-iid-ness challenges, existing state-of-the-art\ncentralized approaches exhibit unsatisfactory performance, while previous FL\nstudies rely on data exchange or repeated server-side aid to improve model's\nperformance. Here, we propose FedLN, a framework to deal with label noise\nacross different FL training stages; namely, FL initialization, on-device model\ntraining, and server model aggregation. Specifically, FedLN computes per-client\nnoise-level estimation in a single federated round and improves the models'\nperformance by correcting (or limiting the effect of) noisy samples. Extensive\nexperiments on various publicly available vision and audio datasets demonstrate\na 24% improvement on average compared to other existing methods for a label\nnoise level of 70%. We further validate the efficiency of FedLN in\nhuman-annotated real-world noisy datasets and report a 9% increase on average\nin models' recognition rate, highlighting that FedLN can be useful for\nimproving FL services provided to everyday users.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Optimal control (OC) using inverse dynamics provides numerical benefits such\nas coarse optimization, cheaper computation of derivatives, and a high\nconvergence rate. However, in order to take advantage of these benefits in\nmodel predictive control (MPC) for legged robots, it is crucial to handle its\nlarge number of equality constraints efficiently. To accomplish this, we first\n(i) propose a novel approach to handle equality constraints based on nullspace\nparametrization. Our approach balances optimality, and both dynamics and\nequality-constraint feasibility appropriately, which increases the basin of\nattraction to good local minima. To do so, we then (ii) adapt our\nfeasibility-driven search by incorporating a merit function. Furthermore, we\nintroduce (iii) a condensed formulation of the inverse dynamics that considers\narbitrary actuator models. We also develop (iv) a novel MPC based on inverse\ndynamics within a perception locomotion framework. Finally, we present (v) a\ntheoretical comparison of optimal control with the forward and inverse\ndynamics, and evaluate both numerically. Our approach enables the first\napplication of inverse-dynamics MPC on hardware, resulting in state-of-the-art\ndynamic climbing on the ANYmal robot. We benchmark it over a wide range of\nrobotics problems and generate agile and complex maneuvers. We show the\ncomputational reduction of our nullspace resolution and condensed formulation\n(up to 47.3%). We provide evidence of the benefits of our approach by solving\ncoarse optimization problems with a high convergence rate (up to 10 Hz of\ndiscretization). Our algorithm is publicly available inside CROCODDYL.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The stability of thin liquid films on a surface depends on the excess free\nenergy of the system involving various short-range and long-range interactions.\nIn an unstable condition, thin liquid films may dewet into multiple small-sized\ndroplets via spinodal, homogeneous, or heterogeneous nucleation process.\nHowever, if the total excess free energy of the system can be manipulated using\nan external stimulus, one can control the stability of thin liquid films on\ndemand. Here, we study the reversible dewetting process of thin lubricating\nfilms underneath aqueous drops on slippery surfaces using an external electric\nfield. Upon applying voltage, stable thin lubricating films dewet in a manner\nidentical to the spinodal dewetting of nanometer-thick liquid films. Upon\nremoving the applied voltage, the dewetted droplets spread, coalesce with\nneighboring ones, and form a uniform film again, however the time taken during\nrewetting is very different compared to dewetting. The characteristic features\nof both the dewetting and rewetting processes are present over multiple cycles.\nDue to the random nature of the spinodal dewetting process, the final dewetting\npattern does not show any correlation over multiple dewetting cycles.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this letter, we investigate Rate-Splitting Multiple Access (RSMA) for an\nuplink communication system with finite blocklength. Considering a two-user\nSingle-Input Single-Output (SISO) Multiple Access Channel (MAC), we study the\nimpact of Signal-to-Noise Ratio (SNR), blocklength, power allocation and target\nrate on the error probability performance of RSMA where one user message is\nsplit. We demonstrate that RSMA can improve the error probability performance\nsignificantly compared to Non-Orthogonal Multiple Access (NOMA) and RSMA can\nhave a larger rate region than NOMA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We derive the first detailed chemical abundances of three star clusters in\nthe Large Magellanic Cloud (LMC), NGC1831 (436+/-22 Myr), NGC1856 (350+/-18\nMyr) and [SL63]268 (1230+/-62 Myr) using integrated-light spectroscopic\nobservations obtained with the Magellan Echelle spectrograph on Magellan Baade\ntelescope. We derive [Fe/H], [Mg/Fe], [Ti/Fe], [Ca/Fe], [Ni/Fe], [Mn/Fe],\n[Cr/Fe] and [Na/Fe] for the three clusters. Overall, our results match the LMC\nabundances obtained in the literature as well as those predicted by detailed\nchemical evolution models. For clusters NGC1831 and NGC1856, the [Mg/Fe] ratios\nappear to be slightly depleted compared to [Ca/Fe] and [Ti/Fe]. This could be\nhinting at the well-known Mg-Al abundance anti-correlation observed in several\nMilky Way globular clusters. We note, however, that higher signal-to-noise\nobservations are needed to confirm such a scenario, particularly for NGC 1831.\nWe also find a slightly enhanced integrated-light [Na/Fe] ratio for cluster\n[SL63]268 compared to those from the LMC field stars, possibly supporting a\nscenario of intracluster abundance variations. We stress that detailed\nabundance analysis of individual stars in these LMC clusters is required to\nconfirm the presence or absence of MSPs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A Proton Exchange Membrane Fuel Cell (PEMFC) provides stable, emission-free,\nhigh-efficiency power. Water management and durability of PEMFCs are directly\naffected by transport phenomena at the cathode side. In the present study,\ntransport phenomena are investigated and optimized in a tapered parallel flow\nfield. Main channels in the flow field are tapered, which increases limiting\ncurrent density by 41%. Two objectives, i.e. water saturation and transport\nresistance, are considered metrics for transport phenomena in a tapered\nparallel flow field PEMFC. Operating pressure, temperature, stoichiometries at\nboth sides, and the porosity of gas diffusion layers are selected as parameters\nto be optimized. Two functions are generated for objectives by integrating 3D\nmultiphase-flow computational fluid dynamics and Response Surface Methodology.\nMulti-Objective Optimization (MOO) is carried out with two different methods.\nMulti-Objective Particle Swarm Optimization (MOPSO) and Non-dominated Sorting\nGenetic Algorithm II (NSGA-II) are employed to produce two challenging Pareto\nfronts. The results demonstrate that MOPSO performs better than NSGA-II. MOPSO\nrecognized quite the same Pareto front with lower runtime. In the last step,\nthe Technique of Order Preference Similarity to the Ideal Solution (TOPSIS) is\nused to select an optimum point from the Pareto front. The results are compared\nagainst experimental data, and good correspondence is observed. The optimum\nfeatures are temperature 323, pressure 1 atm, anode stoichiometry 3, cathode\nstoichiometry 2.62, and porosity 0.68. The porosity and pressure played the\nmost significant roles in determining water saturation and resistance.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we state a hybrid Chebotarev-Sato-Tate conjecture for abelian\nvarieties and we prove it in several particular cases using current potential\nautomorphy theorems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce the cyclic major index of a cycle permutation and give a\nbivariate analogue of enumerative formula for the cyclic shuffles with a given\ncyclic descent numbers due to Adin, Gessel, Reiner and Roichman, which can be\nviewed as a cyclic analogue of Stanley's Shuffle Theorem. This gives an answer\nto a question of Adin, Gessel, Reiner and Roichman, which has been posed by\nDomagalski, Liang, Minnich, Sagan, Schmidt and Sietsema again.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We review the equations determining the photon sphere radius and the black\nhole shadow radius, and calculate the cosmological constant corrections arising\nwhen the dark energy action has the usual form, and when dark energy arises\nfrom a Weyl scaling invariant dark energy action. For black hole targets of the\nEvent Horizon Telescope, the corrections are very small.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Following the first principles the analytic Born-Oppenheimer (BO) potential\ncurve for the ground state $X^1\\Sigma^+$ of the molecule ClF is proposed for\nwhole range of internuclear distances $R \\in [0,\\infty)$. It is based on\nmatching the perturbation theory at small internuclear distances $R$ and\nmultipole expansion at large distances $R$, it has the form of two-point Pade\napproximant and provides 3-4 figures in rovibrational energies. It supports\n5719 rovibrational states with maximal vibrational number $\\nu_{max} = 47$ and\nmaximal angular momentum $L_{max} = 210$ including 36 weakly-bound states close\nto threshold (to dissociation limit) with the energies $\\lesssim 10^{-4}$\nHartree. The van der Waals constant $C^{(ClF)}_6\\ \\sim\\ 29.3$ a.u. is\npredicted.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Stack Overflow has been heavily used by software developers to seek\nprogramming-related information. More and more developers use Community\nQuestion and Answer forums, such as Stack Overflow, to search for code examples\nof how to accomplish a certain coding task. This is often considered to be more\nefficient than working from source documentation, tutorials or full worked\nexamples. However, due to the complexity of these online Question and Answer\nforums and the very large volume of information they contain, developers can be\noverwhelmed by the sheer volume of available information. This makes it hard to\nfind and/or even be aware of the most relevant code examples to meet their\nneeds. To alleviate this issue, in this work we present a query-driven code\nrecommendation tool, named Que2Code, that identifies the best code snippets for\na user query from Stack Overflow posts. Our approach has two main stages: (i)\nsemantically-equivalent question retrieval and (ii) best code snippet\nrecommendation. To evaluate the performance of our proposed model, we conduct a\nlarge scale experiment to evaluate the effectiveness of the\nsemantically-equivalent question retrieval task and best code snippet\nrecommendation task separately on Python and Java datasets in Stack Overflow.\nWe also perform a human study to measure how real-world developers perceive the\nresults generated by our model. Both the automatic and human evaluation results\ndemonstrate the promising performance of our model, and we have released our\ncode and data to assist other researchers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present exploratory studies of the 3D proton tomography through polarized\n$T$-odd gluon TMDs at leading twist, obtained in a spectator-model framework.\nWe embody in our approach a flexible parameterization for the spectator-mass\nspectral function, suited to catch both small- and moderate-$x$ effects. All\nthese studies are relevant to unveil the gluon dynamics inside hadrons, which\nrepresents a core research line of studies at new-generation colliders, such as\nthe Electron-Ion Collider, NICA-SPD, the High-Luminosity LHC, and the Forward\nPhysics Facility.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We provide time lower bounds for sequential and parallel algorithms deciding\nbisimulation on labeled transition systems that use partition refinement. For\nsequential algorithms this is $\\Omega((m \\mkern1mu {+} \\mkern1mu n ) \\mkern-1mu\n\\log \\mkern-1mu n)$ and for parallel algorithms this is $\\Omega(n)$, where $n$\nis the number of states and $m$ is the number of transitions. The lowerbounds\nare obtained by analysing families of deterministic transition systems,\nultimately with two actions in the sequential case, and one action for parallel\nalgorithms.\n  For deterministic transition systems with one action, bisimilarity can be\ndecided sequentially with fundamentally different techniques than partition\nrefinement. In particular, Paige, Tarjan, and Bonic give a linear algorithm for\nthis specific situation. We show, exploiting the concept of an oracle, that\nthis approach is not of help to develop a faster generic algorithm for deciding\nbisimilarity. For parallel algorithms there is a similar situation where these\ntechniques may be applied, too.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Generative models have been found effective for data synthesis due to their\nability to capture complex underlying data distributions. The quality of\ngenerated data from these models is commonly evaluated by visual inspection for\nimage datasets or downstream analytical tasks for tabular datasets. These\nevaluation methods neither measure the implicit data distribution nor consider\nthe data privacy issues, and it remains an open question of how to compare and\nrank different generative models. Medical data can be sensitive, so it is of\ngreat importance to draw privacy concerns of patients while maintaining the\ndata utility of the synthetic dataset. Beyond the utility evaluation, this work\noutlines two metrics called Similarity and Uniqueness for sample-wise\nassessment of synthetic datasets. We demonstrate the proposed notions with\nseveral state-of-the-art generative models to synthesise Cystic Fibrosis (CF)\npatients' electronic health records (EHRs), observing that the proposed metrics\nare suitable for synthetic data evaluation and generative model comparison.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We prove the existence of complex polynomials $p(z)$ of degree $n$ and $q(z)$\nof degree $m<n$ such that the harmonic polynomial $ p(z) + \\ol{q(z)}$ has at\nleast $\\lceil n \\sqrt{m} \\rceil$ many zeros. This provides an array of new\ncounterexamples to Wilmshurst's conjecture that the maximum valence of harmonic\npolynomials $p(z)+\\ol{q(z)}$ taken over polynomials $p$ of degree $n$ and $q$\nof degree $m$ is $m(m-1)+3n-2$. More broadly, these examples show that there\ndoes not exist a linear (in $n$) bound on the valence with a uniform (in $m$)\ngrowth rate. Our proof uses a probabilistic technique based on estimating the\naverage number of zeros of a certain family of random harmonic polynomials.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Short-timescale atomic rearrangements are fundamental to the kinetics of\nglasses and frequently dominated by one atom moving significantly (a\nrearrangement), while others relax only modestly. The rates and directions of\nsuch rearrangements (or hops) are dominated by the distributions of activation\nbarriers (Eact) for rearrangement for a single atom and how those distributions\nvary across the atoms in the system. We have used molecular dynamics\nsimulations of Cu50Zr50 metallic glass below Tg in an isoconfigurational\nensemble to catalog the ensemble of rearrangements from thousands of sites. The\nmajority of atoms are strongly caged by their neighbors, but a tiny fraction\nhas a very high propensity for rearrangement, which leads to a power-law\nvariation in the cage-breaking probability for the atoms in the model. In\naddition, atoms generally have multiple accessible rearrangement vectors, each\nwith its own Eact. However, atoms with lower Eact (or higher rearrangement\nrates) generally explored fewer possible rearrangement vectors, as the low Eact\npath is explored far more than others. We discuss how our results influence\nfuture modeling efforts to predict the rearrangement vector of a hopping atom.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $P_b$ denote the primes expressed in base-$b$. In this note, we prove\nthat $P_b^*$ is not regular. This strengthens a classical result that $P_b$ is\nnot regular, due to Minsky and Papert in 1966.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Downlink coherent multiuser transmission is an essential technique for\ncell-free massive multiple-input multiple output (MIMO) systems, and the\navailability of channel state information (CSI) at the transmitter is a basic\nrequirement. To avoid CSI feedback in a time-division duplex system, the uplink\nchannel parameters should be calibrated to obtain the downlink CSI due to the\nradio frequency circuit mismatch of the transceiver. In this paper, a design of\na reference signal for over-the-air reciprocity calibration is proposed. The\nfrequency domain generated reference signals can make full use of the flexible\nframe structure of the fifth generation (5G) new radio, which can be completely\ntransparent to commercial off-the-shelf (COTS) remote radio units (RRUs) and\ncommercial user equipments. To further obtain the calibration of multiple RRUs,\nan interleaved RRU grouping with a genetic algorithm is proposed, and an\naveraged Argos calibration algorithm is also presented. We develop a cell-free\nmassive MIMO prototype system with COTS RRUs, demonstrate the statistical\ncharacteristics of the calibration error and the effectiveness of the\ncalibration algorithm, and evaluate the impact of the calibration delay on the\ndifferent cooperative transmission schemes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Sinusoidal flows are an important class of explicit stationary solutions of\nthe two-dimensional incompressible Euler equations on a flat torus. For such\nflows, the steam functions are eigenfunctions of the negative Laplacian. In\nthis paper, we prove that any sinusoidal flow related to some least\neigenfunction is, up to phase translations, nonlinearly stable under $L^p$ norm\nof the vorticity for any $1<p<+\\infty$, which improves a classical stability\nresult by Arnold based on the energy-Casimir method. The key point of the proof\nis to distinguish least eigenstates with fixed amplitude from others by using\nisovortical property of the Euler equations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The recent drastic increase in mobile data traffic has pushed the mobile edge\ncomputing systems to the limit of their capacity. A promising solution to this\nproblem is the task migration provided by unmanned aerial vehicles (UAV). Key\nfactors to be taken into account in the design of UAV offloading schemes must\ninclude the number of tasks waiting in the system as well as their\ncorresponding deadlines. An appropriate system cost which is used as an\nobjective function to be minimized comprises two parts. First, an offloading\ncost which can be interpreted as the cost of using computational resources at\nthe UAV. Second, a penalty cost due to potential task expiration. In order to\nminimize the expected (time average) cost over a time horizon, we formulate a\nDynamic Programming (DP) equation and analyze it to describe properties of a\ncandidate optimal offloading policy. The DP equation suffers from the\nwell-known ``Curse of Dimensionality'' that makes computations intractable,\nespecially when the state space is infinite. In order to reduce the\ncomputational burden, we identify three important properties of the optimal\npolicy. Based on these properties, we show that it suffices to evaluate the DP\nequation on a finite subset of the state space only. We then show that the\noptimal task offloading decision associated with a state can be inferred from\nthe decision taken at its ``adjacent'' states, further reducing the\ncomputational load. Finally, we provide numerical results to evaluate the\ninfluence of different parameters on the system performance as well as verify\nthe theoretical results.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In many instances, single nucleon removal reactions from neutron-proton\nasymmetric projectile nuclei populate final states in the residual nuclei that\nare very weakly bound. Familiar examples include neutron removal reactions from\nneutron-rich $^{11}$Be and $^{12}$Be, the latter populating the well-known\n$1/2^+$ halo ground-state and $1/2^-$ excited-state of $^{11}$Be - both states\nless than 1 MeV from the first neutron-decay threshold. Numerous additional\nexamples arise in reactions of asymmetric $p$- and $sd$-shell nuclei. The\nimportance of this weak residue binding upon calculated single-nucleon removal\nreaction cross sections is quantified by means of model calculations that\nneglect or include the dissociation degree of freedom of the residual nuclei.\nThe calculated removal-reaction cross sections for two representative $p$-shell\nprojectiles indicate that an explicit treatment of these residue break-up\neffects is unnecessary and that the differences between the break-up and no\nbreak-up calculations are small provided a consistent description of the\nresidue structure and density is used.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Generative adversarial networks (GANs) have demonstrated impressive image\ngeneration quality and semantic editing capability of real images, e.g.,\nchanging object classes, modifying attributes, or transferring styles. However,\napplying these GAN-based editing to a video independently for each frame\ninevitably results in temporal flickering artifacts. We present a simple yet\neffective method to facilitate temporally coherent video editing. Our core idea\nis to minimize the temporal photometric inconsistency by optimizing both the\nlatent code and the pre-trained generator. We evaluate the quality of our\nediting on different domains and GAN inversion techniques and show favorable\nresults against the baselines.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the common problem of selecting the best $m$ units from a set of $n$\nin the asymptotic regime $m / n \\to \\alpha \\in (0, 1)$, where noisy,\nheteroskedastic measurements of the units' true values are available and the\ndecision-maker wishes to maximize the average true value of the units selected.\nGiven a parametric prior distribution, the empirical Bayesian decision rule\nincurs $\\mathcal{O}_p(n^{-1})$ regret relative to the Bayesian oracle that\nknows the true prior. More generally, if the error in the estimated prior is of\norder $\\mathcal{O}_p(r_n)$, regret is $\\mathcal{O}_p(r_n^2)$. In this sense\nselecting the best units is easier than estimating their values. We show this\nregret bound is sharp, by giving an example in which it is attained. Using\npriors calibrated from a dataset of over four thousand internet experiments, we\nfind that empirical Bayes methods perform well in practice for detecting the\nbest treatments given only a modest number of experiments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the analysis of Twitter debate, the recent literature focused on\ndiscursive communities, i.e. clusters of accounts interacting among themselves\nvia retweets. In the present work, we studied discursive communities in 8\ndifferent thematic Twitter datasets in various languages. Surprisingly, we\nobserved that almost all discursive communities therein display a bow-tie\nstructure during political or societal debates. Instead, they are absent when\nthe argument of the discussion is different as sport events, as in the case of\nEuro2020 Turkish and Italian datasets. We furthermore analysed the quality of\nthe content created in the various sectors of the different discursive\ncommunities, using the domain annotation from the fact-checking website\nNewsguard: we observe that, when the discursive community is affected by\nm/disinformation, the content with the lowest quality is the ones produced and\nshared in SCC and, in particular, a strong incidence of low- or non-reputable\nmessages is present in the flow of retweets between the SCC and the OUT\nsectors. In this sense, in discursive communities affected by m/disinformation,\nthe greatest part of the accounts has access to a great variety of contents,\nbut whose quality is, in general, quite low; such a situation perfectly\ndescribes the phenomenon of infodemic, i.e. the access to \"an excessive amount\nof information about a problem, which makes it difficult to identify a\nsolution\", according to WHO).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Self-supervised-learning-based pre-trained models for speech data, such as\nWav2Vec 2.0 (W2V2), have become the backbone of many speech tasks. In this\npaper, to achieve speaker diarisation and speech recognition using a single\nmodel, a tandem multitask training (TMT) method is proposed to fine-tune W2V2.\nFor speaker diarisation, the tasks of voice activity detection (VAD) and\nspeaker classification (SC) are required, and connectionist temporal\nclassification (CTC) is used for ASR. The multitask framework implements VAD,\nSC, and ASR using an early layer, middle layer, and late layer of W2V2, which\ncoincides with the order of segmenting the audio with VAD, clustering the\nsegments based on speaker embeddings, and transcribing each segment with ASR.\nExperimental results on the augmented multi-party (AMI) dataset showed that\nusing different W2V2 layers for VAD, SC, and ASR from the earlier to later\nlayers for TMT not only saves computational cost, but also reduces diarisation\nerror rates (DERs). Joint fine-tuning of VAD, SC, and ASR yielded 16%/17%\nrelative reductions of DER with manual/automatic segmentation respectively, and\nconsistent reductions in speaker attributed word error rate, compared to the\nbaseline with separately fine-tuned models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Tensor Factor Models (TFM) are appealing dimension reduction tools for\nhigh-order large-dimensional time series, and have wide applications in\neconomics, finance and medical imaging. Two types of TFM have been proposed in\nthe literature, essentially based on the Tucker or CP decomposition of tensors.\nIn this paper, we propose a projection estimator for the Tucker-decomposition\nbased TFM, and provide its least-square interpretation which parallels to the\nleast-square interpretation of the Principal Component Analysis (PCA) for the\nvector factor model. The projection technique simultaneously reduce the\ndimensionality and the magnitudes of the idiosyncratic error matrix, thus\nleading to an increase of signal-to-noise ratio. We derive a faster convergence\nrate of the projection estimator than that of the naive PCA-based estimator,\nunder mild conditions which allow the idiosyncratic noise to have weak\ncross-correlations and weak autocorrelations. Further motivated by the\nleast-squares interpretation, we propose a robust version by utilizing a\nHuber-loss function, which leads to an iterative weighted projection technique.\nExtensive numerical studies are conducted to investigate the empirical\nperformance of the proposed (weighted) projection estimator relative to the\nsate-of-the-art ones. The simulation results shows that the projection\nestimator performs better than the non-projection estimators, and the weighted\nprojection estimator performs much better than the existing ones in the\nheavy-tailed case. We are still working on the theoretical analysis of the\nweighted projection estimator.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a meshless conservative Galerkin method for solving Hamiltonian\nwave equations. We first discretize the equation in space using radial basis\nfunctions in a Galerkin-type formulation. Differ from the traditional RBF\nGalerkin method that directly uses nonlinear functions in its weak form, our\nmethod employs appropriate projection operators in the construction of the\nGalerkin equation, which will be shown to conserve global energies. Moreover,\nwe provide a complete error analysis to the proposed discretization. We further\nderive the fully discretized solution by a second order average vector field\nscheme. We prove that the fully discretized solution preserved the discretized\nenergy exactly. Finally, we provide some numerical examples to demonstrate the\naccuracy and the energy conservation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As a powerful way of realizing semi-supervised segmentation, the cross\nsupervision method learns cross consistency based on independent ensemble\nmodels using abundant unlabeled images. However, the wrong pseudo labeling\ninformation generated by cross supervision would confuse the training process\nand negatively affect the effectiveness of the segmentation model. Besides, the\ntraining process of ensemble models in such methods also multiplies the cost of\ncomputation resources and decreases the training efficiency. To solve these\nproblems, we propose a novel cross supervision method, namely\nuncertainty-guided self cross supervision (USCS). In addition to ensemble\nmodels, we first design a multi-input multi-output (MIMO) segmentation model\nwhich can generate multiple outputs with shared model and consequently impose\nconsistency over the outputs, saving the cost on parameters and calculations.\nOn the other hand, we employ uncertainty as guided information to encourage the\nmodel to focus on the high confident regions of pseudo labels and mitigate the\neffects of wrong pseudo labeling in self cross supervision, improving the\nperformance of the segmentation model. Extensive experiments show that our\nmethod achieves state-of-the-art performance while saving 40.5% and 49.1% cost\non parameters and calculations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study AdS$_5$ black holes from a recently suggested giant graviton\nexpansion formula for the index of $U(N)$ maximal super-Yang-Mills theory. We\ncompute the large $N$ entropy at fixed charges and giant graviton numbers $n_I$\nby a saddle point analysis, and further maximize it in $n_I$. This agrees with\nthe dual black hole entropy in the small black hole limit. To get black holes\nat general sizes, one should note that various giant graviton indices cancel\nbecause gauge theory does not suffer from a Hagedorn-like pathology by an\ninfinite baryonic tower. With one assumption on the mechanism of this\ncancellation, we account for the dual black hole entropy at general sizes. We\ninterpret our results as analytic continuations of the large $N$ free energies\nof SCFTs, and based on it compute the entropies of AdS$_{4,7}$ black holes from\nM5, M2 giant gravitons.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate experimentally and theoretically the collective coupling\nbetween atoms with multilevel ground state manifolds and an optical cavity\nmode. In our setup the cavity field optically pumps populations among the\nground states. The ensuing dynamics can be conveniently described by means of\nan effective dynamical atom-cavity coupling strength that depends on the\noccupation of the individual states and their coupling strengths with the\ncavity mode. This leads to a dynamical backaction of the atomic populations on\nthe atom-cavity coupling strength which results in a non-exponential relaxation\ndynamics. We experimentally observe this effect with laser-cooled $^{87}$Rb\natoms, for which we monitor the collective normal-mode splitting in real time.\nOur results show that the multilevel structure of electronic ground states can\nsignificantly alter the relaxation behavior in atom-cavity settings as compared\nto ensembles of two-level atoms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep learning has innovated the field of computational imaging. One of its\nbottlenecks is unavailable or insufficient training data. This article reviews\nan emerging paradigm, imaging physics-based data synthesis (IPADS), that can\nprovide huge training data in biomedical magnetic resonance without or with few\nreal data. Following the physical law of magnetic resonance, IPADS generates\nsignals from differential equations or analytical solution models, making the\nlearning more scalable, explainable, and better protecting privacy. Key\ncomponents of IPADS learning, including signal generation models, basic deep\nlearning network structures, enhanced data generation, and learning methods are\ndiscussed. Great potentials of IPADS have been demonstrated by representative\napplications in fast imaging, ultrafast signal reconstruction and accurate\nparameter quantification. Finally, open questions and future work have been\ndiscussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To design fast memory devices, we need material combinations which can\nfacilitate fast read and write operation. We present a heterostructure\ncomprising a two-dimensional (2D) magnet and a 2D topological insulator (TI) as\na viable option for designing fast memory devices. We theoretically model\nspin-charge dynamics between the 2D magnets and 2D TIs. Using the adiabatic\napproximation, we combine the non-equilibrium Green's function method for\nspin-dependent electron transport, and time-quantified Monte-Carlo for\nsimulating magnetization dynamics. We show that it is possible to switch the\nmagnetic domain of a ferromagnet using spin-torque from spin-polarized edge\nstates of 2D TI. We further show that the switching between TIs and 2D magnets\nis strongly dependent on the interface exchange ($J_{\\mathrm{int}}$), and an\noptimal interface exchange depending on the exchange interaction within the\nmagnet is required for efficient switching. Finally, we compare the\nexperimentally grown Cr-compounds and show that Cr-compounds with higher\nanisotropy (such as $\\rm CrI_3$) results in lower switching speed but more\nstable magnetic order.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the $S_n$-equivariant log-concavity of the cohomology of flag\nvarieties, also known as the coinvariant ring of $S_n$. Using the theory of\nrepresentation stability, we give computer-assisted proofs of the equivariant\nlog-concavity in low degrees and high degrees and conjecture that it holds for\nall degrees. Furthermore, we make a stronger unimodal conjecture which implies\nthe equivariant log-concavity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Directed evolution is a versatile technique in protein engineering that\nmimics the process of natural selection by iteratively alternating between\nmutagenesis and screening in order to search for sequences that optimize a\ngiven property of interest, such as catalytic activity and binding affinity to\na specified target. However, the space of possible proteins is too large to\nsearch exhaustively in the laboratory, and functional proteins are scarce in\nthe vast sequence space. Machine learning (ML) approaches can accelerate\ndirected evolution by learning to map protein sequences to functions without\nbuilding a detailed model of the underlying physics, chemistry and biological\npathways. Despite the great potentials held by these ML methods, they encounter\nsevere challenges in identifying the most suitable sequences for a targeted\nfunction. These failures can be attributed to the common practice of adopting a\nhigh-dimensional feature representation for protein sequences and inefficient\nsearch methods. To address these issues, we propose an efficient, experimental\ndesign-oriented closed-loop optimization framework for protein directed\nevolution, termed ODBO, which employs a combination of novel low-dimensional\nprotein encoding strategy and Bayesian optimization enhanced with search space\nprescreening via outlier detection. We further design an initial sample\nselection strategy to minimize the number of experimental samples for training\nML models. We conduct and report four protein directed evolution experiments\nthat substantiate the capability of the proposed framework for finding of the\nvariants with properties of interest. We expect the ODBO framework to greatly\nreduce the experimental cost and time cost of directed evolution, and can be\nfurther generalized as a powerful tool for adaptive experimental design in a\nbroader context.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Elton's near unconditionality and quasi-greediness for largest coefficients\nare two properties of bases that made their appearance in functional analysis\nfrom very different areas of research. One of our aims in this note is to show\nthat, oddly enough, they are connected to the extent that they are equivalent\nnotions. We take advantage of this new description of the former property to\nfurther the study of the threshold function associated with near\nunconditionality. Finally, we made a contribution to the isometric theory of\ngreedy bases by characterizing those bases that are $1$-quasi-greedy for\nlargest coefficients.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we study the diagonal restrictions of certain Hilbert theta\nseries for a totally real field $F$, and prove that they span the corresponding\nspace of elliptic modular forms when the $F$ is quadratic or cubic.\nFurthermore, we give evidence of this phenomenon when $F$ is quartic, quintic\nand sextic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The introduction of ISO 12913-2:2018 has provided a framework for\nstandardized data collection and reporting procedures for soundscape\npractitioners. A strong emphasis was placed on the use of calibrated head and\ntorso simulators (HATS) for binaural audio capture to obtain an accurate\nsubjective impression and acoustic measure of the soundscape under evaluation.\nTo auralise the binaural recordings as recorded or at set levels, the audio\nstimuli and the headphone setup are usually calibrated with a HATS. However,\ncalibrated HATS are too financially prohibitive for most research teams,\ninevitably diminishing the availability of the soundscape standard. With the\nincreasing availability of soundscape binaural recording datasets, and the\nimportance of cross-cultural validation of the soundscape ISO standards, e.g.\\\nvia the Soundscape Attributes Translation Project (SATP), it is imperative to\nassess the suitability of cost-effective headphone calibration methods to\nmaximise availability without severely compromising on accuracy. Hence, this\nstudy objectively examines an open-circuit voltage (OCV) calibration method in\ncomparison to a calibrated HATS on various soundcard and headphone\ncombinations. Preliminary experiments found that calibration with the OCV\nmethod differed significantly from the reference binaural recordings in sound\npressure levels, whereas negligible differences in levels were observed with\nthe HATS calibration.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In human neuroscience, machine learning can help reveal lower-dimensional\nneural representations relevant to subjects' behavior. However,\nstate-of-the-art models typically require large datasets to train, so are prone\nto overfitting on human neuroimaging data that often possess few samples but\nmany input dimensions. Here, we capitalized on the fact that the features we\nseek in human neuroscience are precisely those relevant to subjects' behavior.\nWe thus developed a Task-Relevant Autoencoder via Classifier Enhancement\n(TRACE), and tested its ability to extract behaviorally-relevant, separable\nrepresentations compared to a standard autoencoder for two severely truncated\nmachine learning datasets. We then evaluated both models on fMRI data where\nsubjects observed animals and objects. TRACE outperformed both the autoencoder\nand raw inputs nearly unilaterally, showing up to 30% increased classification\naccuracy and up to threefold improvement in discovering \"cleaner\",\ntask-relevant representations. These results showcase TRACE's potential for a\nwide variety of data related to human behavior.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the recent administration change in Mexico, the fluctuations in national\nenergy policy have generated widespread concerns among investors and the\npublic. The debate centers around Mexico's energy dependence on the US and how\nMexico's energy development should move forward. The goal of this study is\ntwo-fold. We first review the history and background of the recent energy\nreforms in Mexico. The focus of the study is on quantifying the state-level\nregional economic impact of the growing US-Mexico natural gas trade in Mexico.\nWe examine both the quantity effect (impact of import volume) and the price\neffect (impact of natural gas price changes). Our empirical analysis adopts a\nfixed-effects regression model and the instrumental variables (IV) estimation\napproach to address spatial heterogeneities and the potential endogeneity\nassociated with natural gas import. The quantity effect analysis suggests a\nstatistically significant positive employment impact of imports in non-mining\nsectors. The impact in the mining sector, however, is insignificant. The\nstate-level average (non-mining) employment impact is 127 jobs per million MCFs\nof natural gas imported from the US. The price effect analysis suggests a\nstatistically significant positive employment impact of price increases in the\nmining sector. A one-percentage increase in natural gas price (1.82 Pesos/GJ,\nin 2015 Peso) leads to an average state-level mining employment increase of 140\n(or 2.38%). We also explored the implications of our findings for Mexico's\nenergy policy, trade policy, and energy security.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Prediction intervals offer an effective tool for quantifying the uncertainty\nof loads in distribution systems. The traditional central PIs cannot adapt well\nto skewed distributions, and their offline training fashion is vulnerable to\nunforeseen changes in future load patterns. Therefore, we propose an optimal PI\nestimation approach, which is online and adaptive to different data\ndistributions by adaptively determining symmetric or asymmetric probability\nproportion pairs for quantiles. It relies on the online learning ability of\nreinforcement learning to integrate the two online tasks, i.e., the adaptive\nselection of probability proportion pairs and quantile predictions, both of\nwhich are modeled by neural networks. As such, the quality of quantiles-formed\nPI can guide the selection process of optimal probability proportion pairs,\nwhich forms a closed loop to improve the quality of PIs. Furthermore, to\nimprove the learning efficiency of quantile forecasts, a prioritized experience\nreplay strategy is proposed for online quantile regression processes. Case\nstudies on both load and net load demonstrate that the proposed method can\nbetter adapt to data distribution compared with online central PIs method.\nCompared with offline-trained methods, it obtains PIs with better quality and\nis more robust against concept drift.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Abstractive summarization models typically generate content unfaithful to the\ninput, thus highlighting the significance of evaluating the faithfulness of\ngenerated summaries. Most faithfulness metrics are only evaluated on news\ndomain, can they be transferred to other summarization tasks? In this work, we\nfirst present a systematic study of faithfulness metrics for dialogue\nsummarization. We evaluate common faithfulness metrics on dialogue datasets and\nobserve that most metrics correlate poorly with human judgements despite\nperforming well on news datasets. Given these findings, to improve existing\nmetrics' performance on dialogue summarization, we first finetune on in-domain\ndataset, then apply unlikelihood training on negative samples, and show that\nthey can successfully improve metric performance on dialogue data. Inspired by\nthe strong zero-shot performance of the T0 language model, we further propose\nT0-Score -- a new metric for faithfulness evaluation, which shows consistent\nimprovement against baseline metrics across multiple domains.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  How fast quantum information scrambles such that it becomes inaccessible by\nlocal probes turns out to be central to various fields. Motivated by recent\nworks on spin systems with nonlocal interactions, we study information\nscrambling in different variants of the Ising model. Our work reveals operator\ndynamics in the presence of nonlocal interactions not precisely captured by\nout-of-time-order correlators (OTOCs). In particular, the operator size\nexhibits a slowdown in systems with generic powerlaw interactions despite a\nhighly nonlinear lightcone. A recently proposed microscopic model for fast\nscrambling does not show this slowdown, which uncovers a distinct analogy\nbetween a local operator under unitary evolution and the entanglement entropy\nfollowing a quantum quench. Our work gives new insights on scrambling\nproperties of systems in reach of current quantum simulation platforms and\ncomplements results on possibly observing features of quantum gravity in the\nlaboratory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The notion of reproducing kernel Hilbert space (RKHS) has emerged in system\nidentification during the past decade. In the resulting framework, the impulse\nresponse estimation problem is formulated as a regularized optimization defined\non an infinite-dimensional RKHS consisting of stable impulse responses. The\nconsequent estimation problem is well-defined under the central assumption that\nthe convolution operators restricted to the RKHS are continuous linear\nfunctionals. Moreover, according to this assumption, the representer theorem\nhold, and therefore, the impulse response can be estimated by solving a\nfinite-dimensional program. Thus, the continuity feature plays a significant\nrole in kernel-based system identification. This paper shows that this central\nassumption is guaranteed to be satisfied in considerably general situations,\nnamely when the kernel is an integrable function and the input signal is\nbounded. Furthermore, the strong convexity of the optimization problem and the\ncontinuity property of the convolution operators imply that the kernel-based\nsystem identification admits a unique solution. Consequently, it follows that\nkernel-based system identification is a well-defined approach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It was conjectured by Ohba, and proved by Noel, Reed and Wu that\n$k$-chromatic graphs $G$ with $|V(G)| \\le 2k+1$ are chromatic-choosable. This\nupper bound on $|V(G)|$ is tight: if $k$ is even, then $K_{3 \\star (k/2+1), 1\n\\star (k/2-1)}$ and $K_{4, 2 \\star (k-1)}$ are $k$-chromatic graphs with $2\nk+2$ vertices that are not chromatic-choosable. It was proved in\n[arXiv:2201.02060] that these are the only non-$k$-choosable complete\n$k$-partite graphs with $2k+2$ vertices. For $G =K_{3 \\star (k/2+1), 1 \\star\n(k/2-1)}$ or $K_{4, 2 \\star (k-1)}$, a bad list assignment of $G$ is a $k$-list\nassignment $L$ of $G$ such that $G$ is not $L$-colourable. Bad list assignments\nfor $G=K_{4, 2 \\star (k-1)}$ were characterized in [Discrete Mathematics 244\n(2002), 55-66]. In this paper, we first give a simpler proof of this result,\nand then we characterize bad list assignments for $G=K_{3 \\star (k/2+1), 1\n\\star (k/2-1)}$. Using these results, we characterize all non-$k$-choosable\n(non-complete) $k$-partite graphs with $2k+2$ vertices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We derive Maxwell equations for electric and magnetic fields in curved\nspacetime from first principles. We show that the latter system enjoys an SO(2)\nduality symmetry, contrary to the results of previous works. We find that in a\ndetector with static background magnetic or electric field, the weak gravity\neffects are analogous to the effects from a medium composed of both\nelectrically and magnetically charged particles. We show that generation of\nelectromagnetic radiation by gravitational waves in an external magnetic field\nis four times more efficient than previously believed. We discuss differences\nbetween axion and gravitational wave electrodynamics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a combined radio/X-ray study of six massive galaxy clusters, aimed\nat determining the potential for heating of the intra-cluster medium (ICM) by\nnon-central radio galaxies. Since X-ray cavities associated with the radio\nlobes of non-central galaxies are generally not detectable, we use Giant\nMetrewave Radio Telescope 610~MHz observations to identify jet sources and\nestimate their size, and Chandra data to estimate the pressure of the\nsurrounding ICM. In the radio, we detect 4.5% of galaxies above the\nspectroscopic survey limit (M*K+2.0) of the Arizona cluster redshift survey\n(ACReS) which covers five of our six clusters. Approximately one tenth of these\nare extended radio sources. Using star formation (SF) rates determined from\nmid-infrared data, we estimate the expected contribution to radio luminosity\nfrom the stellar population of each galaxy, and find that most of the\nunresolved or poorly-resolved radio sources are likely star formation\ndominated. The relatively low frequency and good spatial resolution of our\nradio data allows us to trace star formation emission down to galaxies of\nstellar mass ~10^9.5 Msol. We estimate the enthalpy of the (AGN dominated)\njet/lobe and tailed sources, and place limits on the energy available from\nunresolved radio jets. We find jet powers in the range ~10^43-10^46 erg/s,\ncomparable to those of brightest cluster galaxies. Our results suggest that\nwhile cluster-central sources are the dominant factor balancing ICM cooling\nover the long term, non-central sources may have a significant impact, and that\nfurther investigation is possible and warranted.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose Adversarially Trained Actor Critic (ATAC), a new model-free\nalgorithm for offline reinforcement learning (RL) under insufficient data\ncoverage, based on the concept of relative pessimism. ATAC is designed as a\ntwo-player Stackelberg game: A policy actor competes against an adversarially\ntrained value critic, who finds data-consistent scenarios where the actor is\ninferior to the data-collection behavior policy. We prove that, when the actor\nattains no regret in the two-player game, running ATAC produces a policy that\nprovably 1) outperforms the behavior policy over a wide range of\nhyperparameters that control the degree of pessimism, and 2) competes with the\nbest policy covered by data with appropriately chosen hyperparameters. Compared\nwith existing works, notably our framework offers both theoretical guarantees\nfor general function approximation and a deep RL implementation scalable to\ncomplex environments and large datasets. In the D4RL benchmark, ATAC\nconsistently outperforms state-of-the-art offline RL algorithms on a range of\ncontinuous control tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep Neural Networks (DNNs) have become an essential component in many\napplication domains including web-based services. A variety of these services\nrequire high throughput and (close to) real-time features, for instance, to\nrespond or react to users' requests or to process a stream of incoming data on\ntime. However, the trend in DNN design is toward larger models with many layers\nand parameters to achieve more accurate results. Although these models are\noften pre-trained, the computational complexity in such large models can still\nbe relatively significant, hindering low inference latency. Implementing a\ncaching mechanism is a typical systems engineering solution for speeding up a\nservice response time. However, traditional caching is often not suitable for\nDNN-based services. In this paper, we propose an end-to-end automated solution\nto improve the performance of DNN-based services in terms of their\ncomputational complexity and inference latency. Our caching method adopts the\nideas of self-distillation of DNN models and early exits. The proposed solution\nis an automated online layer caching mechanism that allows early exiting of a\nlarge model during inference time if the cache model in one of the early exits\nis confident enough for final prediction. One of the main contributions of this\npaper is that we have implemented the idea as an online caching, meaning that\nthe cache models do not need access to training data and perform solely based\non the incoming data at run-time, making it suitable for applications using\npre-trained models. Our experiments results on two downstream tasks (face and\nobject classification) show that, on average, caching can reduce the\ncomputational complexity of those services up to 58\\% (in terms of FLOPs count)\nand improve their inference latency up to 46\\% with low to zero reduction in\naccuracy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The ever increasing complexity of machine learning techniques used more and\nmore in practice, gives rise to the need to explain the predictions and\ndecisions of these models, often used as black-boxes. Explainable AI approaches\nare either numerical feature-based aiming to quantify the contribution of each\nfeature in a prediction or symbolic providing certain forms of symbolic\nexplanations such as counterfactuals. This paper proposes a generic agnostic\napproach named ASTERYX allowing to generate both symbolic explanations and\nscore-based ones. Our approach is declarative and it is based on the encoding\nof the model to be explained in an equivalent symbolic representation, this\nlatter serves to generate in particular two types of symbolic explanations\nwhich are sufficient reasons and counterfactuals. We then associate scores\nreflecting the relevance of the explanations and the features w.r.t to some\nproperties. Our experimental results show the feasibility of the proposed\napproach and its effectiveness in providing symbolic and score-based\nexplanations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Heterogeneous graph neural networks (HGNNs) deliver powerful capacity in\nheterogeneous graph representation learning. The execution of HGNNs is usually\naccelerated by GPUs. Therefore, characterizing and understanding the execution\npattern of HGNNs on GPUs is important for both software and hardware\noptimizations. Unfortunately, there is no detailed characterization effort of\nHGNN workloads on GPUs. In this paper, we characterize HGNN workloads at\ninference phase and explore the execution of HGNNs on GPU, to disclose the\nexecution semantic and execution pattern of HGNNs. Given the characterization\nand exploration, we propose several useful guidelines for both software and\nhardware optimizations for the efficient execution of HGNNs on GPUs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Coronal mass ejection spray plasma associated with the M1.5-class flare of 16\nFebruary 2011 is found to exhibit a Doppler blue-shift of 850 km/s - the\nlargest value yet reported from ultraviolet (UV) or extreme ultraviolet (EUV)\nspectroscopy of the solar disk and inner corona. The observation is unusual in\nthat the emission line (Fe XII 193.51 A) is not observed directly, but the\nDoppler shift is so large that the blue-shifted component appears in a\nwavelength window at 192.82 A, intended to observe lines of O V, Fe XI and Ca\nXVII. The Fe XII 195.12 A emission line is used as a proxy for the rest\ncomponent of 193.51 A. The observation highlights the risks of using narrow\nwavelength windows for spectrometer observations when observing highly-dynamic\nsolar phenomena. The consequences of large Doppler shifts for ultraviolet solar\nspectrometers, including the upcoming Multi-slit Solar Explorer (MUSE) mission,\nare discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A new Monte Carlo method has been implemented to describe the angular and\npolarization distributions of anisotropic liquids, like water and linear\nalkylbenzene, by considering orientational fluctuations of polarizability\ntensors. The scattered light of anisotropic liquids is depolarized with an\nangular distribution of $1+(1-\\rho_v)/(1+3\\rho_v)\\cos^2\\theta$, which is\nmodified by the depolarization ratio $\\rho_v$. A standalone experiment has\nvalidated the simulation results of LAB. The new method can provide more\naccurate knowledge on light propagation in large liquid detectors, which is\nbeneficial to the developments of reconstruction for detectors.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop the use of mutual information (MI), a well-established metric in\ninformation theory, to interpret the inner workings of deep learning models. To\naccurately estimate MI from a finite number of samples, we present GMM-MI\n(pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that\ncan be applied to both discrete and continuous settings. GMM-MI is\ncomputationally efficient, robust to the choice of hyperparameters and provides\nthe uncertainty on the MI estimate due to the finite sample size. We\nextensively validate GMM-MI on toy data for which the ground truth MI is known,\ncomparing its performance against established mutual information estimators. We\nthen demonstrate the use of our MI estimator in the context of representation\nlearning, working with synthetic data and physical datasets describing highly\nnon-linear processes. We train deep learning models to encode high-dimensional\ndata within a meaningful compressed (latent) representation, and use GMM-MI to\nquantify both the level of disentanglement between the latent variables, and\ntheir association with relevant physical quantities, thus unlocking the\ninterpretability of the latent representation. We make GMM-MI publicly\navailable.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show a simple method for constructing larger matrices but preserving the\nspectral radius. This yields a sufficient criteria for two square matrices of\narbitrary dimension have the same spectral radius, a way to compare spectral\nradii of two matrices, and a way to derive new upper and lower bounds on\nspectral radius which give the standard row sum bounds as a special case.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this study, we have formulated and analyzed a non-linear compartmental\nmodel (SEIR) for the dynamics of COVID-19 with reference to immigration from\nurban to rural population in Indian scenario. We have captured the effect of\nthe immigration as two separate factors contributing in the rural compartments\nof the model. We have first established the positivity of the solution and the\nboundedness of the solution followed by the existence and uniqueness of the\nsolution for this multi compartment model. We later went on to find out the\nequibria of the system and derived the reproduction number. Further we\nnumerically depicted the local and global stability of the equilibria. Later we\nhave done sensitivity analysis of the model parameters and identified the\nsensitive parameters of the system. The sensitivity analysis is followed up\nwith the two parameter heat plots dealing with the sensitive parameters of the\nsystem. These heat plots gives us the parameter regions in which the system is\nstable. Finally comparative and effectiveness studies were done with reference\nto the control interventions such as Vaccination, Antiviral drugs,\nImmunotheraphy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Class-conditioning offers a direct means to control a Generative Adversarial\nNetwork (GAN) based on a discrete input variable. While necessary in many\napplications, the additional information provided by the class labels could\neven be expected to benefit the training of the GAN itself. On the contrary, we\nobserve that class-conditioning causes mode collapse in limited data settings,\nwhere unconditional learning leads to satisfactory generative ability.\nMotivated by this observation, we propose a training strategy for\nclass-conditional GANs (cGANs) that effectively prevents the observed\nmode-collapse by leveraging unconditional learning. Our training strategy\nstarts with an unconditional GAN and gradually injects the class conditioning\ninto the generator and the objective function. The proposed method for training\ncGANs with limited data results not only in stable training but also in\ngenerating high-quality images, thanks to the early-stage exploitation of the\nshared information across classes. We analyze the observed mode collapse\nproblem in comprehensive experiments on four datasets. Our approach\ndemonstrates outstanding results compared with state-of-the-art methods and\nestablished baselines. The code is available at\nhttps://github.com/mshahbazi72/transitional-cGAN\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In a critical software system, the testers have to spend an enormous amount\nof time and effort to maintain the software due to the continuous occurrence of\ndefects. Among such defects, some severe defects may adversely affect the\nsoftware. To reduce the time and effort of a tester, many machine learning\nmodels have been proposed in the literature, which use the documented defect\nreports to automatically predict the severity of the defective software\nmodules. In contrast to the traditional approaches, in this work we propose a\nmetric-based software defect severity prediction (SDSP) model that uses a\nself-training semi-supervised learning approach to classify the severity of the\ndefective software modules. The approach is constructed on a mixture of\nunlabelled and labelled defect severity data. The self-training works on the\nbasis of a decision tree classifier to assign the pseudo-class labels to the\nunlabelled instances. The predictions are promising since the self-training\nsuccessfully assigns the suitable class labels to the unlabelled instances.\n  On the other hand, numerous research studies have covered proposing\nprediction approaches as well as the methodological aspects of defect severity\nprediction models, the gap in estimating project attributes from the prediction\nmodel remains unresolved. To bridge the gap, we propose five project specific\nmeasures such as the Risk-Factor (RF), the Percent of Saved Budget (PSB), the\nLoss in the Saved Budget (LSB), the Remaining Service Time (RST) and Gratuitous\nService Time (GST) to capture project outcomes from the predictions. Similar to\nthe traditional measures, these measures are also calculated from the observed\nconfusion matrix. These measures are used to analyse the impact that the\nprediction model has on the software project.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We construct wormholes in Einstein-vector-Gauss-Bonnet theory where a real\nmassless vector field is coupled to the higher curvature Gauss-Bonnet\ninvariant. We consider three coupling functions which depend on the square of\nthe vector field. The respective domains of existence of wormholes possess as\ntheir boundaries i) black holes, ii) solutions with a singular throat, iii)\nsolutions with a degenerate throat and iv) solutions with cusp singularities.\nDepending on the coupling function wormhole solutions can feature a single\nthroat or an equator surrounded by a double throat. The wormhole solutions need\na thin shell of matter at the throat, in order to be symmetrically continued\ninto the second asymptotically flat region. These wormhole spacetimes allow for\nbound and unbound particle motion as well as light rings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Search-optimization problems are plentiful in scientific and engineering\ndomains. Artificial intelligence has long contributed to the development of\nsearch algorithms and declarative programming languages geared towards solving\nand modeling search-optimization problems. Automated reasoning and knowledge\nrepresentation are the subfields of AI that are particularly vested in these\ndevelopments. Many popular automated reasoning paradigms provide users with\nlanguages supporting optimization statements: MaxSAT or answer set programming,\nto name a few. These paradigms vary significantly in their languages and in the\nways they express quality conditions on computed solutions. Here we propose a\nunifying framework of so-called weight systems that eliminates syntactic\ndistinctions between paradigms and allows us to see essential similarities and\ndifferences between optimization statements provided by paradigms. This\nunifying outlook has a significant simplifying and explanatory potential in the\nstudies of optimization and modularity in automated reasoning and knowledge\nrepresentation providing technical means for bridging distinct formalisms and\ndeveloping translational solvers. Under consideration in Theory and Practice of\nLogic Programming (TPLP).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this study, we investigated the graphene-ionic liquid (EMImBF4) interface\nto clarify the effects of ambient temperature and potential on the differential\ncapacitance. We complemented molecular dynamics simulations with density\nfunctional theory calculations to unravel the electrolyte and electrode\ncontributions to the differential capacitance. As a result, we show: (1) the\nrelation of characteristic saddle points of the capacitance-potential curve to\nthe structural changes; (2) the smearing effect of temperature on the local\nstructure and, consequently, on the capacitance; (3) rationalization of these\nobservations with the interfacial bilayer model; and, finally, (4) how quantum\ncapacitance correction dampens the influence of temperature and improves the\nagreement with the experimental data. These insights are of fundamental and\npractical importance for the application of similar interfaces in\nelectrochemical energy storage and transformation devices, such as capacitors\nand actuators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this article I will explore some uses of the concept of Boltzmann brains\nin comics (Marvel's The Incredible Hercules), an animated sitcom (Futurama) and\na film (Guardians of the Galaxy Vol. 2). Previous similar uses of the concept\n(before Boltzmann brains were named as such) will be traced, and potential\ninfluences on these artistic outlets will be analysed. I will also explain how\nthe concept was used in a novel published in 2018 by Ediciones Ayarmanot in\nBuenos Aires (La inevitable resurrecci\\'on de los cerebros de Bolzmann, to be\npublished in English as \"Iridio Ennui vs. the Boltzmann brains\"). After briefly\nreviewing the second law of thermodynamics and the concept of Boltzmann brains,\nthe paper analyses the four artistic instances in which has been used,\nanswering two key questions: a) How entities who (in theory) appear in the foam\nof interstellar space and suddenly disappear are presented in the fictional\nscenario? b) How the plot justifies that Boltzmann brains could exist for a\ntime long enough to allow them to become viable fictional characters?\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the initial boundary value problem of Landau-Lifshitz-Bloch\nequation on three-dimensional ferromagnetic films, where the effective field\ncontains the stray field controlled by Maxwell equation and the exchange field\ncontains exchange constant. In this paper, we establish the existence of weak\nsolutions of the equation by using the Faedo-Galerkin approximation method. We\nalso derive its two-dimensional limit equation in a mathematically rigorous way\nwhen the film thickness tends to zero under appropriate compactness conditions.\nMoreover, we obtain an equation that can better describe the magnetic dynamic\nbehavior of ferromagnetic films with negligible thickness at high temperature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hom-Lie superalgebras can be considered as the deformation of Lie\nsuperalgebras; which are $\\mathbb{Z}_2$-graded generalization of Hom-Lie\nalgebras. The motivation of this paper is to introduce the concept of\nisoclinism and factor set in regular Hom-Lie superalgebras. Finally, we obtain\nthat two finite same dimensional regular Hom-Lie superalgebras are isoclinic if\nand only if they are isomorphic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Due to the cost or interference of measurement, we need to control\nmeasurement system. Assuming that each variable can be measured sequentially,\nthere exists optimal policy choosing next measurement for the former\nobservations. Though optimal measurement policy is actually dependent on the\ngoal of measurement, we mainly focus on retrieving complete data, so called as\nimputation. Also, we adapt the imputation method to missingness varying with\nmeasurement policy. However, learning measurement policy and imputation\nrequires complete data which is impossible to be observed, unfortunately. To\ntackle this problem, we propose a data generation method and joint learning\nalgorithm. The main idea is that 1) the data generation method is inherited by\nimputation method, and 2) the adaptation of imputation encourages measurement\npolicy to learn more than individual learning. We implemented some variations\nof proposed algorithm for two different datasets and various missing rates.\nFrom the experimental results, we demonstrate that our algorithm is generally\napplicable and outperforms baseline methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In modern dynamic constantly developing society, more and more people suffer\nfrom chronic and serious diseases and doctors and patients need special and\nsophisticated medical and health support. Accordingly, prominent health\nstakeholders have recognized the importance of development of such services to\nmake patients life easier. Such support requires the collection of huge amount\nof patients complex data like clinical, environmental, nutritional, daily\nactivities, variety of data from smart wearable devices, data from clothing\nequipped with sensors etc. Holistic patients data must be properly aggregated,\nprocessed, analyzed, and presented to the doctors and caregivers to recommend\nadequate treatment and actions to improve patients health related parameters\nand general wellbeing. Advanced artificial intelligence techniques offer the\nopportunity to analyze such big data, consume them, and derive new knowledge to\nsupport personalized medical decisions. New approaches like those based on\nadvanced machine learning, federated learning, transfer learning, explainable\nartificial intelligence open new paths for more quality use of health and\nmedical data in future. In this paper, we will present some crucial aspects and\ncharacteristic examples in the area of application of a range of artificial\nintelligence approaches in personalized medical decisions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In flat space, the scattering amplitudes of certain scalar effective field\ntheories exhibit enhanced soft limits due to the presence of hidden symmetries.\nIn this paper, we show that this phenomenon extends to wavefunction\ncoefficients in de Sitter space. Using a representation in terms of boundary\nconformal generators acting on contact diagrams, we find that imposing enhanced\nsoft limits fixes the masses and four-point couplings (including curvature\ncorrections) in agreement with Lagrangians recently derived from hidden\nsymmetries. Higher-point couplings can then be fixed using a bootstrap\nprocedure which we illustrate at six points. We also discuss implications for\nthe double copy in de Sitter space.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The zero-temperature Ising model is known to reach a fully ordered ground\nstate in sufficiently dense graphs. In sparse random graphs, the dynamics gets\nabsorbed in disordered local minima at magnetization close to zero. Here we\nfind that the non-equilibrium transition between the ordered and the disordered\nregime occurs at an average degree that is a slowly growing function of system\nsize. The system shows bistability. The distribution of the absolute\nmagnetization in the absorbing state reached is bimodal with peaks only at zero\nand unity. For fixed system size, the average time to absorption behaves\nnon-monotonically as a function of average degree. The peak value of the\naverage absorption time grows as a power law of system size.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the urgent need to implement the EU countries pledges and to monitor the\neffectiveness of Green Deal plan, Monitoring Reporting and Verification tools\nare needed to track how emissions are changing for all the sectors. Current\nofficial inventories only provide annual estimates of national CO$_2$ emissions\nwith a lag of 1+ year which do not capture the variations of emissions due to\nrecent shocks including COVID lockdowns and economic rebounds, war in Ukraine.\nHere we present a near-real-time country-level dataset of daily fossil fuel and\ncement emissions from January 2019 through December 2021 for 27 EU countries\nand UK, which called Carbon Monitor Europe. The data are calculated separately\nfor six sectors: power, industry, ground transportation, domestic aviation,\ninternational aviation and residential. Daily CO$_2$ emissions are estimated\nfrom a large set of activity data compiled from different sources. The goal of\nthis dataset is to improve the timeliness and temporal resolution of emissions\nfor European countries, to inform the public and decision makers about current\nemissions changes in Europe.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  On the Internet, fake news exists in various domain (e.g., education,\nhealth). Since news in different domains has different features, researchers\nhave be-gun to use single domain label for fake news detection recently. This\nemerg-ing field is called multi-domain fake news detection (MFND). Existing\nworks show that using single domain label can improve the accuracy of fake news\ndetection model. However, there are two problems in previous works. Firstly,\nthey ignore that a piece of news may have features from different domains. The\nsingle domain label focuses only on the features of the news on particu-lar\ndomain. This may reduce the performance of the model. Secondly, their model\ncannot transfer the domain knowledge to the other dataset without domain label.\nIn this paper, we propose a novel model, FuDFEND, which solves the limitations\nabove by introducing the fuzzy inference mechanism. Specifically, FuDFEND\nutilizes a neural network to fit the fuzzy inference process which constructs a\nfuzzy domain label for each news item. Then, the feature extraction module uses\nthe fuzzy domain label to extract the multi-domain features of the news and\nobtain the total feature representation. Fi-nally, the discriminator module\nuses the total feature representation to dis-criminate whether the news item is\nfake news. The results on the Weibo21 show that our model works better than the\nmodel using only single domain label. In addition, our model transfers domain\nknowledge better to Thu da-taset which has no domain label.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In a Stackelberg game, a leader commits to a randomized strategy, and a\nfollower chooses their best strategy in response. We consider an extension of a\nstandard Stackelberg game, called a discrete-time dynamic Stackelberg game,\nthat has an underlying state space that affects the leader's rewards and\navailable strategies and evolves in a Markovian manner depending on both the\nleader and follower's selected strategies. Although standard Stackelberg games\nhave been utilized to improve scheduling in security domains, their deployment\nis often limited by requiring complete information of the follower's utility\nfunction. In contrast, we consider scenarios where the follower's utility\nfunction is unknown to the leader; however, it can be linearly parameterized.\nOur objective then is to provide an algorithm that prescribes a randomized\nstrategy to the leader at each step of the game based on observations of how\nthe follower responded in previous steps. We design a no-regret learning\nalgorithm that, with high probability, achieves a regret bound (when compared\nto the best policy in hindsight) which is sublinear in the number of time\nsteps; the degree of sublinearity depends on the number of features\nrepresenting the follower's utility function. The regret of the proposed\nlearning algorithm is independent of the size of the state space and polynomial\nin the rest of the parameters of the game. We show that the proposed learning\nalgorithm outperforms existing model-free reinforcement learning approaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We leverage blockchain technology for drone node authentication in internet\nof drone things (IoDT). During the authentication procedure, the credentials of\ndrone nodes are examined to remove malicious nodes from the system. In IoDT,\ndrones are responsible for gathering data and transmitting it to cluster heads\n(CHs) for further processing. The CH collects and organizes data. Due to\ncomputational load, their energy levels rapidly deplete. To overcome this\nproblem, we present a low-energy adaptive clustering hierarchy (R2D) protocol\nbased on distance, degree, and residual energy. R2D is used to replace CHs with\nnormal nodes based on the biggest residual energy, the degree, and the shortest\ndistance from BS. The cost of keeping a big volume of data on the blockchain is\nhigh. We employ the Interplanetary File System (IPFS), to address this issue.\nMoreover, IPFS protects user data using the industry-standard encryption\ntechnique AES-128. This standard compares well to other current encryption\nmethods. Using a consensus mechanism based on proof of work requires a high\namount of computing resources for transaction verification. The suggested\napproach leverages a consensus mechanism known as proof of authority (PoA) to\naddress this problem . The results of the simulations indicate that the\nsuggested system model functions effectively and efficiently. A formal security\nanalysis is conducted to assess the smart contract's resistance to attacks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate the dynamics of a population-imbalanced two-species fermionic\nsystem trapped in an optical lattice. The paired fermions here can form bosonic\nmolecules via Feshbach coupling in the presence of an external magnetic field.\nIt is shown that the natural fluctuations of the condensate fraction is\nperiodic beyond a threshold Feshbach detuning; and below this threshold value\nthe condensate shows no oscillation at all. The oscillation frequency vs.\ndetuning curve is linear in nature. The slope and intercept of this line is\nshown to carry important information about the amount of imbalance present in\nthe system, and also about the nature of the pairing that takes place in the\npopulation-imbalanced system. Most importantly, it can help identifying the\nFulde-Ferrel-Larkin-Ovchinikov (FFLO) phase as its signature is different from\nthe other possible phases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We review holographic renormalization for non-conformal branes using the\nHamilton-Jacobi formalism. We provide the tools required for the computation of\nthe holographic dictionary, while we present some marginally new technical\nresults regarding holographic renormalization for $Dp$-branes, $p<5$, for\nEinstein-Maxwell theory coupled to a dilaton. The computations are based on a\nrecursive algorithm by Papadimitriou, that solves the radial Hamilton-Jacobi\nequation in the asymptotic boundary of the bulk. This paper is a short version\nof the Master thesis of the author written in 2014 at the Instituto de F\\'isica\nTe\\'orica, UAM, Madrid.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The hadronic form factors of $B_c$ semileptonic decays to the $P-$wave\ncharmonium 4-plet can be expressed near the zero-recoil point in terms of\nuniversal functions, performing a systematic expansion in QCD in the relative\nvelocity of the heavy quarks and in $1/m_Q$. Such functions are independent of\nthe member of the multiplet involved in the transitions. We present the results\nof an NLO calculation up to $O(1/m_Q^2)$ classifying the universal functions at\nthis order. We work out a set of relations among the form factors of the same\nmode and of different modes, which should be reproduced by explicit\ncalculations, reducing the hadronic uncertainty affecting such channels. The\napproach is also helpful to investigate the debated nature of\n$\\chi_{c1}(3872)$, studying the production in $B_c$ semileptonic decays and\ncomparing it to the modes involving the other $2P$ charmonia.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Following a regulatory change in Europe which mandates that car manufacturers\ninclude an eCall system in new vehicles, many car manufacturers are adding\nadditional services on top, so that more and more cars become connected\nvehicles and act like IoT sensors. In the following study, we analyse the\nmaturity level of this new technology to build insurance products that would\ntake vehicle usage into account. For this, the connectivity of recent cars\na-priori eligible has been first tested. Then, an ad-hoc platform has been\ndesigned to collect driving data. In particular, 4 cars have been connected to\nthis platform for periods of over one month. Our results highlight that, while\nthis technological innovation appears very promising in the future, the\npricing, the lack of uniformity of data collected and the enrollment process\nare currently three pain points that should be addressed to offer large-scale\nopportunities. In the meantime, this technology might still be used for high\nvalue use cases such as the insurance of luxurious cars.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper applies Benders decomposition to two-stage stochastic problems for\nenergy planning with multiple climatic years, a key problem for the design of\nrenewable energy systems. First, we implement Benders decomposition with\nexisting enhancements suited for the characteristics of the problem, a simple\ncontinuous master-problem and few but large sub-problems. Next, we develop a\nnovel trust-region method using a quadratic constraint that is continuously\nadapted to further improve the algorithm.\n  In a quantitative case-study our method accelerates Benders decomposition by\na factor of four to six slightly increasing solve time of the master-problem,\nbut greatly reducing the number of iterations. With the computational resources\nat our disposal, Benders decomposition with quadratic trust-region outperforms\nclosed optimization if planning covers more than six climatic years, because\nrun-time does not increase with the number of scenarios thanks to distributed\ncomputing. Furthermore, results show that the quadratic trust-region approach\nbenefits from a heuristic starting solution but does not depend on it to be\nperformative.\n  Finally, we suggest further improvements of the algorithm. First, heuristic\nmethods to narrow the solution space of the master-problem. Second,\napproximations of the sub-problems to faster add inexact but valid cuts.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We estimate best-approximation errors using vector-valued finite elements for\nfields with low regularity in the scale of fractional-order Sobolev spaces. By\nassuming additionally that the target field has a curl or divergence property,\nwe establish upper bounds on these errors that can be localized to the mesh\ncells. These bounds are derived using the quasi-interpolation errors with or\nwithout boundary prescription derived in [A. Ern and J.-L. Guermond, ESAIM\nMath. Model. Numer. Anal., 51 (2017), pp.~1367--1385]. By using the\nface-to-cell lifting operators analyzed in [A. Ern and J.-L. Guermond, Found.\nComput. Math., (2021)], and exploiting the additional assumption made on the\ncurl or the divergence of the target field, a localized upper bound on the\nquasi-interpolation error is derived. As an illustration, we show how to apply\nthese results to the error analysis of the curl-curl problem associated with\nMaxwell's equations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Transformer-based pretrained language models (LMs) are ubiquitous across\nnatural language understanding, but cannot be applied to long sequences such as\nstories, scientific articles and long documents, due to their quadratic\ncomplexity. While a myriad of efficient transformer variants have been\nproposed, they are typically based on custom implementations that require\nexpensive pretraining from scratch. In this work, we propose SLED:\nSLiding-Encoder and Decoder, a simple approach for processing long sequences\nthat re-uses and leverages battle-tested short-text pretrained LMs.\nSpecifically, we partition the input into overlapping chunks, encode each with\na short-text LM encoder and use the pretrained decoder to fuse information\nacross chunks (fusion-in-decoder). We illustrate through controlled experiments\nthat SLED offers a viable strategy for long text understanding and evaluate our\napproach on SCROLLS, a benchmark with seven datasets across a wide range of\nlanguage understanding tasks. We find that SLED is competitive with specialized\nmodels that are up to 50x larger and require a dedicated and expensive\npretraining step.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Semi-supervised learning is a critical tool in reducing machine learning's\ndependence on labeled data. It has been successfully applied to structure data,\nsuch as image and language data, by exploiting the inherent spatial and\nsemantic structure therein with pretrained models or data augmentation. Some of\nthese methods are no longer applicable for the data where domain structures are\nnot available because the pretrained models or data augmentation can not be\nused.\n  Due to simplicity, existing pseudo-labeling (PL) methods can be widely used\nwithout any domain assumption, but are vulnerable to noise samples and to\ngreedy assignments given a predefined threshold which is typically unknown.\nThis paper addresses this problem by proposing a Confident Sinkhorn Allocation\n(CSA), which assigns labels to only samples with high confidence scores and\nlearns the best label allocation via optimal transport. CSA outperforms the\ncurrent state-of-the-art in this practically important area of semi-supervised\nlearning. Our code is publicly available at\nhttps://github.com/amzn/confident-sinkhorn-allocation .\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose a novel algorithm, termed Subspace Phase Retrieval\n(SPR), which can accurately recover any $n$-dimensional $k$-sparse signal from\n$\\mathcal O(k\\log n)$ magnitude-only Gaussian samples. This offers a\nsignificant improvement over some existing results that require $\\mathcal O(k^2\n\\log n)$ samples. We also present a geometrical analysis for a subproblem,\nwhere we recover the sparse signal given that at least one support index of\nthis signal is identified already. It is shown that with high probability,\n$\\mathcal O(k\\log k)$ magnitude-only Gaussian samples ensure i) that all local\nminima of our objective function are clustered around the expected global\nminimum within arbitrarily small distances, and ii) that all critical points\noutside of this region have at least one negative curvature. When the input\nsignal is nonsparse (i.e., $k = n$), our result indicates an analogous\ngeometric property with $\\mathcal O(n \\log n)$ samples. This affirmatively\nanswers the open question by Sun-Qu-Wright [1].\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding energy transport in quantum systems is crucial for an\nunderstanding of light-harvesting in nature, and for the creation of new\nquantum technologies. Open quantum systems theory has been successfully applied\nto predict the existence of environmental noise-assisted quantum transport\n(ENAQT) as a widespread phenomenon occurring in biological and artificial\nsystems. That work has been primarily focused on several 'canonical'\nstructures, from simple chains, rings and crystals of varying dimensions, to\nwell-studied light-harvesting complexes. Studying those particular systems has\nproduced specific assumptions about ENAQT, including the notion of a single,\nideal, range of environmental coupling rates that improve energy transport. In\nthis paper we show that a consistent subset of physically modelled transport\nnetworks can have at least two ENAQT peaks in their steady state transport\nefficiency.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The advent of 5G and the design of its architecture has become possible\nbecause of the previous individual scientific works and standardization efforts\non cloud computing and network softwarization. Software-defined Networking and\nNetwork Function Virtualization started separately to find their convolution\ninto 5G network architecture. Then, the ongoing design of the future beyond 5G\n(B5G) and 6G network architecture cannot overlook the pivotal inputs of\ndifferent independent standardization efforts about autonomic networking,\nservice-based communication systems, and multi-access edge computing. This\narticle provides the design and the characteristics of an agent-based,\nsoftwarized, and intelligent architecture, which coherently condenses and\nmerges the independent proposed architectural works by different\nstandardization working groups and bodies. This novel work is a helpful means\nfor the design and standardization process of the futureB5G and 6G network\narchitecture.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To align conditional text generation model outputs with desired behaviors,\nthere has been an increasing focus on training the model using reinforcement\nlearning (RL) with reward functions learned from human annotations. Under this\nframework, we identify three common cases where high rewards are incorrectly\nassigned to undesirable patterns: noise-induced spurious correlation, naturally\noccurring spurious correlation, and covariate shift. We show that even though\nlearned metrics achieve high performance on the distribution of the data used\nto train the reward function, the undesirable patterns may be amplified during\nRL training of the text generation model. While there has been discussion about\nreward gaming in the RL or safety community, in this short discussion piece, we\nwould like to highlight reward gaming in the NLG community using concrete\nconditional text generation examples and discuss potential fixes and areas for\nfuture work.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spatial modulation of electron beams is an essential tool for various\napplications such as nanolithography and imaging, yet its implementations are\nseverely limited and inherently non-tunable. Conversely, light-driven electron\nspatial modulation could potentially allow arbitrary electron wavefront shaping\nvia the underlying mechanism of photon-induced near-field electron microscopy\n(PINEM). Here, we present tunable photon-induced spatial modulation of\nelectrons through their externally-controlled interaction with surface plasmon\npolaritons (SPPs). Using recently developed methods of shaping SPP patterns, we\ndemonstrate a dynamic control of the electron beam with a variety of\nhigh-quality electron distributions. Intriguingly, by utilizing the intrinsic\ninteraction nonlinearity, we attain the first observation of 2D spatial Rabi\noscillations and generate electron features below the SPP wavelength. Our work\npaves the way to on-demand electron wavefront shaping at ultrafast timescales,\nwith prospects for aberration correction, nano-fabrication and material\ncharacterization.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The emergence of the infinite-layer superconducting nickelate thin films\nmarks the Ni age of superconductivity, which has excited a huge surge of\nstudies since the first report in August of 2019. Despite of the tremendous\nattention drawn from the entire material science community and a large body of\ntheoretical studies, the experimental progress has been relatively slow due to\nthe challenging sample fabrication, which may, in turn, be holding back the\nfast development of theoretical research. Therefore, a timely and comprehensive\nreview on all the up-to-date experimental progress of the emergent\ninfinite-layer Ni-based superconductors is urgently needed. In this review, we\nfirst introduce the history of more than 30-year-long Ni-based\nsuperconductivity exploration, then summarize the sample fabrication processes,\nlater present the experimental electrical transport and magnetic properties,\nand finally come up with several key issues deserving intensive studies. This\nreview is thus expected to be helpful for researchers with diverse research\nbackground to readily capture the major progress of this emerging field.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In drug discovery, aqueous solubility is an important pharmacokinetic\nproperty which affects absorption and assay availability of drug. Thus, in\nsilico prediction of solubility has been studied for its utility in virtual\nscreening and lead optimization. Recently, machine learning (ML) methods using\nexperimental data has been popular because physics-based methods like quantum\nmechanics and molecular dynamics are not suitable for high-throughput tasks due\nto its computational costs. However, ML method can exhibit over-fitting problem\nin a data-deficient condition, and this is the case for most chemical property\ndatasets. In addition, ML methods are regarded as a black box function in that\nit is difficult to interpret contribution of hidden features to outputs,\nhindering analysis and modification of structure-activity relationship. To deal\nwith mentioned issues, we developed Bayesian graph neural networks (GNNs) with\nthe self-attention readout layer. Unlike most GNNs using self-attention in node\nupdates, self-attention applied at readout layer enabled a model to improve\nprediction performance as well as to identify atom-wise importance, which can\nhelp lead optimization as exemplified for three FDA-approved drugs. Also,\nBayesian inference enables us to separate more or less accurate results\naccording to uncertainty in solubility prediction task We expect that our\naccurate, reliable and interpretable model can be used for more careful\ndecision-making and various applications in the development of drugs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We have applied Zeeman-Doppler imaging (ZDI) to an extensive\nspectropolarimetric HARPSpol data set of the magnetically active young solar\nanalogue V889 Her, covering 35 spectra obtained during six nights in May 2011.\nThe data set allows us to study Stokes V profiles of the star at almost\nidentical rotational phases, separated by one or more stellar rotations. We use\nthese data to study if the line profiles evolve from one rotation to the next,\nand find that some evolution does indeed occur. We consider two possible\nexplanations for this: abrupt changes in the large-scale magnetic field or\ndifferential rotation. We find it quite difficult to distinguish between the\ntwo alternatives using ZDI alone. A strong differential rotation could,\nhowever, explain the changes in the line profiles, so we conclude that it must\nbe present, and the abrupt magnetic field evolution is left uncertain.\nCommonly, rapidly rotating stars are assumed to have only weak differential\nrotation. If the strong differential rotation of V889 Her is indeed present, as\nhas been found in other studies as well, it could indicate that the theoretical\nand numerical results of differential rotation still need to be revised. The\nrapid changes that may have occurred in the magnetic field indicate that one\nshould be quite cautious when interpreting ZDI maps constructed from data over\nlong time intervals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper is about the arithmetic geometry of the reduction modulo $p$ of\nShimura varieties with parahoric level structure. We realize the\nEKOR-stratification on the special fiber of the Siegel modular stack at\nparahoric level as the fibers of a smooth morphism into an algebraic stack\nparametrizing homogeneously polarized chains of certain truncated displays.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Modern control systems must operate in increasingly complex environments\nsubject to safety constraints and input limits, and are often implemented in a\nhierarchical fashion with different controllers running at multiple time\nscales. Yet traditional constructive methods for nonlinear controller synthesis\ntypically \"flatten\" this hierarchy, focusing on a single time scale, and\nthereby limited the ability to make rigorous guarantees on constraint\nsatisfaction that hold for the entire system. In this work we seek to address\nthe stabilization of constrained nonlinear systems through a\n\\textit{multi-rate} control architecture. This is accomplished by iteratively\nplanning continuous reference trajectories for a nonlinear system using a\nlinearized model and Model Predictive Control (MPC), and tracking said\ntrajectories using the full-order nonlinear model and Control Lyapunov\nFunctions (CLFs). Connecting these two levels of control design in a way that\nensures constraint satisfaction is achieved through the use of\n\\textit{B\\'{e}zier curves}, which enable planning continuous trajectories\nrespecting constraints by planning a sequence of discrete points. Our framework\nis encoded via convex optimization problems which may be efficiently solved, as\ndemonstrated in simulation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It is shown that the coloured isomorphism class of a unital, simple,\n$\\mathcal{Z}$-stable, separable amenable C$^*$-algebra satisfying the Universal\nCoefficient Theorem (UCT) is determined by its tracial simplex.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The digitally disruptive environment has evolved rapidly due to the\nintroduction of new advancements within the field of smart applications.\nApplications of one of the most prominent technologies, Internet of Things\n(IoT), often appear in the retail sector, where smart services have transformed\nthe customer experience holistically. Presented in this paper are the findings\nfrom an exploratory field study in the retail service sector, which drew on the\nviews of experienced practitioners about the smart store experience and the\nassociated changes. The paper presents an overview of the drivers of smart\nretail service diffusion and the relevant challenges, such as the business\nexpectations and the heterogeneity of devices. The arising themes indicate that\nIoT security is a major challenge for businesses installing IoT devices in\ntheir journey towards smart store transformation. The paper highlights the\nimportance of a secure data-sharing IoT environment that respects customer\nprivacy as the smart experience in-store offers data-driven insights and\nservices. Implications for research and practice are discussed in terms of the\ncustomer experience relevant to the identified challenges.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider a single-echelon inventory system under periodic review with two\nsuppliers facing stochastic demand, where excess demand is backlogged. The\nexpedited supplier has a shorter lead time than the regular supplier but\ncharges a higher unit price. We introduce the Projected Expedited Inventory\nPosition (PEIP) policy, and we show that the relative difference between the\nlong run average cost per period of this policy and the optimal policy\nconverges to zero when both the shortage cost and the cost premium for\nexpedited units become large, with their ratio held constant. A corollary of\nthis result is that several existing heuristics are also asymptotically optimal\nin this non-trivial regime. We show through an extensive numerical\ninvestigation that the PEIP policy outperforms the current best performing\nheuristic policies in literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hybrid neural network models combine the advantages of a neural network's\nfitting functionality with differential equation models to reflect actual\nphysical processes and are widely used in analyzing time-series data. Most\nrelated studies have focused on linear hybrid models, but only a few have\nexamined nonlinear problems. In this work, we use a hybrid nonlinear epidemic\nneural network as the entry point to study its power in predicting the correct\ninfection function of an epidemic model. To achieve this goal, we combine the\nbifurcation theory of the nonlinear differential model with the mean-squared\nerror loss and design a novel loss function to ensure model trainability.\nFurthermore, we find the unique existence conditions supporting ordinary\ndifferential equations to estimate the correct infection function. Using the\nRunge Kutta method, we perform numerical experiments on our proposed model and\nverify its soundness. We also apply it to real COVID-19 data to accurately\ndiscover the change law of its infectivity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using the corner-transfer matrix renormalization group to contract the tensor\nnetwork that describes its partition function, we investigate the nature of the\nphase transitions of the hard-square model, one of the exactly solved models of\nstatistical physics for which Baxter has found an integrable manifold. The\nmotivation is twofold: assess the power of tensor networks for such models, and\nprobe the 2D classical analog of a 1D quantum model of hard-core bosons that\nhas recently attracted significant attention in the context of experiments on\nchains of Rydberg atoms. Accordingly, we concentrate on two planes in the 3D\nparameter space spanned by the activity and the coupling constants in the two\ndiagonal directions. We first investigate the only case studied so far with\nMonte Carlo simulations, the case of opposite coupling constants. We confirm\nthat, away and not too far from the integrable 3-state Potts point, the\ntransition out of the period-3 phase appears to be unique in the Huse-Fisher\nchiral universality class, albeit with significantly different exponents as\ncompared to Monte Carlo. We also identify two additional phase transitions not\nreported so far for that model, a Lifshitz disorder line, and an Ising\ntransition for large enough activity. To make contact with 1D quantum models of\nRydberg atoms, we then turn to a plane where the ferromagnetic coupling is kept\nfixed, and we show that the resulting phase diagram is very similar, the only\ndifference being that the Ising transition becomes first-order through a\ntricritical Ising point, in agreement with Baxter's prediction that this plane\nshould contain a tricritical Ising point, and in remarkable, almost\nquantitative agreement with the phase diagram of the 1D quantum version of the\nmodel.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate the unsupervised learning of non-invertible observation\nfunctions in nonlinear state-space models. Assuming abundant data of the\nobservation process along with the distribution of the state process, we\nintroduce a nonparametric generalized moment method to estimate the observation\nfunction via constrained regression. The major challenge comes from the\nnon-invertibility of the observation function and the lack of data pairs\nbetween the state and observation. We address the fundamental issue of\nidentifiability from quadratic loss functionals and show that the function\nspace of identifiability is the closure of a RKHS that is intrinsic to the\nstate process. Numerical results show that the first two moments and temporal\ncorrelations, along with upper and lower bounds, can identify functions ranging\nfrom piecewise polynomials to smooth functions, leading to convergent\nestimators. The limitations of this method, such as non-identifiability due to\nsymmetry and stationarity, are also discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate the internal kinematics of the young star-forming region NGC\n346 in the Small Magellanic Cloud. We used two epochs of deep F555W and F814W\nHubble Space Telescope ACS observations with an 11-year baseline to determine\nproper motions, and study the kinematics of different populations, as\nidentified by their color-magnitude diagram and spatial distribution\ncharacteristics. The proper motion field of the young stars shows a complex\nstructure with spatially coherent patterns. NGC 346 upper-main sequence and\npre-main sequence stars follow very similar motion patterns, with the outer\nparts of the cluster being characterized both by outflows and inflows. The\nproper motion field in the inner ~10 pc shows a combination of rotation and\ninflow, indicative of inspiraling motion. The rotation velocity in this regions\npeaks at ~3 km/s, whereas the inflow velocity peaks at ~1 km/s. Sub-clusters\nand massive young stellar objects in NGC 346 are found at the interface of\nsignificant changes in the coherence of the proper motion field. This suggests\nthat turbulence is the main star formation driver in this region. The similar\nkinematics observed in the metal-poor NGC 346 and the Milky Way star-forming\nregions suggest that the differences in the cooling conditions due to the\ndifferent amounts of metallicity and dust density between the SMC and our\nGalaxy are too small to alter significantly the process of star clusters\nassembly and growth. The main characteristics of our findings are consistent\nwith various proposed star cluster formation models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Yu-Shiba-Rusinov (YSR) state appears as a bound state of a quasiparticle\nat a magnetic atom embedded in a superconductor. We discuss why the YSR state\nhas energy below the superconducting gap and why the pair potential changes the\nsign at the magnetic atom. Although a magnetic atom in a superconductor has\nbeen considered as a pair breaker since 1960s, we propose an alternative\nphysical picture to explain these reasons. We show that a magnetic atom\nconverts a spin-singlet s-wave Cooper pair into an odd-frequency pair rather\nthan breaking it. The odd-frequency pairing correlations always coexist with\nthe quasiparticle states below the gap. The YSR state is an example of such a\nsubgap quasiparticle state. The paramagnetic property of an odd-frequency pair\nexplains the sign change of the pair potential at a magnetic atom and the\ndecrease of superconducting transition temperature in the presence of many\nmagnetic impurities.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given any strictly convex norm $\\|\\cdot\\|$ on $\\mathbb{R}^2$ that is $C^1$ in\n$\\mathbb{R}^2\\setminus\\{0\\}$, we study the generalized Aviles-Giga functional\n\\[I_{\\epsilon}(m):=\\int_{\\Omega} \\left(\\epsilon \\left|\\nabla m\\right|^2 +\n\\frac{1}{\\epsilon}\\left(1-\\|m\\|^2\\right)^2\\right) \\, dx,\\] for\n$\\Omega\\subset\\mathbb R^2$ and $m\\colon\\Omega\\to\\mathbb R^2$ satisfying\n$\\nabla\\cdot m=0$. Using, as in the euclidean case $\\|\\cdot\\|=|\\cdot|$, the\nconcept of entropies for the limit equation $\\|m\\|=1$, $\\nabla\\cdot m=0$, we\nobtain the following. First, we prove compactness in $L^p$ of sequences of\nbounded energy. Second, we prove rigidity of zero-energy states (limits of\nsequences of vanishing energy), generalizing and simplifying a result by\nBochard and Pegon. Third, we obtain optimal regularity estimates for limits of\nsequences of bounded energy, in terms of their entropy productions. Fourth, in\nthe case of a limit map in $BV$, we show that lower bound provided by entropy\nproductions and upper bound provided by one-dimensional transition profiles are\nof the same order. The first two points are analogous to what is known in the\neuclidean case $\\|\\cdot\\|=|\\cdot|$, and the last two points are sensitive to\nthe anisotropy of the norm $\\|\\cdot\\|$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Predicting the presence of major depressive disorder (MDD) using behavioural\nand cognitive signals is a highly non-trivial task. The heterogeneous clinical\nprofile of MDD means that any given speech, facial expression and/or observed\ncognitive pattern may be associated with a unique combination of depressive\nsymptoms. Conventional discriminative machine learning models potentially lack\nthe complexity to robustly model this heterogeneity. Bayesian networks,\nhowever, may instead be well-suited to such a scenario. These networks are\nprobabilistic graphical models that efficiently describe the joint probability\ndistribution over a set of random variables by explicitly capturing their\nconditional dependencies. This framework provides further advantages over\nstandard discriminative modelling by offering the possibility to incorporate\nexpert opinion in the graphical structure of the models, generating explainable\nmodel predictions, informing about the uncertainty of predictions, and\nnaturally handling missing data. In this study, we apply a Bayesian framework\nto capture the relationships between depression, depression symptoms, and\nfeatures derived from speech, facial expression and cognitive game data\ncollected at thymia.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We examine the contribution of institutional integration to the institutional\nquality. To this end, we exploit the 2007 political crisis in Ukraine and\nexamine the effects of staying out of the European Union for 28 Ukrainian\nprovinces in the period 1996-2020. We construct novel subnational estimates of\ninstitutional quality for Ukraine and central and eastern European countries\nbased on the latent residual component extraction of institutional quality from\nthe existing governance indicators by making use of Bayesian posterior analysis\nunder non-informative objective prior function. By comparing the residualized\ninstitutional quality trajectories of Ukrainian provinces with their central\nand eastern European peers that were admitted to the European Union in 2004 and\nafter, we assess the institutional quality cost of being under Russian\npolitical influence and interference. Based on the large-scale synthetic\ncontrol analysis, we find evidence of large-scale negative institutional\nquality effects of staying out of the European Union such as heightened\npolitical instability and rampant deterioration of the rule of law and control\nof corruption. Statistical significance of the estimated effects is evaluated\nacross a comprehensive placebo simulation with more than 34 billion placebo\naverages for each institutional quality outcome.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Black-box functions are broadly used to model complex problems that provide\nno explicit information but the input and output. Despite existing studies of\nblack-box function optimization, the solution set satisfying an inequality with\na black-box function plays a more significant role than only one optimum in\nmany practical situations. Covering as much as possible of the solution set\nthrough limited evaluations to the black-box objective function is defined as\nthe Black-Box Coverage (BBC) problem in this paper. We formalized this problem\nin a sample-based search paradigm and constructed a coverage criterion with\nConfusion Matrix Analysis. Further, we propose LAMBDA (Latent-Action\nMonte-Carlo Beam Search with Density Adaption) to solve BBC problems. LAMBDA\ncan focus around the solution set quickly by recursively partitioning the\nsearch space into accepted and rejected sub-spaces. Compared with La-MCTS,\nLAMBDA introduces density information to overcome the sampling bias of\noptimization and obtain more exploration. Benchmarking shows, LAMBDA achieved\nstate-of-the-art performance among all baselines and was at most 33x faster to\nget 95% coverage than Random Search. Experiments also demonstrate that LAMBDA\nhas a promising future in the verification of autonomous systems in virtual\ntests.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Sonification is the technique of representing data with sound, with potential\napplications in astronomy research for aiding discovery and accessibility.\nSeveral astronomy-focused sonification tools have been developed; however,\nefficacy testing is extremely limited. We performed testing of astronify, a\nprototype tool for sonification functionality within the Barbara A. Mikulski\nArchive for Space Telescopes (MAST). We created synthetic light curves\ncontaining zero, one, or two transit-like signals with a range of\nsignal-to-noise ratios (SNRs=3-100) and applied the default mapping of\nbrightness to pitch. We performed remote testing, asking participants to count\nsignals when presented with light curves as a sonification, visual plot, or\ncombination of both. We obtained 192 responses, of which 118 self-classified as\nexperts in astronomy and data analysis. For high SNRs (=30 and 100), experts\nand non-experts performed well with sonified data (85-100% successful signal\ncounting). At low SNRs (=3 and 5) both groups were consistent with guessing\nwith sonifications. At medium SNRs (=7 and 10), experts performed no better\nthan non-experts with sonifications but significantly better (factor of ~2-3)\nwith visuals. We infer that sonification training, like that experienced by\nexperts for visual data inspection, will be important if this sonification\nmethod is to be useful for moderate SNR signal detection within astronomical\narchives and broader research. Nonetheless, we show that even a very simple,\nand non-optimised, sonification approach allows users to identify high SNR\nsignals. A more optimised approach, for which we present ideas, would likely\nyield higher success for lower SNR signals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider a Tsallis holographic dark energy model with interaction between\ndark energy and matter. The density of dark energy is taken as $\\rho_d \\sim\n3C^2/L^{4-2\\gamma}$, where $C$, $\\gamma$ are constants. The event horizon is\nchosen as the characteristic scale $L$. The cosmological dynamics of the\nuniverse are analyzed, with special attention paid to the possibility of\ncrossing the phantom line $w_{eff}=-1$. It is shown that for certain values of\nparameters this may occur not only once, but also twice.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Pre-training across 3D vision and language remains under development because\nof limited training data. Recent works attempt to transfer vision-language\npre-training models to 3D vision. PointCLIP converts point cloud data to\nmulti-view depth maps, adopting CLIP for shape classification. However, its\nperformance is restricted by the domain gap between rendered depth maps and\nimages, as well as the diversity of depth distributions. To address this issue,\nwe propose CLIP2Point, an image-depth pre-training method by contrastive\nlearning to transfer CLIP to the 3D domain, and adapt it to point cloud\nclassification. We introduce a new depth rendering setting that forms a better\nvisual effect, and then render 52,460 pairs of images and depth maps from\nShapeNet for pre-training. The pre-training scheme of CLIP2Point combines\ncross-modality learning to enforce the depth features for capturing expressive\nvisual and textual features and intra-modality learning to enhance the\ninvariance of depth aggregation. Additionally, we propose a novel Dual-Path\nAdapter (DPA) module, i.e., a dual-path structure with simplified adapters for\nfew-shot learning. The dual-path structure allows the joint use of CLIP and\nCLIP2Point, and the simplified adapter can well fit few-shot tasks without\npost-search. Experimental results show that CLIP2Point is effective in\ntransferring CLIP knowledge to 3D vision. Our CLIP2Point outperforms PointCLIP\nand other self-supervised 3D networks, achieving state-of-the-art results on\nzero-shot and few-shot classification.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The growing amount of data and advances in data science have created a need\nfor a new kind of cloud platform that provides users with flexibility, strong\nsecurity, and the ability to couple with supercomputers and edge devices\nthrough high-performance networks. We have built such a nation-wide cloud\nplatform, called \"mdx\" to meet this need. The mdx platform's virtualization\nservice, jointly operated by 9 national universities and 2 national research\ninstitutes in Japan, launched in 2021, and more features are in development.\nCurrently mdx is used by researchers in a wide variety of domains, including\nmaterials informatics, geo-spatial information science, life science,\nastronomical science, economics, social science, and computer science. This\npaper provides an the overview of the mdx platform, details the motivation for\nits development, reports its current status, and outlines its future plans.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Soft robotic grippers have numerous advantages that address challenges in\ndynamic aerial grasping. Typical multi-fingered soft grippers recently\nshowcased for aerial grasping are highly dependent on the direction of the\ntarget object for successful grasping. This study pushes the boundaries of\ndynamic aerial grasping by developing an omnidirectional system for autonomous\naerial manipulation. In particular, the paper investigates the design,\nfabrication, and experimental verification of a novel, highly integrated,\nmodular, sensor-rich, universal jamming gripper specifically designed for\naerial applications. Leveraging recent developments in particle jamming and\nsoft granular materials, the presented gripper produces a substantial holding\nforce while being very lightweight, energy-efficient and only requiring a low\nactivation force. We show that the holding force can be improved by up to 50%\nby adding an additive to the membrane's silicone mixture. The experiments show\nthat our lightweight gripper can develop up to 15N of holding force with an\nactivation force as low as 2.5N, even without geometric interlocking. Finally,\na pick and release task is performed under real-world conditions by mounting\nthe gripper onto a multi-copter. The developed aerial grasping system features\nmany useful properties, such as resilience and robustness to collisions and the\ninherent passive compliance which decouples the UAV from the environment.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep fake technology became a hot field of research in the last few years.\nResearchers investigate sophisticated Generative Adversarial Networks (GAN),\nautoencoders, and other approaches to establish precise and robust algorithms\nfor face swapping. Achieved results show that the deep fake unsupervised\nsynthesis task has problems in terms of the visual quality of generated data.\nThese problems usually lead to high fake detection accuracy when an expert\nanalyzes them. The first problem is that existing image-to-image approaches do\nnot consider video domain specificity and frame-by-frame processing leads to\nface jittering and other clearly visible distortions. Another problem is the\ngenerated data resolution, which is low for many existing methods due to high\ncomputational complexity. The third problem appears when the source face has\nlarger proportions (like bigger cheeks), and after replacement it becomes\nvisible on the face border. Our main goal was to develop such an approach that\ncould solve these problems and outperform existing solutions on a number of\nclue metrics. We introduce a new face swap pipeline that is based on\nFaceShifter architecture and fixes the problems stated above. With a new eye\nloss function, super-resolution block, and Gaussian-based face mask generation\nleads to improvements in quality which is confirmed during evaluation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper proposes several definitions of robust stability for logic\ndynamical systems (LDSs) with uncertain switching, including robust/uniform\nrobust set stability and asymptotical (or infinitely convergent)/finite-time\nset stability with ratio one. It is proved herein that an LDS is robustly set\nstable if and only if the destination set contains all loops (i.e., the paths\nfrom each state to itself); an LDS is uniformly robustly set stable, or\nfinite-time set stable with ratio one, if and only if all states outside the\ndestination set are unreachable from any self-reachable state; and an LDS is\nasymptotically set stable with ratio one if and only if the largest robustly\ninvariant subset (LRIS) in the destination set is reachable from any state. In\naddition, it is proved that uniform robust set stability implies robust set\nstability, and robust set stability implies asymptotical set stability with\nratio one. However, the inverse claims are not generally true. The relations\nbetween robust stability and stability under random switching are revealed,\nthat is, the asymptotical/finite-time set stability with ratio one under\nuncertain switching is equivalent to asymptotical/finite-time set stability of\nthe LDS under random switching. Furthermore, it is proved that, for uniform set\nstability and asymptotical/finite-time set stability with ratio one, the set\nstability is equivalent to the stability with respect to the LRIS in the\ndestination set. However, robust set stability does not imply robust stability\nwith respect to the LRIS in the destination set. This finding corrects a result\nin a previous study.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In 1956 W. Rudin proved that the Continuum Hypothesis (CH) implies that the\n\\v{C}ech-Stone remainder of $\\mathbb{N}$ (with the discrete topology),\n$\\beta\\mathbb{N}\\setminus\\mathbb{N}$, has $2^{\\mathfrak{c}}$ homeomorphisms. In\n1979, Shelah described a forcing extension of the universe in which every\nautohomeomorphism of $\\beta\\mathbb{N}\\setminus \\mathbb{N}$ is the restriction\nof a continuous map of $\\beta\\mathbb{N}$ into itself. Since there are only\n$\\mathfrak{c}$ such maps, the conclusion contradicts Rudin's. Rudin's result\nis, by today's standards, trivial: By the Stone Duality, autohomeomorphisms of\n$\\beta\\mathbb{N}\\setminus \\mathbb{N}$ correspond to automorphisms of the\nBoolean algebra $\\mathcal{P}(\\mathbb{N})/\\text{Fin}$. This algebra is countably\nsaturated hence CH implies that it is fully saturated. A standard\nback-and-forth argument produces a complete binary tree of height\n$\\aleph_1=\\mathfrak{c}$ whose branches are distinct automorphisms. The fact\nthat the theory of atomless Boolean algebras admits elimination of quantifiers\nis not even used in this argument.\n  On the other hand, Shelah's result is, unlike most of the 1970s memorabilia,\nstill as formidable as when it appeared. Extensions of Shelah's argument\n(nowadays facilitated by Forcing Axioms) show that this rigidity of\n$\\mathcal{P}(\\mathbb{N})/\\text{Fin}$ is shared by other similar quotient\nstructures, and that is what this survey is about.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Out-of-distribution detection is an important capability that has long eluded\nvanilla neural networks. Deep Neural networks (DNNs) tend to generate\nover-confident predictions when presented with inputs that are significantly\nout-of-distribution (OOD). This can be dangerous when employing machine\nlearning systems in the wild as detecting attacks can thus be difficult. Recent\nadvances inference-time out-of-distribution detection help mitigate some of\nthese problems. However, existing methods can be restrictive as they are often\ncomputationally expensive. Additionally, these methods require training of a\ndownstream detector model which learns to detect OOD inputs from\nin-distribution ones. This, therefore, adds latency during inference. Here, we\noffer an information theoretic perspective on why neural networks are\ninherently incapable of OOD detection. We attempt to mitigate these flaws by\nconverting a trained model into a an OOD detector using a handful of steps of\ngradient descent. Our work can be employed as a post-processing method whereby\nan inference-time ML system can convert a trained model into an OOD detector.\nExperimentally, we show how our method consistently outperforms the\nstate-of-the-art in detection accuracy on popular image datasets while also\nreducing computational complexity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Ram pressure stripping (RPS) is known to be a key environmental effect that\ncan remove interstellar gas from galaxies in a cluster. The RPS process is\ncommonly described as a competition between the ram pressure by the\nintracluster medium (ICM) and the anchoring pressure on the interstellar medium\n(ISM) by the gravitational potential of a galaxy. However, the actual gas\nstripping process can be more complicated due to the complexity of gas physics\nsuch as compression and geometrical self-shielding as well as cooling and\nheating. In order to verify how well the observed signatures of the RPS process\ncan be understood as simple momentum transfer, we compare the stripping radii\nof Virgo cluster galaxies in different stages of RPS measured from the HI\nobservation with the predicted gas truncation radii for the given conditions.\nFor the sample undergoing active RPS, we generally find good agreements between\npredictions and observations within a measurement uncertainty. On the other\nhand, galaxies likely in the early or later RPS stage and/or the ones with\nsigns of environmental impacts other than RPS such as tidal interaction or\nstarvation, show some discrepancies. Our results imply that the conventional\nRPS relation works reasonably well in a broad sense when RPS is the most\ndominant process and the galaxy is located where the surrounding environment\ncan be well defined. Otherwise, more careful inspections on the second\nmechanism and local environment are required to assess the impact of RPS on the\ntarget.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The standard cold dark matter plus cosmological constant model predicts that\ngalaxies form within dark-matter haloes, and that low-mass galaxies are more\ndark-matter dominated than massive ones. The unexpected discovery of two\nlow-mass galaxies lacking dark matter immediately provoked concerns about the\nstandard cosmology and ignited explorations of alternatives, including\nself-interacting dark matter and modified gravity. Apprehension grew after\nseveral cosmological simulations using the conventional model failed to form\nadequate numerical analogues with comparable internal characteristics (stellar\nmasses, sizes, velocity dispersions and morphologies). Here we show that the\nstandard paradigm naturally produces galaxies lacking dark matter with internal\ncharacteristics in agreement with observations. Using a state-of-the-art\ncosmological simulation and a meticulous galaxy-identification technique, we\nfind that extreme close encounters with massive neighbours can be responsible\nfor this. We predict that approximately 30 percent of massive central galaxies\n(with at least 1e11 solar masses in stars) harbour at least one\ndark-matter-deficient satellite (with 1e8 - 1e9 solar masses in stars). This\ndistinctive class of galaxies provides an additional layer in our understanding\nof the role of interactions in shaping galactic properties. Future observations\nsurveying galaxies in the aforementioned regime will provide a crucial test of\nthis scenario.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The fitting problem for conjunctive queries (CQs) is the problem to construct\na CQ that fits a given set of labeled data examples. When a fitting CQ exists,\nit is in general not unique. This leads us to proposing natural refinements of\nthe notion of a fitting CQ, such as most-general fitting CQ, most-specific\nfitting CQ, and unique fitting CQ. We give structural characterizations of\nthese notions in terms of (suitable refinements of) homomorphism dualities,\nfrontiers, and direct products, which enable the construction of the refined\nfitting CQs when they exist. We also pinpoint the complexity of the associated\nexistence and verification problems, and determine the size of fitting CQs. We\nstudy the same problems for UCQs and for the more restricted class of tree CQs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Bubbles dispersed in liquids are widely present in many natural and\nindustrial processes, and play a key role in mediating mass transfer during\ntheir lifetime from formation to rising to bursting. In particular,\nnano/micro-sized particulates and organisms, present in the bulk water can be\nhighly enriched in the jet drops ejected during bubble bursting, impacting\nglobal climate and public health. However, the detailed mechanism of this\nenrichment remains obscure, with the enrichment factor being difficult to\npredict. Here, we experimentally investigate the enrichment of nano/micro-sized\nparticles in bubble bursting jet drops and highlight the underlying\nhydrodynamic mechanism, combining the effects of bubble scavenge and bursting\non the transport of particles. Scaling laws for the enrichment factor are\nsubsequently proposed that describe both our and prior experimental results\nreasonably well. Our study may provide new insights for water-to-air transfer\nof microbes related to bubble bursting.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We report the high temperature dielectric and {\\it ac} impedance spectroscopy\ninvestigation of Nb substituted LaCo$_{0.7}$Nb$_{0.3}$O$_3$ polycrystalline\nsample. The maximum dielectric constant value was observed $\\approx$1400 at\naround 400~K where the peak value shows a decreasing trend at higher\ntemperatures and frequency. Similar variation was reflected in the dielectric\nloss (tan$\\delta$) behavior with temperature, which shows the thermal\nactivation of the charge carriers in the material. The analysis of high\ntemperature impedance spectroscopy data shows the grain and grain boundary\ncontributions by fitting the Nyquist plots to the equivalent circuit. From the\nanalysis of the impedance and modulus spectra, it was possible to discern\nbetween the effects of overlapping grains, grain boundaries, and electrode\ninterfaces. The relaxation time decreases with an increase in the temperature\nand the activation energy changes from 0.44~eV to 0.56~eV at around 400~K,\nwhich is due to involvement of thermal activation in the conduction of charge\ncarriers. The conductivity is found to be increased with temperature for a\ngiven frequency, which again shows the semiconducting behavior. Whereas the\nconductivity increases with increase in frequency at lower temperatures. Also,\nthe conductivity almost saturates with frequency at high temperatures.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The nature of the radio-wave radiation generated by particle cascades in both\nthe Earth's atmosphere and dense media such as ice has, historically, been much\ndebated. This situation changed in the early 2010's, with the community\nconverging on the common terminology of \"geomagnetic\" and \"Askaryan\" radiation\nto describe the two emission mechanisms. However, this convergence arose from\ndiscussions at various conferences and workshops, and was ultimately reached\nthrough agreement between simulation codes and experimental measurements. In\nthis article therefore, I use relatively simple geometrical arguments, and a\nminimum of calculations based on single particle tracks, to explain the nature\nof radiation from extensive air showers (EAS) and cascades in dense media such\nas ice. I identify well-determined frequency regimes where the radiation from\nthe Askaryan effect will be bremsstrahlung-like and Cherenkov-like, being\nrespectively below/above 1 GHz in EAS and 100 MHz in dense media; and where\ngeomagnetic emission will be transverse-current-like and where it will resemble\nsynchrotron radiation, respectively below/above a few GHz in EAS, depending on\nthe height of cascade development. I suggest how these transitions in the\nnature of the emission may be experimentally observed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the framework of the development of the SWGO experiment we have performed\na detailed study of the single unit of an extensive air shower observatory\nbased on an array of water Cherenkov detectors. Indeed, one of the possible\nwater Cherenkov detector unit configurations for SWGO consists of tanks, and to\nreach a high detection efficiency and discrimination capability between\ngamma-ray and hadronic air showers, different tank designs are under\ninvestigation. In this study, we considered double-layer tanks with several\nsizes, shapes and number of photo-multiplier tubes (PMTs). Muons, electrons,\nand gamma-rays with energies typical of secondary particles in extensive air\nshowers have been simulated entering the tanks with zenith angles from 0 to 60\ndegrees. The tank response was evaluated considering the number of\nphotoelectrons produced by the PMTs, the detection efficiency, and the time\nresolution of the measurement of the first photon. This analysis allowed to\ncompare the performance of tanks with different size, configuration of PMTs,\nand with circular, hexagonal and square geometry. The method used and the\nresults will be discussed in this paper.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Department of Electrical Engineering and Information Systems, The University\nof Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan Center for Spintronics\nResearch Network (CSRN), The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo\n113-8656, Japan\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  By using Gromov's $\\mu$-bubble technique, we show that the $3$-dimensional\nspherical caps are rigid under perturbations that do not reduce the metric, the\nscalar curvature, and the mean curvature along its boundary. Several\ngeneralizations of this result will be discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper is devoted to the representation theory of quantum coordinate\nalgebra $\\mathbb{C}_q[G]$, for a semisimple Lie group $G$ and a generic\nparameter $q$. By inspecting the actions of normal elements on tensor modules,\nwe generalize a result of Levendorski and Soibelman in [22] for highest weight\nmodules. For a double Bruhat cell $G^{w_1,w_2}$, we describe the primitive\nspectra $\\mathrm{prim}\\,\\mathbb{C}_q[G]_{w_1,w_2}$ in a new fashion, and\nconstruct a bundle of $(w_1,w_2)$ type simple modules onto\n$\\mathrm{prim}\\,\\mathbb{C}_q[G]_{w_1,w_2}$, provided\n$\\mathrm{Supp}(w_1)\\cap\\mathrm{Supp}(w_2)=\\varnothing$ or enough pivot\nelements. The fibers of the bundle are shown to be products of the spectrums of\nsimple modules of 2-dimensional quantum torus $L_q(2)$. As an application of\nour theory, we deduce an equivalent condition for the tensor module to be\nsimple, and construct some simple modules for each primitive ideal when\n$G=SL_3(\\mathbb{C})$. This completes the Dixmier's program for\n$\\mathbb{C}_q[SL_3]$. The wiring diagrams, introduced by Fomin and Zelevinsky\nin their study of total positivity (cf. [3,9]), is the main tool to compute the\naction of generalized quantum minors on tensor modules in the type A case. We\nobtain a quantum version of Lindstr\\\"{o}m's lemma, which plays an important\nrole in transforming representation problems into combinatorial ones of wiring\ndiagrams.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The purpose of this paper is to introduce the construction of a stochastic\nprocess called ``$\\delta$-dimensional Bessel house-moving\" and its properties.\n$\\delta$-dimensional Bessel house-moving is a $\\delta$-dimensional Bessel\nprocess hitting a fixed point at $t=1$ for the first time. We show that this\nprocess can be obtained as the weak limit of $\\delta$-dimensional Bessel\nbridges conditioned from above. Applying this weak convergence, we give the\ndecomposition formula for its distribution and the Radon-Nikodym density for\nthe distribution of the Bessel house-moving with respect to the one of the\nBessel process. We also study sample path properties of the Bessel\nhouse-moving.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  To train image-caption retrieval (ICR) methods, contrastive loss functions\nare a common choice for optimization functions. Unfortunately, contrastive ICR\nmethods are vulnerable to learning shortcuts: decision rules that perform well\non the training data but fail to transfer to other testing conditions. We\nintroduce an approach to reduce shortcut feature representations for the ICR\ntask: latent target decoding (LTD). We add an additional decoder to the\nlearning framework to reconstruct the input caption, which prevents the image\nand caption encoder from learning shortcut features. Instead of reconstructing\ninput captions in the input space, we decode the semantics of the caption in a\nlatent space. We implement the LTD objective as an optimization constraint, to\nensure that the reconstruction loss is below a threshold value while primarily\noptimizing for the contrastive loss. Importantly, LTD does not depend on\nadditional training data or expensive (hard) negative mining strategies. Our\nexperiments show that, unlike reconstructing the input caption, LTD reduces\nshortcut learning and improves generalizability by obtaining higher recall@k\nand r-precision scores. Additionally, we show that the evaluation scores\nbenefit from implementing LTD as an optimization constraint instead of a dual\nloss.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The layered ruthenate family of materials possess an intricate interplay of\nstructural, electronic and magnetic degrees of freedom that yields a plethora\nof delicately balanced ground states. This is exemplified by\nCa$_{3}$Ru$_{2}$O$_{7}$, which hosts a complex transition at which the lattice\nparameters jump, the Fermi surface partially gaps and the spins undergo a\n$90^{\\circ}$ in-plane reorientation. Here, we show how the transition is driven\nby a lattice strain that tunes the electronic bandwidth. We apply uniaxial\nstress to single crystals of Ca$_{3}$Ru$_{2}$O$_{7}$, using neutron and\nresonant x-ray scattering to simultaneously probe the structural and magnetic\nresponses. These measurements demonstrate that the transition can be driven by\nexternally induced strain, stimulating the development of a theoretical model\nin which an internal strain is generated self-consistently to lower the\nelectronic energy. We understand the strain to act by modifying tilts and\nrotations of the RuO$_{6}$ octahedra, which directly influences the\nnearest-neighbour hopping. Our results offer a blueprint for uncovering the\ndriving force behind coupled phase transitions, as well as a novel route to\ncontrolling them.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper presents a comprehensive review of the design of experiments used\nin the surrogate models. In particular, this study demonstrates the necessity\nof the design of experiment schemes for the Physics-Informed Neural Network\n(PINN), which belongs to the supervised learning class. Many complex partial\ndifferential equations (PDEs) do not have any analytical solution; only\nnumerical methods are used to solve the equations, which is computationally\nexpensive. In recent decades, PINN has gained popularity as a replacement for\nnumerical methods to reduce the computational budget. PINN uses physical\ninformation in the form of differential equations to enhance the performance of\nthe neural networks. Though it works efficiently, the choice of the design of\nexperiment scheme is important as the accuracy of the predicted responses using\nPINN depends on the training data. In this study, five different PDEs are used\nfor numerical purposes, i.e., viscous Burger's equation, Shr\\\"{o}dinger\nequation, heat equation, Allen-Cahn equation, and Korteweg-de Vries equation. A\ncomparative study is performed to establish the necessity of the selection of a\nDoE scheme. It is seen that the Hammersley sampling-based PINN performs better\nthan other DoE sample strategies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The application of quantum Langevin equations for the study of\nnon-equilibrium relaxations is illustrated in the exactly solved quantum\nspherical model. Tutorial sections on the physical background of non-markovian\nquantum noise, the spherical model quantum phase transition, the long-time\nlimit of the quantum Langevin equation of the spherical model and physical\nageing are followed by a brief review of the solution of the non-markovian\ntime-dependent spherical constraint and about the consequences for quantum\nageing at zero temperature, after a quantum quench.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Anisotropic X-ray Dark-field Tomography (AXDT) is a recently developed\nimaging modality that enables the visualization of oriented microstructures\nusing lab-based X-ray grating interferometer setups. While there are very\npromising application scenarios, for example in materials testing of fibrous\ncomposites or in medical diagnosis of brain cell connectivity, AXDT faces\nchallenges in practical applicability due to the complex and time-intensive\nacquisitions required to fully sample the anisotropic X-ray scattering\nfunctions. However, depending on the specific imaging task at hand, a full\nsampling may not be required, allowing for reduced acquisitions. In this work\nwe are investigating a performance prediction approach for AXDT using\ntask-specific detectability indices. Based on this approach we present a\ntask-driven acquisition optimization method that enables reduced acquisition\nschemes while keeping the task-specific image quality high. We demonstrate the\nfeasibility and efficacy of the method in experiments with simulated and\nexperimental data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Human mobility prediction is a fundamental task essential for various\napplications, including urban planning, transportation services, and location\nrecommendation. Existing approaches often ignore activity information crucial\nfor reasoning human preferences and routines, or adopt a simplified\nrepresentation of the dependencies between time, activities and locations. To\naddress these issues, we present Hierarchical Graph Attention Recurrent Network\n(HGARN) for human mobility prediction. Specifically, we construct a\nhierarchical graph based on all users' history mobility records and employ a\nHierarchical Graph Attention Module to capture complex time-activity-location\ndependencies. This way, HGARN can learn representations with rich contextual\nsemantics to model user preferences at the global level. We also propose a\nmodel-agnostic history-enhanced confidence (MaHec) label to focus our model on\neach user's individual-level preferences. Finally, we introduce a Recurrent\nEncoder-Decoder Module, which employs recurrent structures to jointly predict\nusers' next activities (as an auxiliary task) and locations. For model\nevaluation, we test the performances of our Hgarn against existing SOTAs in\nrecurring and explorative settings. The recurring setting focuses more on\nassessing models' capabilities to capture users' individual-level preferences.\nIn contrast, the results in the explorative setting tend to reflect the power\nof different models to learn users' global-level preferences. Overall, our\nmodel outperforms other baselines significantly in the main, recurring, and\nexplorative settings based on two real-world human mobility data benchmarks.\nSource codes of HGARN are available at https://github.com/YihongT/HGARN.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a training method for spontaneous speech synthesis models that\nguarantees the consistency of linguistic parts of synthesized speech.\nPersonalized spontaneous speech synthesis aims to reproduce the individuality\nof disfluency, such as filled pauses. Our prior model includes a filled-pause\nprediction model and synthesizes filled-pause-included speech from text without\nfilled pauses. However, inserting the filled pauses degrades the quality of the\nlinguistic parts of the synthesized speech. This might be because filled-pause\ninsertion tendencies differ between training and inference, and the synthesis\nmodel cannot represent connections between filled pauses and surrounding\nphonemes in inference. We, therefore, developed a linguistic-speech consistency\ntraining that guarantees the consistency of linguistic parts of synthetic\nspeech with and without filled pauses. The proposed consistency training\nutilizes not only ground-truth-filled pauses but also pseudo ones. Our\nexperiments demonstrate that this method improves the naturalness of the\nsynthetic linguistic speech and the entire predicted-filled-pause-included\nsynthetic speech.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The availability of electronic health records (EHR) has opened opportunities\nto supplement increasingly expensive and difficult to carry out randomized\ncontrolled trials (RCT) with evidence from readily available real world data.\nIn this paper, we use EHR data to construct synthetic control arms for\ntreatment-only single arm trials. We propose a novel nonparametric Bayesian\ncommon atoms mixture model that allows us to find equivalent population strata\nin the EHR and the treatment arm and then resample the EHR data to create\nequivalent patient populations under both the single arm trial and the\nresampled EHR. Resampling is implemented via a density-free importance sampling\nscheme. Using the synthetic control arm, inference for the treatment effect can\nthen be carried out using any method available for RCTs. Alternatively the\nproposed nonparametric Bayesian model allows straightforward model-based\ninference. In simulation experiments, the proposed method vastly outperforms\nalternative methods. We apply the method to supplement single arm\ntreatment-only glioblastoma studies with a synthetic control arm based on\nhistorical trials.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we propose recurrence relations for the dipole densities in\nQCD, which allows us to find these densities from the solution to the BFKL\nequation. We resolve these relations in the diffusion approximation for the\nBFKL kernel. Based on this solution, we found the sum of large Pomeron loops.\nThis sum generates the scattering amplitude that decreases at large values of\nrapidity $Y$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Topological invariance is a powerful concept in different branches of physics\nas they are particularly robust under perturbations. We generalize the ideas of\ncomputing the statistics of winding numbers for a specific parametric model of\nthe chiral Gaussian Unitary Ensemble to other chiral random matrix ensembles.\nEspecially, we address the two chiral symmetry classes, unitary (AIII) and\nsymplectic (CII), and we analytically compute ensemble averages for ratios of\ndeterminants with parametric dependence. To this end, we employ a technique\nthat exhibits reminiscent supersymmetric structures while we never carry out\nany map to superspace.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many systems in nature are conjectured to exist at a thermal critical point.\nExamples include the brain and earthquake faults. The primary reason for this\nconjecture is that the distribution of clusters (avalanches of firing neurons\nin the brain or regions of slip in earthquake faults) can be described by a\npower law. We discuss other criteria that the cluster critical exponents must\nsatisfy to conclude that the observed power law behavior indicates an\nunderlying thermal critical point rather than some other mechanism. We\nhighlight a possible misinterpretation of the cluster scaling data that can\nlead to incorrectly conclude that that the measured critical exponents do not\nsatisfy these criteria. Examples of the possible misinterpretation of the data\nfor one-dimensional (1D) random site percolation, and the 1D Ising and 1D\nOlami-Feder-Christensen models are presented. We stress that the interpretation\nof the significance of a power law cluster distribution is subtle and its\nmisinterpretation might lead to the abandonment of a promising area of\nresearch.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A numerical detection of the radius-dependent spin transition of dark matter\nhalos is reported. Analyzing the data from the IllustrisTNG simulations, we\nmeasure the halo spin vectors at several inner radii within the virial\nboundaries and investigate their orientations in the principal frames of the\ntidal and velocity shear fields, called the Tweb and Vweb, respectively. The\nhalo spin vectors in the high-mass section exhibit a transition from the Tweb\nintermediate to major principal axes as they are measured at more inner radii,\nwhich holds for both of the dark matter and baryonic components. The radius\nthreshold at which the transition occurs depends on the smoothing scale,\n$R_{f}$, becoming larger as $R_{f}$ decreases. For the case of the Vweb, the\noccurrence of the radius-dependent spin transition is witnessed only when\n$R_{f}\\ge 1\\, h^{-1}$Mpc. Repeating the same analysis but with the vorticity\nvectors, we reveal a critical difference from the spins. The vorticity vectors\nare always perpendicular to the Tweb (Vweb) major principal axes, regardless of\n$R_{f}$, which indicates that the halo inner spins are not strongly affected by\nthe generation of vorticity. It is also shown that the halo spins, as well as\nthe Tweb (Vweb) principal axes, have more directional coherence over a wide\nrange of radial distances in the regions where the vorticity vectors have\nhigher magnitudes. The physical interpretations and implications of our results\nare discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a different point of view on the well-known connection between\nHopf--Galois structures and skew braces, building on a recent paper of A. Koch\nand P. J. Truman.\n  We show that the known results which involve this connection easily carry\nover to this new perspective, and that new ones naturally appear.\n  As an application, we present new insights on the study of the surjectivity\nof the Hopf--Galois correspondence, explaining in more detail the role of\nbi-skew braces in Hopf--Galois theory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Data privacy is an essential issue in publishing data visualizations.\nHowever, it is challenging to represent multiple data patterns in\nprivacy-preserving visualizations. The prior approaches target specific chart\ntypes or perform an anonymization model uniformly without considering the\nimportance of data patterns in visualizations. In this paper, we propose a\nvisual analytics approach that facilitates data custodians to generate multiple\nprivate charts while maintaining user-preferred patterns. To this end, we\nintroduce pattern constraints to model users' preferences over data patterns in\nthe dataset and incorporate them into the proposed Bayesian network-based\nDifferential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is\ndeveloped to assist data custodians in implementing our approach. The\neffectiveness of our approach is demonstrated with quantitative evaluation of\npattern utility under the different levels of privacy protection, case studies,\nand semi-structured expert interviews.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This study compares sequential image classification methods based on\nrecurrent neural networks. We describe methods based on recurrent neural\nnetworks such as Long-Short-Term memory(LSTM), bidirectional Long-Short-Term\nmemory(BiLSTM) architectures, etc. We also review the state-of-the-art\nsequential image classification architectures. We mainly focus on LSTM, BiLSTM,\ntemporal convolution network, and independent recurrent neural network\narchitecture in the study. It is known that RNN lacks in learning long-term\ndependencies in the input sequence. We use a simple feature construction method\nusing orthogonal Ramanujan periodic transform on the input sequence.\nExperiments demonstrate that if these features are given to LSTM or BiLSTM\nnetworks, the performance increases drastically.\n  Our focus in this study is to increase the training accuracy simultaneously\nreducing the training time for the LSTM and BiLSTM architecture, but not on\npushing the state-of-the-art results, so we use simple LSTM/BiLSTM\narchitecture. We compare sequential input with the constructed feature as input\nto single layer LSTM and BiLSTM network for MNIST and CIFAR datasets. We\nobserve that sequential input to the LSTM network with 128 hidden unit training\nfor five epochs results in training accuracy of 33% whereas constructed\nfeatures as input to the same LSTM network results in training accuracy of 90%\nwith 1/3 lesser time.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We explain the current situation of the relationship between the\nKashiwara-Vergne Lie algebra $\\mathfrak{krv}$ and the double shuffle Lie\nalgebra $\\mathfrak{dmr}$. We also show the validity of Ecalle's senary relation\nfor small depths.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper addresses a continuous-time continuous-space chance-constrained\nstochastic optimal control (SOC) problem via a Hamilton-Jacobi-Bellman (HJB)\npartial differential equation (PDE). Through Lagrangian relaxation, we convert\nthe chance-constrained (risk-constrained) SOC problem to a risk-minimizing SOC\nproblem, the cost function of which possesses the time-additive Bellman\nstructure. We show that the risk-minimizing control synthesis is equivalent to\nsolving an HJB PDE whose boundary condition can be tuned appropriately to\nachieve a desired level of safety. Furthermore, it is shown that the proposed\nrisk-minimizing control problem can be viewed as a generalization of the\nproblem of estimating the risk associated with a given control policy. Two\nnumerical techniques are explored, namely the path integral and the finite\ndifference method (FDM), to solve a class of risk-minimizing SOC problems whose\nassociated HJB equation is linearizable via the Cole-Hopf transformation. Using\na 2D robot navigation example, we validate the proposed control synthesis\nframework and compare the solutions obtained using path integral and FDM.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Spontaneous Symmetry breaking in Quantum Finance considers the martingale\ncondition in the stock market as a vacuum state if we express the financial\nequations in the Hamiltonian form. The original analysis for this phenomena\nignores completely the kinetic terms in the neighborhood of the minimal of the\npotential terms. This is correct in most of the cases. However, when we deal\nwith the Martingale condition, it comes out that the kinetic terms can also\nbehave as potential terms and then reproduce a shift on the effective location\nof the vacuum (Martingale). In this paper we analyze the effective symmetry\nbreaking patterns and the connected vacuum degeneracy for these special\ncircumstances. Within the same scenario, we analyze the connection between the\nflow of information and the multiplicity of martingale states, providing in\nthis way powerful tools for analyzing the dynamic of the stock market.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Human physical motion activity identification has many potential applications\nin various fields, such as medical diagnosis, military sensing, sports\nanalysis, and human-computer security interaction. With the recent advances in\nsmartphones and wearable technologies, it has become common for such devices to\nhave embedded motion sensors that are able to sense even small body movements.\nThis study collected human activity data from 60 participants across two\ndifferent days for a total of six activities recorded by gyroscope and\naccelerometer sensors in a modern smartphone. The paper investigates to what\nextent different activities can be identified by utilising machine learning\nalgorithms using approaches such as majority algorithmic voting. More analyses\nare also provided that reveal which time and frequency domain based features\nwere best able to identify individuals motion activity types. Overall, the\nproposed approach achieved a classification accuracy of 98 percent in\nidentifying four different activities: walking, walking upstairs, walking\ndownstairs, and sitting while the subject is calm and doing a typical\ndesk-based activity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Graph neural networks (GNNs) have been extensively developed for graph\nrepresentation learning in various application domains. However, similar to all\nother neural networks models, GNNs suffer from the black-box problem as people\ncannot understand the mechanism underlying them. To solve this problem, several\nGNN explainability methods have been proposed to explain the decisions made by\nGNNs. In this survey, we give an overview of the state-of-the-art GNN\nexplainability methods and how they are evaluated. Furthermore, we propose a\nnew evaluation metric and conduct thorough experiments to compare GNN\nexplainability methods on real world datasets. We also suggest future\ndirections for GNN explainability.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a formalism based on nonequilibrium dynamical mean field theory\n(DMFT) which allows to compute the time-resolved X-ray absorption spectrum\n(XAS) of photo-excited solids. By applying this formalism to the photo-doped\nhalf-filled and quarter-filled two-orbital Hubbard models in the Mott\ninsulating regime we clarify how the time-resolved XAS signal reflects the\nnonequilibrium population of different local states. Apart from the missing\nbroadening associated with continuum excitations, the atomic XAS spectrum\ncomputed with the nonthermal state populations provides a good approximation to\nthe full nonequilibrium DMFT result. This suggest a route to combine the\naccurate DMFT description of nonequilibrum states of solids with cluster\ncalculations of the XAS signal.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We provide sufficient conditions for systems of polynomial equations over\ngeneral (real or complex) algebras to have a solution. This generalizes known\nresults on quaternions, octonions and matrix algebras. We also generalize the\nfundamental theorem of algebra for quaternions to polynomials with two\nmonomials in the leading form, while showing that it fails for three.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper is concerned with noisy matrix completion--the problem of\nrecovering a low-rank matrix from partial and noisy entries. Under uniform\nsampling and incoherence assumptions, we prove that a tuning-free square-root\nmatrix completion estimator (square-root MC) achieves optimal statistical\nperformance for solving the noisy matrix completion problem. Similar to the\nsquare-root Lasso estimator in high-dimensional linear regression, square-root\nMC does not rely on the knowledge of the size of the noise. While solving\nsquare-root MC is a convex program, our statistical analysis of square-root MC\nhinges on its intimate connections to a nonconvex rank-constrained estimator.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present analysis of the proper-motion (PM) field of the red clump stars in\nthe Large Magellanic Cloud (LMC) disk using the Gaia Early Data Release 3\ncatalog. Using a kinematic model based on old stars with 3D velocity\nmeasurements, we construct the residual PM field by subtracting the\ncenter-of-mass motion and internal rotation motion components. The residual PM\nfield reveals asymmetric patterns, including larger residual PMs in the\nsouthern disk. Comparisons between the observed residual PM field with those of\nfive numerical simulations of an LMC analog that is subject to the tidal fields\nof the Milky Way and the Small Magellanic Cloud (SMC) show that the present-day\nLMC is not in dynamical equilibrium. We find that both the observed level of\ndisk heating (PM residual root-mean-square of 0.057$\\pm$0.002 mas yr$^{-1}$)\nand kinematic asymmetry are not reproduced by Milky Way tides or if the SMC\nimpact parameter is larger than the size of the LMC disk. This measured level\nof disk heating provides a novel and important method to validate numerical\nsimulations of the LMC-SMC interaction history. Our results alone put\nconstraints on an impact parameter $\\lesssim$10 kpc and impact timing $<$250\nMyr. When adopting the impact timing constraint of $\\sim$140--160 Myr ago from\nprevious studies, our results suggest that the most recent SMC encounter must\nhave occurred with an impact parameter of $\\sim$5 kpc. We also find consistent\nradial trends in the kinematically- and geometrically-derived disk inclination\nand line-of-node position angles, indicating a common origin.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using the Bragg-Williams construction of an off-shell free energy we compute\nthe topological charge of the Hawking-Page transition point for black holes in\nAdS. A computation following from a related off-shell effective potential in\nthe boundary gauge dual matches the value of topological charge obtained in the\nbulk. We also compute the topological charges of the equilibrium phases of\nthese systems, which follow from the saddle points of the appropriate free\nenergy. The locally stable and unstable phases turn out to have topological\ncharges opposite to each other, with the total being zero, in agreement with\nthe result obtained from a related construction [arXiv:2208.01932].\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We numerically benchmark methods for computing harmonic maps into the unit\nsphere, with particular focus on harmonic maps with singularities. For the\ndiscretization we compare two different approaches, both based on Lagrange\nfinite elements. While the first method enforces the unit-length constraint\nonly at the Lagrange nodes, the other one adds a pointwise projection to\nfulfill the constraint everywhere. For the solution of the resulting algebraic\nproblems we compare a nonconforming gradient flow with a Riemannian\ntrust-region method. Both are energy-decreasing and can be shown to converge\nglobally to a stationary point of the Dirichlet energy. We observe that while\nthe nonconforming and the conforming discretizations both show similar\nbehavior, the second-order trust-region method needs less iterations than the\nsolver based on gradient flow.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a novel de-homogenization approach for efficient design of\nhigh-resolution load-bearing structures. The proposed approach builds upon a\nstreamline-based parametrization of the design domain, using a set of\nspace-filling and evenly-spaced streamlines in the two mutually orthogonal\ndirection fields that are obtained from homogenization-based topology\noptimization. Streamlines in these fields are converted into a graph, which is\nthen used to construct a quad-dominant mesh whose edges follow the direction\nfields. In addition, the edge width is adjusted according to the density and\nanisotropy of the optimized orthotropic cells. In a number of numerical\nexamples, we demonstrate the mechanical performance and regular appearance of\nthe resulting structural designs, and compare them with those from classic and\ncontemporary approaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The associahedron $\\mathcal{A}(G)$ of a graph $G$ has the property that its\nvertices can be thought of as the search trees on $G$ and its edges as the\nrotations between two search trees. If $G$ is a simple path, then\n$\\mathcal{A}(G)$ is the usual associahedron and the search trees on $G$ are\nbinary search trees. Computing distances in the graph of $\\mathcal{A}(G)$ (or\nequivalently, the rotation distance between two binary search trees) is a major\nopen problem. Here, we consider the different case when $G$ is a complete split\ngraph. In that case, $\\mathcal{A}(G)$ interpolates between the stellohedron and\nthe permutohedron, and all the search trees on $G$ are brooms. We show that the\nrotation distance between any two such brooms and therefore the distance\nbetween any two vertices in the graph of the associahedron of $G$ can be\ncomputed in quasi-quadratic time in the number of vertices of $G$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Twin boundaries (TBs) are assumed to be obstacles to dislocation motion and\nincrease the strength of metals. Here, we report the abnormal phenomenon that\nTBs reduce the strength of body-centered cubic (BCC) tungsten (W).\n[1-11]-oriented W nanowires with (121) twin planes and free of dislocations\nwere fabricated by chemical vapor deposition. In situ tensile tests within the\ntransmission electron microscope were performed on single-crystal and twinned W\nnanowires. The fracture strength of the twinned W nanowire was 13.7 GPa, 16%\nlower than that of the single-crystal W nanowire (16.3 GPa). The weakening\nmechanism of the TB was revealed by a combination of atomic-resolution\ncharacterizations and atomistic simulations. Twinned W nanowires failed by the\nearly nucleation of a crack at the intersection of the TB with the surface. The\nstandard strengthening mechanism by dislocation/TB interaction was not\noperative in W because the high Peierls barrier and stacking fault energy in W\nhinder dislocation nucleation and glide. These findings provide a new insight\ninto the influence of TBs on the mechanical properties of BCC metals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Registration of pre-operative and follow-up brain MRI scans is challenging\ndue to the large variation of tissue appearance and missing correspondences in\ntumour recurrence regions caused by tumour mass effect. Although recent deep\nlearning-based deformable registration methods have achieved remarkable success\nin various medical applications, most of them are not capable of registering\nimages with pathologies. In this paper, we propose a 3-step registration\npipeline for pre-operative and follow-up brain MRI scans that consists of 1) a\nmulti-level affine registration, 2) a conditional deep Laplacian pyramid image\nregistration network (cLapIRN) with forward-backward consistency constraint,\nand 3) a non-linear instance optimization method. We apply the method to the\nBrain Tumor Sequence Registration (BraTS-Reg) Challenge. Our method achieves\naccurate and robust registration of brain MRI scans with pathologies, which\nachieves a median absolute error of 1.64 mm and 88\\% of successful registration\nrate in the validation set of BraTS-Reg challenge. Our method ranks 1st place\nin the 2022 MICCAI BraTS-Reg challenge.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Source-free unsupervised domain adaptation (SFUDA) aims to obtain high\nperformance in the unlabeled target domain using the pre-trained source model,\nnot the source data. Existing SFUDA methods assign the same importance to all\ntarget samples, which is vulnerable to incorrect pseudo-labels. To\ndifferentiate between sample importance, in this study, we propose a novel\nsample-wise confidence score, the Joint Model-Data Structure (JMDS) score for\nSFUDA. Unlike existing confidence scores that use only one of the source or\ntarget domain knowledge, the JMDS score uses both knowledge. We then propose a\nConfidence score Weighting Adaptation using the JMDS (CoWA-JMDS) framework for\nSFUDA. CoWA-JMDS consists of the JMDS scores as sample weights and weight Mixup\nthat is our proposed variant of Mixup. Weight Mixup promotes the model make\nmore use of the target domain knowledge. The experimental results show that the\nJMDS score outperforms the existing confidence scores. Moreover, CoWA-JMDS\nachieves state-of-the-art performance on various SFUDA scenarios: closed, open,\nand partial-set scenarios.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We investigate stationary accretion of the collisionless Vlasov gas onto the\nKerr black hole, occurring in the equatorial plane. The solution is specified\nby imposing asymptotic boundary conditions: at infinity the gas obeys the\nMaxwell-J\\\"{u}ttner distribution, restricted to the equatorial plane (both in\npositions and momenta). In the vicinity of the black hole, the motion of the\ngas is governed by the spacetime geometry. We compute accretion rates of the\nrest-mass, the energy, and the angular momentum, as well as the particle number\nsurface density, focusing on the dependence of these quantities on the\nasymptotic temperature of the gas and the black hole spin. The rest-mass and\nenergy accretion rates, normalized by the black hole mass and appropriate\nasymptotic surface densities of the gas, increase with increasing asymptotic\ntemperature. The accretion slows down the rotation of the black hole.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop new tools to study landscapes in nonconvex optimization. Given one\noptimization problem, we pair it with another by smoothly parametrizing the\ndomain. This is either for practical purposes (e.g., to use smooth optimization\nalgorithms with good guarantees) or for theoretical purposes (e.g., to reveal\nthat the landscape satisfies a strict saddle property). In both cases, the\ncentral question is: how do the landscapes of the two problems relate? More\nprecisely: how do desirable points such as local minima and critical points in\none problem relate to those in the other problem?\n  A key finding in this paper is that these relations are often determined by\nthe parametrization itself, and are almost entirely independent of the cost\nfunction. Accordingly, we introduce a general framework to study\nparametrizations by their effect on landscapes. The framework enables us to\nobtain new guarantees for an array of problems, some of which were previously\ntreated on a case-by-case basis in the literature. Applications include:\noptimization over low-rank matrices and tensors by optimizing over a\nfactorization; the Burer--Monteiro approach to semidefinite programs; training\nneural networks by optimizing over their weights and biases; and quotienting\nout symmetries.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Aim. The eccentric von Zeipel-Lidov-Kozai (ZLK) effect is widely used to\nexplain dynamical phenomena in varieties of astrophysical systems. The purpose\nof this work is to make clear the dynamical essence of the eccentric ZLK effect\nby constructing an inherent connection between such an effect and dynamics of\nsecular resonance in restricted hierarchical planetary systems. Methods.\nDynamical structures of apsidal resonance are analytically studied by means of\nperturbative treatments. The resonant model is formulated by averaging the\nHamiltonian (up to octupole order) over rotating ZLK cycles, producing an\nadditional motion integral. The phase portraits under the resonant model can be\nused to analyse dynamical structures, including resonant centres, dynamical\nseparatrices and islands of libration. Results. By analysing phase portraits,\nfive branches of libration centres and eight libration zones are found in the\neccentricity--inclination space. There is an excellent agreement between\nanalytical results of libration zone and numerical distributions of resonant\norbit, indicating that the resonant model for apsidal resonances is valid and\napplicable. Additionally, it is found that, in the test-particle limit,\ndistributions of flipping orbits are dominated by those apsidal resonances\ncentred at the inclination of i = 90 deg. Conclusions. The eccentric ZLK effect\nis dynamically equivalent to the effect of apsidal resonance in restricted\nhierarchical planetary systems. The dynamical response of the eccentric ZLK\neffect (or effect of apsidal resonance) is to significantly excite\neccentricities and/or inclinations of test particles in the very long-term\nevolution.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we consider the problem on uniform estimates for generalized\noscillatory integrals given by Mittag- Leffler functions with the homogeneous\npolynomial phase. We obtain a variant of Ricci-Stein Lemma and invariant\nestimates for corresponding integrals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This thesis covers a few successes of the SM and its limitations. The\nlimitations implement that the SM must be expanded to explain the observables\nwhich can not be addressed by the SM such as mass of neutrinos, a few of\nanomalies, DM, DE, etc. In order to extend the SM, we take the model-dependent\napproach and a minimal extension to the SM. For the minimal extension, we make\nuse of vector-like family, SM-like scalar (plus a singlet flavon), and\n$U(1)^\\prime$ symmetry. A first BSM model in my first work is to explain both\nthe electron and muon anomalous magnetic moments at the same time, while\nkeeping the constraints $\\mu \\rightarrow e \\gamma$ decay and neutrino trident\nproduction. Especially, two mass sources, chirality flip and vector-like mass,\nappear in our analytic prediction for the anomalies and the chirality flip mass\nbetween $0$ and $200 \\operatorname{GeV}$ is searched and then we show no any\nvalue between them can satisfy the anomalies at the same time. A second BSM\nmodel in my second work shares the same motivation, to explain both anomalies,\nhowever it goes one step further by considering the strong hierarchical\nstructure of the SM into the second BSM model. In this BSM model, the SM\nappears as an effective theory spontaneously broken from the $U(1)^\\prime$\nsymmetry. Both anomalies are studied in the neutrino sector first and it\nreveals that the neutrino contributions are too small to explain their\nexperimental bounds, so leading to a conclusion that the neutrinos can not\nexplain both anomalies simultaneously. Next we study the scalar contribution\nand show that the scalar contributions can explain both anomalies\nsimultaneously. My third work investigate the diverse FCNCs within the same\nsecond BSM model to constrain mass range of vector-like fermions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Humans tend to mine objects by learning from a group of images or several\nframes of video since we live in a dynamic world. In the computer vision area,\nmany researches focus on co-segmentation (CoS), co-saliency detection (CoSD)\nand video salient object detection (VSOD) to discover the co-occurrent objects.\nHowever, previous approaches design different networks on these similar tasks\nseparately, and they are difficult to apply to each other, which lowers the\nupper bound of the transferability of deep learning frameworks. Besides, they\nfail to take full advantage of the cues among inter- and intra-feature within a\ngroup of images. In this paper, we introduce a unified framework to tackle\nthese issues, term as UFO (Unified Framework for Co-Object Segmentation).\nSpecifically, we first introduce a transformer block, which views the image\nfeature as a patch token and then captures their long-range dependencies\nthrough the self-attention mechanism. This can help the network to excavate the\npatch structured similarities among the relevant objects. Furthermore, we\npropose an intra-MLP learning module to produce self-mask to enhance the\nnetwork to avoid partial activation. Extensive experiments on four CoS\nbenchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks\n(Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal\nand SegV2) show that our method outperforms other state-of-the-arts on three\ndifferent tasks in both accuracy and speed by using the same network\narchitecture , which can reach 140 FPS in real-time.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $(X, \\omega, J)$ be a toric variety of dimension $2n$ determined by a\nDelzant polytope. In this paper, we first construct the polarizations\n$\\shP_{k}$ by the Hamiltonian $T^{k}$-action on $X$ (see Theorem 3.11). We will\nshow that $\\shP_{k}$ is a singular mixed polarization for $1\\le k < n$, and\n$\\shP_{n}$ is a singular real polarization which coincides with the real\npolarization studied in \\cite{BFMN} on the open dense subset of $X$. Then for\neach $1\\le k \\le n$, we will find a one-parameter family of K\\\"ahler\npolarizations $\\shP_{k,t}$ on $X$ that converges to $\\shP_{k}$ (see Theorem\n3.12). Finally, we will show that $\\shH_{k,t}^{T}$ the space of\n$T^{k}$-invariant $J_{k,t}$-holomorphic sections converges to $\\shH_{k}^{0}$\n(see Theorem 3.18).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the Zhang sandpile model in one-dimension (1D) with locally\nconservative (or dissipative) dynamics and examine its total energy\nfluctuations at the external drive time scale. The bulk-driven system leads to\nLorentzian spectra, with a cutoff time $T$ growing linearly with the system\nsize $L$. The fluctuations show $1/f^{\\alpha}$ behavior with $\\alpha \\sim 1$\nfor the boundary drive, and the cutoff time varies non-linearly. For\nconservative local dynamics, the cutoff time shows a power-law growth $T \\sim\nL^{\\lambda}$ that differs from an exponential form $ \\sim \\exp(\\mu L)$ observed\nfor the nonconservative case. We suggest that the local dissipation is not a\nnecessary ingredient of the system in 1D to get the $1/f$ noise, and the cutoff\ntime can reveal the distinct nature of the local dynamics. We also discuss the\nenergy fluctuations for locally nonconservative dynamics with random\ndissipation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Every 2-dimensional spine of an aspherical 3-manifold has the nonpositive\ntowers property, but every collapsed 2-dimensional spine of a 3-ball containing\na 2-cell has an immersed sphere.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present an estimator of the covariance matrix $\\Sigma$ of random\n$d$-dimensional vector from an i.i.d. sample of size $n$. Our sole assumption\nis that this vector satisfies a bounded $L^p-L^2$ moment assumption over its\none-dimensional marginals, for some $p\\geq 4$. Given this, we show that\n$\\Sigma$ can be estimated from the sample with the same high-probability error\nrates that the sample covariance matrix achieves in the case of Gaussian data.\nThis holds even though we allow for very general distributions that may not\nhave moments of order $>p$. Moreover, our estimator can be made to be optimally\nrobust to adversarial contamination. This result improves recent results in the\nliterature by Mendelson and Zhivotovskiy and Catoni and Giulini, and matches\nparallel work by Abdalla and Zhivotovskiy (the exact relationship with this\nlast work is described in the paper).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many bacterial species are helical in form, including the widespread pathogen\nH. pylori. Motivated by recent experiments on H. pylori showing that cell wall\nsynthesis is not uniform, we investigate the possible formation of helical cell\nshape induced by elastic heterogeneity. We show, experimentally and\ntheoretically, that helical morphogenesis can be produced by pressurizing an\nelastic cylindrical vessel with helical reinforced lines. The properties of the\npressurized helix are highly dependent on the initial helical angle of the\nreinforced region. We find that steep angles result in crooked helices with,\nsurprisingly, reduced end-to-end distance upon pressurization. This work helps\nto explain the possible mechanisms for the generation of helical cell\nmorphologies and may inspire the design of novel pressure-controlled helical\nactuators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Supersymmetric warped product dS4 solutions in D=11 supergravity are\nclassified. The Killing spinor is associated with two possible stabilizer\ngroups, SU(3) and G_2. We show that there are no solutions to the Killing\nSpinor equations in the G_2 stabilizer case. For the SU(3) stablilzer case, all\nof the conditions imposed from supersymmetry on the 4-form flux, and the\ngeometry of the internal manifold, are determined in terms of SU(3) invariant\nspinor bilinears.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  By putting two harmonic oscillator potential $x^2$ side by side with a\nseparation $2d$, two exactly solvable piecewise analytic quantum systems with a\nfree parameter $d>0$ are obtained. Due to the mirror symmetry, their\neigenvalues $E$ for the even and odd parity sectors are determined exactly as\nthe zeros of certain combinations of the confluent hypergeometric function\n${}_1F_1$ of $d$ and $E$, which are common to $V_{D}$ and $V_{S}$ but in two\ndifferent branches. The eigenfunctions are the piecewise square integrable\ncombinations of ${}_1F_1$, the so called $U$ functions. By comparing the\neigenvalues and eigenfunctions for various values of the separation $d$, vivid\npictures unfold showing the tunneling effects between the two wells.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Nonreciprocity in superconductors is attracting much interest owing to its\nfundamental importance as well as its potential applicability to engineering.\nIn this paper, we generalize the previous theories of the intrinsic\nsuperconducting diode effect (SDE) and microscopically elucidate its\nrelationship with the nonreciprocity of the transition lines under\nsupercurrent. We derive a general formula for the intrinsic SDE by using the\nphenomenological Ginzburg-Landau theory and thereby show that the SDE is\ndetermined by the relative angle between the magnetic field and an effective\nanti-symmetric spin-orbit coupling defined from the Ginzburg-Landau\ncoefficients. The obtained formula offers a convenient criterion to obtain a\nfinite SDE. We also study the SDE and the nonreciprocal phase transitions of\nthe $s$-wave and $d$-wave superconductors by using the mean-field theory. It is\nestablished that the sign reversal of the SDE accompanied by the crossover of\nthe helical superconductivity is a general feature irrespective of the system\ndetails. We study the phase transition lines in the temperature-magnetic-field\nphase diagram under the supercurrent, and clarify that the sign reversal of the\nSDE generally accompanies the crossings of the transition lines under positive\nand negative current directions. Furthermore, the superconducting phases under\nthe supercurrent even become re-entrant under moderate strength of the electric\ncurrent, implying the current-induced first-order phase transitions. Our\nfindings establish the electric current as the control parameter and the\npowerful probe to study the superconducting properties related to the\nfinite-momentum Cooper pairs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A logic has uniform interpolation if its formulas can be projected down to\ngiven subsignatures, preserving all logical consequences that do not mention\nthe removed symbols; the weaker property of (Craig) interpolation allows the\nprojected formula -- the interpolant -- to be different for each logical\nconsequence of the original formula. These properties are of importance, e.g.,\nin the modularization of logical theories. We study interpolation in the\ncontext of coalgebraic modal logics, i.e. modal logics axiomatized in rank 1,\nrestricting for clarity to the case with finitely many modalities. Examples of\nsuch logics include the modal logics K and KD, neighbourhood logic and its\nmonotone variant, finite-monoid-weighted logics, and coalition logic. We\nintroduce a notion of one-step (uniform) interpolation, which refers only to a\nrestricted logic without nesting of modalities, and show that a coalgebraic\nmodal logic has uniform interpolation if it has one-step interpolation.\nMoreover, we identify preservation of finite surjective weak pullbacks as a\nsufficient, and in the monotone case necessary, condition for one-step\ninterpolation. We thus prove or reprove uniform interpolation for most of the\nexamples listed above.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Nowadays, most studies on the dynamic properties of annular gaps focus only\non the force characteristics due to translational motions, while the tilt and\nmoment coefficients are less well studied. Therefore, there is hardly any\nreliable experimental data for the additional coefficients that can be used for\nvalidation purpose. To improve this, a test rig first presented by Kuhr (2022)\nis used to experimentally determine the dynamic force and moment\ncharacteristics of three annuli of different lengths. By using active magnetic\nbearings, the rotor is excited with user-defined frequencies and the rotor\nposition and the forces and moments induced by the flow field in the annulus\nare measured. To obtain accurate and reliable experimental data, extensive\npreliminary studies are carried out to determine the known characteristics of\nthe test rig rotor and the added mass and inertia imposed by the test rig.\nSubsequently, an elaborate uncertainty quantification is carried out to\nquantify the measurement uncertainties. The experimental results, i.e. the 48\nrotordynamic coefficients, are compared to a new calculation method. It is\nshown that the presented experimental data agree well with the calculation\nmethod, especially for the additional rotordynamic tilt and moment\ncoefficients. Furthermore, it is shown that the annulus length significantly\ninfluences the coefficients of the first sub-matrix. A dependence of the\nadditional coefficients on the length is recognisable, but less pronounced.\nEven for the shortest investigated annulus, i.e. L = 1, the stiffness\ncoefficients due to the forces from the angular motion of the rotor are of the\nsame order of magnitude as the stiffness coefficients due to the forces from\nthe translational motion. This supports recent results, indicating that the\nadditional coefficients become relevant much earlier than assumed throughout\nthe literature, cf. Childs (1993).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given a irreducible automorphic representation $\\Pi$ of a similitude group\n$G/\\mathbb Q$ giving rise to a KHT-Shimura variety, given a local congruence of\nthe local component of $\\Pi$ at a fixed place $p$, we justify the existence of\na global automorphic representation $\\Pi'$ with the same weight and the same\nlevel outside $p$ than $\\Pi$, such that $\\Pi$ and $\\Pi'$ are weakly congruent.\nThe arguments rest on the separation of the various contributions coming either\nfrom torsion or on the distinct families of automorphic representations, to the\nmodulo $l$ reduction of the cohomology of Harris-Taylor perverse sheaves.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Aim: There is increasing interest in the role of chronic inflammation on\npathogenesis of various disease, and one of its markers, high NLR is associated\nwith various mortality and morbidity risk. Insulin resistance (IR) might be one\npotential associate factors, as suggested in preclinical studies. However,\nepidemiological studies are scarce which investigated the association between\nNLR, and insulin resistance (IR) and they included only diabetes mellitus\npatients, not the general population. This study aims to determine if there is\na direct correlation between NLR and IR in the US general population. Methods:\nThe sample consists of 3,307 from general population, provided by National\nHealth and Nutrition Examination Survey (NHANES). Homeostasis Model Assessment\nof Insulin Resistance (HOMA-IR) value was calculated to evaluate insulin\nresistance. We investigated the relationship between their NLR and HOMA-IR\nvalues by bivariate and multivariate linear regression analyses. As insulin use\ncould results in inaccurate HOMA-IR estimation, we excluded them and ran the\nanalyses in subgroup analyses. Results: There was a relationship shown when\ninsulin users were included, having a beta coefficient value of 0.010 (95%\nconfidence interval [CI] of 0.003-0.017). However, when insulin users were\nexcluded, the beta value decreased to 0.004 (95% CI of -0.006-0.015). The\nstatistical significance was not reached when age, sex, and body mass index\nwere adjusted for in the multivariate analyses. Conclusion: There is no visible\nrelationship between IR and NLR in the general population. IR might not explain\nthe variation of NLR value in healthy people, and further studies are needed to\nreveal the associated factor of high NLR.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The van der Waals ferromagnet (FM), VI$_3$, was studied by muon spin\nrelaxation ($\\mu^+$SR) and first principle calculations based on density\nfunctional theory (DFT). Temperature dependent zero field muon spin relaxation\n($\\mu^+$SR) measurements confirm the onset of long range FM order and the time\nspectra exhibits clear muon spin precession frequencies for $T<T_{\\rm\nC}=50.03(1)$~K. The calculated internal magnetic fields at the predicted muon\nsites, based on the established magnetic structure from neutron diffraction, is\ninconsistent with the measured one. This inconsistency is because of strong\nincoherent neutron scattering and absorption originating from the elements V\nand I. Instead, a new and a more accurate magnetic structure is derived based\non a combined study using $\\mu^+$SR and DFT. These results suggest strong\ncontritions from orbital angular momentum, providing experimental evidence for\nthe existence of unquenched orbital angular momentum of V$^{3+}$ in VI$_3$.\nFinally, an unusual form of a short range ordering is present above $T_{\\rm\nC}$. Its temperature dependence is unlike previously reported cases in other\nlayered compounds and its microscopic origin is discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that if $E$ is an ample vector bundle of rank at least two with some\ncurvature bound on $O_{P(E^*)}(1)$, then $E^*\\otimes \\det E$ is Kobayashi\npositive. The proof relies on comparing the curvature of $(\\det E^*)^k$ and\n$S^kE$ for large $k$ and using duality of convex Finsler metrics. Following the\nsame thread of thought, we show if $E$ is ample with similar curvature bounds\non $O_{P(E^*)}(1)$ and $O_{P(E\\otimes \\det E^*)}(1)$, then $E$ is Kobayashi\npositive. With additional assumptions, we can furthermore show that $E^*\\otimes\n\\det E$ and $E$ are Griffiths positive.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The tensor network, as a facterization of tensors, aims at performing the\noperations that are common for normal tensors, such as addition, contraction\nand stacking. However, due to its non-unique network structure, only the tensor\nnetwork contraction is so far well defined. In this paper, we propose a\nmathematically rigorous definition for the tensor network stack approach, that\ncompress a large amount of tensor networks into a single one without changing\ntheir structures and configurations. We illustrate the main ideas with the\nmatrix product states based machine learning as an example. Our results are\ncompared with the for loop and the efficient coding method on both CPU and GPU.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Online advertising driven by auctions brings billions of dollars in revenue\nfor social networking services and e-commerce platforms. GSP auction, which is\nsimple and easy to understand for advertisers, has almost become the benchmark\nfor ad auction mechanisms in the industry. However, the allocation stability of\nGSP depends on the separable CTR assumption, which means that GSP considers\nneither position-dependent externalities nor ad-dependent externalities in\nmulti-slot scenario, leading to suboptimal performance. Some GSP-based deep\nauctions (e.g., DeepGSP, DNA) have attempted to upgrade GSP with deep neural\nnetworks, while only modeling local externalities and thus still suboptimal. On\nthe other hand, although VCG-based multi-slot auctions (e.g., VCG, WVCG) take\nexternalities into consideration, they lack an efficient balance of both\nrevenue and social welfare. In this paper, we propose a novel auction named\nNeural Multi-slot Auction (NMA) to tackle the above-mentioned challenges.\nSpecifically, we model the global externalities effectively with a\ncontext-aware list-wise prediction module to achieve better performance. We\ndesign a list-wise deep rank module to guarantee incentive compatibility in\nend-to-end learning. Furthermore, we propose an auxiliary loss for social\nwelfare to effectively reduce the decline of social welfare while maximizing\nrevenue. Experiment results on both offline large-scale datasets and online A/B\ntests demonstrate that NMA obtains higher revenue with balanced social welfare\nthan other existing auction mechanisms (i.e., GSP, DNA, WVCG) in industrial\npractice, and we have successfully deployed NMA on Meituan food delivery\nplatform.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Existing approaches to constructing training data for Natural Language\nInference (NLI) tasks, such as for semi-structured table reasoning, are either\nvia crowdsourcing or fully automatic methods. However, the former is expensive\nand time-consuming and thus limits scale, and the latter often produces naive\nexamples that may lack complex reasoning. This paper develops a realistic\nsemi-automated framework for data augmentation for tabular inference. Instead\nof manually generating a hypothesis for each table, our methodology generates\nhypothesis templates transferable to similar tables. In addition, our framework\nentails the creation of rational counterfactual tables based on human written\nlogical constraints and premise paraphrasing. For our case study, we use the\nInfoTabs, which is an entity-centric tabular inference dataset. We observed\nthat our framework could generate human-like tabular inference examples, which\ncould benefit training data augmentation, especially in the scenario with\nlimited supervision.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding object affordance can help in designing better and more robust\nrobotic grasping. Existing work in the computer vision community formulates the\nobject affordance understanding as a grasping pose generation problem, which\ntreats the problem as a black box by learning a mapping between objects and the\ndistributions of possible grasping poses for the objects. On the other hand, in\nthe robotics community, estimating object affordance represented by contact\nmaps is of the most importance as localizing the positions of the possible\naffordance can help the planning of grasping actions. In this paper, we propose\nto formulate the object affordance understanding as both contacts and grasp\nposes generation. we factorize the learning task into two sequential stages,\nrather than the black-box strategy: (1) we first reason the contact maps by\nallowing multi-modal contact generation; (2) assuming that grasping poses are\nfully constrained given contact maps, we learn a one-to-one mapping from the\ncontact maps to the grasping poses. Further, we propose a penetration-aware\npartial optimization from the intermediate contacts. It combines local and\nglobal optimization for the refinement of the partial poses of the generated\ngrasps exhibiting penetration. Extensive validations on two public datasets\nshow our method outperforms state-of-the-art methods regarding grasp generation\non various metrics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a study on planar equilibria of a terminally loaded elastic rod\nwrapped around a rigid circular capstan. Both frictionless and frictional\ncontact between the rod and the capstan are considered. We identify three cases\nof frictionless contact -- namely where the rod touches the capstan at one\npoint, along a continuous arc, and at two points. We show that, in contrast to\na fully flexible filament, an elastic rod of \\emph{finite length} wrapped\naround a capstan does not require friction to support unequal loads at its two\nends. Furthermore, we classify rod equilibria corresponding to the three\naforementioned cases in a limit where the length of the rod is much larger than\nthe radius of the capstan. In the same limit, we incorporate frictional\ninteraction between the rod and the capstan, and compute limiting equilibria of\nthe rod. Our solution to the frictional case fully generalizes the\n\\emph{classic capstan problem} to include the effects of finite thickness and\nbending elasticity of a flexible filament wrapped around a circular capstan.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Extracting phenotypes from clinical text has been shown to be useful for a\nvariety of clinical use cases such as identifying patients with rare diseases.\nHowever, reasoning with numerical values remains challenging for phenotyping in\nclinical text, for example, temperature 102F representing Fever. Current\nstate-of-the-art phenotyping models are able to detect general phenotypes, but\nperform poorly when they detect phenotypes requiring numerical reasoning. We\npresent a novel unsupervised methodology leveraging external knowledge and\ncontextualized word embeddings from ClinicalBERT for numerical reasoning in a\nvariety of phenotypic contexts. Comparing against unsupervised benchmarks, it\nshows a substantial performance improvement with absolute gains on generalized\nRecall and F1 scores up to 79% and 71%, respectively. In the supervised\nsetting, it also surpasses the performance of alternative approaches with\nabsolute gains on generalized Recall and F1 scores up to 70% and 44%,\nrespectively.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Parikh's Theorem says that the Parikh image of a context-free language is\nsemilinear. We give a short proof of Parikh's Theorem using the formulation of\nVerma, Seidl, and Schwentick in terms of Presburger arithmetic. The proof\nrelies on an Eulerian property of derivation trees of context-free languages\nand was inspired by Hierholzer's algorithm; it does not use the Chomsky normal\nform.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The technology of hyperspectral imaging (HSI) records the visual information\nupon long-range-distributed spectral wavelengths. A representative\nhyperspectral image acquisition procedure conducts a 3D-to-2D encoding by the\ncoded aperture snapshot spectral imager (CASSI), and requires a software\ndecoder for the 3D signal reconstruction. Based on this encoding procedure, two\nmajor challenges stand in the way of a high-fidelity reconstruction: (i) To\nobtain 2D measurements, CASSI dislocates multiple channels by disperser-titling\nand squeezes them onto the same spatial region, yielding an entangled data\nloss. (ii) The physical coded aperture (mask) will lead to a masked data loss\nby selectively blocking the pixel-wise light exposure. To tackle these\nchallenges, we propose a spatial-spectral (S2-) transformer architecture with a\nmask-aware learning strategy. Firstly, we simultaneously leverage spatial and\nspectral attention modelings to disentangle the blended information in the 2D\nmeasurement along both two dimensions. A series of Transformer structures\nacross spatial & spectral clues are systematically designed, which considers\nthe information inter-dependency between the two-fold cues. Secondly, the\nmasked pixels will induce higher prediction difficulty and should be treated\ndifferently from unmasked ones. Thereby, we adaptively prioritize the loss\npenalty attributing to the mask structure by inferring the difficulty-level\nupon the mask-aware prediction. Our proposed method not only sets a new\nstate-of-the-art quantitatively, but also yields a better perceptual quality\nupon structured areas.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Touch is the primary way that users interact with smartphones. However,\nbuilding mobile user interfaces where touch interactions work well for all\nusers is a difficult problem, because users have different abilities and\npreferences. We propose a system, Reflow, which automatically applies small,\npersonalized UI adaptations, called refinements -- to mobile app screens to\nimprove touch efficiency. Reflow uses a pixel-based strategy to work with\nexisting applications, and improves touch efficiency while minimally disrupting\nthe design intent of the original application. Our system optimizes a UI by (i)\nextracting its layout from its screenshot, (ii) refining its layout, and (iii)\nre-rendering the UI to reflect these modifications. We conducted a user study\nwith 10 participants and a heuristic evaluation with 6 experts and found that\napplications optimized by Reflow led to, on average, 9% faster selection time\nwith minimal layout disruption. The results demonstrate that Reflow's\nrefinements useful UI adaptations to improve touch interactions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  MicroRNAs play an indispensable role in numerous biological processes ranging\nfrom organismic development to tumor progression.In oncology,these microRNAs\nconstitute a fundamental regulation role in the pathology of cancer that\nprovides the basis for probing into the influences on clinical features through\ntranscriptome data. Previous work focused on machine learning (ML) for\nsearching biomarkers in different cancer databases, but the functions of these\nbiomarkers are fully not clear. Taking lung cancer as a prototype case of\nstudy. Through integrating clinical information into the transcripts expression\ndata, we systematically analyzed the effect of microRNA on diagnostic and\nprognostic factors at deteriorative lung adenocarcinoma (LUAD). After dimension\nreduction, unsupervised hierarchical clustering was used to find the diagnostic\nfactors which represent the unique expression patterns of microRNA at various\npatient's stages. In addition, we developed a classification framework, Light\nGradient Boosting Machine (LightGBM) and SHAPley Additive explanation (SHAP)\nalgorithm, to screen out the prognostic factors. Enrichment analyses show that\nthe diagnostic and prognostic factors are not only enriched in cancer-related\nathways, but also involved in many vital cellular signaling transduction and\nimmune responses. These key microRNAs also impact the survival risk of LUAD\npatients at all (or a specific) stage(s) and some of them target some important\nTranscription Factors (TF).The key finding is that five microRNAs\n(hsa-mir-196b, hsa-mir-31, hsa-mir-891a, hsa-mir-34c, and hsa-mir-653) can then\nserve as not only potential diagnostic factors but also prognostic tools in the\nmonitoring of lung cancer.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Geometric Deep Learning has recently attracted significant interest in a wide\nrange of machine learning fields, including document analysis. The application\nof Graph Neural Networks (GNNs) has become crucial in various document-related\ntasks since they can unravel important structural patterns, fundamental in key\ninformation extraction processes. Previous works in the literature propose\ntask-driven models and do not take into account the full power of graphs. We\npropose Doc2Graph, a task-agnostic document understanding framework based on a\nGNN model, to solve different tasks given different types of documents. We\nevaluated our approach on two challenging datasets for key information\nextraction in form understanding, invoice layout analysis and table detection.\nOur code is freely accessible on https://github.com/andreagemelli/doc2graph.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This review describes the composition of the Compact Muon Solenoid (CMS)\ndetector and the methodology for modelling the heterogeneous CMS magnetic\nsystem, starting with the formulation of the magnetostatics problem for\nmodelling the magnetic flux of the CMS superconducting solenoid enclosed in a\nsteel flux-return yoke. The review includes a section on the magnetization\ncurves of various types of steel used in the CMS magnet yoke. The evolution of\nthe magnetic system model over 20 years is presented in the discussion section\nand is well illustrated by the CMS model layouts and the magnetic flux\ndistribution.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Energy systems modellers often resort to simplified system representations\nand deterministic model formulations (i.e., not considering uncertainty) to\npreserve computational tractability. However, reduced levels of detail and\nneglected uncertainties can both lead to sub-optimal system designs. In this\npaper, we present a novel method that quantitatively compares the impact of\ndetail and uncertainty to guide model development and help prioritisation of\nthe limited computational resources. By considering modelling choices as an\nadditional 'uncertain' parameter in a global sensitivity analysis, the method\ndetermines their qualitative ranking against conventional input parameters. As\na case study, the method is applied to a peer-reviewed heat decarbonisation\nmodel for the United Kingdom with the objective of assessing the importance of\nspatial resolution. The results show that while for the optimal total system\ncost the impact of spatial resolution is negligible, it is the most important\nfactor determining the capacities of electricity, gas and heat networks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The influence of pre-strain and temperature on the superior properties\nexhibited by an Nb nanowire embedded in a NiTi shape memory alloy (SMA) are\ninvestigated via molecular dynamics simulations. To this end, a new Nb-Ni-Ti\nternary interatomic potential based on the second nearest-neighbor modified\nembedded-atom method (2NN MEAM) is developed and employed. The origin of the\nunique phenomena of quasi-linear elasticity, slim hysteresis, and reduction in\nYoung's modulus observed for pre-strained nanowire-SMA composites is uncovered.\nThe results demonstrate the importance of plastic deformation in the embedded\nNb nanowires and reveal how the deformation facilitates the just-mentioned,\nunprecedented phenomena. A simple and straightforwardly obtainable descriptor\nto correlate and monitor Young's modulus evolution during pre-straining is\nproposed. Furthermore, our simulations suggest that the desired Young's modulus\ncan be obtained for a wide range of application temperatures through\nappropriate pre-straining.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we propose a Non-Linear Predictive Vector quantizer (PVQ) for\nspeech coding, based on Multi-Layer Perceptrons. With this scheme we have\nimproved the results of our previous ADPCM coder with nonlinear prediction, and\nwe have reduced the bit rate up to 1 bit per sample.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A Gaussian function of discrete variable can be associated in a natural way\nto a Gaussian function of continuous variable in each odd-dimensional Hilbert\nspace. The discrete Fourier transform and discrete Wigner distribution of such\na function can be expressed in terms of the corresponding continuous\ncounterpart. By extending this continuous-discrete correspondence up to all the\ncontinuous variable Gaussian states, we define some new (to our knowledge) pure\nand mixed discrete variable Gaussian states, and investigate some of their\nproperties. The discrete variable Gaussian states are described by using the\ncorresponding discrete Wigner functions defined as the sum of some convergent\nseries of a particular form. The chosen form is suggested by the explicit\nexpression obtained in certain particular cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Explaining changes in bitcoin's price and predicting its future have been the\nfoci of many research studies. In contrast, far less attention has been paid to\nthe relationship between bitcoin's mining costs and its price. One popular\nnotion is the cost of bitcoin creation provides a support level below which\nthis cryptocurrency's price should never fall because if it did, mining would\nbecome unprofitable and threaten the maintenance of bitcoin's public ledger.\nOther research has used mining costs to explain or forecast bitcoin's price\nmovements. Competing econometric analyses have debunked this idea, showing that\nchanges in mining costs follow changes in bitcoin's price rather than preceding\nthem, but the reason for this behavior remains unexplained in these analyses.\nThis research aims to employ economic theory to explain why econometric studies\nhave failed to predict bitcoin prices and why mining costs follow movements in\nbitcoin prices rather than precede them. We do so by explaining the chain of\ncausality connecting a bitcoin's price to its mining costs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A large amount of information is stored in data tables. Users can search for\ndata tables using a keyword-based query. A table is composed primarily of data\nvalues that are organized in rows and columns providing implicit structural\ninformation. A table is usually accompanied by secondary information such as\nthe caption, page title, etc., that form the textual information. Understanding\nthe connection between the textual and structural information is an important\nyet neglected aspect in table retrieval as previous methods treat each source\nof information independently. In addition, users can search for data tables\nthat are similar to an existing table, and this setting can be seen as a\ncontent-based table retrieval. In this paper, we propose StruBERT, a\nstructure-aware BERT model that fuses the textual and structural information of\na data table to produce context-aware representations for both textual and\ntabular content of a data table. StruBERT features are integrated in a new\nend-to-end neural ranking model to solve three table-related downstream tasks:\nkeyword- and content-based table retrieval, and table similarity. We evaluate\nour approach using three datasets, and we demonstrate substantial improvements\nin terms of retrieval and classification metrics over state-of-the-art methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This note re-visits the rolling-horizon control approach to the problem of a\nMarkov decision process (MDP) with infinite-horizon discounted expected reward\ncriterion. Distinguished from the classical value-iteration approach, we\ndevelop an asynchronous on-line algorithm based on policy iteration integrated\nwith a multi-policy improvement method of policy switching. A sequence of\nmonotonically improving solutions to the forecast-horizon sub-MDP is generated\nby updating the current solution only at the currently visited state, building\nin effect a rolling-horizon control policy for the MDP over infinite horizon.\nFeedbacks from \"supervisors,\" if available, can be also incorporated while\nupdating. We focus on the convergence issue with a relation to the transition\nstructure of the MDP. Either a global convergence to an optimal\nforecast-horizon policy or a local convergence to a \"locally-optimal\"\nfixed-policy in a finite time is achieved by the algorithm depending on the\nstructure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work we present updated forecasts on parameterised modifications of\ngravity that can capture deviations of the behaviour of cosmological density\nperturbations beyond $\\Lambda$CDM. For these forecasts we adopt the SKA\nObservatory (SKAO) as a benchmark for future cosmological surveys at radio\nfrequencies, combining a continuum survey for weak lensing and angular galaxy\nclustering with an HI galaxy survey for spectroscopic galaxy clustering that\ncan detect baryon acoustic oscillations and redshift space distortions.\nMoreover, we also add 21cm HI intensity mapping, which provides invaluable\ninformation at higher redshifts, and can complement tomographic resolution,\nthus allowing us to probe redshift-dependent deviations of modified gravity\nmodels. For some of these cases, we combine the probes with other optical\nsurveys, such as the Dark Energy Spectroscopic Instrument (DESI) and the Vera\nC. Rubin Observatory (VRO). We show that such synergies are powerful tools to\nremove systematic effects and degeneracies in the non-linear and small-scale\nmodelling of the observables. Overall, we find that the combination of all SKAO\nradio probes will have the ability to constrain the present value of the\nfunctions parameterising deviations from $\\Lambda$CDM ($\\mu$ and $\\Sigma$) with\na precision of $2.7\\%$ and $1.8\\%$ respectively, competitive with the\nconstraints expected from optical surveys and with constraints we have on\ngravitational interactions in the standard model. Exploring the radio-optical\nsynergies, we find that the combination of VRO with SKAO can yield extremely\ntight constraints on $\\mu$ and $\\Sigma$ ($0.9\\%$ and $0.7\\%$ respectively),\nwhich are further improved when the cross-correlation between intensity mapping\nand DESI galaxies is included.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Kimura equation is a degenerated partial differential equation of\ndrift-diffusion type used in population genetics. Its solution is required to\nsatisfy not only the equation but a series of conservation laws formulated as\nintegral constraints. In this work, we consider a population of two types\nevolving without mutation or selection, the so-called neutral evolution. We\nobtain explicit solutions in terms of Gegenbauer polynomials. To satisfy the\nintegral constraints it is necessary to prove new relations satisfied by the\nGegenbauer polynomials. The long-term in time asymptotic is also studied.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Secure and reliable data communication in optical networks is critical for\nhigh-speed internet. We propose a data driven approach for the anomaly\ndetection and faults identification in optical networks to diagnose physical\nattacks such as fiber breaks and optical tapping. The proposed methods include\nan autoencoder-based anomaly detection and an attention-based bidirectional\ngated recurrent unit algorithm for the fiber fault identification and\nlocalization. We verify the efficiency of our methods by experiments under\nvarious attack scenarios using real operational data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Magnetic skyrmions are topologically protected spin swirling vertices, which\nare promising in device applications due to their particle-like nature and\nexcellent controlability. Magnetic skyrmions have been extensively studied in a\nvariaty of materials, and were proposed to exist in the extreme two-dimensional\nlimit, i.e., in twisted bilayer CrI$_3$ (TBCI). Unfortunately, the magnetic\nstates of TBCIs with small twist angles are disorderly distributed\nferromagnetic (FM) and antiferromagnetic (AFM) domains in previous experiments,\nand thus the method to get rid of disorders in TBCIs is highly desirable. Here\nwe propose the functions of interlayer exchange interactions obtained using\nfirst-principles calculations and stored in symmetry-adapted artificial neural\nnetworks. Based on them, the subsequent Landau-Lifshitz-Gillbert equation\ncalculations explain the disorderly distributed FM-AFM domains in TBCIs with\nsmall twist angles and predict the orderly distributed skyrmions in TBCIs with\nlarge twist angles, which can be used in both spintronics and fundamental\nresearch.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A nonlinear self-focusing material can amplify random small-amplitude phase\nmodulations present in an optical beam, leading to the formation of amplitude\nsingularities commonly referred to as optical caustics. By imposing\npolarization structuring on the beam, we demonstrate the suppression of\namplitude singularities caused by nonlinear self-phase modulation. Our results\nare the first to indicate that polarization-structured beams can suppress\nnonlinear caustic formation in a saturable self-focusing medium and add to the\ngrowing understanding of catastrophic self-focusing effects in beams containing\npolarization structure.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  An algorithm for solving nonconvex smooth optimization problems is proposed,\nanalyzed, and tested. The algorithm is an extension of the Trust Region\nAlgorithm with Contractions and Expansions (TRACE) [Math. Prog. 162(1):132,\n2017]. In particular, the extension allows the algorithm to use inexact\nsolutions of the arising subproblems, which is an important feature for solving\nlarge-scale problems. Inexactness is allowed in a manner such that the optimal\niteration complexity of ${\\cal O}(\\epsilon^{-3/2})$ for attaining an\n$\\epsilon$-approximate first-order stationary point is maintained while the\nworst-case complexity in terms of Hessian-vector products may be significantly\nimproved as compared to the original TRACE. Numerical experiments show the\nbenefits of allowing inexact subproblem solutions and that the algorithm\ncompares favorably to a state-of-the-art technique.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  NanoSQUIDs are quantum sensors that excel in detecting a small change in\nmagnetic flux with high sensitivity and high spatial resolution. Here, we\nemploy resist-free direct-write Ga+ Focused Ion Beam Induced Deposition (FIBID)\ntechniques to grow W-C nanoSQUIDs, and we investigate their electrical response\nto changes in the magnetic flux. Remarkably, FIBID allows the fast\n($3~\\mathrm{nm}$) growth of $700~\\mathrm{nm}\\times 300~\\mathrm{nm}$\nDayem-bridge nanoSQUIDs based on narrow nanowires ($50~\\mathrm{nm}$ wide) that\nact as Josephson junctions. The observed transfer coefficient (output voltage\nto magnetic flux change) is very high (up to $1301~\\mathrm{\\mu V/\\Phi_0}$),\nwhich correlates with the high resistivity of W-C in the normal state. We\ndiscuss here the potential of this approach to reduce the active area of the\nnanoSQUIDs to gain spatial resolution as well as their integration on\ncantilevers for scanning-SQUID applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding the assembly of our Galaxy requires us to also characterize the\nsystems that helped build it. In this work, we accomplish this by exploring the\nchemistry of accreted halo stars from the Gaia-Enceladus/Gaia-Sausage (GES)\nselected in the infrared from the Apache Point Observatory Galactic Evolution\nExperiment (APOGEE) Data Release 16. We use high resolution optical spectra for\n62 GES stars to measure abundances in 20 elements spanning the $\\alpha$,\nFe-peak, light, odd-Z, and notably, the neutron-capture groups of elements to\nunderstand their trends in the context of and in contrast to the Milky Way and\nother stellar populations. Using these derived abundances we find that the\noptical and the infrared abundances agree to within 0.15 dex except for O, Co,\nNa, Cu, and Ce. These stars have enhanced neutron-capture abundance trends\ncompared to the Milky Way, and their [Eu/Mg] and neutron-capture abundance\nratios (e.g., [Y/Eu], [Ba/Eu], [Zr/Ba], [La/Ba], and [Nd/Ba]) point to\nr-process enhancement and a delay in s-process enrichment. Their [$\\alpha$/Fe]\ntrend is lower than the Milky Way trend for [Fe/H]$>$-1.5 dex, similar to\nprevious studies of GES stars and consistent with the picture that these stars\nformed in a system with a lower rate of star formation. This is further\nsupported by their depleted abundances in Ni, Na, and Cu abundances, again,\nsimilar to previous studies of low-$\\alpha$ stars with accreted origins.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Comparator-overdrive-delay conditioning is a new control conditioning\napproach for high-frequency current-mode control. No existing literature\nrigorously studies the effect of the comparator overdrive delay on the\ncurrent-mode control. The results in this paper provide insights into the\nmechanism of comparator-overdrive-delay conditioning.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Atmospheric fronts are associated with precipitation and strong diabatic\nprocesses. Therefore, detecting fronts objectively from reanalyses is a\nprerequisite for the long-term study of their weather impacts. For this\npurpose, several algorithms exist, e.g., based on the thermic front parameter\n(TFP) or the F diagnostic. It is shown that both methods have problems to\nidentify weak warm fronts since they are characterized by low baroclinicity. To\navoid this inaccuracy, a new algorithm is developed that considers fronts as\ndeviation from an adiabatic and steady state. These deviations can be\naccurately measured using the dynamic state index (DSI). The DSI shows a\ncoherent dipole structure along fronts and is strongly correlated with\nprecipitation sums. It is shown that the North Atlantic storm tracks can be\nclearly identified by the DSI method. Compared to other front identification\nmethods, fronts identified with the DSI method have particularly high specific\nhumidity. Using a simple estimate for front speed, it is shown that fronts\nidentified using DSI method move faster then fronts identified with TFP method,\ndemonstrating the potential of the DSI to indicate movement speed and direction\nin atmospheric flows.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the problem of propagation of an input electromagnetic pulse through\na long two-level laser amplifier under trivial initial conditions. In this\npaper, we consider an unstable model described by the Maxwell-Bloch equations\nwithout spectral broadening. Previously, this model was studied by S.V. Manakov\nin 1982 and together with V.Yu. Novokshenov in 1986. We consider this model in\na more natural formulation as an initial-boundary (mixed) problem using a\nmodern version of the inverse scattering transform method in the form of a\nsuitable Riemann-Hilbert (RH) problem. The RH problem arises as a result of\napplying the Fokas-Its method of simultaneous analysis of the corresponding\nspectral problems for the Ablowitz-Kaup-Newell-Segur (AKNS) equations. This\napproach makes it possible to obtain rigorous asymptotic results at large\ntimes, which differ significantly from the previous ones. Differences take\nplace both near the light cone and in the tail region, where a new type of\nsolitons is found against an oscillating background. These solitons are\nphysically relevant, their velocities are smaller than the speed of light. The\nnumber of such solitons can be either finite or infinite (in the latter case,\nthe set of zeros has a condensation point at infinity). Such solitons can not\nbe reflectionless, they are generated by zeros of the reflection coefficient of\nthe input pulse (and not by poles of the transmission coefficient).\n  Thus our approach shows the presence of a new phenomenon in soliton theory,\nnamely, the boundary condition (input pulse) of a mixed problem under trivial\ninitial conditions can generate solitons due to the zeros of the reflection\ncoefficient, while the poles of the transmission coefficient do not contribute\nto the asymptotics of the solution.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spatial autocorrelation measures such as Moran's index can be expressed as a\npair of equations based on a standardized size variable and a globally\nnormalized weight matrix. One is based on inner product, and the other is based\non outer product of the size variable. The inner product equation is actually a\nspatial autocorrelation model. However, the theoretical basis of the inner\nproduct equation for Moran's index is not clear. This paper is devoted to\nrevealing the antecedents and consequences of the inner product equation of\nMoran's index. The method is mathematical derivation and empirical analysis.\nThe main results are as follows. First, the inner product equation is derived\nfrom a simple spatial autoregressive model, and thus the relation between\nMoran's index and spatial autoregressive coefficient is clarified. Second, the\nleast squares regression is proved to be one of effective approaches for\nestimating spatial autoregressive coefficient. Third, the value ranges of the\nspatial autoregressive coefficient can be identified from three angles of view.\nA conclusion can be drawn that a spatial autocorrelation model is actually an\ninverse spatial autoregressive model, and Moran's index and spatial\nautoregressive models can be integrated into the same framework through inner\nproduct and outer product equations. This work may be helpful for understanding\nthe connections and differences between spatial autocorrelation measurements\nand spatial autoregressive modeling.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In 1968, Ore determined the maximum size of $k$-connected graphs with given\norder and diameter. In this note, we give a new short proof.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Optimisation algorithms designed to work on quantum computers or other\nspecialised hardware have been of research interest in recent years. Many of\nthese solver can only optimise problems that are in binary and quadratic form.\nQuadratic Unconstrained Binary Optimisation (QUBO) is therefore a common\nformulation used by these solvers.\n  There are many combinatorial optimisation problems that are naturally\nrepresented as permutations e.g., travelling salesman problem. Encoding\npermutation problems using binary variables however presents some challenges.\nMany QUBO solvers are single flip solvers, it is therefore possible to generate\nsolutions that cannot be decoded to a valid permutation. To create bias towards\ngenerating feasible solutions, we use penalty weights. The process of setting\nstatic penalty weights for various types of problems is not trivial. This is\nbecause values that are too small will lead to infeasible solutions being\nreturned by the solver while values that are too large may lead to slower\nconvergence. In this study, we explore some methods of setting penalty weights\nwithin the context of QUBO formulations. We propose new static methods of\ncalculating penalty weights which lead to more promising results than existing\nmethods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recurrent models have been dominating the field of neural machine translation\n(NMT) for the past few years. Transformers \\citep{vaswani2017attention}, have\nradically changed it by proposing a novel architecture that relies on a\nfeed-forward backbone and self-attention mechanism. Although Transformers are\npowerful, they could fail to properly encode sequential/positional information\ndue to their non-recurrent nature. To solve this problem, position embeddings\nare defined exclusively for each time step to enrich word information. However,\nsuch embeddings are fixed after training regardless of the task and the word\nordering system of the source or target language.\n  In this paper, we propose a novel architecture with new position embeddings\ndepending on the input text to address this shortcoming by taking the order of\ntarget words into consideration. Instead of using predefined position\nembeddings, our solution generates new embeddings to refine each word's\nposition information. Since we do not dictate the position of source tokens and\nlearn them in an end-to-end fashion, we refer to our method as dynamic position\nencoding (DPE). We evaluated the impact of our model on multiple datasets to\ntranslate from English into German, French, and Italian and observed meaningful\nimprovements in comparison to the original Transformer.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Enhancing quantum entanglement is important for many quantum information\nprocessing applications. In this paper, we consider a protocol for entanglement\nenhancing in a two-mode squeezed vacuum state (TMSVS), attained based on photon\nsubtraction, photon catalysis, and photon addition. Central to such an\noperation is the task of mixing and detecting number states with each mode of\nTMSVS. We analyze various settings and find an optimal setup for improving the\nentanglement of the state.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Randomized iterative algorithms for solving a factorized linear system,\n$\\mathbf A\\mathbf B\\mathbf x=\\mathbf b$ with $\\mathbf A\\in{\\mathbb{R}}^{m\\times\n\\ell}$, $\\mathbf B\\in{\\mathbb{R}}^{\\ell\\times n}$, and $\\mathbf\nb\\in{\\mathbb{R}}^m$, have recently been proposed. They take advantage of the\nfactorized form and avoid forming the matrix $\\mathbf C=\\mathbf A\\mathbf B$\nexplicitly. However, they can only find the minimum norm (least squares)\nsolution. In contrast, the regularized randomized Kaczmarz (RRK) algorithm can\nfind solutions with certain structures from consistent linear systems. In this\nwork, by combining the randomized Kaczmarz algorithm or the randomized\nGauss--Seidel algorithm with the RRK algorithm, we propose two novel\nregularized randomized iterative algorithms to find (least squares) solutions\nwith certain structures of $\\mathbf A\\mathbf B\\mathbf x=\\mathbf b$. We prove\nlinear convergence of the new algorithms. Computed examples are given to\nillustrate that the new algorithms can find sparse (least squares) solutions of\n$\\mathbf A\\mathbf B\\mathbf x=\\mathbf b$ and can be better than the existing\nrandomized iterative algorithms for the corresponding full linear system\n$\\mathbf C\\mathbf x=\\mathbf b$ with $\\mathbf C=\\mathbf A\\mathbf B$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In long-wavelength moir\\'e superlattices of stacked transition metal\ndichalcogenides (TMDs), structural reconstruction ubiquitously occurs, which\nhas reported to impact significantly their electronic properties. However,\ncomplete microscopic understandings of the interplay between the lattice\nreconstruction and alteration of electronic properties, and their further\nresponse to external perturbations in the reconstructed TMDs moir\\'e\nsuperlattice are still lacking. Here, using scanning tunneling microscopy (STM)\nand scanning tunneling spectroscopy (STS) combined with first-principles\ncalculation, we study the strain-dependent structural reconstruction and its\ncorrelated electronic reconstruction in long-wavelength H-type WS$_{2}$ moir\\'e\nsuperlattice at nanometer scale. We observe that the long-wavelength WS$_{2}$\nmoir\\'e superlattices experiencing strong atomic reconstruction transform into\na hexagonal array of screw dislocations separating large-sized H-stacked\ndomains. Both the geometry and the moir\\'e wavelength of the moir\\'e\nsuperlattice are dramatically tuned by external intralayer heterostrain in our\nexperiment. Remarkably, the STS measurements further demonstrate that the\nlocation of the K point in conduction band is modulated sensitively by\nstrain-induced lattice deformation at nanometer scale in this system, with the\nmaximum energy shift reaching up to 300 meV. Our results highlight that\nintralayer strain plays a vital role in determining structural and electronic\nproperties in TMD moir\\'e superlattice.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  One of the essential elements in implementing a closed-loop irrigation system\nis soil moisture estimation based on a limited number of available sensors. One\nassociated problem is the determination of the optimal locations to install the\nsensors such that good soil moisture estimation can be obtained. In our\nprevious work, the modal degree of observability was employed to address the\nproblem of optimal sensor placement for soil moisture estimation of\nagro-hydrological systems. It was demonstrated that the optimally placed\nsensors can improve the soil moisture estimation performance. However, it is\nunclear whether the optimal sensor placement can significantly improve the soil\nmoisture estimation performance in actual applications. In this work, we\ninvestigate the impact of sensor placement in soil moisture estimation for an\nactual agricultural field in Lethbridge, Alberta, Canada. In an experiment on\nthe studied field, 42 soil moisture sensors were installed at different depths\nto collect the soil moisture measurements for one growing season. A\nthree-dimensional agro-hydrological model with heterogeneous soil parameters of\nthe studied field is developed. The modal degree of observability is applied to\nthe three-dimensional system to determine the optimal sensor locations. The\nextended Kalman filter (EKF) is chosen as the data assimilation tool to\nestimate the soil moisture content of the studied field. Soil moisture\nestimation results for different scenarios are obtained and analyzed to\ninvestigate the effects of sensor placement on the performance of soil moisture\nestimation in the actual applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the long term, reinforcement learning (RL) is considered by many AI\ntheorists to be the most promising path to artificial general intelligence.\nThis places RL practitioners in a position to design systems that have never\nexisted before and lack prior documentation in law and policy. Public agencies\ncould intervene on complex dynamics that were previously too opaque to\ndeliberate about, and long-held policy ambitions would finally be made\ntractable. In this whitepaper we illustrate this potential and how it might be\ntechnically enacted in the domains of energy infrastructure, social media\nrecommender systems, and transportation. Alongside these unprecedented\ninterventions come new forms of risk that exacerbate the harms already\ngenerated by standard machine learning tools. We correspondingly present a new\ntypology of risks arising from RL design choices, falling under four\ncategories: scoping the horizon, defining rewards, pruning information, and\ntraining multiple agents. Rather than allowing RL systems to unilaterally\nreshape human domains, policymakers need new mechanisms for the rule of reason,\nforeseeability, and interoperability that match the risks these systems pose.\nWe argue that criteria for these choices may be drawn from emerging subfields\nwithin antitrust, tort, and administrative law. It will then be possible for\ncourts, federal and state agencies, and non-governmental organizations to play\nmore active roles in RL specification and evaluation. Building on the \"model\ncards\" and \"datasheets\" frameworks proposed by Mitchell et al. and Gebru et\nal., we argue the need for Reward Reports for AI systems. Reward Reports are\nliving documents for proposed RL deployments that demarcate design choices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Data-aided channel estimation is a promising solution to improve channel\nestimation accuracy by exploiting data symbols as pilot signals for updating an\ninitial channel estimate. In this paper, we propose a semi-data-aided channel\nestimator for multiple-input multiple-output communication systems. Our\nstrategy is to leverage reinforcement learning (RL) for selecting reliable\ndetected symbols among the symbols in the first part of transmitted data block.\nThis strategy facilitates an update of the channel estimate before the end of\ndata block transmission and therefore achieves a significant reduction in\ncommunication latency compared to conventional data-aided channel estimation\napproaches. Towards this end, we first define a Markov decision process (MDP)\nwhich sequentially decides whether to use each detected symbol as an additional\npilot signal. We then develop an RL algorithm to efficiently find the best\npolicy of the MDP based on a Monte Carlo tree search approach. In this\nalgorithm, we exploit the a-posteriori probability for approximating both the\noptimal future actions and the corresponding state transitions of the MDP and\nderive a closed-form expression for the best policy. Simulation results\ndemonstrate that the proposed channel estimator effectively mitigates both\nchannel estimation error and detection performance loss caused by insufficient\npilot signals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We give a new, simpler proof of a compactness result in $GSBD^p$, $p>1$,\n  by the same authors, which is also valid in $GBD$ (the case $p=1$),\n  and shows that bounded sequences converge a.e., after removal of a suitable\n  sequence of piecewise infinitesimal rigid motions, subject to a fixed\npartition.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Photonic topological phases offering unprecedented manipulation of\nelectromagnetic waves have attracted much research interest which, however,\nhave been mostly restricted to a single band gap. Here, we report on the\nexperimental discovery of hybrid topological photonic crystals which host\nsimultaneously quantum anomalous Hall and valley Hall phases in different\nphotonic band gaps. The underlying hybrid topological phase manifests itself in\nthe edge responses as the coexistence of the chiral edge states and valley Hall\nedge states in different frequency ranges. We experimentally verify such an\nemergent phenomenon and show that such a feature enables novel multiplexing of\nphoton transport in the edge channels. Our study reveals a situation with\ncoexisting topology of distinct nature in a single photonic system that may\nenable frequency-dependent filtering and manipulation of topological edge\nphotons.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We use Goncharov's coproduct of multiple polylogarithms to define a Lie\ncoalgebra over an arbitrary field. It is generated by symbols subject to\ninductively defined relations, which we think of as functional relations for\nmultiple polylogarithms. In particular, we have inversion relations and shuffle\nrelations. We relate our definition to Goncharov's Bloch groups, and to the\nconcrete model in weight less than 5 by Goncharov and Rudenko.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Nonconvex-concave minimax optimization has received intense interest in\nmachine learning, including learning with robustness to data distribution,\nlearning with non-decomposable loss, adversarial learning, to name a few.\nNevertheless, most existing works focus on the gradient-descent-ascent (GDA)\nvariants that can only be applied in smooth settings. In this paper, we\nconsider a family of minimax problems whose objective function enjoys the\nnonsmooth composite structure in the variable of minimization and is concave in\nthe variables of maximization. By fully exploiting the composite structure, we\npropose a smoothed proximal linear descent ascent (\\textit{smoothed} PLDA)\nalgorithm and further establish its $\\mathcal{O}(\\epsilon^{-4})$ iteration\ncomplexity, which matches that of smoothed GDA~\\cite{zhang2020single} under\nsmooth settings. Moreover, under the mild assumption that the objective\nfunction satisfies the one-sided Kurdyka-\\L{}ojasiewicz condition with exponent\n$\\theta \\in (0,1)$, we can further improve the iteration complexity to\n$\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$. To the best of our knowledge,\nthis is the first provably efficient algorithm for nonsmooth nonconvex-concave\nproblems that can achieve the optimal iteration complexity\n$\\mathcal{O}(\\epsilon^{-2})$ if $\\theta \\in (0,1/2]$. As a byproduct, we\ndiscuss different stationarity concepts and clarify their relationships\nquantitatively, which could be of independent interest. Empirically, we\nillustrate the effectiveness of the proposed smoothed PLDA in variation\nregularized Wasserstein distributionally robust optimization problems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many modern cyber physical systems incorporate computer vision technologies,\ncomplex sensors and advanced control software, allowing them to interact with\nthe environment autonomously. Testing such systems poses numerous challenges:\nnot only should the system inputs be varied, but also the surrounding\nenvironment should be accounted for. A number of tools have been developed to\ntest the system model for the possible inputs falsifying its requirements.\nHowever, they are not directly applicable to autonomous cyber physical systems,\nas the inputs to their models are generated while operating in a virtual\nenvironment. In this paper, we aim to design a search based framework, named\nAmbieGen, for generating diverse fault revealing test scenarios for autonomous\ncyber physical systems. The scenarios represent an environment in which an\nautonomous agent operates. The framework should be applicable to generating\ndifferent types of environments. To generate the test scenarios, we leverage\nthe NSGA II algorithm with two objectives. The first objective evaluates the\ndeviation of the observed system behaviour from its expected behaviour. The\nsecond objective is the test case diversity, calculated as a Jaccard distance\nwith a reference test case. We evaluate AmbieGen on three scenario generation\ncase studies, namely a smart-thermostat, a robot obstacle avoidance system, and\na vehicle lane keeping assist system. We compared three configurations of\nAmbieGen: based on a single objective genetic algorithm, multi objective, and\nrandom search. Both single and multi objective configurations outperform the\nrandom search. Multi objective configuration can find the individuals of the\nsame quality as the single objective, producing more unique test scenarios in\nthe same time budget.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the UTokyo-SaruLab mean opinion score (MOS) prediction system\nsubmitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS\nvalues of speech samples collected from previous Blizzard Challenges and Voice\nConversion Challenges for two tracks: a main track for in-domain prediction and\nan out-of-domain (OOD) track for which there is less labeled data from\ndifferent listening tests. Our system is based on ensemble learning of strong\nand weak learners. Strong learners incorporate several improvements to the\nprevious fine-tuning models of self-supervised learning (SSL) models, while\nweak learners use basic machine-learning methods to predict scores from SSL\nfeatures. In the Challenge, our system had the highest score on several metrics\nfor both the main and OOD tracks. In addition, we conducted ablation studies to\ninvestigate the effectiveness of our proposed methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The PANDA (anti-Proton ANnihilation at DArmstadt) experiment at the Facility\nfor Anti-proton and Ion Research is going to study strong interactions at the\nscale at which quarks are confined to form hadrons. A continuous beam of\nantiproton, provided by the High Energy Storage Ring (HESR), will impinge on a\nfixed hydrogen target. The antiproton beam momentum spans from\n1.5~GeV\\footnote{Natural units, c=1} to 15~GeV~\\cite{physics2009report}, will\ncreate optimal conditions for studying many different aspects of hadron\nphysics, including hyperon physics.\n  Precision physics studies require a highly efficient particle track\nreconstruction. The Straw Tube Tracker in PANDA is the main component for that\npurpose. It has a hexagonal geometry, consisting of 4224 gas-filled tubes\narranged in 26 layers and six sectors. However, the challenge is reconstructing\nlow momentum charged particles given the complex detector geometry and the\nstrongly curved particle trajectory. This paper presents the first application\nof a geometric deep learning pipeline to track reconstruction in the PANDA\nexperiment. The pipeline reconstructs more than 95\\% of particle tracks and\ncreates less than 0.3\\% fake tracks. The promising results make the pipeline a\nstrong candidate algorithm for the experiment.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Motivated by practical applications, recent works have considered\nmaximization of sums of a submodular function $g$ and a linear function $\\ell$.\nAlmost all such works, to date, studied only the special case of this problem\nin which $g$ is also guaranteed to be monotone. Therefore, in this paper we\nsystematically study the simplest version of this problem in which $g$ is\nallowed to be non-monotone, namely the unconstrained variant, which we term\nRegularized Unconstrained Submodular Maximization (RegularizedUSM).\n  Our main algorithmic result is the first non-trivial guarantee for general\nRegularizedUSM. For the special case of RegularizedUSM in which the linear\nfunction $\\ell$ is non-positive, we prove two inapproximability results,\nshowing that the algorithmic result implied for this case by previous works is\nnot far from optimal. Finally, we reanalyze the known Double Greedy algorithm\nto obtain improved guarantees for the special case of RegularizedUSM in which\nthe linear function $\\ell$ is non-negative; and we complement these guarantees\nby showing that it is not possible to obtain (1/2, 1)-approximation for this\ncase (despite intuitive arguments suggesting that this approximation guarantee\nis natural).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  DNNs are vulnerable to adversarial examples, which poses great security\nconcerns for security-critical systems. In this paper, a novel\nadaptive-patch-based physical attack (AP-PA) framework is proposed, which aims\nto generate adversarial patches that are adaptive in both physical dynamics and\nvarying scales, and by which the particular targets can be hidden from being\ndetected. Furthermore, the adversarial patch is also gifted with attack\neffectiveness against all targets of the same class with a patch outside the\ntarget (No need to smear targeted objects) and robust enough in the physical\nworld. In addition, a new loss is devised to consider more available\ninformation of detected objects to optimize the adversarial patch, which can\nsignificantly improve the patch's attack efficacy (Average precision drop up to\n87.86% and 85.48% in white-box and black-box settings, respectively) and\noptimizing efficiency. We also establish one of the first comprehensive,\ncoherent, and rigorous benchmarks to evaluate the attack efficacy of\nadversarial patches on aerial detection tasks. Finally, several proportionally\nscaled experiments are performed physically to demonstrate that the elaborated\nadversarial patches can successfully deceive aerial detection algorithms in\ndynamic physical circumstances. The code is available at\nhttps://github.com/JiaweiLian/AP-PA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The problem of inferring object shape from a single 2D image is\nunderconstrained. Prior knowledge about what objects are plausible can help,\nbut even given such prior knowledge there may still be uncertainty about the\nshapes of occluded parts of objects. Recently, conditional neural radiance\nfield (NeRF) models have been developed that can learn to infer good point\nestimates of 3D models from single 2D images. The problem of inferring\nuncertainty estimates for these models has received less attention. In this\nwork, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy\nfor learning probabilistic generative models of 3D objects' shapes and\nappearances, and for doing posterior inference to recover those properties from\n2D images. ProbNeRF is trained as a variational autoencoder, but at test time\nwe use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D\nimages of an object (which may be partially occluded), ProbNeRF is able not\nonly to accurately model the parts it sees, but also to propose realistic and\ndiverse hypotheses about the parts it does not see. We show that key to the\nsuccess of ProbNeRF are (i) a deterministic rendering scheme, (ii) an\nannealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and\n(iv) doing inference over a full set of NeRF weights, rather than just a\nlow-dimensional code.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum computer provides new opportunities for quantum chemistry. In this\narticle, we present a versatile, extensible, and efficient software package,\nnamed Q$^2$Chemistry, for developing quantum algorithms and quantum inspired\nclassical algorithms in the field of quantum chemistry. In Q$^2$Chemistry, wave\nfunction and Hamiltonian can be conveniently mapped into the qubit space, then\nquantum circuits can be generated according to a specific quantum algorithm\nalready implemented in the package or newly developed by the users. The\ngenerated circuits can be dispatched to either a physical quantum computer, if\navailable, or to the internal virtual quantum computer realized by simulating\nquantum circuit on classical supercomputers. As demonstrated by our benchmark\nsimulations with up to 72 qubit, Q$^2$Chemistry achieves excellent performance\nin simulating medium scale quantum circuits. Application of Q$^2$Chemistry to\nsimulate molecules and periodic systems are given with performance analysis.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For text classification tasks, finetuned language models perform remarkably\nwell. Yet, they tend to rely on spurious patterns in training data, thus\nlimiting their performance on out-of-distribution (OOD) test data. Among recent\nmodels aiming to avoid this spurious pattern problem, adding extra\ncounterfactual samples to the training data has proven to be very effective.\nYet, counterfactual data generation is costly since it relies on human\nannotation. Thus, we propose a novel solution that only requires annotation of\na small fraction (e.g., 1%) of the original training data, and uses automatic\ngeneration of extra counterfactuals in an encoding vector space. We demonstrate\nthe effectiveness of our approach in sentiment classification, using IMDb data\nfor training and other sets for OOD tests (i.e., Amazon, SemEval and Yelp). We\nachieve noticeable accuracy improvements by adding only 1% manual\ncounterfactuals: +3% compared to adding +100% in-distribution training samples,\n+1.3% compared to alternate counterfactual approaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Following up large numbers of candidates in continuous gravitational wave\nsearches presents a challenge, particularly in regard to computational power\nand the time required to manually scrutinize each of the candidates. It is\nimportant to design and test good follow-up procedures that are safe (i.e.,\nminimize false dismissals) and computationally efficient across many search\nconfigurations. We investigate two follow-up procedures, or \"vetoes,\" both of\nwhich exploit the Doppler modulation predicted in astrophysical signals. In\nparticular, we introduce the concept of using an effective point spread\nfunction as part of our veto criteria. We take advantage of a well-established\nsemicoherent search algorithm based on a hidden Markov model to study various\nsearch configurations and to generalize the veto criteria by considering the\noverall veto performance in terms of efficiency and safety. The results can\nserve as a guideline for follow-up studies in future continuous gravitational\nwave searches using a hidden Markov model algorithm. The results also apply\nqualitatively to other semicoherent search algorithms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Long Document retrieval (DR) has always been a tremendous challenge for\nreading comprehension and information retrieval. The pre-training model has\nachieved good results in the retrieval stage and Ranking for long documents in\nrecent years. However, there is still some crucial problem in long document\nranking, such as data label noises, long document representations, negative\ndata Unbalanced sampling, etc. To eliminate the noise of labeled data and to be\nable to sample the long documents in the search reasonably negatively, we\npropose the bag sampling method and the group-wise Localized Contrastive\nEstimation(LCE) method. We use the head middle tail passage for the long\ndocument to encode the long document, and in the retrieval, stage Use dense\nretrieval to generate the candidate's data. The retrieval data is divided into\nmultiple bags at the ranking stage, and negative samples are selected in each\nbag. After sampling, two losses are combined. The first loss is LCE. To fit bag\nsampling well, after query and document are encoded, the global features of\neach group are extracted by convolutional layer and max-pooling to improve the\nmodel's resistance to the impact of labeling noise, finally, calculate the LCE\ngroup-wise loss. Notably, our model shows excellent performance on the MS MARCO\nLong document ranking leaderboard.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the problem of $N$ identical fermions interacting via a\nzero-range attractive potential with a lighter atom in one dimension. Using the\nfew-body approach based on the Skorniakov and Ter-Martirosian equation, we\ndetermine the energies and the critical mass ratios for the emergence of the\ntetramer, pentamer, and hexamer. For large $N$, we solve the problem\nanalytically by using the mean-field theory based on the Thomas-Fermi\napproximation. The system becomes bound when the heavy-to-light mass ratio\nexceeds a critical value which grows as $N^3$ at large $N$. We also employ a\nmore sophisticated Hartree-Fock approach, which turns out to be equivalent to\nthe Thomas-Fermi approximation for determining the energies, but provides a\nbetter description of the microscopic structure of the clusters.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Physics-informed neural networks (PINNs) are revolutionizing science and\nengineering practice by bringing together the power of deep learning to bear on\nscientific computation. In forward modeling problems, PINNs are meshless\npartial differential equation (PDE) solvers that can handle irregular,\nhigh-dimensional physical domains. Naturally, the neural architecture\nhyperparameters have a large impact on the efficiency and accuracy of the PINN\nsolver. However, this remains an open and challenging problem because of the\nlarge search space and the difficulty of identifying a proper search objective\nfor PDEs. Here, we propose Auto-PINN, the first systematic, automated\nhyperparameter optimization approach for PINNs, which employs Neural\nArchitecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually\nor exhaustively searching the hyperparameter space associated with PINNs. A\ncomprehensive set of pre-experiments using standard PDE benchmarks allows us to\nprobe the structure-performance relationship in PINNs. We find that the\ndifferent hyperparameters can be decoupled, and that the training loss function\nof PINNs is a good search objective. Comparison experiments with baseline\nmethods demonstrate that Auto-PINN produces neural architectures with superior\nstability and accuracy over alternative baselines.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Owing to the big data the extension of physical laws on nonmaterial has seen\nnumerous successes, and human mobility is one of the scientific frontier\ntopics. Recent GPS technology has made it possible to trace detailed\ntrajectories of millions of people, macroscopic approaches such as the gravity\nlaw for human flow between cities and microscopic approaches of individual\norigin-destination distributions are attracting much attention. However, we\nneed a more general basic model with wide applicability to realize traffic\nforecasting and urban planning of metropolis fully utilizing the GPS data.\nHere, based on a novel idea of treating moving people as charged particles, we\nintroduce a sophisticated method to map macroscopic human flows into currents\non an imaginary electric circuit defined over a metropolitan area. Conductance\nis found to be nearly proportional to the maximum current in each location and\nsynchronized human flows in the morning and evening are well described by the\ntemporal changes of electric potential. Surprisingly, the famous fluctuation\ndissipation theorem holds, namely, the variances of currents are proportional\nto the conductivities akin to an ordinary material. Especially during the\npandemic, such a tool may offer an invaluable insight for policy-making in\nmanaging human flows.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Current methods for LIDAR semantic segmentation are not robust enough for\nreal-world applications, e.g., autonomous driving, since it is closed-set and\nstatic. The closed-set assumption makes the network only able to output labels\nof trained classes, even for objects never seen before, while a static network\ncannot update its knowledge base according to what it has seen. Therefore, in\nthis work, we propose the open-world semantic segmentation task for LIDAR point\nclouds, which aims to 1) identify both old and novel classes using open-set\nsemantic segmentation, and 2) gradually incorporate novel objects into the\nexisting knowledge base using incremental learning without forgetting old\nclasses. For this purpose, we propose a REdundAncy cLassifier (REAL) framework\nto provide a general architecture for both the open-set semantic segmentation\nand incremental learning problems. The experimental results show that REAL can\nsimultaneously achieves state-of-the-art performance in the open-set semantic\nsegmentation task on the SemanticKITTI and nuScenes datasets, and alleviate the\ncatastrophic forgetting problem with a large margin during incremental\nlearning.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Convergence and convergence rate analyses of adaptive methods, such as\nAdaptive Moment Estimation (Adam) and its variants, have been widely studied\nfor nonconvex optimization. The analyses are based on assumptions that the\nexpected or empirical average loss function is Lipschitz smooth (i.e., its\ngradient is Lipschitz continuous) and the learning rates depend on the\nLipschitz constant of the Lipschitz continuous gradient. Meanwhile, numerical\nevaluations of Adam and its variants have clarified that using small constant\nlearning rates without depending on the Lipschitz constant and hyperparameters\n($\\beta_1$ and $\\beta_2$) close to one is advantageous for training deep neural\nnetworks. Since computing the Lipschitz constant is NP-hard, the Lipschitz\nsmoothness condition would be unrealistic. This paper provides theoretical\nanalyses of Adam without assuming the Lipschitz smoothness condition in order\nto bridge the gap between theory and practice. The main contribution is to show\ntheoretical evidence that Adam using small learning rates and hyperparameters\nclose to one performs well, whereas the previous theoretical results were all\nfor hyperparameters close to zero. Our analysis also leads to the finding that\nAdam performs well with large batch sizes. Moreover, we show that Adam performs\nwell when it uses diminishing learning rates and hyperparameters close to one.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The seismic events that preceded the leaks in the Nordstream pipelines in the\nBaltic Sea have been interpreted as explosions on the seabed, most likely\nman-made. We use a polarization-based location method initially developed for\nmarsquakes to locate the source region without a subsurface velocity model. We\nshow that the 2 largest seismic events can be unambiguously attributed to the\nmethane plumes observed on the sea surface. The two largest events can be\nlocated with this method, using 4 and 5 stations located around the source,\nwith location uncertainties of 30km and 10x60km. We can further show that both\nevents emitted seismic energy for at least ten minutes after the initial\nexplosion, indicative of resonances in the water column or the depressurizing\npipeline.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the symbology of planar Feynman integrals in dimensional\nregularization by considering geometric configurations in momentum twistor\nspace corresponding to their leading singularities (LS). Cutting propagators in\nmomentum twistor space amounts to intersecting lines associated with loop and\nexternal dual momenta, including the special line associated with the point at\ninfinity, which breaks dual conformal symmetry. We show that cross-ratios of\nintersection points on these lines, especially those on the infinity line,\nnaturally produce symbol letters for Feynman integrals in $D=4-2\\epsilon$,\nwhich include and generalize their LS. At one loop, we obtain all symbol\nletters using intersection points from quadruple cuts for integrals up to\npentagon kinematics with two massive corners, which agree perfectly with\ncanonical differential equation (CDE) results. We then obtain all two-loop\nletters, for up to four-mass box and one-mass pentagon kinematics, by\nconsidering more intersections arising from two-loop cuts. Finally we comment\non how cluster algebras appear from this construction, and importantly how we\nmay extend the method to non-planar integrals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Query understanding plays a key role in exploring users' search intents and\nfacilitating users to locate their most desired information. However, it is\ninherently challenging since it needs to capture semantic information from\nshort and ambiguous queries and often requires massive task-specific labeled\ndata. In recent years, pre-trained language models (PLMs) have advanced various\nnatural language processing tasks because they can extract general semantic\ninformation from large-scale corpora. Therefore, there are unprecedented\nopportunities to adopt PLMs for query understanding. However, there is a gap\nbetween the goal of query understanding and existing pre-training strategies --\nthe goal of query understanding is to boost search performance while existing\nstrategies rarely consider this goal. Thus, directly applying them to query\nunderstanding is sub-optimal. On the other hand, search logs contain user\nclicks between queries and urls that provide rich users' search behavioral\ninformation on queries beyond their content. Therefore, in this paper, we aim\nto fill this gap by exploring search logs. In particular, to incorporate search\nlogs into pre-training, we first construct a query graph where nodes are\nqueries and two queries are connected if they lead to clicks on the same urls.\nThen we propose a novel graph-enhanced pre-training framework, GE-BERT, which\ncan leverage both query content and the query graph. In other words, GE-BERT\ncan capture both the semantic information and the users' search behavioral\ninformation of queries. Extensive experiments on various query understanding\ntasks have demonstrated the effectiveness of the proposed framework.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For a hereditary family of graphs $\\FF$, let $\\FF_n$ denote the set of all\nmembers of $\\FF$ on $n$ vertices. The speed of $\\FF$ is the function\n$f(n)=|\\FF_n|$. An implicit representation of size $\\ell(n)$ for $\\FF_n$ is a\nfunction assigning a label of $\\ell(n)$ bits to each vertex of any given graph\n$G \\in \\FF_n$, so that the adjacency between any pair of vertices can be\ndetermined by their labels. Bonamy, Esperet, Groenland and Scott proved that\nthe minimum possible size of an implicit representation of $\\FF_n$ for any\nhereditary family $\\FF$ with speed $2^{\\Omega(n^2)}$ is $(1+o(1)) \\log_2\n|\\FF_n|/n~(=\\Theta(n))$. A recent result of Hatami and Hatami shows that the\nsituation is very different for very sparse hereditary families. They showed\nthat for every $\\delta>0$ there are hereditary families of graphs with speed\n$2^{O(n \\log n)}$ that do not admit implicit representations of size smaller\nthan $n^{1/2-\\delta}$. In this note we show that even a mild speed bound\nensures an implicit representation of size $O(n^c)$ for some $c<1$.\nSpecifically we prove that for every $\\eps>0$ there is an integer $d \\geq 1$ so\nthat if $\\FF$ is a hereditary family with speed $f(n) \\leq 2^{(1/4-\\eps)n^2}$\nthen $\\FF_n$ admits an implicit representation of size $O(n^{1-1/d} \\log n)$.\nMoreover, for every integer $d>1$ there is a hereditary family for which this\nis tight up to the logarithmic factor.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper presents considerations towards an information and control\narchitecture for future electric energy systems driven by massive changes\nresulting from the societal goals of decarbonization and electrification. This\npaper describes the new requirements and challenges of an extended information\nand control architecture that need to be addressed for continued reliable\ndelivery of electricity. It identifies several new actionable information and\ncontrol loops, along with their spatial and temporal scales of operation, which\ncan together meet the needs of future grids and enable deep decarbonization of\nthe electricity sector. The present architecture of electric power grids\ndesigned in a different era is thereby extensible to allow the incorporation of\nincreased renewables and other emerging electric loads.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Is dynamics prediction indispensable for physical reasoning? If so, what kind\nof roles do the dynamics prediction modules play during the physical reasoning\nprocess? Most studies focus on designing dynamics prediction networks and\ntreating physical reasoning as a downstream task without investigating the\nquestions above, taking for granted that the designed dynamics prediction would\nundoubtedly help the reasoning process. In this work, we take a closer look at\nthis assumption, exploring this fundamental hypothesis by comparing two\nlearning mechanisms: Learning from Dynamics (LfD) and Learning from Intuition\n(LfI). In the first experiment, we directly examine and compare these two\nmechanisms. Results show a surprising finding: Simple LfI is better than or on\npar with state-of-the-art LfD. This observation leads to the second experiment\nwith Ground-truth Dynamics, the ideal case of LfD wherein dynamics are obtained\ndirectly from a simulator. Results show that dynamics, if directly given\ninstead of approximated, would achieve much higher performance than LfI alone\non physical reasoning; this essentially serves as the performance upper bound.\nYet practically, LfD mechanism can only predict Approximate Dynamics using\ndynamics learning modules that mimic the physical laws, making the following\ndownstream physical reasoning modules degenerate into the LfI paradigm; see the\nthird experiment. We note that this issue is hard to mitigate, as dynamics\nprediction errors inevitably accumulate in the long horizon. Finally, in the\nfourth experiment, we note that LfI, the extremely simpler strategy when done\nright, is more effective in learning to solve physical reasoning problems.\nTaken together, the results on the challenging benchmark of PHYRE show that LfI\nis, if not better, as good as LfD for dynamics prediction. However, the\npotential improvement from LfD, though challenging, remains lucrative.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Predicting mechanical properties in metal additive manufacturing (MAM) is\nvital to ensure the printed parts' performance, reliability, and whether they\ncan fulfill requirements for a specific application. Conducting experiments to\nestimate mechanical properties in MAM processes, however, is a laborious and\nexpensive task. Also, they can solely be designed for a particular material in\na certain MAM process. Nonetheless, Machine learning (ML) methods, which are\nmore flexible and cost-effective solutions, can be utilized to predict\nmechanical properties based on the processing parameters and material\nproperties. To this end, in this work, a comprehensive framework for\nbenchmarking ML for mechanical properties is introduced. An extensive\nexperimental dataset is collected from more than 90 MAM articles and 140 MAM\ncompanies' data sheets containing MAM processing conditions, machines,\nmaterials, and resultant mechanical properties, including yield strength,\nultimate tensile strength, elastic modulus, elongation, hardness as well as\nsurface roughness. Physics-aware MAM featurization, adjustable ML models, and\nevaluation metrics are proposed to construct a comprehensive learning framework\nfor mechanical properties prediction. Additionally, the Explainable AI method,\ni.e., SHAP analysis was studied to explain and interpret the ML models'\npredicted values for mechanical properties. Moreover, data-driven explicit\nmodels have been identified to estimate mechanical properties based on the\nprocessing parameters and material properties with more interpretability as\ncompared to the employed ML models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the analysis of the optical variability of the early,\nnitrogen-rich Wolf-Rayet (WR) star WR7. The analysis of multi-sector Transiting\nExoplanet Survey Satellite (TESS) light curves and high-resolution\nspectroscopic observations confirm multi-periodic variability that is modulated\non time-scales of years. We detect a dominant period of $2.6433 \\pm 0.0005$ d\nin the TESS sectors 33 and 34 light curves in addition to the previously\nreported high-frequency features from sector 7. We discuss the plausible\nmechanisms that may be responsible for such variability in WR7, including\npulsations, binarity, co-rotating interacting regions (CIRs) and clumpy winds.\nGiven the lack of strong evidence for the presence of a stellar or compact\ncompanion, we suggest that WR7 may pulsate in quasi-coherent modes in addition\nto wind variability likely caused by CIRs on top of stochastic low-frequency\nvariability. WR7 is certainly a worthy target for future monitoring in both\nspectroscopy and photometry to sample both the short ($\\lesssim 1$ d) and long\n($\\gtrsim 1000$ d) variability time scales.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we prove some rigidity results for both shrinking and\nexpanding Ricci solitons. First, we prove that compact shrinking Ricci solitons\nare Einstein if we control the maximum value of the potential function. Then,\nwe prove some rigidity results for non-compact gradient expanding and shrinking\nRicci solitons with pinched Ricci and scalar curvatures, assuming an asymptotic\ncondition on the scalar curvature at infinity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the coming years, third-generation detectors such as the Einstein\nTelescope and the Cosmic Explorer will enter the network of ground-based\ngravitational-wave detectors. Their current design predicts a significantly\nimproved sensitivity band with a lower minimum frequency than existing\ndetectors. This, combined with the increased arm length, leads to two major\neffects: the detection of more signals and the detection of longer signals.\nBoth will result in a large number of overlapping signals.\n  It has been shown that such overlapping signals can lead to biases in the\nrecovered parameters, which would adversely affect the science extracted from\nthe observed binary merger signals. In this work, we analyze overlapping binary\nblack hole coalescences with two methods to analyze multi-signal observations:\nhierarchical subtraction and joint parameter estimation. We find that these\nmethods enable a reliable parameter extraction in most cases and that joint\nparameter estimation is usually more precise but comes with higher\ncomputational costs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Compared to conventional robots, flexible manipulators offer many advantages,\nsuch as faster end-effector velocities and less energy consumption. However,\ntheir flexible structure can lead to undesired oscillations. Therefore, the\napplied control strategy should account for these elasticities. A feedforward\ncontroller based on an inverse model of the system is an efficient way to\nimprove the performance. However, unstable internal dynamics arise for many\ncommon flexible robots and stable inversion must be applied. In this\ncontribution, an approximation of the original stable inversion approach is\nproposed. The approximation simplifies the problem setup, since the internal\ndynamics do not need to be derived explicitly for the definition of the\nboundary conditions. From a practical point of view, this makes the method\napplicable to more complex systems with many unactuated degrees of freedom.\nFlexible manipulators modeled by the absolute nodal coordinate formulation\n(ANCF) are considered as an application example.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce two parametrized families of piecewise affine maps on $[0,1]^2$\nand $[0,1]^3$, as generalizations of the heterochaos baker maps which were\nintroduced and investigated in [Y. Saiki, H. Takahasi, J. A. Yorke,\nNonlinearity, 34 (2021), 5744--5761] as minimal models of the unstable\ndimension variability in multidimensional dynamical systems. We show that\nnatural coding spaces of these maps coincide with the Dyck system that has come\nfrom the theory of languages. Based on this coincidence, we start to develop a\ncomplementary analysis on their invariant measures. As a first attempt, we show\nthe existence of two ergodic measures of maximal entropy for the generalized\nheterochaos baker maps. We also clarify a mechanism for the breakdown of\nentropy approachability.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Cryogenic characterization of transition-edge sensor (TES) bolometers is a\ntime- and labor-intensive process. As new experiments deploy larger and larger\narrays of TES bolometers, the testing process will become more of a bottleneck.\nThus it is desirable to develop a method for evaluating detector performance at\nroom temperature. One possibility is using machine learning to correlate\ndetectors' visual appearance with their cryogenic properties. Here, we use\nthree engineering-grade TES bolometer wafers from the production cycle for\nSPT-3G, the current receiver on the South Pole Telescope, to train and test\nsuch an algorithm. High-resolution images of these TES bolometers were captured\nand relevant features were calculated from the images. Cryogenic performance\nmetrics, including a detector's ability to tune and superconducting parameters\nsuch as normal resistance, critical temperature, and transition width, were\nalso measured. A random forest algorithm was trained to predict these\nperformance metrics from the visual features. Analysis of the images proved\nhighly successful. While the ability of the random forest algorithm to predict\ncryogenic features was limited with the chosen set of input image features, it\nis possible that an increase in data volume or the addition of more image\nfeatures will solve this problem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In 1976 Frank and Gy{\\'a}rf{\\'a}s gave a necessary and sufficient condition\nfor the existence of an orientation in an arbitrary graph $G$ such that for\neach vertex $v$, the out-degree $d^+_G(v)$ of it satisfies $p(v)\\le d^+_G(v)\\le\nq(v)$, where $p$ and $q$ are two integer-valued functions on $V(G)$ with $p\\le\nq$. In this paper, we give a sufficient edge-connectivity condition for the\nexistence of an orientation in $G$ such that for each vertex $v$, $d^+_G(v)\\in\n\\{p(v),q(v)\\}$, provided that for each vertex $v$, $p(v)\\le \\frac{1}{2}d_G(v)\n\\le q(v)$, $|q(v)-p(v)|\\le k$, and there is $t(v)\\in \\{p(v),q(v)\\}$ in which\n$|E(G)|=\\sum_{v\\in V(G)}t(v)$. This result is a generalization of a theorem due\nto Thomassen (2012) on the existence of modulo orientations in highly\nedge-connected graphs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It is well-known that for any inner function $\\theta$ defined in the unit\ndisk $D$ the following two conditons: $(i)$ there exists a sequence of\npolynomials $\\{p_n\\}_n$ such that $\\lim_{n \\to \\infty} \\theta(z) p_n(z) = 1$\nfor all $z \\in D$, and $(ii)$ $\\sup_n \\| \\theta p_n \\|_\\infty < \\infty$, are\nincompatible, i.e., cannot be satisfied simultaneously. In this note we discuss\nand apply a consequence of a result by Thomas Ransford, which shows that if we\nrelax the second condition to allow for arbitrarily slow growth of the sequence\n$\\{ \\theta(z) p_n(z)\\}_n$ as $|z| \\to 1$, then condition $(i)$ can be met. In\nother words, every growth class of analytic functions contains cyclic singular\ninner functions. We apply this observation to properties of decay of Taylor\ncoefficients and moduli of continuity of functions in model spaces $K_\\theta$.\nIn particular, we establish a variant of a result of Khavinson and Dyakonov on\nnon-existence of functions with certain smoothness properties in $K_\\theta$,\nand we show that the classical Aleksandrov theorem on density of continuous\nfunctions in $K_\\theta$, and its generalization to de Branges-Rovnyak spaces\n$\\mathcal{H}(b)$, is essentially sharp.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this letter first we show that the equation of null geodesics in\nspherically symmetric spacetimes in isotropic coordinates is identical to the\nequation of light ray trajectories in isotropic media in flat spacetime. Based\non this analogy we introduce an exact simulation of the light ray trajectories\nboth in these spacetimes and in their metamaterial analogs in terms of the\nspacetime index of refraction. As unstable light trajectories, the photon\nspheres form in these metamaterial analogs at {\\it exactly} the same radial\ndistances as expected from the corresponding black hole geometries. Using the\nsame ray-tracing simulation we find the analog of a simple black hole shadow\nformed by the metamaterial analog of a Schwarzschild black hole, eclipsing a\nline of light sources near its analog horizon.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Almost 25 years ago, Jarzynski published a paper in which it was asserted:\nthe work done, W, in driving a system from state A to state B, characterized by\nthe Helmholtz free energies FA and FB, satisfies an equality in which an\naverage over an ensemble of measurements for W determines the difference in\nFree energy. Several features of this result require more detailed description,\nto be given in the text. The equality is significant and unexpected. So is the\nstatement that the equality is independent of the rate of change from state A\nto state B. A few years ago, I had presented three papers in which the\ncontraction of the description from full phase space to coordinate space only\nwas made. This was motivated by the large difference in time scales for momenta\nrelaxation and coordinate relaxation. The Jarzynski equality (JE) will be shown\nhere to be correct only in the limit of slow processes. The proposed example is\nfor a macromolecular harmonic oscillator with a Hooke's spring constant, k,\nthat during the time interval linearly changes from k1 to k2. When performed\nslowly, we obtain JE in a form in which the free energy is related to the ratio\nof the k's. However, when the transition is performed more rapidly the equality\nis lost. Jarzynski has suggested to me that Hummer and Szabo have shown that JE\nfollows directly from the Feynman-Kac formula. We show here that F-K does not\nreproduce the direct calculation executed in this paper and that it has been\nmisapplied in earlier papers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Dense retrievers encode documents into fixed dimensional embeddings. However,\nstoring all the document embeddings within an index produces bulky indexes\nwhich are expensive to serve. Recently, BPR (Yamada et al., 2021) and JPQ (Zhan\net al., 2021a) have been proposed which train the model to produce binary\ndocument vectors, which reduce the index 32x and more. The authors showed these\nbinary embedding models significantly outperform more traditional index\ncompression techniques like Product Quantization (PQ). Previous work evaluated\nthese approaches just in-domain, i.e. the methods were evaluated on tasks for\nwhich training data is available. In practice, retrieval models are often used\nin an out-of-domain setting, where they have been trained on a publicly\navailable dataset, like MS MARCO, but are then used for some custom dataset for\nwhich no training data is available.\n  In this work, we show that binary embedding models like BPR and JPQ can\nperform significantly worse than baselines once there is a domain-shift\ninvolved. We propose a modification to the training procedure of BPR and JPQ\nand combine it with a corpus specific generative procedure which allow the\nadaptation of BPR and JPQ to any corpus without requiring labeled training\ndata. Our domain-adapted strategy known as GPL is model agnostic, achieves an\nimprovement by up-to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark\nin comparison to BPR and JPQ while maintaining its 32x memory efficiency.\nJPQ+GPL even outperforms our upper baseline: uncompressed TAS-B model on\naverage by 2.0 points.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  RISTRETTO is a visible high-resolution spectrograph fed by an extreme\nadaptive optics (XAO) system, to be proposed as a visitor instrument on ESO\nVLT. The main science goal of RISTRETTO is the detection and atmospheric\ncharacterization of exoplanets in reflected light, in particular the temperate\nrocky planet Proxima b. RISTRETTO will be able to measure albedos and detect\natmospheric features in a number of exoplanets orbiting nearby stars for the\nfirst time. It will do so by combining a high-contrast AO system working at the\ndiffraction limit of the telescope to a high-resolution spectrograph, via a\n7-spaxel integral-field unit (IFU) feeding single-mode fibers. Further science\ncases for RISTRETTO include the study of accreting protoplanets such as PDS 70\nb & c through spectrally-resolved H-alpha emission; and spatially-resolved\nstudies of Solar System objects such as icy moons and the ice giants Uranus and\nNeptune. The project is in an advanced design phase for the spectrograph and\nIFU/fiber-link sub-systems, and a preliminary design phase for the AO\nfront-end. Construction of the spectrograph and IFU/fiber-link will start at\nthe end of 2022. RISTRETTO is a pathfinder instrument in view of similar\ndevelopments at ESO ELT, in particular the SCAO-IFU mode of ELT-ANDES and the\nfuture ELT-PCS instrument.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The symmetry of the whole experimental setups, including specific sample\nenvironments and measurables, can be compared with that of specimens for\nobservable physical phenomena. We, first, focus on one-dimensional (1D)\nexperimental setups, independent from any spatial rotation around one\ndirection, and show that eight kinds of 1D objects (four; vectorlike, the other\nfour; director-like), defined in terms of symmetry, and their dot and cross\nproducts are an effective way for the symmetry consideration. The dot products\nform a Z2xZ2xZ2 group with Abelian additive operation, and the cross products\nform a Z2xZ2 group with Abelian additive operation or Q8, a non-abelian group\nof order eight, depending on their signs. Those 1D objects are associated with\ncharacteristic physical phenomena. When a 3D specimen has Symmetry Operational\nSimilarity (SOS) with (identical or lower, but not higher, symmetries than) an\n1D object with a particular phenomenon, the 3D specimen can exhibit the\nphenomenon. This SOS approach can be a transformative and unconventional avenue\nfor symmetry-guided materials designs and discoveries.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Increasing anthropogenic carbon dioxide (CO$_2$) emissions have led to rising\nglobal temperatures and climate change. Using earth-abundant metal-oxide\ncatalysts such as Cu$_2$O for reducing CO$_2$ through RWGS reaction seems\nlucrative. In this work, we have used Cu$_2$O nanostructures and identified its\nactivity, stability, and selectivity for reducing CO$_2$ to carbon monoxide\n(CO) which can be further hydrogenated to higher hydrocarbons using Fisher\nTropsch synthesis. We have observed that the rate of CO$_2$ conversion\nincreases by 4 times and significantly drops at 300 C where the catalyst was\nreduced to metallic Cu and the rate increases slightly as the temperature is\nfurther increased. The selectivity of CO$_2$ reduction is majorly towards CO\nwith a trace amount of methane. We can further exploit the Mie resonance\ncharacteristics of Cu$_2$O nanocatalysts and in-situ generation of hydrogen for\nhydrogenation of CO$_2$ to enhance the activity of the catalysts. We can\nfurther identify the optimum size and shape of the nanocatalysts required and\nuse hybrid nanostructures which can favor RWGS reaction thus improving the\nstability of these catalysts.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper addresses issues related to the process of informal socialization\ninto physics, particularly for senior graduate students and postdoctoral\nscholars. Many physicists' careers are built on the relationships they have and\ndevelop during these critical years.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study scattering amplitudes in the shadow conformal primary basis, which\nsatisfies the same defining properties as the original conformal primary basis\nand has many advantages over it. The shadow celestial amplitudes exhibit\nlocality manifestly on the celestial sphere, and behave like correlation\nfunctions in conformal field theory under the operator product expansion (OPE)\nlimit. We study the OPE limits for three-point shadow celestial amplitude, and\ngeneral $2\\to n-2$ shadow celestial amplitudes from a large class of Feynman\ndiagrams. In particular, we compute the conformal block expansion of the\n$s$-channel four-point shadow celestial amplitude of massless scalars at\ntree-level, and show that the expansion coefficients factorize as products of\nOPE coefficients.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many effective Threat Analysis (TA) techniques exist that focus on analyzing\nthreats to targeted assets (e.g., components, services). These techniques\nconsider static interconnections among the assets. However, in dynamic\nenvironments, such as the Cloud, resources can instantiate, migrate across\nphysical hosts, or decommission to provide rapid resource elasticity to the\nusers. It is evident that existing TA techniques cannot address all these\nrequirements. In addition, there is an increasing number of complex\nmulti-layer/multi-asset attacks on Cloud systems, such as the Equifax data\nbreach. Hence, there is a need for threat analysis approaches that are designed\nto analyze threats in complex, dynamic, and multi-layer Cloud environments. In\nthis paper, we propose ThreatPro that addresses the analysis of multi-layer\nattacks and supports dynamic interconnections in the Cloud. ThreatPro\nfacilitates threat analysis by developing a technology-agnostic information\nflow model, which represents the Cloud's functionality through a set of\nconditional transitions. The model establishes the basis to capture the\nmulti-layer and dynamic interconnections during the life-cycle of a Virtual\nMachine (VM). Specifically, ThreatPro contributes in (a) enabling the\nexploration of a threat's behavior and its propagation across the Cloud, and\n(b) assessing the security of the Cloud by analyzing the impact of multiple\nthreats across various operational layers/assets. Using public information on\nthreats from the National Vulnerability Database (NVD), we validate ThreatPro's\ncapabilities, i.e., (a) identify and trace actual Cloud attacks and (b)\nspeculatively postulate alternate potential attack paths.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we analytically design a simple configuration of a broadband\nTHz and polarization-insensitive absorber. The mentioned absorber consists of\ntwo layers of graphene disks, and the transmission line model is considered for\nthe whole of the proposed absorber's structure to design it accurately.\nTherefore, the input admittance of the designed absorber is obtained by the\ntransmission line model. Also, the real part of the input admittance is\napproximately tuned to be matched to the free space admittance. In contrast,\nthe imaginary part of it is closely adjusted to zero around the central\nfrequency of the THz absorber. Using only just two layers of Periodic Arrays of\nGraphene Disks (PAGDs) with one kind of dielectric as the material of\nsubstrates, it causes that the absorption of the structure can be achieved\nhigher than 90 % by the Finite Element Method (FEM). Normalized bandwidth has\nreached up to 75.4% in 5 THz as the central frequency of the device. As the\nnext step, we use the CST studio software to validate our designed absorber,\nand it will show that the numerical results will have the best matching with\nthe analytical method.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Electrocardiograms (ECG) analysis is one of the most important ways to\ndiagnose heart disease. This paper proposes an efficient ECG classification\nmethod based on Wasserstein scalar curvature to comprehend the connection\nbetween heart disease and mathematical characteristics of ECG. The newly\nproposed method converts an ECG into a point cloud on the family of Gaussian\ndistribution, where the pathological characteristics of ECG will be extracted\nby the Wasserstein geometric structure of the statistical manifold.\nTechnically, this paper defines the histogram dispersion of Wasserstein scalar\ncurvature, which can accurately describe the divergence between different heart\ndiseases. By combining medical experience with mathematical ideas from geometry\nand data science, this paper provides a feasible algorithm for the new method,\nand the theoretical analysis of the algorithm is carried out. Digital\nexperiments on the classical database with large samples show the new\nalgorithm's accuracy and efficiency when dealing with the classification of\nheart disease.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Empowering agents with a compositional understanding of their environment is\na promising next step toward solving long-horizon planning problems. On the one\nhand, we have seen encouraging progress on variational inference algorithms for\nobtaining sets of object-centric latent representations (\"slots\") from\nunstructured scene observations. On the other hand, generating scenes from\nslots has received less attention, in part because it is complicated by the\nlack of a canonical object order. A canonical object order is useful for\nlearning the object correlations necessary to generate physically plausible\nscenes similar to how raster scan order facilitates learning pixel correlations\nfor pixel-level autoregressive image generation. In this work, we address this\nlack by learning a fixed object order for a hierarchical variational\nautoencoder with a single level of autoregressive slots and a global scene\nprior. We cast autoregressive slot inference as a set-to-sequence modeling\nproblem. We introduce an auxiliary loss to train the slot prior to generate\nobjects in a fixed order. During inference, we align a set of inferred slots to\nthe object order obtained from a slot prior rollout. To ensure the rolled out\nobjects are meaningful for the given scene, we condition the prior on an\ninferred global summary of the input. Experiments on compositional environments\nand ablations demonstrate that our model with global prior, inference with\naligned slot order, and auxiliary loss achieves state-of-the-art sample\nquality.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Meditation, or mindfulness, is widely used to improve mental health. With the\nemergence of Virtual Reality technology, many studies have provided evidence\nthat meditation with VR can bring health benefits. However, to our knowledge,\nthere are no guidelines and comprehensive reviews in the literature on how to\nconduct such research in virtual reality. In order to understand the role of VR\ntechnology in meditation and future research opportunities, we conducted a\nsystematic literature review in the IEEE and ACM databases. Our process yielded\n19 eligible papers and we conducted a structured analysis. We understand the\nstate-of-art of meditation type, design consideration and VR and technology\nthrough these papers and conclude research opportunities and challenges for the\nfuture.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $\\mathfrak{g}$ be a simple finite dimensional Lie algebra of type $A_n$\n($n \\geqslant 2$), $D_n$ ($n \\geqslant 4$) or $E_6$, $E_7$, $E_8$, and let\n$\\hat{\\mathfrak{g}}$ be the corresponding affine Lie algebra. Kac and Wakimoto\nobserved that in some cases the coefficients in the character formula for a\nsimple highest weight $\\hat{\\mathfrak{g}}$-module are either bounded or are\ngiven by a linear function of the weight. We explain and generalize this\nobservation by means of Kazhdan-Lusztig theory, namely, by computing values at\n$q=1$ of certain (parabolic) affine inverse Kazhdan-Lusztig polynomials. The\ncalculation relies on the explicit description of the canonical basis in the\ncell quotient of the anti-spherical module over the affine Hecke algebra\ncorresponding to the subregular cell. We also present an explicit description\nof the corresponding objects in the derived category of equivariant coherent\nsheaves on the Springer resolution, they correspond to irreducible objects in\nthe heart of a certain $t$-structure related to the so called non-commutative\nSpringer resolution.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent Language Models (LMs) achieve breakthrough performance in code\ngeneration when trained on human-authored problems, even solving some\ncompetitive-programming problems. Self-play has proven useful in games such as\nGo, and thus it is natural to ask whether LMs can generate their own\ninstructive programming problems to improve their performance. We show that it\nis possible for an LM to synthesize programming problems and solutions, which\nare filtered for correctness by a Python interpreter. The LM's performance is\nthen seen to improve when it is fine-tuned on its own synthetic problems and\nverified solutions; thus the model 'improves itself' using the Python\ninterpreter. Problems are specified formally as programming puzzles [Schuster\net al., 2021], a code-based problem format where solutions can easily be\nverified for correctness by execution. In experiments on publicly-available\nLMs, test accuracy more than doubles. This work demonstrates the potential for\ncode LMs, with an interpreter, to generate instructive problems and improve\ntheir own performance.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work we have explored the imaginary part of the Heavy Quark (HQ)\npotential and subsequently the dissociation of heavy quarkonia at finite\ntemperature and magnetic field. With respect to earlier investigations on this\ntopic, present work contain three new ingredients. First one is considering all\nLandau level summation, for which present work can be applicable in entire\nmagnetic field domain - from weak to strong. Second one is the general\nstructure of the gauge boson propagator in a hot magnetized medium, which is\nused here in heavy quark potential problem first time. Third one is a rich\nanisotropic structure of the complex heavy quark potential, which explicitly\ndepends on the longitudinal and transverse distance. By comparing with earlier\nreferences, we have attempted to display our new contributions by plotting\nheavy quark potential tomography and dissociation probability at finite\ntemperature and magnetic field.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider finite-dimensional systems of linear stochastic differential\nequations ${\\partial_t}{x_k}\\left( t \\right) = {A_{kp}}\\left( t\n\\right){x_p}\\left( t \\right)$, ${\\bf A}(t)$ being a stationary continuous\nstatistically isotropic stochastic process with values in real $d \\times d$\nmatrices. We suppose also that the laws of ${\\bf A}(t)$ satisfy the large\ndeviation principle. For these systems, we find exact expressions for the\nLyapunov and generalized Lyapunov exponents and show that they are determined\nin a precise way only by the rate function of the diagonal elements of ${\\bf\nA}$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The channel coding problem in the moderate deviations regime is studied;\nhere, the error probability decays sub-exponentially to zero, and the rate\napproaches the capacity slower than $O(1/\\sqrt{n})$. Our result refines\nAltu\\u{g} and Wagner's moderate deviations result by deriving lower and upper\nbounds on the third-order term in the asymptotic expansion of the maximum\nachievable message set size. The third-order term of our expansion employs a\nnew quantity here called the channel skewness. For the binary symmetric channel\nand most practically important $(n, \\epsilon)$ pairs, including $n \\in [100,\n500]$ and $\\epsilon \\in [10^{-10}, 10^{-1}]$, an approximation up to the\nchannel skewness is the most accurate among several expansions in the\nliterature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a computational database with calculated structural,\nthermodynamic, electronic, magnetic, and optical properties of 820\none-dimensional materials. The materials are systematically selected and\nexfoliated from experimental databases of crystal structures based on a\ndimensionality scoring parameter. The database is furthermore expanded by\nchemical element substitution in the materials. The materials are investigated\nin both their bulk form and as isolated one-dimensional components. We discuss\nthe methodology behind the database, give an overview of some of the calculated\nproperties, and look at patterns and correlations in the data. The database is\nfurthermore applied in computational screening to identify materials, which\ncould exhibit Majorana bound states.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Optically active spin defects in van der Waals materials are promising\nplatforms for modern quantum technologies. Here we investigate the coherent\ndynamics of strongly interacting ensembles of negatively charged boron-vacancy\n($\\mathrm{V}_{\\mathrm{B}}^-$) centers in hexagonal boron nitride (hBN) with\nvarying defect density. By employing advanced dynamical decoupling sequences to\nselectively isolate different dephasing sources, we observe more than 5-fold\nimprovement in the measured coherence times across all hBN samples. Crucially,\nwe identify that the many-body interaction within the\n$\\mathrm{V}_{\\mathrm{B}}^-$ ensemble plays a substantial role in the coherent\ndynamics, which is then used to directly determine the precise concentration of\n$\\mathrm{V}_{\\mathrm{B}}^-$. We find that at high ion implantation dosage, only\n$\\lesssim5~\\%$ of the created boron vacancy defects are in the desired\nnegatively charged state. Finally, we investigate the spin response of\n$\\mathrm{V}_{\\mathrm{B}}^-$ to the local charged defects induced electric field\nsignals, and estimate its transverse electric field susceptibility. Our results\nprovide new insights on the spin and charge properties of\n$\\mathrm{V}_{\\mathrm{B}}^-$, which are important for future use of defects in\nhBN as quantum sensors and simulators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $\\{E_{(p,q)}\\}$ be a family of elliptic curves over a rational field such\nthat we have $E_{(p,q)} : y^2 = x^3 - p^2x + q^2$, where $p$ and $q$ are prime\nnumbers greater than five. Earlier work showed that the elliptic curve\n$E_{(p,q)}$ had ranked at least two for all $p, q > 5$ and two independent\npoints. This paper shows that two points that can be extended to a basis for\n$E_{(p,q)}$ under conditions are confident that we will fully recover.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Displacement is an important measurement for the assessment of structural\nconditions, but its field measurement is often hindered by difficulties\nassociated with sensor installation and measurement accuracy. To overcome the\ndisadvantages of conventional displacement measurement, computer vision\n(CV)-based methods have been implemented due to their remote sensing\ncapabilities and accuracy. This paper presents a strategy for non-target\nstructural displacement measurement that makes use of CV to avoid the need to\ninstall a target on the structure while calibrating the displacement using\nstructured light. The proposed system called as LAVOLUTION calculates the\nrelative position of the camera with regard to the structure using four equally\nspaced beams of structured light and obtains a scale factor to convert pixel\nmovement into structural displacement. A jig for the four beams of structured\nlight is designed and a corresponding alignment process is proposed. A method\nfor calculating the scale factor using the designed jig for tunable\nstructured-light is proposed and validated via numerical simulations and\nlab-scale experiments. To confirm the feasibility of the proposed displacement\nmeasurement process, experiments on a shaking table and a full-scale bridge are\nconducted and the accuracy of the proposed method is compared with that of a\nreference laser doppler vibrometer.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Entanglement-based quantum key distribution can enable secure communication\nin trusted node-free networks and over long distances. Although implementations\nexist both in fiber and in free space, the latter approach is often considered\nchallenging due to environmental factors. Here, we implement a quantum\ncommunication protocol during daytime for the first time using a quantum dot\nsource. This technology presents advantages in terms of narrower spectral\nbandwidth -- beneficial for filtering out sunlight -- and negligible\nmultiphoton emission at peak brightness. We demonstrate continuous operation\nover the course of three and a half days, across an urban 270-m-long free-space\noptical link, under different light and weather conditions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Robots operating in human environments must be able to rearrange objects into\nsemantically-meaningful configurations, even if these objects are previously\nunseen. In this work, we focus on the problem of building physically-valid\nstructures without step-by-step instructions. We propose StructDiffusion, which\ncombines a diffusion model and an object-centric transformer to construct\nstructures out of a single RGB-D image based on high-level language goals, such\nas \"set the table.\" Our method shows how diffusion models can be used for\ncomplex multi-step 3D planning tasks. StructDiffusion improves success rate on\nassembling physically-valid structures out of unseen objects by on average 16%\nover an existing multi-modal transformer model, while allowing us to use one\nmulti-task model to produce a wider range of different structures. We show\nexperiments on held-out objects in both simulation and on real-world\nrearrangement tasks. For videos and additional results, check out our website:\nhttp://weiyuliu.com/StructDiffusion/.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we have took reissner nordstrom blackhole with cloud of strings\nand surrounds it with quintessence. we processed the metric through newman\njanis algorithm to get its rotating counterpart. the blackhole in study now is\na roating charged blackhole with clouds of string surrounded by quintessence.\nwe studied its nature of effective potential and unstable photon orbits.\nFinally we have plotted the blackhole shadow for various variable profiles.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Elastomers swollen with non-polar fluids show potential as anti-adhesive\nmaterials. We study the effect of oil fraction and contact time on the adhesion\nbetween swollen spherical probes of PDMS (polydimethylsiloxane) and flat glass\nsurfaces. The PDMS probes are swollen with pre-determined amount of 10 cSt\nsilicone oil to span the range where the PDMS is fluid free (via solvent\nextraction) up to the limit where it is oil saturated. Probe tack measurements\nshow that adhesion decreases rapidly with an increase in oil fraction. The\ndecrease in adhesion is attributed to excess oil present at the PDMS-air\ninterface. Contact angle measurements and optical microscopy images support\nthis observation. Adhesion also increases with contact time for a given oil\nfraction. The increase in adhesion with contact time can be interpreted through\ndifferent competing mechanisms that depend on the oil fraction where the\ndominant mechanism changes from extracted to fully swollen PDMS. For partially\nswollen PDMS, we observe that adhesion initially increases because of\nviscoelastic relaxation and at long times increases because of contact aging.\nIn contrast, adhesion between fully swollen PDMS and glass barely increases\nover time and is mainly due to capillary forces. While the relaxation of PDMS\nin contact is well-described by a visco-poroelastic model, we do not see\nevidence that poroelastic relaxation of the PDMS contributes to an increase of\nadhesion with glass whether it is partially or fully swollen.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The time series captured by a single scalp electrode (plus the reference\nelectrode) of refractory epileptic patients is used to forecast seizures\nsusceptibility. The time series is preprocessed, segmented, and each segment\ntransformed into an image, using three different known methods: Recurrence\nPlot, Gramian Angular Field, Markov Transition Field. The likelihood of the\noccurrence of a seizure in a future predefined time window is computed by\naveraging the output of the softmax layer of a CNN, differently from the usual\nconsideration of the output of the classification layer. By thresholding this\nlikelihood, seizure forecasting has better performance. Interestingly, for\nalmost every patient, the best threshold was different from 50%. The results\nshow that this technique can predict with good results for some seizures and\npatients. However, more tests, namely more patients and more seizures, are\nneeded to better understand the real potential of this technique.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It is known that the nuclear-spin-dependent parity-violating contributions to\nthe NMR shielding and the nuclear spin-rotation tensors (${\\bf \\sigma}^{PV}$\nand ${\\bf M}^{PV}$, respectively) are formally related each other within the\nnon-relativistic (NR) regime. Such a formal relationship is not any longer\nvalid within the relativistic domain. A new more general formal relationship,\nthat is valid within the relativistic framework is shown here, being developed\nthrough the use of the LRESC model. The formalism of polarization propagators\nis applied to write the different contributions to both properties within both\nregimes, relativistic and NR. In the relativistic regime the Dirac-Coulomb\nHamiltonian was selected as the unperturbed Hamiltonian. Theoretical\ndevelopments together with results of calculations performed on the H$_2X_2$\nseries of molecules ($X =$ $^{17}$O, $^{33}$S, $^{77}$Se, $^{125}$Te and\n$^{209}$Po) show that also within the relativistic regime there is a close\nrelationship between the parity-violation contributions to both properties. In\nparticular, spin-dependent contributions are the most important in the\nfour-component calculations of electroweak effects on the isotropic values of\nboth tensors, ${\\bf \\sigma}^{PV}$ and ${\\bf M}^{PV}$, being also responsible\nfor the breakdown of the previously mentioned NR relationship among them. This\nlast relationship is still fulfilled when the scalar-relativistic effects are\nconsidered.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep Reinforcement Learning (DRL) is being used in many domains. One of the\nbiggest advantages of DRL is that it enables the continuous improvement of a\nlearning agent. Secondly, the DRL framework is robust and flexible enough to be\napplicable to problems of varying nature and domain. Presented work is evidence\nof using the DRL technique to solve an Optimal Power Flow (OPF) problem. Two\nclassical algorithms have been presented to solve the OPF problem. The\ndrawbacks of the vanilla DRL application are discussed, and an algorithm is\nsuggested to improve the performance. Secondly, a reward function for the OPF\nproblem is presented that enables the solution of inherent issues in DRL.\nReasons for divergence and degeneration in DRL are discussed, and the correct\nstrategy to deal with them with respect to OPF is presented.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Out of the many exciting results obtained with the lattice approach to QCD\nunder extreme conditions, I discuss a few selected items related to chiral\nsymmetry: the chiral condensate as an approximate order parameter, meson\nscreening masses, and masses of baryons and mesons, including D(s) mesons, when\napproaching the crossover from the hadronic side.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  An accurate and detailed field map is important for cyclotron beam dynamics\nstudies. During the long history of cyclotron studies, many techniques have\nbeen developed by cyclotron pioneers for the treatment of median plane field\nmap. In this paper, we take the TRIUMF 500 MeV cyclotron as an example to study\nthe asymmetric field resulting from the imperfect median plane symmetry. The\n``Gordon approach'' and a highly accurate compact finite differentiation method\nare used to investigate the historical field survey data. The redundancy in the\nsurvey data is revealed by the expansion method, which also makes it possible\nto correct the error in the measurement. Finally, both the azimuthal field\n$B_\\theta$ and the axial gradient of the axial field $dB_z/dz$ in the median\nplane are corrected using the radial field map $B_r$. The influence of the\ncorrection is examined by recalculating the equilibrium orbit properties of the\nTRIUMF cyclotron. The result shows significantly increased vertical centering\nerrors of the closed orbits. Further simulation study suggests that these\ncentering errors can be reduced to below 1.5 cm by adjusting the trim coils'\n$B_r$ field within the output limits of our trim coils' power supplies. The\nerror in the measurement field data may explain why the calculated trim coils'\nsettings during the cyclotron commissioning in 1974 encountered difficulty.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Foucaud, Krishna and Lekshmi recently introduced the concept of monitoring\nedge-geodetic sets in graphs, and a related graph invariant. These are sets of\nvertices such that the removal of any edge changes the distance between some\npair of vertices in the set. They studied the minimum possible size of such a\nset in a given graph, which we call the monitoring edge-geodetic number.\n  We show that the decision problem for the monitoring edge-geodetic number is\nNP-complete. We also give best-possible upper and lower bounds for the\nCartesian and strong products of two graphs. These bounds establish the exact\nvalue in many cases, including many new examples of graphs whose only\nmonitoring edge-geodetic set is the whole vertex set.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We formulate the task of detecting lines and paragraphs in a document into a\nunified two-level clustering problem. Given a set of text detection boxes that\nroughly correspond to words, a text line is a cluster of boxes and a paragraph\nis a cluster of lines. These clusters form a two-level tree that represents a\nmajor part of the layout of a document. We use a graph convolutional network to\npredict the relations between text detection boxes and then build both levels\nof clusters from these predictions. Experimentally, we demonstrate that the\nunified approach can be highly efficient while still achieving state-of-the-art\nquality for detecting paragraphs in public benchmarks and real-world images.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper presents a novel ML-based methodology for geothermal exploration\ntowards PFA applications. Our methodology is provided through our open-source\nML framework, GeoThermalCloud\n\\url{https://github.com/SmartTensors/GeoThermalCloud.jl}. The GeoThermalCloud\nuses a series of unsupervised, supervised, and physics-informed ML methods\navailable in SmartTensors AI platform \\url{https://github.com/SmartTensors}.\nHere, the presented analyses are performed using our unsupervised ML algorithm\ncalled NMF$k$, which is available in the SmartTensors AI platform. Our ML\nalgorithm facilitates the discovery of new phenomena, hidden patterns, and\nmechanisms that helps us to make informed decisions. Moreover, the\nGeoThermalCloud enhances the collected PFA data and discovers signatures\nrepresentative of geothermal resources. Through GeoThermalCloud, we could\nidentify hidden patterns in the geothermal field data needed to discover blind\nsystems efficiently. Crucial geothermal signatures often overlooked in\ntraditional PFA are extracted using the GeoThermalCloud and analyzed by the\nsubject matter experts to provide ML-enhanced PFA, which is informative for\nefficient exploration. We applied our ML methodology to various open-source\ngeothermal datasets within the U.S. (some of these are collected by past PFA\nwork). The results provide valuable insights into resource types within those\nregions. This ML-enhanced workflow makes the GeoThermalCloud attractive for the\ngeothermal community to improve existing datasets and extract valuable\ninformation often unnoticed during geothermal exploration.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Cosmic background neutrinos ($C_{\\nu}B)$ helicity composition is different\nfor Dirac or Majorana neutrinos making detectors based on $C_{\\nu}B$ capture\nsensitive to the nature of neutrinos. We calculate, for the first time, the\nhelicity changes of neutrinos crossing dark matter fields, to quantitatively\ncalculate this effect on the capture rate. We show that a fraction of neutrinos\nchange their helicity, regardless of them being deflected by a void or a dark\nmatter halo. The average signal from the 100 most massive voids or halos in a\nGpc$^3$ gives a prediction that if neutrinos are Dirac, the density of the\n$C_{\\nu} B$ background measured on Earth should be 48 cm${^{-3}}$ for\nleft-helical neutrinos, a decrease of 15% (53.6 cm${^{-3}}$; 5%) for a halo\n(void) with respect to the standard calculation without including gravitational\neffects due to large scale structures. In terms of the total capture rate in a\n100 g tritium detector, this translates in $4.9^{+1.1}_{-0.8}$ neutrinos per\nyear for the Dirac case, as a function of the unknown neutrino mass scale, or\n8.1 per year if neutrinos are Majorana. Thus although smaller than the factor\ntwo for the non-relativistic case, it is still large enough to be detected and\nit highlights the power of future $C_{\\nu} B$ detectors, as an alternative to\nneutrinoless double beta decay experiments, to discover the neutrino nature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Sintering refers to particle coalescence by heat, which has been known as a\nthermal phenomenon involving all aspects of natural science for centuries. It\nis particularly important in heterogeneous catalysis because normally sintering\nresults in deactivation of the catalysts. In previous studies, the enthalpy\ncontribution was considered to be dominant in sintering and the entropy effect\nis generally considered neglectable. However, we unambiguously demonstrate in\nthis work that entropy could prevail over the enthalpy contribution to dominate\nthe sintering behavior of supported nanoparticles (NPs) by designed experiments\nand improved theoretical framework. Using in situ Cs-corrected environmental\nscanning transmission electron microscopy and synchrotron-based ambient\npressure X-ray photoelectron spectroscopy, we observe the unprecedent\nentropy-driven phenomenon that supported NPs reversibly redisperse upon heating\nand sinter upon cooling in three systems (Pd-CeO2, Cu-TiO2, Ag-TiO2). We\nquantitatively show that the configurational entropy of highly dispersed\nad-atoms is large enough to reverse their sintering tendency at the elevated\ntemperature. This work reshapes the basic understanding of sintering at the\nnanoscale and opens the door for various de-novo designs of thermodynamically\nstable nanocatalysts.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this article, we explore the cooling of isolated quark stars. These\nobjects are structured of a homogeneous quark matter core and crusted by\nmatter. To do this, we adopt two kinds of crust: (i) a crust made of purely\nnuclear matter following the Baym-Pethick-Sutherland (BPS) equation of state\n(EoS) and (ii) a crust made of nuggets of strange quark matter (strangelets).\nBoth models have the same quark matter core described by the MIT bag model EoS.\nOur main purpose is to quantify the effects of a strangelet crust on the\ncooling and relaxation times of these strange stars. We also perform a thorough\nstudy of the thermal relaxation of quark stars, in which we have found that\nobjects with a strangelet crust have a significantly different thermal\nrelaxation time. Our study also includes the possible effects of color\nsuperconductivity in the quark core.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In order to develop complex relationships between their inputs and outputs,\ndeep neural networks train and adjust large number of parameters. To make these\nnetworks work at high accuracy, vast amounts of data are needed. Sometimes,\nhowever, the quantity of data needed is not present or obtainable for training.\nNeuron-specific dropout (NSDropout) is a tool to address this problem.\nNSDropout looks at both the training pass, and validation pass, of a layer in a\nmodel. By comparing the average values produced by each neuron for each class\nin a data set, the network is able to drop targeted units. The layer is able to\npredict what features, or noise, the model is looking at during testing that\nisn't present when looking at samples from validation. Unlike dropout, the\n\"thinned\" networks cannot be \"unthinned\" for testing. Neuron-specific dropout\nhas proved to achieve similar, if not better, testing accuracy with far less\ndata than traditional methods including dropout and other regularization\nmethods. Experimentation has shown that neuron-specific dropout reduces the\nchance of a network overfitting and reduces the need for large training samples\non supervised learning tasks in image recognition, all while producing\nbest-in-class results.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In recent years, many applications have deployed incentive mechanisms to\npromote users' attention and engagement. Most incentive mechanisms determine\nspecific incentive values based on users' attributes (e.g., preferences), while\nsuch information is unavailable in many real-world applications. Meanwhile, due\nto budget restrictions, realizing successful incentivization for all users can\nbe challenging to complete. In this light, we consider leveraging social\ninfluence to maximize the incentivization result. We can directly incentivize\ninfluential users to affect more users, so the cost of incentivizing these\nusers can be decreased. However, identifying influential users in a social\nnetwork requires complete information about influence strength among users,\nwhich is impractical to acquire in real-world situations. In this research, we\npropose an end-to-end reinforcement learning-based framework, called Geometric\nActor-Critic (GAC), to tackle the abovementioned problem. The proposed approach\ncan realize effective incentive allocation without having prior knowledge about\nusers' attributes. Three real-world social network datasets have been adopted\nin the experiments to evaluate the performance of GAC. The experimental results\nindicate that GAC can learn and apply effective incentive allocation policies\nin unknown social networks and outperform existing incentive allocation\napproaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The observability of Lyman-alpha emitting galaxies (LAEs) during the Epoch of\nReionization can provide a sensitive probe of the evolving neutral hydrogen gas\ndistribution, thus setting valuable constraints to distinguish different\nreionization models. In this study, we utilize the new THESAN suite of\nlarge-volume (95.5 cMpc) cosmological radiation-hydrodynamic simulations to\ndirectly model the Ly$\\alpha$ emission from individual galaxies and the\nsubsequent transmission through the intergalactic medium. THESAN combines the\nAREPO-RT radiation-hydrodynamic solver with the IllustrisTNG galaxy formation\nmodel and includes high- and medium-resolution simulations designed to\ninvestigate the impacts of halo-mass-dependent escape fractions, alternative\ndark matter models, and numerical convergence. We find important differences in\nthe Ly$\\alpha$ transmission based on reionization history, bubble morphology,\nfrequency offset from line centre, and galaxy brightness. For a given global\nneutral fraction, Ly$\\alpha$ transmission reduces slightly when low mass haloes\ndominate reionization over high mass haloes. Furthermore, the variation across\nsightlines for a single galaxy is greater than the variation across all\ngalaxies. This collectively affects the visibility of LAEs, directly impacting\nobserved Ly$\\alpha$ luminosity functions (LFs). We employ Gaussian Process\nRegression using SWIFTEmulator to rapidly constrain an empirical model for dust\nescape fractions and emergent spectral line profiles to match observed LFs. We\nfind that dust strongly impacts the Ly$\\alpha$ transmission and covering\nfractions of $M_{UV} < -19$ galaxies in $M_{vir} > 10^{11} {\\rm M}_{\\odot}$\nhaloes, such that the dominant mode of removing Ly$\\alpha$ photons in non-LAEs\nchanges from low IGM transmission to high dust absorption around $z \\sim 7$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  MCL-1 and its natural inhibitors, the BH3-only proteins PUMA, BIM, and NOXA\nregulate apoptosis by interacting promiscuously within an entangled binding\nnetwork. Little is known about the transient processes and dynamic\nconformational fluctuations that are the basis for the formation and stability\nof the MCL-1/BH3-only complex. In this study, we designed photoswitchable\nversions of MCL-1/PUMA and MCL-1/NOXA, and investigated the protein response\nafter an ultrafast photo-perturbation with transient infrared spectroscopy. We\nobserved partial $\\alpha$-helical unfolding in all cases, albeit on strongly\nvarying timescales (1.6~ns for PUMA, 9.7~ns for the previously studied BIM, and\n85~ns for NOXA). These differences are interpreted as a BH3-only-specific\n\"structural resilience\" to defy the perturbation while remaining in MCL-1's\nbinding pocket. Thus, the presented insights could help to better understand\nthe differences between PUMA, BIM, and NOXA, the promiscuity of MCL-1 in\ngeneral, and the role of the proteins in the apoptotic network.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In dimension N=3 the cubic nonlinear Schrodinger equation has solutions which\nbecome singular, i.e. at a spatial point they blow up to infinity in finite\ntime. In 1972 Zakharov famously investigated finite time singularity formation\nin the cubic nonlinear Schrodinger equation as a model for spatial collapse of\nLangmuir waves in plasma, the most abundant form of observed matter in the\nuniverse. Zakharov assumed that (NLS) blow up of solutions is self-similar and\nradially symmetric, and that singularity formation can be modeled by a solution\nof an associated self-similar, complex ordinary differential equation~(ODE). A\nparameter a>0 appears in the ODE, and the dependent variable, Q, satisfies\n(Q(0),Q'(0))=(Q_{0},0), where Q(0)>0. A fundamentally important step towards\nputting the Zakharov model on a firm mathematical footing is to prove, when\nN=3, whether values a>0 and Q_{0}>0 exist such that Q also satisfies the\nphysically important `zero-energy' integral constraint. Since 1972 this has\nremained an open problem. Here, we resolve this issue by proving that for every\na>0 and Q(0)>0, Q satisfies the the `zero-energy' integral constraint.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a possible modification to the tensionless string model with\ncontact interactions. The proposed model aims to reproduce an expectation value\nof the non-Abelian Wilson loop in the Yang-Mills theory when integrating out\nstring degrees of freedom with a fixed worldsheet boundary. Lie algebra-valued\nfields whose dynamics are determined by the topological BF action are\nintroduced on the string worldsheet to reproduce path-ordering along the\nworldsheet boundary. Without bulk contributions, we show that the model\ndescribes the non-Abelian Wilson loop discarding effects of self-interactions.\nFinally, a reproduction of the Wilson loop with three-point interaction is\ntested in the case of $SU(2)$\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Objective: This study takes further step on understanding personality\nstructure in order to cope with the mental health during the COVID-19 global\npandemic situation. Methods: Categorized the independent variables into\nbiological, family and cultural predictors according to the datasets of the\nBig-5 personality survey online. And established multiple regression prediction\nmodels and exhaustive CHAID decision tree model of each personality trait.\nResults: Females are different from males in personality. The personality\nchanges when growing. One-handed dominants are less agreeable and open than\nthose who use both hands. Different sexual orientation does have variety\npersonality. Native language used and education attainment is significantly\nrelated to personality accordingly. Marriage did help shaping personality to be\nmore extroverted, less neurotic or agreeable and more conscientious and open.\nPeople raised in urban are more agreeable and open. Neurotic and open people\noften come from small families. person participated in voting are more\nextroverted, conscientious and open but less neurotic and agreeable. Different\nreligions and races have different characteristics in each dimension of\npersonality and there is no clear pattern have been found. Conclusion:\nPersonality traits are indeed affected by multiple confounding factors. but the\nexploration on multiple cultures predictors still needed more details\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Automated simplification models aim to make input texts more readable. Such\nmethods have the potential to make complex information accessible to a wider\naudience, e.g., providing access to recent medical literature which might\notherwise be impenetrable for a lay reader. However, such models risk\nintroducing errors into automatically simplified texts, for instance by\ninserting statements unsupported by the corresponding original text, or by\nomitting key information. Providing more readable but inaccurate versions of\ntexts may in many cases be worse than providing no such access at all. The\nproblem of factual accuracy (and the lack thereof) has received heightened\nattention in the context of summarization models, but the factuality of\nautomatically simplified texts has not been investigated. We introduce a\ntaxonomy of errors that we use to analyze both references drawn from standard\nsimplification datasets and state-of-the-art model outputs. We find that errors\noften appear in both that are not captured by existing evaluation metrics,\nmotivating a need for research into ensuring the factual accuracy of automated\nsimplification models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Neural networks are vulnerable to adversarial attacks: adding well-crafted,\nimperceptible perturbations to their input can modify their output. Adversarial\ntraining is one of the most effective approaches to training robust models\nagainst such attacks. Unfortunately, this method is much slower than vanilla\ntraining of neural networks since it needs to construct adversarial examples\nfor the entire training data at every iteration. By leveraging the theory of\ncoreset selection, we show how selecting a small subset of training data\nprovides a principled approach to reducing the time complexity of robust\ntraining. To this end, we first provide convergence guarantees for adversarial\ncoreset selection. In particular, we show that the convergence bound is\ndirectly related to how well our coresets can approximate the gradient computed\nover the entire training data. Motivated by our theoretical analysis, we\npropose using this gradient approximation error as our adversarial coreset\nselection objective to reduce the training set size effectively. Once built, we\nrun adversarial training over this subset of the training data. Unlike existing\nmethods, our approach can be adapted to a wide variety of training objectives,\nincluding TRADES, $\\ell_p$-PGD, and Perceptual Adversarial Training. We conduct\nextensive experiments to demonstrate that our approach speeds up adversarial\ntraining by 2-3 times while experiencing a slight degradation in the clean and\nrobust accuracy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A space-time crystal is defined as a quantum mechanical system with both\nspatial and temporal periodicity. Such a system can be described by the\nFloquet-Bloch (FB) theory. We first formulate a semiclassical theory by\nconstructing a wave-packet through the superposition of the FB wave functions\nand derive the equations of motion of FB electrons subjected to slowly varying\nexternal fields, revealing behaviors similar to ordinary Bloch electrons but\nwith quantities modified in the Floquet context. Specifically, we study local\nmagnetic moment due to the self-rotation of the wave-packet, a contribution to\ntotal magnetization from the Berry curvature in k-space, and the polarization\nof a fully occupied FB band. Based on the semiclassical theory, we can also\nshow the fingerprint of the energy flow in such an energy-non-conserved system.\nWe then discuss the density matrix of a FB system attached to a thermal bath,\nwhich allows us to investigate quantities involving many electrons in the\nnon-interacting limit. As an application, we calculate the intrinsic current\nresponse in an oblique spacetime metal showing the non-equilibrium nature of\nthe FB system. The current response can also be related to the acoustoelectric\neffect. Overall, we develop a systematic approach for studying space-time\ncrystals and provide a powerful tool to explore the electronic properties of\nthis exotic system with coupled space and time.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  One of the main promises of technology development is for it to be adopted by\npeople, organizations, societies, and governments -- incorporated into their\nlife, work stream, or processes. Often, this is socially beneficial as it\nautomates mundane tasks, frees up more time for other more important things, or\notherwise improves the lives of those who use the technology. However, these\nbeneficial results do not apply in every scenario and may not impact everyone\nin a system the same way. Sometimes a technology is developed which produces\nboth benefits and inflicts some harm. These harms may come at a higher cost to\nsome people than others, raising the question: {\\it how are benefits and harms\nweighed when deciding if and how a socially consequential technology gets\ndeveloped?} The most natural way to answer this question, and in fact how\npeople first approach it, is to compare the new technology to what used to\nexist. As such, in this work, I make comparative analyses between humans and\nmachines in three scenarios and seek to understand how sentiment about a\ntechnology, performance of that technology, and the impacts of that technology\ncombine to influence how one decides to answer my main research question.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we introduce a codification of the paths towards\nsynchronization for synchronizing flows defined over a network. The collection\nof paths toward synchronization defines a combinatorial structure: the\ntransition diagram. We describe the transition diagram corresponding to the\nLaplacian flow over the completely connected graph. This applies to the\nKuramoto flow over the same graph when initial conditions close to the diagonal\nare considered. We present as well some results concerning the Laplacian and\nKuramoto flows over the complete bipartite graph.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Prosody plays a vital role in verbal communication. Acoustic cues of prosody\nhave been examined extensively. However, prosodic characteristics are not only\nperceived auditorily, but also visually based on head and facial movements. The\npurpose of this report is to present a method for examining audiovisual prosody\nusing virtual reality. We show that animations based on a virtual human provide\nmotion cues similar to those obtained from video recordings of a real talker.\nThe use of virtual reality opens up new avenues for examining multimodal\neffects of verbal communication. We discuss the method in the framework of\nexamining prosody perception in cochlear implant listeners.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Knowledge distillation (KD) has been extensively employed to transfer the\nknowledge from a large teacher model to the smaller students, where the\nparameters of the teacher are fixed (or partially) during training. Recent\nstudies show that this mode may cause difficulties in knowledge transfer due to\nthe mismatched model capacities. To alleviate the mismatch problem,\nteacher-student joint training methods, e.g., online distillation, have been\nproposed, but it always requires expensive computational cost. In this paper,\nwe present a parameter-efficient and student-friendly knowledge distillation\nmethod, namely PESF-KD, to achieve efficient and sufficient knowledge transfer\nby updating relatively few partial parameters. Technically, we first\nmathematically formulate the mismatch as the sharpness gap between their\npredictive distributions, where we show such a gap can be narrowed with the\nappropriate smoothness of the soft label. Then, we introduce an adapter module\nfor the teacher and only update the adapter to obtain soft labels with\nappropriate smoothness. Experiments on a variety of benchmarks show that\nPESF-KD can significantly reduce the training cost while obtaining competitive\nresults compared to advanced online distillation methods. Code will be released\nupon acceptance.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Machine Learning (ML) methods have become a useful tool for tackling vehicle\nrouting problems, either in combination with popular heuristics or as\nstandalone models. However, current methods suffer from poor generalization\nwhen tackling problems of different sizes or different distributions. As a\nresult, ML in vehicle routing has witnessed an expansion phase with new\nmethodologies being created for particular problem instances that become\ninfeasible at larger problem sizes.\n  This paper aims at encouraging the consolidation of the field through\nunderstanding and improving current existing models, namely the attention model\nby Kool et al. We identify two discrepancy categories for VRP generalization.\nThe first is based on the differences that are inherent to the problems\nthemselves, and the second relates to architectural weaknesses that limit the\nmodel's ability to generalize. Our contribution becomes threefold: We first\ntarget model discrepancies by adapting the Kool et al. method and its loss\nfunction for Sparse Dynamic Attention based on the alpha-entmax activation. We\nthen target inherent differences through the use of a mixed instance training\nmethod that has been shown to outperform single instance training in certain\nscenarios. Finally, we introduce a framework for inference level data\naugmentation that improves performance by leveraging the model's lack of\ninvariance to rotation and dilation changes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The objective of this study is to propose a system-level framework with\nquantitative measures to assess the resilience of road networks. The framework\nproposed in this paper can help transportation agencies incorporate resilience\nconsiderations into project development proactively and to understand the\nresilience performance of current road networks effectively. This study\nidentified and implemented four quantitative metrics to classify the\ncriticality of road segments based on critical dimensions of road network\nresilience, and two integrated metrics were proposed to combine all metrics to\nshow the overall resilience performance of road segments. A case study was\nconducted on the Texas road networks to demonstrate the effectiveness of\nimplementing this framework in a practical scenario. Since the data used in\nthis study is available to other states and countries, the framework presented\nin this study can be adopted by other transportation agencies across the globe\nfor regional transportation resilience assessments.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Drug resistant bacteria, prions and nosocomial infections underline the need\nof more effective sterilizing technologies. The cold plasma technology is\nexpected to bring a benefit in this context. Six different plasma sources,\nbased on printed circuit boards, were evaluated fourfold. This include\nmeasurements of the power consumption, the ignition behavior by an ICCD-camera\nand ozone formation by absorption spectroscopy at 254 nm. To evaluate the\nbiocidal effect, four bacterial test series were performed with Escherichia\ncoli. The entirety of the tests analyze the plasma inactivation process from\nthe input parameters to the desired biocidal effect. The discharge current and\ntime resolved ignition behaviors indicated a simultaneous formation of\nfilaments at the beginning of the negative half-cycle. The dynamics of the\nozone production showed a saturated exponential growth upon a maximum value of\n435 ppm. Additionally, the microbiological test series unveiled differences\nbetween the plasma source concepts. A total reduction rate of Log-4 within a\nminute was achievable. An air flow through slits within the plasma sources\ndestabilized the plasma. Minor changes of the electrode geometry changed all\nmeasured parameters. Hence, to develop a pathogen inactivating plasma source,\nthese results recommend a comb-shaped electrode design, which is laminated on a\ndielectric.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The median of a graph $G$ with weighted vertices is the set of all vertices\n$x$ minimizing the sum of weighted distances from $x$ to the vertices of $G$.\nFor any integer $p\\ge 2$, we characterize the graphs in which, with respect to\nany non-negative weights, median sets always induce connected subgraphs in the\n$p$th power $G^p$ of $G$. This extends some characterizations of graphs with\nconnected medians (case $p=1$) provided by Bandelt and Chepoi (2002). The\ncharacteristic conditions can be tested in polynomial time for any $p$. We also\nshow that several important classes of graphs in metric graph theory, including\nbridged graphs (and thus chordal graphs), graphs with convex balls, bucolic\ngraphs, and bipartite absolute retracts, have $G^2$-connected medians.\nExtending the result of Bandelt and Chepoi that basis graphs of matroids are\ngraphs with connected medians, we characterize the isometric subgraphs of\nJohnson graphs and of halved-cubes with connected medians.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a framework that allows for the construction of sequent systems\nfor expressive description logics extending ALC. Our framework not only covers\na wide array of common description logics, but also allows for sequent systems\nto be obtained for extensions of description logics with special formulae that\nwe call \"role relational axioms.\" All sequent systems are sound, complete, and\npossess favorable properties such as height-preserving admissibility of common\nstructural rules and height-preserving invertibility of rules.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the first numerical analysis of causal, stable first-order\nrelativistic hydrodynamics with ideal gas microphysics, based in the formalism\ndeveloped by Bemfica, Disconzi, Noronha, and Kovtun (BDNK theory). The BDNK\napproach provides definitions for the conserved stress-energy tensor and baryon\ncurrent, and rigorously proves causality, local well-posedness, strong\nhyperbolicity, and linear stability (about equilibrium) for the equations of\nmotion, subject to a set of coupled nonlinear inequalities involving the\nundetermined model coefficients (the choice for which defines the \"hydrodynamic\nframe\"). We present a class of hydrodynamic frames derived from the\nrelativistic ideal gas \"gamma-law\" equation of state which satisfy the BDNK\nconstraints, and explore the properties of the resulting model for a series of\n(0+1)D and (1+1)D tests in 4D Minkowski spacetime. These tests include a\ncomparison of the dissipation mechanisms in Eckart, BDNK, and\nMuller-Israel-Stewart theories, as well as investigations of the impact of\nhydrodynamic frame on the causality and stability properties of Bjorken flow,\nplanar shockwave, and heat flow solutions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study sample average approximations (SAA) of chance constrained programs.\nSAA methods typically approximate the actual distribution in the chance\nconstraint using an empirical distribution constructed from random samples\nassumed to be independent and identically distributed according to the actual\ndistribution. In this paper, we consider a nonstationary variant of this\nproblem, where the random samples are assumed to be independently drawn in a\nsequential fashion from an unknown and possibly time-varying distribution. This\nnonstationarity may be driven by changing environmental conditions present in\nmany real-world applications. To account for the potential nonstationarity in\nthe data generation process, we propose a novel robust SAA method exploiting\ninformation about the Wasserstein distance between the sequence of\ndata-generating distributions and the actual chance constraint distribution. As\na key result, we obtain distribution-free estimates of the sample size required\nto ensure that the robust SAA method will yield solutions that are feasible for\nthe chance constraint under the actual distribution with high confidence.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We report 491 new near-infrared spectroscopic measurements of 420 near-Earth\nobjects (NEOs) collected on the NASA InfraRed Telescope Facility (IRTF) as part\nof the MIT-Hawaii NEO Spectroscopic Survey (MITHNEOS). These measurements were\ncombined with previously published data (Binzel et al. 2019) and bias-corrected\nto derive the intrinsic compositional distribution of the overall NEO\npopulation, as well as of subpopulations coming from various escape routes\n(ERs) in the asteroid belt and beyond. The resulting distributions reflect well\nthe overall compositional gradient of the asteroid belt, with decreasing\nfractions of silicate-rich (S- and Q-type) bodies and increasing fractions of\ncarbonaceous (B-, C-, D- and P-type) bodies as a function of increasing ER\ndistance from the Sun. The close compositional match between NEOs and their\npredicted source populations validates dynamical models used to identify ERs\nand argues against any strong composition change with size in the asteroid belt\nbetween ~5 km down to ~100 m. A notable exception comes from the over-abundance\nof D-type NEOs from the 5:2J and, to a lesser extend, the 3:1J and nu6 ERs,\nhinting at the presence of a large population of small D-type asteroids in the\nmain belt. Alternatively, this excess may indicate preferential spectral\nevolution from D-type surfaces to C- and P-types as a consequence of space\nweathering, or to the fact that D-type objects fragment more often than other\nspectral types in the NEO space. No further evidence for the existence of\ncollisional families in the main belt, below the detection limit of current\nmain-belt surveys, was found in this work.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  There exists a phenomenon that subjectivity highly lies in the daily\nevaluation process. Our research primarily concentrates on a multi-person\nevaluation system with anomaly detection to minimize the possible inaccuracy\nthat subjective assessment brings. We choose the two-stage screening method,\nwhich consists of rough screening and score-weighted Kendall-$\\tau$ Distance to\nwinnow out abnormal data, coupled with hypothesis testing to narrow global\ndiscrepancy. Then we use Fuzzy Synthetic Evaluation Method(FSE) to determine\nthe significance of scores given by reviewers as well as their reliability,\nculminating in a more impartial weight for each reviewer in the final\nconclusion. The results demonstrate a clear and comprehensive ranking instead\nof unilateral scores, and we get to have an efficiency in filtering out\nabnormal data as well as a reasonably objective weight determination mechanism.\nWe can sense that through our study, people will have a chance of modifying a\nmulti-person evaluation system to attain both equity and a relatively superior\ncompetitive atmosphere.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the explosive growth of multi-source data, multi-view clustering has\nattracted great attention in recent years. Most existing multi-view methods\noperate in raw feature space and heavily depend on the quality of original\nfeature representation. Moreover, they are often designed for feature data and\nignore the rich topology structure information. Accordingly, in this paper, we\npropose a generic framework to cluster both attribute and graph data with\nheterogeneous features. It is capable of exploring the interplay between\nfeature and structure. Specifically, we first adopt graph filtering technique\nto eliminate high-frequency noise to achieve a clustering-friendly smooth\nrepresentation. To handle the scalability challenge, we develop a novel\nsampling strategy to improve the quality of anchors. Extensive experiments on\nattribute and graph benchmarks demonstrate the superiority of our approach with\nrespect to state-of-the-art approaches.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The general analysis of the differential cross section and various\npolarization observables is performed for the process $e^+ + e^- \\to N + \\bar N\n+\\pi^0 $ assuming that the annihilation occurs through the exchange of one\nvirtual photon. The dependence of the differential distributions over invariant\nvariables is derived for the reaction $e^+ + e^- \\to N + \\bar N +\\pi^0 $ in the\nso-called non-resonant mechanism, applying the conservation of the hadron\nelectromagnetic currents and the P-invariance of the hadron electromagnetic\ninteraction. The detection in an exclusive experimental set up where the\nnucleon (or antinucleon) and pion are detected in coincidence is considered. A\nnumber of single and double differential distributions have been calculated\nanalytically and numerical estimates are given for the $p\\bar p\\pi^0$ and\n$n\\bar n\\pi^0$ channels, in the Born (non-resonant) approximation, in the\nenergy range from threshold up to $s=16$ GeV$^2$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Discussions on causal relations in real life often consider variables for\nwhich the definition of causality is unclear since the notion of interventions\non the respective variables is obscure. Asking 'what qualifies an action for\nbeing an intervention on the variable X' raises the question whether the action\nimpacted all other variables only through X or directly, which implicitly\nrefers to a causal model.\n  To avoid this known circularity, we instead suggest a notion of\n'phenomenological causality' whose basic concept is a set of elementary\nactions. Then the causal structure is defined such that elementary actions\nchange only the causal mechanism at one node (e.g. one of the causal\nconditionals in the Markov factorization). This way, the Principle of\nIndependent Mechanisms becomes the defining property of causal structure in\ndomains where causality is a more abstract phenomenon rather than being an\nobjective fact relying on hard-wired causal links between tangible objects. We\ndescribe this phenomenological approach to causality for toy and hypothetical\nreal-world examples and argue that it is consistent with the causal Markov\ncondition when the system under consideration interacts with other variables\nthat control the elementary actions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Gradient-enhanced Kriging (GE-Kriging) is a well-established surrogate\nmodelling technique for approximating expensive computational models. However,\nit tends to get impractical for high-dimensional problems due to the large\ninherent correlation matrix and the associated high-dimensional hyper-parameter\ntuning problem. To address these issues, we propose a new method in this paper,\ncalled sliced GE-Kriging (SGE-Kriging) for reducing both the size of the\ncorrelation matrix and the number of hyper-parameters. Firstly, we perform a\nderivative-based global sensitivity analysis to detect the relative importance\nof each input variable with respect to model response. Then, we propose to\nsplit the training sample set into multiple slices, and invoke Bayes' theorem\nto approximate the full likelihood function via a sliced likelihood function,\nin which multiple small correlation matrices are utilized to describe the\ncorrelation of the sample set. Additionally, we replace the original\nhigh-dimensional hyper-parameter tuning problem with a low-dimensional\ncounterpart by learning the relationship between the hyper-parameters and the\nglobal sensitivity indices. Finally, we validate SGE-Kriging by means of\nnumerical experiments with several benchmarks problems. The results show that\nthe SGE-Kriging model features an accuracy and robustness that is comparable to\nthe standard one but comes at much less training costs. The benefits are most\nevident in high-dimensional problems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Drude model successfully quantifies the optical constants for bulk matter,\nbut it is not suitable for subwavelength objects. In this paper, terahertz\nnear-field optical microscopy and finite element simulation are used to study\ngold patches fabricated by Gallium etching. Electron transport is discovered in\ndetermining the optical signal strength. The signal from substrate is more\ncomplicated and still not fully understood. As the etching area decreases,\nnear-field interaction is not dominated by doping concentration, and a higher\nsignal is observed near connected metals. With the help of simulation, the\nabnormal enhancement phenomenon is discussed in detail, which lays the\nfoundation for further experimental verification.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a new abstraction set (SynopSet) that has a continuum of visual\nrepresentations for the explanatory analysis of molecular dynamics simulations\n(MDS) in the DNA nanotechnology domain. By re-purposing the commonly used\nprogress bar and designing novel visuals, as well as transforming the data from\nthe domain format to a format that better fits the newly designed visuals, we\ncompose this new set of representations. This set is also designed to be\ncapable of showing all spatial and temporal details, and all structural\ncomplexity, or abstracting these to various degrees, enabling both the slow\nplayback of the simulation for detailed examinations or very fast playback for\nan overview that helps to efficiently identify events of interest, as well as\nseveral intermediate levels between these two extremes. For any pair of\nsuccessive representations, we demonstrate smooth, continuous transitions,\nenabling users to keep track of relevant information from one representation to\nthe next. By providing multiple representations suited to different temporal\nresolutions and connected by smooth transitions, we enable time-efficient\nsimulation analysis, giving users the opportunity to examine and present\nimportant phases in great detail, or leverage abstract representations to go\nover uneventful phases much faster. Domain experts can thus gain actionable\ninsight about their simulations and communicate it in a much shorter time.\nFurther, the novel representations are more intuitive and also enable\nresearchers unfamiliar with MDS analysis graphs to better understand the\nsimulation results. We assessed the effectiveness of SynopSet on 12 DNA\nnanostructure simulations together with a domain expert. We have also shown\nthat our set of representations can be systematically located in a\nvisualization space, dubbed SynopSpace.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%\nrelative gain under an annotation budget of 18/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https://github.com/HKUNLP/icl-selective-annotation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the problem of robustly testing the norm of a high-dimensional\nsparse signal vector under two different observation models. In the first\nmodel, we are given $n$ i.i.d. samples from the distribution\n$\\mathcal{N}\\left(\\theta,I_d\\right)$ (with unknown $\\theta$), of which a small\nfraction has been arbitrarily corrupted. Under the promise that\n$\\|\\theta\\|_0\\le s$, we want to correctly distinguish whether $\\|\\theta\\|_2=0$\nor $\\|\\theta\\|_2>\\gamma$, for some input parameter $\\gamma>0$. We show that any\nalgorithm for this task requires $n=\\Omega\\left(s\\log\\frac{ed}{s}\\right)$\nsamples, which is tight up to logarithmic factors. We also extend our results\nto other common notions of sparsity, namely, $\\|\\theta\\|_q\\le s$ for any $0 < q\n< 2$. In the second observation model that we consider, the data is generated\naccording to a sparse linear regression model, where the covariates are i.i.d.\nGaussian and the regression coefficient (signal) is known to be $s$-sparse.\nHere too we assume that an $\\epsilon$-fraction of the data is arbitrarily\ncorrupted. We show that any algorithm that reliably tests the norm of the\nregression coefficient requires at least $n=\\Omega\\left(\\min(s\\log\nd,{1}/{\\gamma^4})\\right)$ samples. Our results show that the complexity of\ntesting in these two settings significantly increases under robustness\nconstraints. This is in line with the recent observations made in robust mean\ntesting and robust covariance testing.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The statistical combination of disjoint signal regions in reinterpretation\nstudies uses more of the data of an analysis and gives more robust results than\nthe single signal region approach. We present the implementation and usage of\nsignal region combination in MadAnalysis 5 through two methods: an interface to\nthe pyhf package making use of statistical models in JSON-serialised format\nprovided by the ATLAS collaboration, and a simplified likelihood calculation\nmaking use of covariance matrices provided by the CMS collaboration. The gain\nin physics reach is demonstrated 1.) by comparison with official mass limits\nfor 4 ATLAS and 5 CMS analyses from the Public Analysis Database of MadAnalysis\n5 for which signal region combination is currently available, and 2.) by a case\nstudy for an MSSM scenario in which both stops and sbottoms can be produced and\nhave a variety of decays into charginos and neutralinos.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose two new definitions of local differential privacy\nfor belief functions. One is based on Shafer's semantics of randomly coded\nmessages and the other from the perspective of imprecise probabilities. We show\nthat such basic properties as composition and post-processing also hold for our\nnew definitions. Moreover, we provide a hypothesis testing framework for these\ndefinitions and study the effect of \"don't know\" in the trade-off between\nprivacy and utility in discrete distribution estimation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Merge Resolution proof system (M-Res) for QBFs, proposed by Beyersdorff\net al. in 2019, explicitly builds partial strategies inside refutations. The\noriginal motivation for this approach was to overcome the limitations\nencountered in long-distance Q-Resolution proof system (LD-Q-Res), where the\nsyntactic side-conditions, while prohibiting all unsound resolutions, also end\nup prohibiting some sound resolutions. However, while the advantage of M-Res\nover many other resolution-based QBF proof systems was already demonstrated, a\ncomparison with LD-Q-Res itself had remained open. In this paper, we settle\nthis question. We show that M-Res has an exponential advantage over not only\nLD-Q-Res, but even over LQU$^+$-Res and IRM, the most powerful among currently\nknown resolution-based QBF proof systems. Combining this with results from\nBeyersdorff et al. 2020, we conclude that M-Res is incomparable with LQU-Res\nand LQU$^+$-Res.\n  Our proof method reveals two additional and curious features about M-Res: (i)\nMRes is not closed under restrictions, and is hence not a natural proof system,\nand (ii) weakening axiom clauses with existential variables provably yields an\nexponential advantage over M-Res without weakening. We further show that in the\ncontext of regular derivations, weakening axiom clauses with universal\nvariables provably yields an exponential advantage over M-Res without\nweakening. These results suggest that MRes is better used with weakening,\nthough whether M-Res with weakening is closed under restrictions remains open.\nWe note that even with weakening, M-Res continues to be simulated by eFrege $+$\n$\\forall$red (the simulation of ordinary M-Res was shown recently by Chew and\nSlivovsky).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Schwarzschild black hole in the cold dark matter (CDM) halo is deeply\nstudied, and the radiation laws of the thin accretion disk near the black hole\nis discussed and summarized. The light orbits around the black hole are\ncalculated. According to the number of times that the light from the north pole\npasses through the equatorial plane of the black hole, the light orbits can be\ndivided into three categories. At the same time, the innermost stable circular\norbit of the black hole is also calculated. By analyzing the calculation\nresults of radiation intensity of three kinds of radiation which emitted by\nthin accretion disk on the equatorial plane of black hole and observed at\ninfinity away from the north pole of black hole, the radiation laws of the thin\naccretion disk of Schwarzschild black hole suitable for the background of the\nCDM halo is obtained.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Efficient neural network backbones for mobile devices are often optimized for\nmetrics such as FLOPs or parameter count. However, these metrics may not\ncorrelate well with latency of the network when deployed on a mobile device.\nTherefore, we perform extensive analysis of different metrics by deploying\nseveral mobile-friendly networks on a mobile device. We identify and analyze\narchitectural and optimization bottlenecks in recent efficient neural networks\nand provide ways to mitigate these bottlenecks. To this end, we design an\nefficient backbone MobileOne, with variants achieving an inference time under 1\nms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that MobileOne\nachieves state-of-the-art performance within the efficient architectures while\nbeing many times faster on mobile. Our best model obtains similar performance\non ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3%\nbetter top-1 accuracy on ImageNet than EfficientNet at similar latency.\nFurthermore, we show that our model generalizes to multiple tasks - image\nclassification, object detection, and semantic segmentation with significant\nimprovements in latency and accuracy as compared to existing efficient\narchitectures when deployed on a mobile device.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The electron spectral shapes corresponding to the low-$Q$ $\\beta^-$-decay\ntransitions $^{151}$Sm$(5/2^-_{\\rm g.s.})\\to\\,^{151}\\textrm{Eu}(5/2^+_{\\rm\ng.s.})$, $^{151}$Sm$(5/2^-_{\\rm g.s.})\\to\\,^{151}\\textrm{Eu}(7/2^+_{1})$,\n$^{171}$Tm$(1/2^+_{\\rm g.s.})\\to\\,^{171}\\textrm{Yb}(1/2^-_{\\rm g.s.})$,\n$^{171}$Tm$(1/2^+_{\\rm g.s.})\\to\\,^{171}\\textrm{Yb}(3/2^-_{1})$,\n$^{210}\\textrm{Pb}(0^+_{\\rm g.s.})\\to\\,^{210}\\textrm{Bi}(1^-_{\\rm g.s.})$, and\n$^{210}\\textrm{Pb}(0^+_{\\rm g.s.})\\to\\,^{210}\\textrm{Bi}(0^-_{1})$ have been\ncomputed using beta-decay theory with several refinements for these\nfirst-forbidden nonunique (ff-nu) $\\beta^-$ transitions. These ff-nu $\\beta^-$\ntransitions have non-trivial electron spectral shapes with transition nuclear\nmatrix elements (NMEs) computed by using the microscopic Interacting\nBoson-Fermion Model (IBFM-2) for the decays of $^{151}$Sm and $^{171}$Tm, and\nthe nuclear shell model (NSM) for the decay of $^{210}$Pb. Within the\nrespective $Q$ windows, the computed ff-nu electron spectral shapes deviate\nmaximally at sub-percent level from the universal allowed shape, except for the\ntransition $^{210}\\textrm{Pb}(0^+_{\\rm g.s.})\\to\\,^{210}\\textrm{Bi}(1^-_{\\rm\ng.s.})$, where the maximal deviation is some 2.7$\\%$. This confirms that the\nso-called $\\xi$ approximation is fairly good for most of these low-$Q$\n$\\beta^-$ transitions and thus the allowed shape is a rather good first\napproximation. Our computed spectral shapes could be of interest for\nexperiments aiming to measure the cosmic neutrino background (C$\\nu$B), like\nthe PTOLEMY experiment. We have also derived C$\\nu$B cross sections for the\nground-state transitions of the considered nuclei at the $\\beta$ endpoint. Our\nfindings indicate that more work on the atomic mismatch correction is needed in\nthe future in order to extract reliable and precise C$\\nu$B cross sections for\nany nuclear target.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The detection of the hyper-bright gamma-ray burst (GRB) 221009A enables us to\nexplore the nature of GRB emission and the origin of very-high-energy (VHE)\ngamma-rays. We analyze the ${\\it Fermi}$-LAT data and investigate GeV-TeV\nemission in the framework of the external reverse shock model. We show that\nearly $\\sim1-10$ GeV emission can be explained by the external inverse-Compton\nmechanism via upscattering MeV gamma-rays by electrons accelerated at the\nreverse shock, in addition to the synchrotron self-Compton component. The\npredicted early optical flux could have been brighter than the naked-eye GRB\n080319B. We also show that proton synchrotron emission from accelerated\nultra-high-energy cosmic rays (UHECRs) is detectable, and could potentially\nexplain $\\gtrsim \\rm TeV$ photons detected by LHAASO or UHECR acceleration can\nbe constrained. Our model suggests that the detection of\n$\\mathcal{O}(10\\rm~TeV)$ photons with energy up to $\\sim18$ TeV is possible for\nreasonable models of the extragalactic background light without invoking new\nphysics, and predicts anti-correlations between MeV photons and TeV photons,\nwhich can be tested with the LHAASO data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, deep learning (DL)-aided data detection of spatial\nmultiplexing (SMX) multiple-input multiple-output (MIMO) transmission with\nindex modulation (IM) (Deep-SMX-IM) has been proposed. Deep-SMX-IM has been\nconstructed by combining a zero-forcing (ZF) detector and DL technique. The\nproposed method uses the significant advantages of DL techniques to learn\ntransmission characteristics of the frequency and spatial domains. Furthermore,\nthanks to using subblockbased detection provided by IM, Deep-SMX-IM is a\nstraightforward method, which eventually reveals reduced complexity. It has\nbeen shown that Deep-SMX-IM has significant error performance gains compared to\nZF detector without increasing computational complexity for different system\nconfigurations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Turing test for comparing computer performance to that of humans is well\nknown, but, surprisingly, there is no widely used test for comparing how much\nbetter human-computer systems perform relative to humans alone, computers\nalone, or other baselines. Here, we show how to perform such a test using the\nratio of means as a measure of effect size. Then we demonstrate the use of this\ntest in three ways. First, in an analysis of 79 recently published experimental\nresults, we find that, surprisingly, over half of the studies find a decrease\nin performance, the mean and median ratios of performance improvement are both\napproximately 1 (corresponding to no improvement at all), and the maximum ratio\nis 1.36 (a 36% improvement). Second, we experimentally investigate whether a\nhigher performance improvement ratio is obtained when 100 human programmers\ngenerate software using GPT-3, a massive, state-of-the-art AI system. In this\ncase, we find a speed improvement ratio of 1.27 (a 27% improvement). Finally,\nwe find that 50 human non-programmers using GPT-3 can perform the task about as\nwell as--and less expensively than--the human programmers. In this case,\nneither the non-programmers nor the computer would have been able to perform\nthe task alone, so this is an example of a very strong form of human-computer\nsynergy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In many multi-agent settings, participants can form teams to achieve\ncollective outcomes that may far surpass their individual capabilities.\nMeasuring the relative contributions of agents and allocating them shares of\nthe reward that promote long-lasting cooperation are difficult tasks.\nCooperative game theory offers solution concepts identifying distribution\nschemes, such as the Shapley value, that fairly reflect the contribution of\nindividuals to the performance of the team or the Core, which reduces the\nincentive of agents to abandon their team. Applications of such methods include\nidentifying influential features and sharing the costs of joint ventures or\nteam formation. Unfortunately, using these solutions requires tackling a\ncomputational barrier as they are hard to compute, even in restricted settings.\nIn this work, we show how cooperative game-theoretic solutions can be distilled\ninto a learned model by training neural networks to propose fair and stable\npayoff allocations. We show that our approach creates models that can\ngeneralize to games far from the training distribution and can predict\nsolutions for more players than observed during training. An important\napplication of our framework is Explainable AI: our approach can be used to\nspeed-up Shapley value computations on many instances.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Information sharing on social networks is ubiquitous, intuitive, and\noccasionally accidental. However, people may be unaware of the potential\nnegative consequences of disclosures, such as reputational damages. Yet, people\nuse social networks to disclose information about themselves or others, advised\nonly by their own experiences and the context-invariant informed consent\nmechanism. In two online experiments (N=515 and N=765), we investigated how to\naid informed sharing decisions and associate them with the potential outcomes\nvia notifications. Based on the measurements of sharing attitudes, our results\nshowed that the effectiveness of informing the users via notifications may\ndepend on the timing, content, and layout of the notifications, as well as on\nthe users' curiosity and rational cognitive style, motivating information\nprocessing. Furthermore, positive emotions may result in disregard of important\ninformation. We discuss the implications for user privacy and\nself-presentation. We provide recommendations on privacy-supporting system\ndesign and suggest directions for further research.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In order to adapt the Wasserstein distance to the large sample multivariate\nnon-parametric two-sample problem, making its application computationally\nfeasible, permutation tests based on the Sinkhorn divergence between\nprobability vectors associated to data dependent partitions are considered.\nDifferent ways of implementing these tests are evaluated and the asymptotic\ndistribution of the underlying statistic is established in some cases. The\nstatistics proposed are compared, in simulated examples, with the test of\nSchilling's, one of the best non-parametric tests available in the literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Neutrinos are expected to freestream (i.e. not interact with anything) since\nthey decouple in the early Universe at a temperature $T\\sim 2~{\\rm MeV}$.\nHowever, there are many relevant particle physics scenarios that can make\nneutrinos interact at $T< 2~{\\rm MeV}$. In this work, we take a global\nperspective and aim to identify the temperature range in which neutrinos can\ninteract given current cosmological observations. We consider a generic set of\nrates parametrizing neutrino interactions and by performing a full Planck\ncosmic microwave background (CMB) analysis we find that neutrinos cannot\ninteract significantly for redshifts $2000 \\lesssim z \\lesssim 10^5$, which we\nrefer to as the freestreaming window. We also derive a redshift dependent upper\nbound on a suitably defined interaction rate $\\Gamma_\\text{nfs}(z)$, finding\n$\\Gamma_\\text{nfs}(z)/H(z)\\lesssim 1-10$ within the freestreaming window. We\nshow that these results are largely model independent under some broad\nassumptions, and contextualize them in terms of neutrino decays, neutrino\nself-interactions, neutrino annihilations, and majoron models. We provide\nexamples of how to use our model independent approach to obtain bounds in\nspecific scenarios, and demonstrate agreement with existing results. We also\ninvestigate the reach of upcoming cosmological data finding that CMB Stage-IV\nexperiments can improve the bound on $\\Gamma_\\text{nfs}(z)/H(z)$ by up to a\nfactor $10$. Moreover, we comment on large-scale structure observations,\nfinding that the ongoing DESI survey has the potential to probe uncharted\nregions of parameter space of interacting neutrinos. Finally, we point out a\npeculiar scenario that has so far not been considered, and for which relatively\nlarge interactions around recombination are still allowed by Planck data due to\nsome degeneracy with $n_s$, $A_s$ and $H_0$. This scenario can be fully tested\nwith CMB-S4.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Subgraph GNNs are a recent class of expressive Graph Neural Networks (GNNs)\nwhich model graphs as collections of subgraphs. So far, the design space of\npossible Subgraph GNN architectures as well as their basic theoretical\nproperties are still largely unexplored. In this paper, we study the most\nprominent form of subgraph methods, which employs node-based subgraph selection\npolicies such as ego-networks or node marking and deletion. We address two\ncentral questions: (1) What is the upper-bound of the expressive power of these\nmethods? and (2) What is the family of equivariant message passing layers on\nthese sets of subgraphs?. Our first step in answering these questions is a\nnovel symmetry analysis which shows that modelling the symmetries of node-based\nsubgraph collections requires a significantly smaller symmetry group than the\none adopted in previous works. This analysis is then used to establish a link\nbetween Subgraph GNNs and Invariant Graph Networks (IGNs). We answer the\nquestions above by first bounding the expressive power of subgraph methods by\n3-WL, and then proposing a general family of message-passing layers for\nsubgraph methods that generalises all previous node-based Subgraph GNNs.\nFinally, we design a novel Subgraph GNN dubbed SUN, which theoretically unifies\nprevious architectures while providing better empirical performance on multiple\nbenchmarks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Stability, reachability, and safety are crucial properties of dynamical\nsystems. While verification and control synthesis of reach-avoid-stay\nobjectives can be effectively handled by abstraction-based formal methods, such\napproaches can be computationally expensive due to the use of state-space\ndiscretization. In contrast, Lyapunov methods qualitatively characterize\nstability and safety properties without any state-space discretization. Recent\nwork on converse Lyapunov-barrier theorems also demonstrates an approximate\ncompleteness or verifying reach-avoid-stay specifications of systems modelled\nby nonlinear differential equations. In this paper, based on the topology of\nhybrid arcs, we extend the Lyapunov-barrier characterization to more general\nhybrid systems described by differential and difference inclusions. We show\nthat Lyapunov-barrier functions are not only sufficient to guarantee\nreach-avoid-stay specifications for well-posed hybrid systems, but also\nnecessary for arbitrarily slightly perturbed systems under mild conditions.\nNumerical examples are provided to illustrate the main results.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Trajectory prediction aims to predict the movement trend of the agents like\npedestrians, bikers, vehicles. It is helpful to analyze and understand human\nactivities in crowded spaces and widely applied in many areas such as\nsurveillance video analysis and autonomous driving systems. Thanks to the\nsuccess of deep learning, trajectory prediction has made significant progress.\nThe current methods are dedicated to studying the agents' future trajectories\nunder the social interaction and the sceneries' physical constraints. Moreover,\nhow to deal with these factors still catches researchers' attention. However,\nthey ignore the \\textbf{Semantic Shift Phenomenon} when modeling these\ninteractions in various prediction sceneries. There exist several kinds of\nsemantic deviations inner or between social and physical interactions, which we\ncall the \"\\textbf{Gap}\". In this paper, we propose a \\textbf{C}ontextual\n\\textbf{S}emantic \\textbf{C}onsistency \\textbf{Net}work (\\textbf{CSCNet}) to\npredict agents' future activities with powerful and efficient context\nconstraints. We utilize a well-designed context-aware transfer to obtain the\nintermediate representations from the scene images and trajectories. Then we\neliminate the differences between social and physical interactions by aligning\nactivity semantics and scene semantics to cross the Gap. Experiments\ndemonstrate that CSCNet performs better than most of the current methods\nquantitatively and qualitatively.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given the impact of health literacy (HL) on patients outcomes, limited health\nliteracy (LHL) is a major barrier in cancer care globally. HL refers to the\ndegree in which an individual is able to acquire, process and comprehend\ninformation in a way to be actively involved in their health decisions.\nPrevious research found that almost half of the population in developed\ncountries have difficulties in understanding health related information. With\nthe gradual shift toward the shared decision making (SDM) process and digital\ntransformation in oncology, the need for dealing with low HL issues is more\ncrucial. Decision making in oncology is often accompanied by considerable\nconsequences on patients lives, which requires patients to understand complex\ninformation and be able to compare treatment methods by considering their own\nvalues. How health information is perceived by patients is influenced by\nvarious factors including patients characteristics and the way information is\npresented to patients. Based on the findings, identifying patients with low HL\nand using simple data visualizations are the best practice to help patients and\nclinicians in dealing with LHL. Furthermore, preparing reliable sources of\ninformation in tools such as patient decision aids (PDA), as well as involving\nHL mediators in consultation sessions supports patients to make sense of\ncomplex information.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present an updated version of the 2015-Atlas of Nuclear Isomers\n\\cite{jain2015}, compiling and evaluating experimental data for the isomers\nwith half-life $\\ge 10$ $\\it{ns}$, together with their spectroscopic properties\nsuch as excitation-energies, half-lives, decay modes, spins and parities,\nenergies and multipolarities of isomeric transitions, along with the relevant\noriginal references in literature. The current version of Atlas presents many\nre-evaluated half-lives as compared to the 2015 edition, where values were\nreferred to Nuclear Data Sheets publications, when no new data existed. The\nENSDF database \\cite{Ensdf}, together with the XUNDL \\cite{Xundl} and the\nNUBASE2020 \\cite{Kondev2021} databases have been consulted for completeness,\nyet, data from original papers from journals were considered in the present\nevaluation, and the NSR bibliographic database \\cite{Nsr} has been searched to\nensure that this work is as complete and current as possible. Several useful\nsystematic features of nuclear isomers covered in this Atlas have been\ndiscussed. Literature cutoff date for the extraction of data is July 21, 2022.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, a novel multi-party quantum private comparison (MQPC) protocol\nfor equality comparison with n-level single-particle states is constructed,\nwhere the encoded particles are transmitted in a circular way. Here, n parties\nemploy the qudit shifting operation to encode their private secrets and can\ncompare the equality of their private secrets within one time execution of\nprotocol. The proposed MQPC protocol can overcome both the outside attack and\nthe participant attack. Specially, each party's secret can be kept unknown to\nother parties and the third party (TP).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that under $\\BMM$ and \"there exists a Woodin cardinal$\"$, the\nnonstationary ideal on $\\omega_1$ can not be defined by a $\\Sigma_1$ formula\nwith parameter $A \\subset \\omega_1$. We show that the same conclusion holds\nunder the assumption of Woodin's $(\\ast)$-axiom. We further show that there are\nuniverses where $\\BPFA$ holds and $\\NS$ is $\\Sigma_1(\\omega_1)$-definable. Last\nwe show that if the canonical inner model with one Woodin cardinal $M_1$\nexists, there is a universe where $\\NS$ is saturated,\n$\\Sigma_1(\\omega_1)$-definable and $\\MA$ holds.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the target space entanglement in quantum mechanics of\nnon-interacting fermions at finite temperature. Unlike pure states investigated\nin arXiv:2105.13726, the (R\\'enyi) entanglement entropy for thermal states does\nnot follow a simple bound because all states in the infinite-dimensional\nHilbert space are involved. We investigate a general formula of the target\nspace R\\'enyi entropy for $N$ fermions at finite temperature, and present\nnumerical results of the entropy in a one-dimensional model. We also argue the\nlarge $N$ behaviors with a comparison to the grand canonical ensemble.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As reinforcement learning (RL) has achieved near human-level performance in a\nvariety of tasks, its robustness has raised great attention. While a vast body\nof research has explored test-time (evasion) attacks in RL and corresponding\ndefenses, its robustness against training-time (poisoning) attacks remains\nlargely unanswered. In this work, we focus on certifying the robustness of\noffline RL in the presence of poisoning attacks, where a subset of training\ntrajectories could be arbitrarily manipulated. We propose the first\ncertification framework, COPA, to certify the number of poisoning trajectories\nthat can be tolerated regarding different certification criteria. Given the\ncomplex structure of RL, we propose two certification criteria: per-state\naction stability and cumulative reward bound. To further improve the\ncertification, we propose new partition and aggregation protocols to train\nrobust policies. We further prove that some of the proposed certification\nmethods are theoretically tight and some are NP-Complete problems. We leverage\nCOPA to certify three RL environments trained with different algorithms and\nconclude: (1) The proposed robust aggregation protocols such as temporal\naggregation can significantly improve the certifications; (2) Our certification\nfor both per-state action stability and cumulative reward bound are efficient\nand tight; (3) The certification for different training algorithms and\nenvironments are different, implying their intrinsic robustness properties. All\nexperimental results are available at https://copa-leaderboard.github.io.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Symptom information is primarily documented in free-text clinical notes and\nis not directly accessible for downstream applications. To address this\nchallenge, information extraction approaches that can handle clinical language\nvariation across different institutions and specialties are needed. In this\npaper, we present domain generalization for symptom extraction using\npretraining and fine-tuning data that differs from the target domain in terms\nof institution and/or specialty and patient population. We extract symptom\nevents using a transformer-based joint entity and relation extraction method.\nTo reduce reliance on domain-specific features, we propose a domain\ngeneralization method that dynamically masks frequent symptoms words in the\nsource domain. Additionally, we pretrain the transformer language model (LM) on\ntask-related unlabeled texts for better representation. Our experiments\nindicate that masking and adaptive pretraining methods can significantly\nimprove performance when the source domain is more distant from the target\ndomain.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we establish $L^p(\\mathbb{R}^d,\\gamma_\\infty)$-boundedness\nproperties for square functions involving time and spatial derivatives of\nOrnstein-Uhlenbeck semigroups. Here $\\gamma_\\infty$ denotes the invariant\nmeasure. In order to prove the strong type results for $1<p<\\infty$ we use\n$R$-boundedness. The weak type (1,1) property is established by studying\nseparately global and local operators defined for the square Littlewood-Paley\nfunctions. By the way we prove $L^p(\\mathbb{R}^d,\\gamma_\\infty)$-boundedness\nproperties for maximal and variation operators for Ornstein-Uhlenbeck\nsemigroups.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Channel estimation is a critical task in multiple-input multiple-output\n(MIMO) digital communications that substantially effects end-to-end system\nperformance. In this work, we introduce a novel approach for channel estimation\nusing deep score-based generative models. A model is trained to estimate the\ngradient of the logarithm of a distribution and is used to iteratively refine\nestimates given measurements of a signal. We introduce a framework for training\nscore-based generative models for wireless MIMO channels and performing channel\nestimation based on posterior sampling at test time. We derive theoretical\nrobustness guarantees for channel estimation with posterior sampling in\nsingle-input single-output scenarios, and experimentally verify performance in\nthe MIMO setting. Our results in simulated channels show competitive\nin-distribution performance, and robust out-of-distribution performance, with\ngains of up to $5$ dB in end-to-end coded communication performance compared to\nsupervised deep learning methods. Simulations on the number of pilots show that\nhigh fidelity channel estimation with $25$% pilot density is possible for MIMO\nchannel sizes of up to $64 \\times 256$. Complexity analysis reveals that model\nsize can efficiently trade performance for estimation latency, and that the\nproposed approach is competitive with compressed sensing in terms of\nfloating-point operation (FLOP) count.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Several recent works show impressive results in mapping language-based human\ncommands and image scene observations to direct robot executable policies\n(e.g., pick and place poses). However, these approaches do not consider the\nuncertainty of the trained policy and simply always execute actions suggested\nby the current policy as the most probable ones. This makes them vulnerable to\ndomain shift and inefficient in the number of required demonstrations. We\nextend previous works and present the PARTNR algorithm that can detect\nambiguities in the trained policy by analyzing multiple modalities in the pick\nand place poses using topological analysis. PARTNR employs an adaptive,\nsensitivity-based, gating function that decides if additional user\ndemonstrations are required. User demonstrations are aggregated to the dataset\nand used for subsequent training. In this way, the policy can adapt promptly to\ndomain shift and it can minimize the number of required demonstrations for a\nwell-trained policy. The adaptive threshold enables to achieve the\nuser-acceptable level of ambiguity to execute the policy autonomously and in\nturn, increase the trustworthiness of our system. We demonstrate the\nperformance of PARTNR in a table-top pick and place task.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the differential equation $\\frac{\\partial G}{\\partial\\bar z}=g$ with\nan unbounded Banach-valued Bochner measurable function $g$ on the open unit\ndisk $\\mathbb D\\subset\\mathbb C$. We prove that under some conditions on the\ngrowth and essential support of $g$ such equation has a bounded solution given\nby a continuous linear operator. The obtained results are applicable to the\nBanach-valued corona problem for the algebra of bounded holomorphic functions\non $\\mathbb D$ with values in a complex commutative unital Banach algebra.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a variant of the voter model in which agents may have different\ndegrees of confidence on their opinions. Those with low confidence are normal\nvoters whose state can change upon a single contact with a different\nneighboring opinion. However, confidence increases with opinion reinforcement\nand, above a certain threshold, these agents become zealots that do not change\nopinion. We show that both strategies, normal voters and zealots, may coexist,\nleading to a competition between two different kinetic mechanisms:\ncurvature-driven growth and interfacial noise. The kinetically constrained\nzealots are formed well inside the clusters, away from the different opinions\nat the surfaces that help keep the confidence not so high. Normal voters\nconcentrate in a region around the interfaces and their number, that is related\nwith the distance between the surface and the zealotry bulk, depends on the\nrate the confidence changes. Despite this interface being rough and fragmented,\ntypical of the voter model, the presence of zealots in the bulk of these\ndomains, induces a curvature-driven dynamics, similar to the low temperature\ncoarsening behavior of the non-conserved Ising model after a temperature\nquench.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Biological vision systems are unparalleled in their ability to learn visual\nrepresentations without supervision. In machine learning, contrastive learning\n(CL) has led to major advances in forming object representations in an\nunsupervised fashion. These systems learn representations invariant to\naugmentation operations over images, like cropping or flipping. In contrast,\nbiological vision systems exploit the temporal structure of the visual\nexperience. This gives access to augmentations not commonly used in CL, like\nwatching the same object from multiple viewpoints or against different\nbackgrounds. Here, we systematically investigate and compare the potential\nbenefits of such time-based augmentations for learning object categories. Our\nresults show that time-based augmentations achieve large performance gains over\nstate-of-the-art image augmentations. Specifically, our analyses reveal that:\n1) 3-D object rotations drastically improve the learning of object categories;\n2) viewing objects against changing backgrounds is vital for learning to\ndiscard background-related information. Overall, we conclude that time-based\naugmentations can greatly improve contrastive learning, narrowing the gap\nbetween artificial and biological vision systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this study, working with the task of object retrieval in clutter, we have\ndeveloped a robot learning framework in which Monte Carlo Tree Search (MCTS) is\nfirst applied to enable a Deep Neural Network (DNN) to learn the intricate\ninteractions between a robot arm and a complex scene containing many objects,\nallowing the DNN to partially clone the behavior of MCTS. In turn, the trained\nDNN is integrated into MCTS to help guide its search effort. We call this\napproach learning-guided Monte Carlo tree search for Object REtrieval (MORE),\nwhich delivers significant computational efficiency gains and added solution\noptimality. MORE is a self-supervised robotics framework/pipeline capable of\nworking in the real world that successfully embodies the System 2 to System 1\nlearning philosophy proposed by Kahneman, where learned knowledge, used\nproperly, can help greatly speed up a time-consuming decision process over\ntime. Videos and supplementary material can be found at\nhttps://github.com/arc-l/more\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep Reinforcement Learning (DRL) solutions are becoming pervasive at the\nedge of the network as they enable autonomous decision-making in a dynamic\nenvironment. However, to be able to adapt to the ever-changing environment, the\nDRL solution implemented on an embedded device has to continue to occasionally\ntake exploratory actions even after initial convergence. In other words, the\ndevice has to occasionally take random actions and update the value function,\ni.e., re-train the Artificial Neural Network (ANN), to ensure its performance\nremains optimal. Unfortunately, embedded devices often lack processing power\nand energy required to train the ANN. The energy aspect is particularly\nchallenging when the edge device is powered only by a means of Energy\nHarvesting (EH). To overcome this problem, we propose a two-part algorithm in\nwhich the DRL process is trained at the sink. Then the weights of the fully\ntrained underlying ANN are periodically transferred to the EH-powered embedded\ndevice taking actions. Using an EH-powered sensor, real-world measurements\ndataset, and optimizing for Age of Information (AoI) metric, we demonstrate\nthat such a DRL solution can operate without any degradation in the\nperformance, with only a few ANN updates per day.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  There are several approaches to define an eigenvector decomposition of the\nfinite Fourier Transform, which is in some sense unique, and at best resembles\nthe eigenstates of the quantum harmonic oscillator. A solution given by Balian\nand Itzykson in 1986 for prime dimensions d = 3 (mod 4) is revisited. It is\nshown, that by applying the Weyl-Heisenberg matrices to this eigenvector basis,\na projective 2-design is generated.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this report, we describe the technical details of our submission to the\nEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action\nRecognition. To tackle the domain-shift which exists under the UDA setting, we\nfirst exploited a recent Domain Generalization (DG) technique, called Relative\nNorm Alignment (RNA). Secondly, we extended this approach to work on unlabelled\ntarget data, enabling a simpler adaptation of the model to the target\ndistribution in an unsupervised fashion. To this purpose, we included in our\nframework UDA algorithms, such as multi-level adversarial alignment and\nattentive entropy. By analyzing the challenge setting, we notice the presence\nof a secondary concurrence shift in the data, which is usually called\nenvironmental bias. It is caused by the existence of different environments,\ni.e., kitchens. To deal with these two shifts (environmental and temporal), we\nextended our system to perform Multi-Source Multi-Target Domain Adaptation.\nFinally, we employed distinct models in our final proposal to leverage the\npotential of popular video architectures, and we introduced two more losses for\nthe ensemble adaptation. Our submission (entry 'plnet') is visible on the\nleaderboard and ranked in 2nd position for 'verb', and in 3rd position for both\n'noun' and 'action'.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We stimulate mycelian networks of oyster fungi Pleurotus ostreatus with low\nfrequency sinusoidal electrical signals. We demonstrate that the fungal\nnetworks can discriminate between frequencies in a fuzzy or threshold based\nmanner. Details about the mixing of frequencies by the mycelium networks are\nprovided. The results advance the novel field of fungal electronics and pave\nground for the design of living, fully recyclable, electron devices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Compression plays an important role on the efficient transmission and storage\nof images and videos through band-limited systems such as streaming services,\nvirtual reality or videogames. However, compression unavoidably leads to\nartifacts and the loss of the original information, which may severely degrade\nthe visual quality. For these reasons, quality enhancement of compressed images\nhas become a popular research topic. While most state-of-the-art image\nrestoration methods are based on convolutional neural networks, other\ntransformers-based methods such as SwinIR, show impressive performance on these\ntasks.\n  In this paper, we explore the novel Swin Transformer V2, to improve SwinIR\nfor image super-resolution, and in particular, the compressed input scenario.\nUsing this method we can tackle the major issues in training transformer vision\nmodels, such as training instability, resolution gaps between pre-training and\nfine-tuning, and hunger on data. We conduct experiments on three representative\ntasks: JPEG compression artifacts removal, image super-resolution (classical\nand lightweight), and compressed image super-resolution. Experimental results\ndemonstrate that our method, Swin2SR, can improve the training convergence and\nperformance of SwinIR, and is a top-5 solution at the \"AIM 2022 Challenge on\nSuper-Resolution of Compressed Image and Video\".\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The improvement of air-quality in urban areas is one of the main concerns of\npublic government bodies. This concern emerges from the evidence between the\nair quality and the public health. Major efforts from government bodies in this\narea include monitoring and forecasting systems, banning more pollutant motor\nvehicles, and traffic limitations during the periods of low-quality air. In\nthis work, a proposal for dynamic prices in regulated parking services is\npresented. The dynamic prices in parking service must discourage motor vehicles\nparking when low-quality episodes are predicted. For this purpose, diverse deep\nlearning strategies are evaluated. They have in common the use of collective\nair-quality measurements for forecasting labels about air quality in the city.\nThe proposal is evaluated by using economic parameters and deep learning\nquality criteria at Madrid (Spain).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deploying deep neural networks on low-resource edge devices is challenging\ndue to their ever-increasing resource requirements. Recent investigations\npropose multiplication-free neural networks to reduce computation and memory\nconsumption. Shift neural network is one of the most effective tools towards\nthese reductions. However, existing low-bit shift networks are not as accurate\nas their full precision counterparts and cannot efficiently transfer to a wide\nrange of tasks due to their inherent design flaws. We propose DenseShift\nnetwork that exploits the following novel designs. First, we demonstrate that\nthe zero-weight values in low-bit shift networks are neither useful to the\nmodel capacity nor simplify the model inference. Therefore, we propose to use a\nzero-free shifting mechanism to simplify inference while increasing the model\ncapacity. Second, we design a new metric to measure the weight freezing issue\nin training low-bit shift networks, and propose a sign-scale decomposition to\nimprove the training efficiency. Third, we propose the low-variance random\ninitialization strategy to improve the model's performance in transfer learning\nscenarios. We run extensive experiments on various computer vision and speech\ntasks. The experimental results show that DenseShift network significantly\noutperforms existing low-bit multiplication-free networks and can achieve\ncompetitive performance to the full-precision counterpart. It also exhibits\nstrong transfer learning performance with no drop in accuracy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the era of industry 4.0, procurement in supply chain management is the key\nto developing information management systems. It directly affects production\nplanning failure. In this case, it is the process to prepare and confirming the\nmaterial inventory is in the ordinal stages and be able to produce the products\nin any production line. In terms of industrial informatics, it can provide\ninformation management approaches for leveraging data sharing between\nfactories. The multiobjective optimization will be enabled by integrating\nmaterial inventory, production planning and monitoring, and transportation\nplanning collaboration. The material-inventory transportation problem is the\nvirtual factory situation when production plan failure occurs. It becomes the\ncost to transport material between each factory and the distribution to\nclients. In this study, the question of the material-inventory transportation\nproblem is: How can we transport other materials from one factory into another\nfactory? This study proposed a model to find out about the adjustment of\nmaterial inventory through transportation. The objective of this model is to\nminimize the whole production cost and total transportation cost.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We analyze the hypercyclicity, chaoticity, and spectral structure of (bounded\nand unbounded) weighted backward shifts in a nonclassical sequence space, which\nthe space $l_1$ of summable sequences is both isometrically isomorphic to and\ncontinuously and densely embedded into. Based on the weighted backward shifts,\nwe further construct new bounded and unbounded linear hypercyclic and chaotic\noperators both in the nonclassical sequence space and the classical space\n$l_1$, including those that are hypercyclic but not chaotic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that the notion of ribbon rational homology cobordism yields a\npartial order on the set of aspherical $3$-manifolds, thus supporting a\nconjecture formulated by Daemi, Lidman, Vela-Vick and Wong. Our proof is built\non Agol's recent proof of the fact that ribbon concordance yields a partial\norder on the set of knots in the 3-sphere.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Experience and reasoning occur across multiple temporal scales: milliseconds,\nseconds, hours or days. The vast majority of computer vision research, however,\nstill focuses on individual images or short videos lasting only a few seconds.\nThis is because handling longer videos require more scalable approaches even to\nprocess them. In this work, we propose a framework enabling research on\nhour-long videos with the same hardware that can now process second-long\nvideos. We replace standard video compression, e.g. JPEG, with neural\ncompression and show that we can directly feed compressed videos as inputs to\nregular video networks. Operating on compressed videos improves efficiency at\nall pipeline levels -- data transfer, speed and memory -- making it possible to\ntrain models faster and on much longer videos. Processing compressed signals\nhas, however, the downside of precluding standard augmentation techniques if\ndone naively. We address that by introducing a small network that can apply\ntransformations to latent codes corresponding to commonly used augmentations in\nthe original video space. We demonstrate that with our compressed vision\npipeline, we can train video models more efficiently on popular benchmarks such\nas Kinetics600 and COIN. We also perform proof-of-concept experiments with new\ntasks defined over hour-long videos at standard frame rates. Processing such\nlong videos is impossible without using compressed representation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present an assessment of the greenhouse gases emissions of the Institute\nfor Research in Astrophysics and Planetology (IRAP), located in Toulouse\n(France). It was performed following the established \"Bilan Carbone\"\nmethodology, over a large scope compared to similar previous studies, including\nin particular the contribution from the purchase of goods and services as well\nas IRAP's use of external research infrastructures, such as ground-based\nobservatories and space-borne facilities. The carbon footprint of the institute\nfor the reference year 2019 is 7400 +/- 900 tCO2e. If we exclude the\ncontribution from external research infrastructures to focus on a restricted\nperimeter over which the institute has some operational control, IRAP's\nemissions in 2019 amounted to 3300 +/- 400 tCO2e. Over the restricted\nperimeter, the contribution from purchasing goods and services is dominant,\nabout 40% of the total, slightly exceeding the contribution from professional\ntravel including hotel stays, which accounts for 38%. Local infrastructures\nmake a smaller contribution to IRAP's carbon footprint, about 25% over the\nrestricted perimeter. We note that this repartition may be specific to IRAP,\nsince the energy used to produce the electricity and heating has a relatively\nlow carbon footprint. Over the full perimeter, the large share from the use of\nground-based observatories and space-borne facilities and the fact that the\nmajority of IRAP purchases are related to instrument development indicate that\nresearch infrastructures represent the most significant challenge for reducing\nthe carbon footprint of research at our institute. With ~260 staff members\nemployed, our results imply that performing research in astronomy and\nastrophysics at IRAP according to the standards of 2019 produces average GHG\nemissions of 28 tCO2e/yr per person involved in that activity (Abridged).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Collaborative filtering algorithms capture underlying consumption patterns,\nincluding the ones specific to particular demographics or protected information\nof users, e.g. gender, race, and location. These encoded biases can influence\nthe decision of a recommendation system (RS) towards further separation of the\ncontents provided to various demographic subgroups, and raise privacy concerns\nregarding the disclosure of users' protected attributes. In this work, we\ninvestigate the possibility and challenges of removing specific protected\ninformation of users from the learned interaction representations of a RS\nalgorithm, while maintaining its effectiveness. Specifically, we incorporate\nadversarial training into the state-of-the-art MultVAE architecture, resulting\nin a novel model, Adversarial Variational Auto-Encoder with Multinomial\nLikelihood (Adv-MultVAE), which aims at removing the implicit information of\nprotected attributes while preserving recommendation performance. We conduct\nexperiments on the MovieLens-1M and LFM-2b-DemoBias datasets, and evaluate the\neffectiveness of the bias mitigation method based on the inability of external\nattackers in revealing the users' gender information from the model. Comparing\nwith baseline MultVAE, the results show that Adv-MultVAE, with marginal\ndeterioration in performance (w.r.t. NDCG and recall), largely mitigates\ninherent biases in the model on both datasets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We examine the classical problem of the height of a static liquid interface\nthat forms on the outside of a solid vertical cylinder in an unbounded stagnant\npool exposed to air. Gravitational and surface tension effects compete to\naffect the interface shape as characterized by the Bond number, $B = \\rho g\nR^{2}/\\sigma$, where $\\rho$ is fluid density, $g$ is the gravitational\nconstant, $R$ is the radius of the cylinder, and $\\sigma$ is surface tension.\nHere, we provide a convergent power series solution for interface shapes that\nrise above the horizontal pool as a function of Bond number. The power series\nsolution is expressed in terms of a pre-factor that matches the large distance\nasymptotic behavior -- the modified Bessel function of zeroth order -- and an\nEuler transformation that moves the influence of convergence-limiting\nsingularities out of the physical domain. The power series solution is\nvalidated through comparison with a numerical solution, and the $B\\rightarrow0$\nmatched asymptotic solutions of Lo (1983, J. Fluid Mech., 132, p.65-78). For a\nbenchmark static contact angle of $45$ degrees, the power series approach\nexceeds the accuracy of matched asymptotic solutions for $B>0.01$; this lower\nlimit on $B$ arises from round-off error in the computation of the series\ncoefficients in double precision arithmetic, and is reduced as the contact\nangle is increased.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The adoption of deep learning-based healthcare decision support systems such\nas the detection of irregular cardiac rhythm is hindered by challenges such as\nlack of access to quality data and the high costs associated with the\ncollection and annotation of data. The collection and processing of large\nvolumes of healthcare data is a continuous process. The performance of\ndata-hungry Deep Learning models (DL) is highly dependent on the quantity and\nquality of the data. While the need for data quantity has been established\nthrough research adequately, we show how a selection of good quality data\nimproves deep learning model performance. In this work, we take\nElectrocardiogram (ECG) data as a case study and propose a model performance\nimprovement methodology for algorithm developers, that selects the most\ninformative data samples from incoming streams of multi-class ECG data. Our\nCore-Set selection methodology uses metrics-based explanations to select the\nmost informative ECG data samples. This also provides an understanding (for\nalgorithm developers) as to why a sample was selected as more informative over\nothers for the improvement of deep learning model performance. Our experimental\nresults show a 9.67% and 8.69% precision and recall improvement with a\nsignificant training data volume reduction of 50%. Additionally, our proposed\nmethodology asserts the quality and annotation of ECG samples from incoming\ndata streams. It allows automatic detection of individual data samples that do\nnot contribute to model learning thus minimizing possible negative effects on\nmodel performance. We further discuss the potential generalizability of our\napproach by experimenting with a different dataset and deep learning\narchitecture.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Equity, Diversity, and Inclusion (EDI) committees and Codes of Conduct (CoC)\nhave become common in laboratories and physics departments across the country.\nHowever, very often these EDI committees and CoC are not equipped to provide\npractical consequences for violations, and therefore are mostly performative in\nnature. A considerable effort has been devoted by various groups within APS\nunits and beyond the APS in developing instead what are now called Community\nGuidelines. Community Guidelines help implement the core principles in CoC, by\nsetting expectations for participation in in-person events and virtual\ncommunication. When further accompanied by accountability and enforcement\nprocesses, they develop into Community Agreements. This White Paper discusses\nthe elements necessary to create and implement an effective Community\nAgreement, reviews examples of Community Agreements in physics, and argues that\nphysics collaborations, physics departments, and ultimately as many physics\norganizations as possible, however large or small, should have a Community\nAgreement in place. We advocate that Community Agreements should become part of\nthe bylaws of any entity that has bylaws.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Network models are useful tools for modelling complex associations. If a\nGaussian graphical model is assumed, conditional independence is determined by\nthe non-zero entries of the inverse covariance (precision) matrix of the data.\nThe Bayesian graphical horseshoe estimator provides a robust and flexible\nframework for precision matrix inference, as it introduces local, edge-specific\nparameters which prevent over-shrinkage of non-zero off-diagonal elements.\nHowever, for many applications such as statistical omics, the current\nimplementation based on Gibbs sampling becomes computationally inefficient or\neven unfeasible in high dimensions. Moreover, the graphical horseshoe has only\nbeen formulated for a single network, whereas interest has grown in the network\nanalysis of multiple data sets that might share common structures. We propose\n(i) a scalable expectation conditional maximisation (ECM) algorithm for\nobtaining the posterior mode of the precision matrix in the graphical\nhorseshoe, and (ii) a novel joint graphical horseshoe estimator, which borrows\ninformation across multiple related networks to improve estimation. We show, on\nboth simulated and real omics data, that our single-network ECM approach is\nmore scalable than the existing graphical horseshoe Gibbs implementation, while\nachieving the same level of accuracy. We also show that our joint-network\nproposal successfully leverages shared edge-specific information between\nnetworks while still retaining differences, outperforming state-of-the-art\nmethods at any level of network similarity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider multivariable polynomials over a fixed number field, linear in\nsome of the variables. For a system of such polynomials satisfying certain\ntechnical conditions we prove the existence of search bounds for simultaneous\nzeros with respect to height. For a single such polynomial, we prove the\nexistence of search bounds with respect to height for zeros lying outside of a\nprescribed algebraic set. We also obtain search bounds in the case of\nhomogeneous multilinear polynomials, which are related to a so-called \"sparse\"\nversion of Siegel's lemma. Among the tools we develop are height inequalities\nthat are of some independent interest.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Cuspidal robots can travel from one inverse kinematic solution to another\nwithout meeting a singularity. The name cuspidal was coined based on the\nexistence of a cusp point in the workspace of 3R serial robots. The existence\nof a cusp point was proved to be a necessary and sufficient condition for\northogonal robots to be cuspidal, but it was not possible to extend this\ncondition to non-orthogonal robots. The goal of this paper is to prove that\nthis condition stands for any generic 3R robot. This result would give the\ndesigner more flexibility. In the presented work, the geometrical\ninterpretation of the inverse kinematics of 3R robots is revisited and\nimportant observations on the nonsingular change of posture are noted. The\npaper presents a theorem regarding the existence of reduced aspects in any\ngeneric 3R serial robot. Based on these observations and on this theorem, we\nprove that the existence of a cusp point is a necessary and sufficient\ncondition for any 3R generic robot to be cuspidal.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a protocol for probing the state of a quantum system by its\nresonant coupling and entanglement with a meter system. By continuous\nmeasurement of a time evolving meter observable, we infer the evolution of the\nentangled systems and, ultimately, the state and dynamics of the system of\ninterest. The photon number in a cavity field is thus resolved by simulated\nmonitoring of the time dependent excited state population of a resonantly\ncoupled two-level system, and we propose to regard this as an extension of\nquantum non-demolition measurements with potential applications in quantum\nmetrology and quantum computing.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The ongoing amalgamation of UAV and ML techniques is creating a significant\nsynergy and empowering UAVs with unprecedented intelligence and autonomy. This\nsurvey aims to provide a timely and comprehensive overview of ML techniques\nused in UAV operations and communications and identify the potential growth\nareas and research gaps. We emphasise the four key components of UAV operations\nand communications to which ML can significantly contribute, namely, perception\nand feature extraction, feature interpretation and regeneration, trajectory and\nmission planning, and aerodynamic control and operation. We classify the latest\npopular ML tools based on their applications to the four components and conduct\ngap analyses. This survey also takes a step forward by pointing out significant\nchallenges in the upcoming realm of ML-aided automated UAV operations and\ncommunications. It is revealed that different ML techniques dominate the\napplications to the four key modules of UAV operations and communications.\nWhile there is an increasing trend of cross-module designs, little effort has\nbeen devoted to an end-to-end ML framework, from perception and feature\nextraction to aerodynamic control and operation. It is also unveiled that the\nreliability and trust of ML in UAV operations and applications require\nsignificant attention before full automation of UAVs and potential cooperation\nbetween UAVs and humans come to fruition.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study momentum-resolved tunneling into a system of spinless chiral\none-dimensional fermions, such as electrons at the edge of an integer quantum\nHall system. Interactions between particles give rise to broadening of the\nspectral function of the system. We develop an approach that enables one to\nobtain the shape of the peak in the spectral function in the regime of strong\ninteraction. We apply our technique to the special cases of short-range and\nCoulomb interactions.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We follow the heavy Higgs searches at the Large Hadron Collider (LHC) for $H\n\\to \\tau \\mu$ by CMS, and $H\\to\\tau\\tau$ by ATLAS and CMS, to study the\nprospects of discovering the extra Higgs states in $pp \\to H,A \\to \\tau \\mu,\n\\tau\\tau$ with $e\\mu + {E}^{\\rm miss}_T$ and $j_{\\tau} \\mu + {E}^{\\rm miss}_T $\nfinal states, where $j_{\\tau} = \\pi \\, , \\rho\\, , a_1$ and ${E}^{miss}_T$ is\nmissing transverse energy. In a general two Higgs doublet model framework\nwithout $Z_2$ symmetry, extra Yukawa couplings $\\rho_{\\tau\\tau}$ and\n$\\rho_{\\tau\\mu}$ can drive $H,A \\to \\tau\\tau$ and $\\tau\\mu$ channels at hadron\ncolliders, following gluon-gluon fusion production with extra $\\rho_{tt}$\ncouplings. The light Higgs boson $h(125)$ is found to resemble closely the\nStandard Model Higgs boson. In the alignment limit of $\\cos\\gamma \\to 0$ for\n$h$--$H$ mixing, flavor changing neutral Higgs couplings such as $h \\to\n\\tau\\mu$ are naturally suppressed, but the couplings of the heavier $H$ is\noptimized by $\\sin \\gamma \\to 1$. We define various signal regions for $H,A \\to\n\\tau\\mu$ and $\\tau \\tau$, and evaluate physics backgrounds from dominant\nprocesses with realistic acceptance cuts and tagging efficiencies. Two\ndifferent studies are presented: we first perform a parton level study without\nany hadronization and with minimal detector smearing; we then include\nhadronization using PYTHIA 8.2 and fast detector simulation using DELPHES to\ngive event level simulation. Results for $\\sqrt{s}= 13$ TeV appear promising,\nwhich we extend further to $\\sqrt{s} = 14$ TeV for the High Luminosity LHC.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Investigating period changes of classical Cepheids through the framework of\n$O-C$ diagrams provides a unique insight to the evolution and nature of these\nvariable stars. In this work, the new or extended $O-C$ diagrams for 148\nGalactic classical Cepheids are presented. By correlating the calculated period\nchange rates with the Gaia EDR3 colours, we obtain observational indications\nfor the non-negligible dependence of the period change rate on the horizontal\nposition within the instability strip. We find period fluctuations in 59\nCepheids with a confidence level of 99%, which are distributed uniformly over\nthe inspected period range. Correlating the fluctuation amplitude with the\npulsation period yields a clear dependence, similar to the one valid for longer\nperiod pulsating variable stars. The non-negligible amount of Cepheids showing\nchanges in their $O-C$ diagrams that are not or not only of evolutionary origin\npoints toward the need for further studies for the complete understanding of\nthese effects. One such peculiar behaviour is the large amplitude period\nfluctuation in short period Cepheids, which occurs in a significant fraction of\nthe investigated stars. The period dependence of the fluctuation strength and\nits minimum at the bump Cepheid region suggests a stability enhancing mechanism\nfor this period range, which agrees with current pulsation models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We analyze the power spectrum and the bispectrum of BOSS galaxy-clustering\ndata using the prediction from the Effective Field Theory of Large-Scale\nStructure at one-loop order for $\\textit{both}$ the power spectrum\n$\\textit{and}$ the bispectrum. With $\\Lambda$CDM parameters fixed to Planck\npreferred values, we set limits on three templates of non-Gaussianities\npredicted by many inflationary models: the equilateral, the orthogonal, and the\nlocal shapes. After validating our analysis against simulations, we find\n$f_{\\rm NL}^{\\rm equil.}=2 \\pm 212$, $f_{\\rm NL}^{\\rm orth.}= 126 \\pm 72$,\n$f_{\\rm NL}^{\\rm loc.}=-30\\pm 29$, at $68\\%$ confidence level. These\nbispectrum-based constraints from Large-Scale Structure, not far from the ones\nof WMAP, suggest promising results from upcoming surveys.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The motivation for this paper comes from the ongoing SARS-CoV-2 Pandemic. Its\ngoal is to present a previously neglected approach to non-adaptive group\ntesting and describes it in terms of residuated pairs on partially ordered\nsets. Our investigation has the advantage, as it naturally yields an efficient\ndecision scheme (decoder) for any given testing scheme. This decoder allows to\ndetect a large amount of infection patterns. Apart from this, we devise a\nconstruction of good group testing schemes that are based on incidence matrices\nof finite partial linear spaces. The key idea is to exploit the structure of\nthese matrices and make them available as test matrices for group testing.\nThese matrices may generally be tailored for different estimated disease\nprevalence levels. As an example, we discuss the group testing schemes based on\ngeneralized quadrangles. In the context at hand, we state our results only for\nthe error-free case so far. An extension to a noisy scenario is desirable and\nwill be treated in a subsequent account on the topic.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study real interpolation, but instead of interpolating between Banach\nspaces, we interpolate between general functions taking values in $[0,\\infty].$\nWe show the equivalence of the mean method and the $K$-method and apply the\ngeneral theory to interpolation between the norm on a Banach space and the set\nnorm associated with an m-accretive operator on such a space.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We advocate for a paradigm shift in supporting free/libre and open source\nsoftware (FLOSS) ecosystem maintenance, from focusing on individual projects to\nmonitoring a whole organic system of the entire FLOSS ecosystem, which we call\nsoftware meta-maintenance. We discuss challenges of building a global source\ncode management system, a global issue management system, and FLOSS human\ncapital index, based on the blockchain technologies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a Bayesian inference methodology for the estimation of orbital\nparameters on single-line spectroscopic binaries with astrometric data, based\non the No-U-Turn sampler Markov chain Monte Carlo algorithm. Our approach is\ndesigned to provide a precise and efficient estimation of the joint posterior\ndistribution of the orbital parameters in the presence of partial and\nheterogeneous observations. This scheme allows us to directly incorporate prior\ninformation about the system - in the form of a trigonometric parallax, and an\nestimation of the mass of the primary component from its spectral type - to\nconstrain the range of solutions, and to estimate orbital parameters that\ncannot be usually determined (e.g. the individual component masses), due to the\nlack of observations or imprecise measurements. Our methodology is tested by\nanalyzing the posterior distributions of well-studied double-line spectroscopic\nbinaries treated as single-line binaries by omitting the radial velocity data\nof the secondary object. Our results show that the system's mass ratio can be\nestimated with an uncertainty smaller than 10% using our approach. As a proof\nof concept, the proposed methodology is applied to twelve single-line\nspectroscopic binaries with astrometric data that lacked a joint\nastrometric-spectroscopic solution, for which we provide full orbital elements.\nOur sample-based methodology allows us also to study the impact of different\nposterior distributions in the corresponding observations space. This novel\nanalysis provides a better understanding of the effect of the different sources\nof information on the shape and uncertainty of the orbit and radial velocity\ncurve.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Although Automatic Speech Recognition (ASR) systems have achieved human-like\nperformance for a few languages, the majority of the world's languages do not\nhave usable systems due to the lack of large speech datasets to train these\nmodels. Cross-lingual transfer is an attractive solution to this problem,\nbecause low-resource languages can potentially benefit from higher-resource\nlanguages either through transfer learning, or being jointly trained in the\nsame multilingual model. The problem of cross-lingual transfer has been well\nstudied in ASR, however, recent advances in Self Supervised Learning are\nopening up avenues for unlabeled speech data to be used in multilingual ASR\nmodels, which can pave the way for improved performance on low-resource\nlanguages. In this paper, we survey the state of the art in multilingual ASR\nmodels that are built with cross-lingual transfer in mind. We present best\npractices for building multilingual models from research across diverse\nlanguages and techniques, discuss open questions and provide recommendations\nfor future work.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multilingual text-video retrieval methods have improved significantly in\nrecent years, but the performance for other languages lags behind English. We\npropose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve\nmultilingual text-video retrieval. Inspired by the fact that English text-video\nretrieval outperforms other languages, we train a student model using input\ntext in different languages to match the cross-modal predictions from teacher\nmodels using input text in English. We propose a cross entropy based objective\nwhich forces the distribution over the student's text-video similarity scores\nto be similar to those of the teacher models. We introduce a new multilingual\nvideo dataset, Multi-YouCook2, by translating the English captions in the\nYouCook2 video dataset to 8 other languages. Our method improves multilingual\ntext-video retrieval performance on Multi-YouCook2 and several other datasets\nsuch as Multi-MSRVTT and VATEX. We also conducted an analysis on the\neffectiveness of different multilingual text models as teachers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We find the characteristic identities for the split Casimir operator in the\ndefining and adjoint representations of the $osp(M|N)$ and $s\\ell(M|N)$ Lie\nsuperalgebras. These identities are used to build the projectors onto invariant\nsubspaces of the representation $T^{\\otimes 2}$ of the $osp(M|N)$ and\n$s\\ell(M|N)$ Lie superalgebras in the cases when $T$ is the defining and\nadjoint representations. For defining representations, the $osp(M|N)$- and\n$s\\ell(M|N)$-invariant solutions of the Yang-Baxter equation are expressed as\nrational functions of the split Casimir operator. For the adjoint\nrepresentation, the characteristic identities and invariant projectors obtained\nare considered from the viewpoint of a universal description of Lie\nsuperalgebras by means of the Vogel parametrization. We also construct a\nuniversal generating function for higher Casimir operators of the $osp(M|N)$\nand $s\\ell(M|N)$ Lie superalgebras in the adjoint representation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Scientific knowledge develops through cumulative discoveries that build on,\ncontradict, contextualize, or correct prior findings. Scientists and\njournalists often communicate these incremental findings to lay people through\nvisualizations and text (e.g., the positive and negative effects of caffeine\nintake). Consequently, readers need to integrate diverse and contrasting\nevidence from multiple sources to form opinions or make decisions. However, the\nunderlying mechanism for synthesizing information from multiple visualizations\nremains underexplored. To address this knowledge gap, we conducted a series of\nfour experiments (N = 1166) in which participants synthesized empirical\nevidence from a pair of line charts presented sequentially. In Experiment 1, we\nadministered a baseline condition with charts depicting no specific context\nwhere participants held no strong belief. To test for the generalizability, we\nintroduced real-world scenarios to our visualizations in Experiment 2, and\nadded accompanying text descriptions similar to on-line news articles or blog\nposts in Experiment 3. In all three experiments, we varied the relative\ndirection and magnitude of line slopes within the chart pairs. We found that\nparticipants tended to weigh the positive slope more when the two charts\ndepicted relationships in the opposite direction (e.g., one positive slope and\none negative slope). Participants tended to weigh the less steep slope when the\ntwo charts depicted relationships in the same direction (e.g., both positive).\nThrough these experiments, we characterize participants' synthesis behaviors\ndepending on the relationship between the information they viewed, contribute\nto theories describing underlying cognitive mechanisms in information\nsynthesis, and describe design implications for data storytelling.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given a way to evaluate an unknown polynomial with integer coefficients, we\npresent new algorithms to recover its nonzero coefficients and corresponding\nexponents. As an application, we adapt this interpolation algorithm to the\nproblem of computing the exact quotient of two given polynomials. These methods\nare efficient in terms of the bit-length of the sparse representation, that is,\nthe number of nonzero terms, the size of coefficients, the number of variables,\nand the logarithm of the degree. At the core of our results is a new Monte\nCarlo randomized algorithm to recover a polynomial $f(x)$ with integer\ncoefficients given a way to evaluate $f(\\theta) \\bmod m$ for any chosen\nintegers $\\theta$ and $m$. This algorithm has nearly-optimal bit complexity,\nmeaning that the total bit-length of the probes, as well as the computational\nrunning time, is softly linear (ignoring logarithmic factors) in the bit-length\nof the resulting sparse polynomial. To our knowledge, this is the first sparse\ninterpolation algorithm with soft-linear bit complexity in the total output\nsize. For polynomials with integer coefficients, the best previously known\nresults have at least a cubic dependency on the bit-length of the exponents.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce a new algorithm for constructing the generalized suffix array of\na collection of highly similar strings. As a first step, we construct a\ncompressed representation of the matching statistics of the collection with\nrespect to a reference string. We then use this data structure to distribute\nsuffixes into a partial order, and subsequently to speed up suffix comparisons\nto complete the generalized suffix array. Our experimental evidence with a\nprototype implementation (a tool we call sacamats) shows that on string\ncollections with highly similar strings we can construct the suffix array in\ntime competitive with or faster than the fastest available methods. Along the\nway, we describe a heuristic for fast computation of the matching statistics of\ntwo strings, which may be of independent interest.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The social structure of an animal population can often influence movement and\ninform researchers on a species' behavioral tendencies. Animal social networks\ncan be studied through movement data; however, modern sources of data can have\nidentification issues that result in multiply-labeled individuals. Since all\navailable social movement models rely on unique labels, we extend an existing\nBayesian hierarchical movement model in a way that makes use of a latent social\nnetwork and accommodates multiply-labeled movement data (MLMD). We apply our\nmodel to drone-measured movement data from Risso's dolphins (Grampus griseus)\nand estimate the effects of sonar exposure on the dolphins' social structure.\nOur proposed framework can be applied to MLMD for various social movement\napplications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The first direct determination of the ground-state-to-ground-state $Q$ values\nof the $\\beta^-$ decay $^{76}$As $\\rightarrow$ $^{76}$Se and the\nelectron-capture decay $^{155}$Tb $\\rightarrow$ $^{155}$Gd was performed\nutilizing the double Penning trap mass spectrometer JYFLTRAP. By measuring the\natomic mass difference of the decay pairs via the phase-imaging\nion-cyclotron-resonance (PI-ICR) technique, the $Q$ values of $^{76}$As\n$\\rightarrow$ $^{76}$Se and $^{155}$Tb $\\rightarrow$ $^{155}$Gd were determined\nto be 2959.265(74) keV and 814.94(18) keV, respectively. The precision was\nincreased relative to earlier measurements by factors of 12 and 57,\nrespectively. The new $Q$ values are 1.33 keV and 5 keV lower compared to the\nvalues adopted in the most recent Atomic Mass Evaluation 2020. With the newly\ndetermined ground-state-to-ground-state $Q$ values combined with the excitation\nenergy from $\\gamma$-ray spectroscopy, the $Q$ values for\nground-state-to-excited-state transitions $^{76}$As (ground state)\n$\\rightarrow$ $^{76}$Se$^*$ (2968.4(7) keV) and $^{155}$Tb (ground state)\n$\\rightarrow$ $^{155}$Gd$^*$ (815.731(3) keV) were derived to be -9.13(70) keV\nand -0.79(18) keV. Thus we have confirmed that both of the $\\beta^{-}$-decay\nand EC-decay candidate transitions are energetically forbidden at a level of at\nleast 4$\\sigma$, thus definitely excluding these two cases from the list of\npotential candidates for the search of low-$Q$-value $\\beta^-$ or EC decays to\ndetermine the electron-(anti)neutrino mass.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The pioneering method for unsupervised meta-learning, CACTUs, is a\nclustering-based approach with pseudo-labeling. This approach is model-agnostic\nand can be combined with supervised algorithms to learn from unlabeled data.\nHowever, it often suffers from label inconsistency or limited diversity, which\nleads to poor performance. In this work, we prove that the core reason for this\nis lack of a clustering-friendly property in the embedding space. We address\nthis by minimizing the inter- to intra-class similarity ratio to provide\nclustering-friendly embedding features, and validate our approach through\ncomprehensive experiments. Note that, despite only utilizing a simple\nclustering algorithm (k-means) in our embedding space to obtain the\npseudo-labels, we achieve significant improvement. Moreover, we adopt a\nprogressive evaluation mechanism to obtain more diverse samples in order to\nfurther alleviate the limited diversity problem. Finally, our approach is also\nmodel-agnostic and can easily be integrated into existing supervised methods.\nTo demonstrate its generalization ability, we integrate it into two\nrepresentative algorithms: MAML and EP. The results on three main few-shot\nbenchmarks clearly show that the proposed method achieves significant\nimprovement compared to state-of-the-art models. Notably, our approach also\noutperforms the corresponding supervised method in two tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Traditionally, Text Simplification is treated as a monolingual translation\ntask where sentences between source texts and their simplified counterparts are\naligned for training. However, especially for longer input documents,\nsummarizing the text (or dropping less relevant content altogether) plays an\nimportant role in the simplification process, which is currently not reflected\nin existing datasets. Simultaneously, resources for non-English languages are\nscarce in general and prohibitive for training new solutions. To tackle this\nproblem, we pose core requirements for a system that can jointly summarize and\nsimplify long source documents. We further describe the creation of a new\ndataset for joint Text Simplification and Summarization based on German\nWikipedia and the German children's lexicon \"Klexikon\", consisting of almost\n2900 documents. We release a document-aligned version that particularly\nhighlights the summarization aspect, and provide statistical evidence that this\nresource is well suited to simplification as well. Code and data are available\non Github: https://github.com/dennlinger/klexikon\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Heart rate variability results from the combined activity of several\nphysiological systems, including the cardiac, vascular, and respiratory systems\nwhich have their own internal regulation, but also interact with each other to\npreserve the homeostatic function. These control mechanisms operate across\nmultiple temporal scales, resulting in the simultaneous presence of short-term\ndynamics and long-range correlations. The Network Physiology framework provides\nstatistical tools based on information theory able to quantify structural\naspects of multivariate and multiscale interconnected mechanisms driving the\ndynamics of complex physiological networks. In this work, the multiscale\nrepresentation of Transfer Entropy from Systolic Arterial Pressure (S) and\nRespiration (R) to Heart Period (H) and of its decomposition into unique,\nredundant and synergistic contributions is obtained using a Vector\nAutoRegressive Fractionally Integrated (VARFI) framework for Gaussian\nprocesses. This novel approach allows to quantify the directed information flow\naccounting for the simultaneous presence of short-term dynamics and long-range\ncorrelations among the analyzed processes. The approach is first illustrated in\nsimulated VARFI processes and then applied to H, S and R time series measured\nin healthy subjects monitored at rest and during mental and postural stress.\nOur results highlight the dependence of the information transfer on the balance\nbetween short-term and long-range correlations in coupled dynamical systems,\nwhich cannot be observed using standard methods that do not consider long-range\ncorrelations. The proposed methodology shows that postural stress induces\nlarger redundant effects at short time scales and mental stress induces larger\ncardiovascular information transfer at longer time scales.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper introduces AGAPECert, an Auditable, Generalized, Automated,\nPrivacy-Enabling, Certification framework capable of performing auditable\ncomputation on private data and reporting real-time aggregate certification\nstatus without disclosing underlying private data. AGAPECert utilizes a novel\nmix of trusted execution environments, blockchain technologies, and a real-time\ngraph-based API standard to provide automated, oblivious, and auditable\ncertification. Our technique allows a privacy-conscious data owner to run\npre-approved Oblivious Smart Contract code in their own environment on their\nown private data to produce Private Automated Certifications. These\ncertifications are verifiable, purely functional transformations of the\navailable data, enabling a third party to trust that the private data must have\nthe necessary properties to produce the resulting certification. Recently, a\nmultitude of solutions for certification and traceability in supply chains have\nbeen proposed. These often suffer from significant privacy issues because they\ntend to take a\" shared, replicated database\" approach: every node in the\nnetwork has access to a copy of all relevant data and contract code to\nguarantee the integrity and reach consensus, even in the presence of malicious\nnodes. In these contexts of certifications that require global coordination,\nAGAPECert can include a blockchain to guarantee ordering of events, while\nkeeping a core privacy model where private data is not shared outside of the\ndata owner's own platform. AGAPECert contributes an open-source certification\nframework that can be adopted in any regulated environment to keep sensitive\ndata private while enabling a trusted automated workflow.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Bonding resonant modes of plasmonic nanoantennas with narrow gaps exhibit\nvery large local field enhancement. These hotspots are highly attractive for\nboosting optical nonlinearities, such as second harmonic generation. However,\nfor resonant symmetric gap antennas, the strong second harmonic sources created\nat the gap interfaces oscillate out-of-phase and therefore interfere\ndestructively in the far-field. Here, we use an advanced nanofabrication\napproach to systematically break the local symmetry of nanoscopic antenna gaps\nwhile retaining the bonding resonant antenna mode at the fundamental frequency\nand the concomitant intensity hotspot. We find that antennas with the strongest\nlocal symmetry breaking emit correspondingly intense second harmonic radiation\nas compared to symmetric reference structures. By combining these findings with\nsecond harmonic radiation patterns as well as quantitative nonlinear\nsimulations, we obtain remarkably detailed insights into the mechanism of\nsecond harmonic generation at the nanoscale. Our findings open new perspectives\nfor the realization of non-reciprocal nanoscale systems, where local symmetry\nbreaking is crucial to create unique functionalities.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we consider weighted Morrey spaces ${\\mathcal M}_{\\lambda,\n{\\mathcal F}}^p(w)$ adapted to a family of cubes ${\\mathcal F}$, with norm\n$$\\|f\\|_{{\\mathcal M}_{\\lambda, {\\mathcal F}}^p(w)}:=\\sup_{Q\\in {\\mathcal\nF}}\\left(\\frac{1}{|Q|^{\\lambda}}\\int_Q|f|^pw\\right)^{1/p},$$ and the question\nwe deal with is whether a Muckenhoupt-type condition characterizes the\nboundedness of the Hardy--Littlewood maximal operator on ${\\mathcal\nM}_{\\lambda, {\\mathcal F}}^p(w)$.\n  In the case of the global Morrey spaces (when ${\\mathcal F}$ is the family of\nall cubes in ${\\mathbb R}^n$) this question is still open. In the case of the\nlocal Morrey spaces (when ${\\mathcal F}$ is the family of all cubes centered at\nthe origin) this question was answered positively in a recent work of\nDuoandikoetxea--Rosenthal \\cite{DR21}.\n  We obtain an extension of \\cite{DR21} by showing that the answer is positive\nwhen ${\\mathcal F}$ is the family of all cubes centered at a sequence of points\nin ${\\mathbb R}^n$ satisfying a certain lacunary-type condition.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The growth of a tissue, which depends on cell-cell interactions and\nbiologically relevant process such as cell division and apoptosis, is regulated\nby a mechanical feedback mechanism. We account for these effects in a minimal\ntwo-dimensional model in order to investigate the consequences of mechanical\nfeedback, which is controlled by a critical pressure, $p_c$. A cell can only\ngrow and divide if the pressure it experiences, due to interaction with its\nneighbors, is less than $p_c$. Because temperature is an irrelevant variable in\nthe model, the cell dynamics is driven by self-generated active forces (SGAFs)\nthat are created by cell division. It is shown that even in the absence of\nintercellular interactions, cells undergo diffusive behavior. The SGAF driven\ndiffusion is indistinguishable from the well-known dynamics of a free Brownian\nparticle at a fixed finite temperature. When intercellular interactions are\ntaken into account, we find persistent temporal correlations in the force-force\nautocorrelation function ($FAF$) that extends over timescale of several cell\ndivision times. The time-dependence of the $FAF$ reveals memory effects, which\nincreases as pc increases. The observed non-Markovian effects emerge due to the\ninterplay of cell division and mechanical feedback, and is inherently a\nnon-equilibrium phenomenon.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Conventional model quantization methods use a fixed quantization scheme to\ndifferent data samples, which ignores the inherent \"recognition difficulty\"\ndifferences between various samples. We propose to feed different data samples\nwith varying quantization schemes to achieve a data-dependent dynamic\ninference, at a fine-grained layer level. However, enabling this adaptive\ninference with changeable layer-wise quantization schemes is challenging\nbecause the combination of bit-widths and layers is growing exponentially,\nmaking it extremely difficult to train a single model in such a vast searching\nspace and use it in practice. To solve this problem, we present the Arbitrary\nBit-width Network (ABN), where the bit-widths of a single deep network can\nchange at runtime for different data samples, with a layer-wise granularity.\nSpecifically, first we build a weight-shared layer-wise quantizable\n\"super-network\" in which each layer can be allocated with multiple bit-widths\nand thus quantized differently on demand. The super-network provides a\nconsiderably large number of combinations of bit-widths and layers, each of\nwhich can be used during inference without retraining or storing myriad models.\nSecond, based on the well-trained super-network, each layer's runtime bit-width\nselection decision is modeled as a Markov Decision Process (MDP) and solved by\nan adaptive inference strategy accordingly. Experiments show that the\nsuper-network can be built without accuracy degradation, and the bit-widths\nallocation of each layer can be adjusted to deal with various inputs on the\nfly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement\nwhile saving 36.2% BitOps.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Face presentation attack detection (PAD) is critical to secure face\nrecognition (FR) applications from presentation attacks. FR performance has\nbeen shown to be unfair to certain demographic and non-demographic groups.\nHowever, the fairness of face PAD is an understudied issue, mainly due to the\nlack of appropriately annotated data. To address this issue, this work first\npresents a Combined Attribute Annotated PAD Dataset (CAAD-PAD) by combining\nseveral well-known PAD datasets where we provide seven human-annotated\nattribute labels. This work then comprehensively analyses the fairness of a set\nof face PADs and its relation to the nature of training data and the\nOperational Decision Threshold Assignment (ODTA) on different data groups by\nstudying four face PAD approaches on our CAAD-PAD. To simultaneously represent\nboth the PAD fairness and the absolute PAD performance, we introduce a novel\nmetric, namely the Accuracy Balanced Fairness (ABF). Extensive experiments on\nCAAD-PAD show that the training data and ODTA induce unfairness on gender,\nocclusion, and other attribute groups. Based on these analyses, we propose a\ndata augmentation method, FairSWAP, which aims to disrupt the identity/semantic\ninformation and guide models to mine attack cues rather than attribute-related\ninformation. Detailed experimental results demonstrate that FairSWAP generally\nenhances both the PAD performance and the fairness of face PAD.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape\nrepresentation. To capture geometry details, current methods usually learn DIF\nusing local latent codes, which discretize the space into a regular 3D grid (or\noctree) and store local codes in grid points (or octree nodes). Given a query\npoint, the local feature is computed by interpolating its neighboring local\ncodes with their positions. However, the local codes are constrained at\ndiscrete and regular positions like grid points, which makes the code positions\ndifficult to be optimized and limits their representation ability. To solve\nthis problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF.\nOur method explicitly associates local codes with learnable position vectors,\nand the position vectors are continuous and can be dynamically optimized, which\nimproves the representation ability. In addition, we propose a novel code\nposition loss to optimize the code positions, which heuristically guides more\nlocal codes to be distributed around complex geometric details. In contrast to\nprevious methods, our DCC-DIF represents 3D shapes more efficiently with a\nsmall amount of local codes, and improves the reconstruction quality.\nExperiments demonstrate that DCC-DIF achieves better performance over previous\nmethods. Code and data are available at https://github.com/lity20/DCCDIF.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hyper-spectral images are images captured from a satellite that gives spatial\nand spectral information of specific region.A Hyper-spectral image contains\nmuch more number of channels as compared to a RGB image, hence containing more\ninformation about entities within the image. It makes them well suited for the\nclassification of objects in a snap. In the past years, the efficiency of\nhyper-spectral image recognition has increased significantly with deep\nlearning. The Convolution Neural Network(CNN) and Multi-Layer Perceptron(MLP)\nhas demonstrated to be an excellent process of classifying images. However,\nthey suffer from the issues of long training time and requirement of large\namounts of the labeled data, to achieve the expected outcome. These issues\nbecome more complex while dealing with hyper-spectral images. To decrease the\ntraining time and reduce the dependence on large labeled data-set, we propose\nusing the method of transfer learning.The features learned by CNN and MLP\nmodels are then used by the transfer learning model to solve a new\nclassification problem on an unseen dataset. A detailed comparison of CNN and\nmultiple MLP architectural models is performed, to determine an optimum\narchitecture that suits best the objective. The results show that the scaling\nof layers not always leads to increase in accuracy but often leads to\nover-fitting, and also an increase in the training time.The training time is\nreduced to greater extent by applying the transfer learning approach rather\nthan just approaching the problem by directly training a new model on large\ndata-sets, without much affecting the accuracy.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper provides the first large-scale data-driven analysis to evaluate\nthe predictive power of different attributes for assessing risk of cyberattack\ndata breaches. Furthermore, motivated by rapid increase in third party enabled\ncyberattacks, the paper provides the first quantitative empirical evidence that\ndigital supply-chain attributes are significant predictors of enterprise cyber\nrisk. The paper leverages outside-in cyber risk scores that aim to capture the\nquality of the enterprise internal cybersecurity management, but augment these\nwith supply chain features that are inspired by observed third party\ncyberattack scenarios, as well as concepts from network science research. The\nmain quantitative result of the paper is to show that supply chain network\nfeatures add significant detection power to predicting enterprise cyber risk,\nrelative to merely using enterprise-only attributes. Particularly, compared to\na base model that relies only on internal enterprise features, the supply chain\nnetwork features improve the out-of-sample AUC by 2.3\\%. Given that each cyber\ndata breach is a low probability high impact risk event, these improvements in\nthe prediction power have significant value. Additionally, the model highlights\nseveral cybersecurity risk drivers related to third party cyberattack and\nbreach mechanisms and provides important insights as to what interventions\nmight be effective to mitigate these risks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Existing studies on multimodal sentiment analysis heavily rely on textual\nmodality and unavoidably induce the spurious correlations between textual words\nand sentiment labels. This greatly hinders the model generalization ability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sentiment analysis. This task aims to estimate and mitigate the bad\neffect of textual modality for strong OOD generalization. To this end, we\nembrace causal inference, which inspects the causal relationships via a causal\ngraph. From the graph, we find that the spurious correlations are attributed to\nthe direct effect of textual modality on the model prediction while the\nindirect one is more reliable by considering multimodal semantics. Inspired by\nthis, we devise a model-agnostic counterfactual framework for multimodal\nsentiment analysis, which captures the direct effect of textual modality via an\nextra text model and estimates the indirect one by a multimodal model. During\nthe inference, we first estimate the direct effect by the counterfactual\ninference, and then subtract it from the total effect of all modalities to\nobtain the indirect effect for reliable prediction. Extensive experiments show\nthe superior effectiveness and generalization ability of our proposed\nframework.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Local Unitarity (LU) representation of differential cross-sections\nlocally realises the cancellations of infrared singularities predicted by the\nKinoshita-Lee-Nauenberg theorem. In this work we solve the two remaining\nchallenges to enable practical higher-loop computations within the LU\nformalism. The first concerns the generalisation of the LU representation to\ngraphs with raised propagators. The solution to this problem results in a\ngeneralisation of distributional Cutkosky rules. The second concerns the\nregularisation of ultraviolet and spurious soft singularities, solved using a\nfully automated and local renormalisation procedure based on Bogoliubov's\nR-operation. We detail an all-order construction for the hybrid\n$\\overline{\\text{MS}}$ and On-Shell scheme whose only analytic input is\nsingle-scale vacuum diagrams. Using this novel technology, we provide\n(semi-)inclusive results for two multi-leg processes at NLO, study limits of\nindividual supergraphs up to N3LO and present the first physical NNLO\ncross-sections computed fully numerically in momentum-space, namely for the\nprocesses $\\gamma^* \\rightarrow j j$ and $\\gamma^* \\rightarrow t \\bar{t}$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Computation of a tensor singular value decomposition (t-SVD) with a few\nnumber of passes over the underlying data tensor is crucial in using modern\ncomputer architectures, where the main concern is the communication cost. The\ncurrent subspace randomized algorithms for computation of the t-SVD, need 2q +\n2 number of passes over the data tensor where q is a non-negative integer\nnumber (power iteration parameter). In this paper, we propose a new and\nflexible randomized algorithm which works for any number of passes v, not\nnecessarily being an even number. It is a generalization of the methods\ndeveloped for matrices to tensors. The expected error bound of the proposed\nalgorithm is derived. Several numerical experiments are conducted and the\nresults confirmed that the proposed algorithm is efficient and applicable. We\nalso use our proposed method to develop a fast algorithm for tensor completion\nproblem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Non-orthogonal multiple access (NOMA) is a powerful transmission technique\nthat enhances the spectral efficiency of communication links, and is being\ninvestigated for 5G standards and beyond. A major drawback of NOMA is the need\nto apply successive interference cancellation (SIC) at the receiver on a\nsymbol-by-symbol basis, which limits its practicality. To circumvent this, in\nthis paper a novel constructive multiple access (CoMA) scheme is proposed and\ninvestigated. CoMA aligns the superimposed signals to the different users\nconstructively to the signal of interest. Since the superimposed signal aligns\nwith the data signal, there is no need to remove it at the receiver using SIC.\nAccordingly, SIC component can be removed at the receiver side. In this regard\nand in order to provide a comprehensive investigation and comparison, different\noptimization problems for user paring NOMA multiple-input-single-output (MISO)\nsystems are considered. Firstly, an optimal precoder to minimize the total\ntransmission power for CoMA subject to a quality-of-service constraint is\nobtained, and compared to conventional NOMA. Then, a precoder that minimizes\nthe CoMA symbol error rate (SER) subject to power constraint is investigated.\nFurther, the computational complexity of CoMA is considered and compared with\nconventional NOMA scheme in terms of total number of complex operations. The\nresults in this paper prove the superiority of the proposed CoMA scheme over\nthe conventional NOMA technique, and demonstrate that CoMA is an attractive\nsolution for user paring NOMA MISO systems with low number of BS antennas,\nwhile circumventing the receive SIC complexity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this research, we propose a tiny image segmentation model, L^3U-net, that\nworks on low-resource edge devices in real-time. We introduce a data folding\ntechnique that reduces inference latency by leveraging the parallel\nconvolutional layer processing capability of the CNN accelerators. We also\ndeploy the proposed model to such a device, MAX78000, and the results show that\nL^3U-net achieves more than 90% accuracy over two different segmentation\ndatasets with 10 fps.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Liesel is a probabilistic programming framework focusing on but not limited\nto semi-parametric regression. It comprises a graph-based model building\nlibrary, a Markov chain Monte Carlo (MCMC) library with support for modular\ninference algorithms combining multiple kernels (both implemented in Python),\nand an R interface (RLiesel) for the configuration of semi-parametric\nregression models. Each component can be used independently of the others, e.g.\nthe MCMC library also works with third-party model implementations. Our goal\nwith Liesel is to facilitate a new research workflow in computational\nstatistics: In a first step, the researcher develops a model graph with\npre-implemented and well-tested building blocks as a base model, e.g. using\nRLiesel. Then, the graph can be manipulated to incorporate new research ideas,\nbefore the MCMC library can be used to run and analyze a default or\nuser-defined MCMC procedure. The researcher has the option to combine powerful\nMCMC algorithms such as the No U-Turn Sampler (NUTS) with self-written kernels.\nVarious tools for chain post-processing and diagnostics are also provided.\nConsidering all its components, Liesel enables efficient and reliable\nstatistical research on complex models and estimation algorithms. It depends on\nJAX as a numerical computing library. This way, it can benefit from the latest\nmachine learning technology such as automatic differentiation, just-in-time\n(JIT) compilation, and the use of high-performance computing devices such as\ntensor processing units (TPUs).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we propose a probabilistic continuous-time visual-inertial\nodometry (VIO) for rolling shutter cameras. The continuous-time trajectory\nformulation naturally facilitates the fusion of asynchronized high-frequency\nIMU data and motion-distorted rolling shutter images. To prevent intractable\ncomputation load, the proposed VIO is sliding-window and keyframe-based. We\npropose to probabilistically marginalize the control points to keep the\nconstant number of keyframes in the sliding window. Furthermore, the line\nexposure time difference (line delay) of the rolling shutter camera can be\nonline calibrated in our continuous-time VIO. To extensively examine the\nperformance of our continuous-time VIO, experiments are conducted on\npublicly-available WHU-RSVI, TUM-RSVI, and SenseTime-RSVI rolling shutter\ndatasets. The results demonstrate the proposed continuous-time VIO\nsignificantly outperforms the existing state-of-the-art VIO methods. The\ncodebase of this paper will also be open-sourced at\n\\url{https://github.com/APRIL-ZJU/Ctrl-VIO}.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we investigate the equilibrium properties of bidirectional\nassociative memories (BAMs). Introduced by Kosko in 1988 as a generalization of\nthe Hopfield model to a bipartite structure, the simplest architecture is\ndefined by two layers of neurons, with synaptic connections only between units\nof different layers: even without internal connections within each layer,\ninformation storage and retrieval are still possible through the reverberation\nof neural activities passing from one layer to another. We characterize the\ncomputational capabilities of a stochastic extension of this model in the\nthermodynamic limit, by applying rigorous techniques from statistical physics.\nA detailed picture of the phase diagram at the replica symmetric level is\nprovided, both at finite temperature and in the noiseless regime. An analytical\nand numerical inspection of the transition curves (namely critical lines\nsplitting the various modes of operation of the machine) is carried out as the\ncontrol parameters - noise, load and asymmetry between the two layer sizes -\nare tuned. In particular, with a finite asymmetry between the two layers, it is\nshown how the BAM can store information more efficiently than the Hopfield\nmodel by requiring less parameters to encode a fixed number of patterns.\nComparisons are made with numerical simulations of neural dynamics. Finally, a\nlow-load analysis is carried out to explain the retrieval mechanism in the BAM\nby analogy with two interacting Hopfield models. A potential equivalence with\ntwo coupled Restricted Boltmzann Machines is also discussed.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We examine the extreme situation of radiation from an electron that is\nasymptotically accelerated to the speed of light, resulting in finite emission\nenergy. The analytic solution explicitly demonstrates the difference between\nradiation power loss and kinetic power loss (null).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop, analyze and test adaptive penalty parameter methods. We prove\nunconditional stability for velocity when adapting the penalty parameter,\n$\\epsilon,$ and stability of the velocity time derivative under a condition on\nthe change of the penalty parameter, $\\epsilon(t_{n+1})-\\epsilon(t_n)$. The\nanalysis and tests show that adapting $\\epsilon(t_{n+1})$ in response to\n$\\nabla\\cdot u(t_n)$ removes the problem of picking $\\epsilon$ and yields good\napproximations for the velocity. We provide error analysis and numerical tests\nto support these results. We supplement the adaptive-$\\epsilon$ method by also\nadapting the time-step. The penalty parameter $\\epsilon$ and time-step are\nadapted independently. We further compare first, second and variable order\ntime-step algorithms. Accurate recovery of pressure remains an open problem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A generalization of the KP equation involving higher-order dispersion is\nstudied. This equation appears in several physical applications. As new\nresults, the Lie point symmetries are obtained and used to derive conservation\nlaws via Noether's theorem by introduction of a potential which gives a\nLagrangian formulation for the equation. The meaning and properties of the\nsymmetries and the conserved quantities are described. Explicit line soliton\nsolutions are found and their features are discussed. They are shown to\ndescribe dark solitary waves on a background which depends on a dispersion\nratio and on the speed and direction of the waves. The zero-background case is\nexplored.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the observational implications of a class of inflationary models\nwherein the inflaton is coupled to the Einstein tensor through a generalized\nnon-minimal derivative coupling (GNMDC). Such a coupling can be realized in the\nframework of Horndeski theories or generalized Galileon theories and leads to\nnovel and distinguishable inflationary predictions. In particular, we explore\nwhether such models can provide a possible explanation to the large scale\nanomalies such as the power suppression and other localized features associated\nwith the CMB temperature and polarisation anisotropies at low multipoles or\nlarge angular scales. For a specific choice of the GNMDC coupling function, we\nfind that these models can lead to suitable localized features in the power\nspectrum on large scales. To our knowledge, such models have not been used\nearlier to explain the origin of these modulations in the CMB power spectrum.\nWe work in the regime of parameter space such that we avoid the gradient\ninstability and the superluminal propagation of scalar perturbations. A very\ninteresting aspect of our analysis is that a class of inflationary models such\nas the hilltop-quartic model results in a better agreement with the Planck data\nin the presence of an additional GNMDC term. Further, we compare the GNMDC\nmodel with the data using CosmoMC and find that these models provide a\nconsiderable improvement over the best fit reference $\\Lambda$CDM model with a\nfeatureless, power law, primordial spectrum. Future CMB experiments such as\nCMB-S4 will certainly impose better constraints on various parameters of the\nGNMDC scenario. Finally, we discuss the wider implications of our results and\nargue that these models may also be useful in providing an explanation to other\nanomalies associated with the CMB observations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Weyl semimetals realize massless relativistic fermions with two Weyl nodes\nseparated in energy and momentum space, whose low-energy physics is described\nby Dirac fermions with an axial gauge constant. Here, we study their\nelectromagnetic linear responses based on the effective field theory and on the\nchiral kinetic theory. Although the static chiral magnetic effect is canceled\nby the Chern-Simons current under the Pauli-Villars regularization, a dynamical\nmagnetic field is found capable of driving an electric current along its\ndirection, with the total transported charge being independent of temperature\nand chemical potential for a uniform field. We also incorporate dissipation in\nthe relaxation-time approximation and study collective excitations coupled with\nMaxwell electromagnetic fields. Their dispersion relations at low frequency and\nlong wavelength are determined only by electric, chiral magnetic, and anomalous\nHall conductivities, which predict unstable modes leading to anisotropic\ngeneration of electromagnetic waves oriented to the direction of Weyl node\nseparation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In robotic networks relying on noisy range measurements between agents for\ncooperative localization, the achievable positioning accuracy strongly strongly\ndepends on the network geometry. This motivates the problem of planning robot\ntrajectories in such multi-robot systems in a way that maintains high\nlocalization accuracy. We present potential-based planning methods, where\nlocalizability potentials are introduced to characterize the quality of the\nnetwork geometry for cooperative position estimation. These potentials are\nbased on Cramer Rao Lower Bounds (CRLB) and provide a theoretical lower bound\non the error covariance achievable by any unbiased position estimator. In the\nprocess, we establish connections between CRLBs and the theory of graph\nrigidity, which has been previously used to plan the motion of robotic\nnetworks. We develop decentralized deployment algorithms appropriate for large\nnetworks, and we use equality-constrained CRLBs to extend the concept of\nlocalizability to scenarios where additional information about the relative\npositions of the ranging sensors is known. We illustrate the resulting robot\ndeployment methodology through simulated examples and an experiment.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Harris and Venkatesh made a conjecture relating the derived Hecke operators\nand the adjoint motivic cohomology in the setting of weight one modular forms.\nThis conjecture was proved under some conditions in the dihedral case by\nDarmon--Harris--Rotger--Venkatesh. We use a new approach to prove more general\ncases of the conjecture (up to sign). Our approach relies on Waldspurger's\nformula for the central value of Rankin L-series and Ichino's formula for the\ntriple product L-function.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Polypharmacy, defined as the use of multiple drugs together, is a standard\ntreatment method, especially for severe and chronic diseases. However, using\nmultiple drugs together may cause interactions between drugs. Drug-drug\ninteraction (DDI) is the activity that occurs when the impact of one drug\nchanges when combined with another. DDIs may obstruct, increase, or decrease\nthe intended effect of either drug or, in the worst-case scenario, create\nadverse side effects. While it is critical to detect DDIs on time, it is\ntimeconsuming and expensive to identify them in clinical trials due to their\nshort duration and many possible drug pairs to be considered for testing. As a\nresult, computational methods are needed for predicting DDIs. In this paper, we\npresent a novel heterogeneous graph attention model, HAN-DDI to predict\ndrug-drug interactions. We create a heterogeneous network of drugs with\ndifferent biological entities. Then, we develop a heterogeneous graph attention\nnetwork to learn DDIs using relations of drugs with other entities. It consists\nof an attention-based heterogeneous graph node encoder for obtaining drug node\nrepresentations and a decoder for predicting drug-drug interactions. Further,\nwe utilize comprehensive experiments to evaluate of our model and to compare it\nwith state-of-the-art models. Experimental results show that our proposed\nmethod, HAN-DDI, outperforms the baselines significantly and accurately\npredicts DDIs, even for new drugs.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hi-5 is the L'-band (3.5-4.0 $\\mu$m) high-contrast imager of Asgard, an\ninstrument suite in preparation for the visitor focus of the VLTI. The system\nis optimized for high-contrast and high-sensitivity imaging within the\ndiffraction limit of a single UT/AT telescope. It is designed as a\ndouble-Bracewell nulling instrument producing spectrally-dispersed (R=20, 400,\nor 2000) complementary nulling outputs and simultaneous photometric outputs for\nself-calibration purposes. In this paper, we present an update of the project\nwith a particular focus on the overall architecture, opto-mechanical design of\nthe warm and cold optics, injection system, and development of the photonic\nbeam combiner. The key science projects are to survey (i) nearby young\nplanetary systems near the snow line, where most giant planets are expected to\nbe formed, and (ii) nearby main sequence stars near the habitable zone where\nexozodiacal dust that may hinder the detection of Earth-like planets. We\npresent an update of the expected instrumental performance based on full\nend-to-end simulations using the new GRAVITY+ specifications of the VLTI and\nthe latest planet formation models.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A lot of research on elastic interfaces has been done on systems where the\ninterface is pushed with a constant force. We studied the velocity of an\ninterface under a periodic subcritical driving, which is relevant in for\nexample magnetic hysteresis and fracturing of materials under repetitive\nloading. We obtained a modified version of the creep velocity that describes\ninterfaces under small constant drivings and thermal noise. For short-range\nelastic systems, the velocity follows an approximate power law with a material\ndependent exponent. Long ranged systems have simpler behaviour without a\nmaterial dependent exponent.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The XRootD system is used to transfer, store, and cache large datasets from\nhigh-energy physics (HEP). In this study we focus on its capability as\ndistributed on-demand storage cache. Through exploring a large set of daily log\nfiles between 2020 and 2021, we seek to understand the data access patterns\nthat might inform future cache design. Our study begins with a set of summary\nstatistics regarding file read operations, file lifetimes, and file transfers.\nWe observe that the number of read operations on each file remains nearly\nconstant, while the average size of a read operation grows over time.\nFurthermore, files tend to have a consistent length of time during which they\nremain open and are in use. Based on this comprehensive study of the cache\naccess statistics, we developed a cache simulator to explore the behavior of\ncaches of different sizes. Within a certain size range, we find that increasing\nthe XRootD cache size improves the cache hit rate, yielding faster overall file\naccess. In particular, we find that increase the cache size from 40TB to 56TB\ncould increase the hit rate from 0.62 to 0.89, which is a significant increase\nin cache effectiveness for modest cost.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the combined Chandra and Swift-BAT spectral analysis of nine\nlow-redshift (z <= 0.10), candidate heavily obscured active galactic nuclei\n(AGN) selected from the Swift-BAT 150-month catalog. We located soft (1-10 keV)\nX-ray counterparts to these BAT sources and joint fit their spectra with\nphysically motivated models.The spectral analysis in the 1-150 keV energy band\ndetermined that all sources are obscured, with a line-of-sight column density\nNH >= 10^22 cm^-2 at a 90% confidence level. Four of these sources show\nsignificant obscuration with NH >= 10^23 cm^-2 and two additional sources are\ncandidate Compton-thick Active Galactic Nuclei (CT-AGNs) with NH >= 10^24\ncm^-2.These two sources, 2MASX J02051994-0233055 and IRAS 11058-1131, are the\nlatest addition to the previous 3 CT-AGN candidates found using our strategy\nfor soft X-ray follow-up of BAT sources. In here we present the results of our\nmethodology so far, and analyze the effectiveness of applying different\nselection criteria to discover CT-AGN in the local Universe. Our selection\ncriteria has a ~20% success rate of discovering heavily obscured AGN whose CT\nnature is confirmed by follow-up NuSTAR observations. This is much higher than\nthe ~5% found in blind surveys.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose generative channel modeling to learn statistical channel models\nfrom channel input-output measurements. Generative channel models can learn\nmore complicated distributions and represent the field data more faithfully.\nThey are tractable and easy to sample from, which can potentially speed up the\nsimulation rounds. To achieve this, we leverage advances in GAN, which helps us\nlearn an implicit distribution over stochastic MIMO channels from observed\nmeasurements. In particular, our approach MIMO-GAN implicitly models the\nwireless channel as a distribution of time-domain band-limited impulse\nresponses. We evaluate MIMO-GAN on 3GPP TDL MIMO channels and observe\nhigh-consistency in capturing power, delay and spatial correlation statistics\nof the underlying channel. In particular, we observe MIMO-GAN achieve errors of\nunder 3.57 ns average delay and -18.7 dB power.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The numbers $f_\\lambda$ of standard tableaux of shape $\\lambda\\vdash n$\nsatisfy 2 fundamental recursions: $f_\\lambda = \\sum f_{\\lambda^-}$ and $(n +\n1)f_\\lambda=\\sum f_{\\lambda^+}$, where $\\lambda^-$ and $\\lambda^+$ run over all\nshapes obtained from $\\lambda$ by adding or removing a square respectively. The\nfirst of these recursions is trivial; the second can be proven algebraically\nfrom the first. These recursions together imply algebraically the dimension\nformula $n! =\\sum f_\\lambda^2$ for the irreducible representations of $S_n$. We\nshow that a combinatorial analysis of this classical algebraic argument\nproduces an infinite family of algorithms, among which are the classical\nRobinson-Schensted row and column insertion algorithms. Each of our algorithms\nyields a bijective proof of the dimension formula.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Asynchronous learning protocols have regained attention lately, especially in\nthe Federated Learning (FL) setup, where slower clients can severely impede the\nlearning process. Herein, we propose \\texttt{AsyncDrop}, a novel asynchronous\nFL framework that utilizes dropout regularization to handle device\nheterogeneity in distributed settings. Overall, \\texttt{AsyncDrop} achieves\nbetter performance compared to state of the art asynchronous methodologies,\nwhile resulting in less communication and training time overheads. The key idea\nrevolves around creating ``submodels'' out of the global model, and\ndistributing their training to workers, based on device heterogeneity. We\nrigorously justify that such an approach can be theoretically characterized. We\nimplement our approach and compare it against other asynchronous baselines,\nboth by design and by adapting existing synchronous FL algorithms to\nasynchronous scenarios. Empirically, \\texttt{AsyncDrop} reduces the\ncommunication cost and training time, while matching or improving the final\ntest accuracy in diverse non-i.i.d. FL scenarios.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We initiate a systematic study of continuously self-similar (CSS)\ngravitational dynamics in two dimensions, motivated by critical phenomena\nobserved in higher dimensional gravitational theories. We consider CSS\nspacetimes admitting a homothetic Killing vector (HKV) field. For a general\ntwo-dimensional gravitational theory coupled to a dilaton field and Maxwell\nfield, we find that the assumption of continuous self-similarity determines the\nform of the dilaton coupling to the curvature. Certain limits produce two\nimportant classes of models, one of which is closely related to two-dimensional\ntarget space string theory and the other being Liouville gravity. The gauge\nfield is shown to produce a shift in the dilaton potential strength. We\nconsider static black hole solutions and find spacetimes with uncommon\nasymptotic behaviour. We show the vacuum self-similar spacetimes to be special\nlimits of the static solutions. We add matter fields consistent with\nself-similarity (including a certain model of semi-classical gravity) and write\ndown the autonomous ordinary differential equations governing the gravitational\ndynamics. Based on the phenomenon of finite-time blow-up in ODEs, we argue that\nspacetime singularities are generic in our models. We present qualitatively\ndiverse results from analytical and numerical investigations regarding matter\nfield collapse and singularities. We find interesting hints of a Choptuik-like\nscaling law.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Latent space exploration is a technique that discovers interpretable latent\ndirections and manipulates latent codes to edit various attributes in images\ngenerated by generative adversarial networks (GANs). However, in previous work,\nspatial control is limited to simple transformations (e.g., translation and\nrotation), and it is laborious to identify appropriate latent directions and\nadjust their parameters. In this paper, we tackle the problem of editing the\nStyleGAN image layout by annotating the image directly. To do so, we propose an\ninteractive framework for manipulating latent codes in accordance with the user\ninputs. In our framework, the user annotates a StyleGAN image with locations\nthey want to move or not and specifies a movement direction by mouse dragging.\nFrom these user inputs and initial latent codes, our latent transformer based\non a transformer encoder-decoder architecture estimates the output latent\ncodes, which are fed to the StyleGAN generator to obtain a result image. To\ntrain our latent transformer, we utilize synthetic data and pseudo-user inputs\ngenerated by off-the-shelf StyleGAN and optical flow models, without manual\nsupervision. Quantitative and qualitative evaluations demonstrate the\neffectiveness of our method over existing methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Having a model and being able to implement open-ended evolutionary systems is\nimportant for advancing our understanding of open-endedness. Complex systems\nscience and newest generation high-level programming languages provide\nintriguing possibilities to do so. First, some recent advances in modelling and\nimplementing open-ended evolutionary systems are reviewed. Then, the so-called\nallagmatic method is introduced that describes, models, implements, and allows\ninterpretation of complex systems. After highlighting some current modelling\nand implementation challenges, model building blocks of open-ended evolutionary\nsystems are identified, a system metamodel of open-ended evolution is\nformalised in the allagmatic method, an implementation self-modifying code\nprototype with a high-level programming language is provided, and guidance from\nthe allagmatic method to create code blocks is described. The proposed\nprototype allows modifying code at runtime in a controlled way within a system\nmetamodel. Since the allagmatic method has been built based on metaphysical\nconcepts borrowed from Gilbert Simondon and Alfred N. Whitehead, the proposed\nprototype provides a promising starting point to interpret novelty generated at\nruntime with the help of a metaphysical framework.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The recently published Gaia DR3 catalog of 181327 spectroscopic binaries (SB)\nincludes the Keplerian elements of each orbit but not the measured radial\nvelocities (RVs) and their epochs. Instead, the catalog lists a few parameters\nthat characterize the robustness of each solution. In this work, we use two\nexternal sources to validate the orbits - 17563 LAMOST DR6 and 6018 GALAH DR3\nstars with measured RVs that have Gaia-SB orbits. We compare the expected RVs,\nbased on the Gaia orbits, with the LAMOST and GALAH measurements. Finding some\norbits that are inconsistent with these measurements, we constructed a function\nthat estimates the probability of each of the Gaia orbits to be correct, using\nthe published robust parameters. We devise a clean but still very large Gaia\nSB1 sample of 91740 orbits. The sample differs from the parent sample by the\nabsence of - physically unlikely and hence presumably spurious - short-period\nbinaries with high eccentricity. The clean SB1 sample offers the prospect of\nthorough statistical studies of the binary population after carefully modeling\nof the remaining selection effects. At a first look, two possible features\nemerge from the clean sample - a paucity of short-period binaries with low-mass\nprimaries, which might be a result of some observational bias, and a sub-sample\nof main-sequence binaries on circular orbits, probable evidence for\ncircularization processes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  An efficient approach for extracting 3D local averages in spherical\nsubdomains is proposed and applied to study the intermittency of small-scale\nvelocity and scalar fields in direct numerical simulations of isotropic\nturbulence. We focus on the inertial-range scaling exponents of locally\naveraged energy dissipation rate, enstrophy and scalar dissipation rate\ncorresponding to the mixing of a passive scalar $\\theta$ in the presence of a\nuniform mean gradient. The Taylor-scale Reynolds number $R_\\lambda$ goes up to\n$1300$, and the Schmidt number $Sc$ up to $512$ (albeit at smaller\n$R_\\lambda$). The intermittency exponent of the energy dissipation rate is $\\mu\n\\approx 0.23$, whereas that of enstrophy is slightly larger; trends with\n$R_\\lambda$ suggest that this will be the case even at extremely large\n$R_\\lambda$. The intermittency exponent of the scalar dissipation rate is\n$\\mu_\\theta \\approx 0.35$ for $Sc=1$. These findings are in essential agreement\nwith previously reported results in the literature. We further show that\n$\\mu_\\theta$ decreases monotonically with increasing $Sc$, either as $1/\\log\nSc$ or a weak power law, suggesting that $\\mu_\\theta \\to 0$ as $Sc \\to \\infty$,\nreaffirming recent results on the breakdown of scalar dissipation anomaly in\nthis limit.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Precision measurements of transitions between singlet ($S=0$) Rydberg states\nof H$_2$ belonging to series converging on the\n$\\mathrm{X}^+\\,^2\\Sigma_g^+(v^+=0,N^+=0)$ state of H$_2^+$ have been carried\nout by millimetre-wave spectroscopy under field-free conditions and in the\npresence of weak static electric fields. The Stark effect mixes states with\ndifferent values of the orbital-angular-momentum quantum number $\\ell$ and\nleads to quadratic Stark shifts of low-$\\ell$ states and to linear Stark shifts\nof the nearly degenerate manifold of high-$\\ell$ states. Transitions to the\nStark manifold were observed for the principal numbers 50 and 70, at fields\nbelow 50 mV/cm, with linewidths below 500~kHz. The energy-level structure was\ncalculated using a matrix-diagonalisation approach, in which the zero-field\npositions of the $\\ell\\leq 3$ Rydberg states were obtained either from\nmultichannel-quantum-defect-theory calculations or experiment, and those of the\n$\\ell\\geq 4$ Rydberg states from a long-range core-polarisation model. This\napproach offers the advantage of including rovibronic channel interactions\nthrough the MQDT treatment while retaining the advantages of a spherical basis\nfor the determination of the off-diagonal elements of the Stark operator.\nComparison of experimental and calculated transition frequencies enabled the\nquantitative description of the Stark manifolds, with residuals typically below\n50 kHz. We demonstrate how the procedure leads to quantum defects and binding\nenergies of high Rydberg states with unprecedented accuracy, opening up new\nprospects for the determination of ionisation energies in molecules.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Aerial robots can enhance their safe and agile navigation in complex and\ncluttered environments by efficiently exploiting the information collected\nduring a given task. In this paper, we address the learning model predictive\ncontrol problem for quadrotors. We design a learning receding--horizon\nnonlinear control strategy directly formulated on the system nonlinear manifold\nconfiguration space SO(3)xR^3. The proposed approach exploits past successful\ntask iterations to improve the system performance over time while respecting\nsystem dynamics and actuator constraints. We further relax its computational\ncomplexity making it compatible with real-time quadrotor control requirements.\nWe show the effectiveness of the proposed approach in learning a minimum time\ncontrol task, respecting dynamics, actuators, and environment constraints.\nSeveral experiments in simulation and real-world set-up validate the proposed\napproach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Self-trapped droplets stabilized by quantum fluctuations have been\nexperimentally realized in dipolar gases and binary Boson mixtures. We propose\nspinor Bose gases as another candidate for droplet formation in this work. For\nspin-1 gas, we find that spin fluctuations give a dilute but self-trapped state\nfor two different order parameters where the mean-field picture predicts\ncollapse. A polar droplet phase can be stabilized by spin fluctuations for both\nantiferromagnetic and ferromagnetic spin-dependent coupling. An\nantiferromagnetic droplet phase can be stabilized similarly with a negative\nquadratic Zeeman shift. Furthermore, the beyond mean-field energy of the system\ndepends on the quadratic Zeeman coupling, which provides a mechanism to tune\nthe droplet formation and its density. We discuss the parameters necessary for\nthe experimental realization of such spinor droplets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Starburst galaxies are well-motivated astrophysical emitters of high-energy\ngamma-rays. They are well-known cosmic-ray \"reservoirs\", thanks to their large\nmagnetic fields which confine high-energy protons for $\\sim 10^5$ years. Over\nsuch long times, cosmic-ray transport can be significantly affected by\nscatterings with sub-GeV dark matter. Here we point out that this scattering\ndistorts the cosmic-ray spectrum, and the distortion can be indirectly observed\nby measuring the gamma-rays produced by cosmic-rays via hadronic collisions.\nPresent gamma-ray data show no sign of such a distortion, leading to stringent\nbounds on the cross section between protons and dark matter. These are\ncompetitive with current bounds, but have large room for improvement with the\nfuture gamma-ray measurements in the 0.1--10 TeV range from the Cherenkov\nTelescope Array, which can strengthen the limits by as much as two orders of\nmagnitude.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present the development of a Skipper Charge-Coupled Device (CCD) focal\nplane prototype for the SOAR Telescope Integral Field Spectrograph (SIFS). This\nmosaic focal plane consists of four 6k $\\times$ 1k, 15 $\\mu$m pixel Skipper\nCCDs mounted inside a vacuum dewar. We describe the process of packaging the\nCCDs so that they can be easily tested, transported, and installed in a mosaic\nfocal plane. We characterize the performance of $\\sim 650 \\mu$m thick,\nfully-depleted engineering-grade Skipper CCDs in preparation for performing\nsimilar characterization tests on science-grade Skipper CCDs which will be\nthinned to 250$\\mu$m and backside processed with an antireflective coating. We\nachieve a single-sample readout noise of $4.5 e^{-} rms/pix$ for the best\nperforming amplifiers and sub-electron resolution (photon counting\ncapabilities) with readout noise $\\sigma \\sim 0.16 e^{-} rms/pix$ from 800\nmeasurements of the charge in each pixel. We describe the design and\nconstruction of the Skipper CCD focal plane and provide details about the\nsynchronized readout electronics system that will be implemented to\nsimultaneously read 16 amplifiers from the four Skipper CCDs (4-amplifiers per\ndetector). Finally, we outline future plans for laboratory testing,\ninstallation, commissioning, and science verification of our Skipper CCD focal\nplane.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose Hyper-pool, an analytical, offline, utility-driven ride-pooling\nalgorithm to aggregate individual trip requests into attractive shared rides of\nhigh-occupancy. We depart from our ride-pooling ExMAS algorithm where single\nrides are pooled into attractive door-to-door rides and propose two novel\ndemand-side algorithms for further aggregating individual demand towards more\ncompact pooling. First, we generate stop-to-stop rides, with a single pick up\nand drop off points optimal for all the travellers. Second, we bundle such\nrides again, resulting with hyper-pooled rides compact enough to resemble\npublic transport operations. We propose a bottom-up framework where the pooling\ndegree of identified rides is gradually increased, thereby ensuring\nattractiveness at subsequent aggregation levels. Our Hyper-pool method outputs\nthe set of attractive pooled rides per service variant for a given travel\ndemand. The algorithms are publicly available and reproducible. It is\napplicable for real-size demand datasets and opens new opportunities for\nexploiting the limits of ride-pooling potential. In our Amsterdam case-study we\nmanaged to pool over 220 travellers into 40 hyper-pooled rides of average\noccupancy 5.8 pax/veh.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work we investigate the stability and instability properties of a\nclass of naked singularity spacetimes. The first rigorous study of naked\nsingularity formation in the spherically symmetric Einstein-scalar field system\nwas due to Christodoulou, who constructed a family $(\\overline{g}_k,\n\\overline{\\phi}_k)$ of $k$-self-similar solutions, for any $k^2 \\in\n(0,\\frac{1}{3})$. We extend the construction to produce examples of interior\nand exterior regions of naked singularity spacetimes locally modeled on the\n$(\\overline{g}_k, \\overline{\\phi}_k)$, without requiring exact self-similarity.\n  The main result is a global stability statement under fine-tuned data\nperturbations, for a class of naked singularity spacetimes satisfying\nself-similar bounds. Given the well-known blueshift instability for suitably\nregular naked singularities in the Einstein-scalar field model, we require\nnon-generic conditions on the data perturbations. In particular, the scalar\nfield perturbation along the past lightcone of the singular point $\\mathcal{O}$\nvanishes to high order near $\\mathcal{O}$. Technical difficulties arise from\nthe singular behavior of the background solution, as well as regularity\nconsiderations at the axis and past lightcone of the singularity. The interior\nregion is constructed via a backwards stability argument, thereby avoiding\nactivating the blueshift instability. The extension to the exterior region is\ntreated as a global existence problem to the future of $\\mathcal{O}$, adapting\ntechniques of Rodnianski and Shlapentokh-Rothman for vacuum spacetimes.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The main objective of this work is to study the structure, composition, and\noscillation modes of color superconducting quark stars with intense magnetic\nfields. We adopted the MIT bag model within the color superconductivity CFL\nframework, and we included the effects of strong magnetic fields to construct\nthe equation of state of stable quark matter. We calculated observable\nquantities, such as the mass, radius, frequency, and damping time of the\noscillation fundamental $f$ mode of quark stars, taking into account current\nastrophysical constraints. The results obtained show that color superconducting\nmagnetized quark stars satisfy the constraints imposed by the observations of\nmassive pulsars and gravitational wave events. Furthermore, the quantities\nassociated with the oscillation $f$ mode of these objects fit the universal\nrelationships for compact objects. In the context of the new multi-messenger\ngravitational wave astronomy era and the future asteroseismology of neutron\nstars, we hope that our results contribute to the understanding of the behavior\nof dense matter and compact objects.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The human proteome contains a vast network of interacting kinases and\nsubstrates. Even though some kinases have proven to be immensely useful as\ntherapeutic targets, a majority are still understudied. In this work, we\npresent a novel knowledge graph representation learning approach to predict\nnovel interaction partners for understudied kinases. Our approach uses a\nphosphoproteomic knowledge graph constructed by integrating data from iPTMnet,\nProtein Ontology, Gene Ontology and BioKG. The representation of kinases and\nsubstrates in this knowledge graph are learned by performing directed random\nwalks on triples coupled with a modified SkipGram or CBOW model. These\nrepresentations are then used as an input to a supervised classification model\nto predict novel interactions for understudied kinases. We also present a\npost-predictive analysis of the predicted interactions and an ablation study of\nthe phosphoproteomic knowledge graph to gain an insight into the biology of the\nunderstudied kinases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The formation of Primordial Black Holes (PBHs) through the collapse of large\nfluctuations in the early universe is a rare event. This manifests itself, for\ninstance, through the non-Gaussian tail of the formation probability. To\ncompute such probability and the abundance of PBHs, the curvature perturbation\nis frequently adopted. In this note we emphasize that its use does not provide\nthe correct PBH formation probability. Through a path-integral approach we show\nthat the exact calculation of the PBH abundance demands the knowledge of\nmultivariate joint probabilities of the curvature perturbation or,\nequivalently, of all the corresponding connected correlators.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The pentagram map on polygons in the projective plane was introduced by R.\nSchwartz in 1992 and is by now one of the most popular and classical discrete\nintegrable systems. In the present paper we introduce and prove integrability\nof long-diagonal pentagram maps on polygons in $\\mathbb{R}\\mathrm{P}^d$,\nencompassing all known integrable cases. We also establish an equivalence of\nlong-diagonal and bi-diagonal maps and present a simple self-contained\nconstruction of the Lax form for both. Finally, we prove the continuous limit\nof all these maps is equivalent to the $ (2,d+1)$-KdV equation, generalizing\nthe Boussinesq equation for $d=2$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Natarajan dimension is a fundamental tool for characterizing multi-class\nPAC learnability, generalizing the Vapnik-Chervonenkis (VC) dimension from\nbinary to multi-class classification problems. This note establishes upper\nbounds on Natarajan dimensions for certain function classes, including (i)\nmulti-class decision tree and random forests, and (ii) multi-class neural\nnetworks with binary, linear and ReLU activations. These results may be\nrelevant for describing the performance of certain multi-class learning\nalgorithms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In recent years, understanding the implicit regularization of neural networks\n(NNs) has become a central task of deep learning theory. However, implicit\nregularization is in itself not completely defined and well understood. In this\nwork, we make an attempt to mathematically define and study the implicit\nregularization. Importantly, we explore the limitation of a common approach of\ncharacterizing the implicit regularization by data-independent functions. We\npropose two dynamical mechanisms, i.e., Two-point and One-point Overlapping\nmechanisms, based on which we provide two recipes for producing classes of\none-hidden-neuron NNs that provably cannot be fully characterized by a type of\nor all data-independent functions. Our results signify the profound\ndata-dependency of implicit regularization in general, inspiring us to study in\ndetail the data-dependency of NN implicit regularization in the future.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spiking neural networks (SNNs) are known as a typical kind of brain-inspired\nmodels with their unique features of rich neuronal dynamics, diverse coding\nschemes and low power consumption properties. How to obtain a high-accuracy\nmodel has always been the main challenge in the field of SNN. Currently, there\nare two mainstream methods, i.e., obtaining a converted SNN through converting\na well-trained Artificial Neural Network (ANN) to its SNN counterpart or\ntraining an SNN directly. However, the inference time of a converted SNN is too\nlong, while SNN training is generally very costly and inefficient. In this\nwork, a new SNN training paradigm is proposed by combining the concepts of the\ntwo different training methods with the help of the pretrain technique and\nBP-based deep SNN training mechanism. We believe that the proposed paradigm is\na more efficient pipeline for training SNNs. The pipeline includes pipeS for\nstatic data transfer tasks and pipeD for dynamic data transfer tasks. SOTA\nresults are obtained in a large-scale event-driven dataset ES-ImageNet. For\ntraining acceleration, we achieve the same (or higher) best accuracy as similar\nLIF-SNNs using 1/10 training time on ImageNet-1K and 2/5 training time on\nES-ImageNet and also provide a time-accuracy benchmark for a new dataset\nES-UCF101. These experimental results reveal the similarity of the functions of\nparameters between ANNs and SNNs and also demonstrate the various potential\napplications of this SNN training pipeline.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the generator $G$ of the one-dimensional damped wave equation with\nunbounded damping. We show that the norm of the corresponding resolvent\noperator, $\\| (G - \\lambda)^{-1} \\|$, is approximately constant as $|\\lambda|\n\\to +\\infty$ on vertical strips of bounded width contained in the closure of\nthe left-hand side complex semi-plane, $\\overline{\\mathbb{C}}_{-} := \\{\\lambda\n\\in \\mathbb{C}: \\operatorname{Re} \\lambda \\le 0\\}$. Our proof rests on a\nprecise asymptotic analysis of the norm of the inverse of $T(\\lambda)$, the\nquadratic operator associated with $G$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the Next-to-Minimal Supersymmetric Standard Model there is a strong\ncorrelation between the mass terms corresponding to the singlet Higgs and the\nsinglino interaction states, both of which are proportional to the parameter\n$\\kappa$. If this parameter is complex, explicit CP-violation occurs in the\nHiggs as well as the neutralino sectors of the model at the tree level, unlike\nin the minimal scenario. A small magnitude of $\\kappa$ typically yields a\n$\\cal{O}$(10) GeV lightest neutralino with a dominant singlino component. In\nsuch a scenario, the phase of $\\kappa$, beside modifying the properties of the\nfive Higgs bosons, can also have a crucial impact on the phenomenology of the\nneutralino dark matter. In this study we perform a first investigation of this\nimpact on the relic abundance of the dark matter solutions with sub-100 GeV\nmasses, obtained for parameter space configurations of the model that are\nconsistent with a variety of current experimental data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Biochemical reactions inside living cells often occur in the presence of\ncrowders -- molecules that do not participate in the reactions but influence\nthe reaction rates through excluded volume effects. However the standard\napproach to modelling stochastic intracellular reaction kinetics is based on\nthe chemical master equation (CME) whose propensities are derived assuming no\ncrowding effects. Here, we propose a machine learning strategy based on\nBayesian Optimisation utilising synthetic data obtained from spatial cellular\nautomata (CA) simulations (that explicitly model volume-exclusion effects) to\nlearn effective propensity functions for CMEs. The predictions from a small CA\ntraining data set can then be extended to the whole range of parameter space\ndescribing physiologically relevant levels of crowding by means of Gaussian\nProcess regression. We demonstrate the method on an enzyme-catalyzed reaction\nand a genetic feedback loop, showing good agreement between the time-dependent\ndistributions of molecule numbers predicted by the effective CME and CA\nsimulations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a phase-field crystal (PFC) model for solidification that accounts\nfor thermal transport and a temperature-dependent lattice parameter. Elasticity\neffects are characterized through the continuous elastic field computed from\nthe microscopic density field. We showcase the model capabilities via selected\nnumerical investigations which focus on the prototypical growth of\ntwo-dimensional crystals from the melt, resulting in faceted shapes and\ndendrites. This work sets the grounds for a comprehensive mesoscale model of\nsolidification including thermal expansion.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Modern multi-agent reinforcement learning frameworks rely on centralized\ntraining and reward shaping to perform well. However, centralized training and\ndense rewards are not readily available in the real world. Current multi-agent\nalgorithms struggle to learn in the alternative setup of decentralized training\nor sparse rewards. To address these issues, we propose a self-supervised\nintrinsic reward ELIGN - expectation alignment - inspired by the\nself-organization principle in Zoology. Similar to how animals collaborate in a\ndecentralized manner with those in their vicinity, agents trained with\nexpectation alignment learn behaviors that match their neighbors' expectations.\nThis allows the agents to learn collaborative behaviors without any external\nreward or centralized training. We demonstrate the efficacy of our approach\nacross 6 tasks in the multi-agent particle and the complex Google Research\nfootball environments, comparing ELIGN to sparse and curiosity-based intrinsic\nrewards. When the number of agents increases, ELIGN scales well in all\nmulti-agent tasks except for one where agents have different capabilities. We\nshow that agent coordination improves through expectation alignment because\nagents learn to divide tasks amongst themselves, break coordination symmetries,\nand confuse adversaries. These results identify tasks where expectation\nalignment is a more useful strategy than curiosity-driven exploration for\nmulti-agent coordination, enabling agents to do zero-shot coordination.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The paper aims to study some invariants and conservation laws relevant to\nelectromagnetic and gravitational fields, by means of the rotational\ntransformations of octonion coordinate systems. The scholars utilize the\noctonions to analyze the electromagnetic and gravitational fields\nsimultaneously, including the octonion field potential, field strength, field\nsource, linear momentum, angular momentum, torque and force. When the octonion\ncoordinate system transforms rotationally, the vector part of one octonion may\nalter, while the scalar part of the octonion will remain unchanged. This\nproperty allows one to deduce a few invariants, such as the scalar part of\noctonion radius vector, speed of light, and norm of octonion radius vector.\nThese invariants are the basic postulates for the Galilean transformation and\nLorentz transformation. Further, from the rotation transform of octonion\ncoordinate systems, it is capable of deducing several invariants, including the\nmass, energy and power relevant to gravitational fields, in one octonion space\n$\\mathbb{O}$. And the term relevant to the electric charge will transform with\nthe rotation of coordinate systems. In another octonion space $\\mathbb{O}_u$,\nit is capable of inferring a few invariants, including the electric charge\nrelated to electromagnetic fields. And the terms relevant to the mass and\nenergy will vary with the rotation of octonion coordinate systems. So the\ninvariants are divided into two different groups. In particular, the mass\nconservation law and energy conservation law can be effective simultaneously.\nBut the charge conservation law and mass conservation law are unable to be\nvalid simultaneously, in the strict sense. It is beneficial to further\nunderstand the laws of conservation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Polarization is an unprecedented coding technique in that it not only\nachieves channel capacity, but also does so at a faster speed of convergence\nthan any other technique. This speed is measured by the \"scaling exponent\" and\nits importance is three-fold. Firstly, estimating the scaling exponent is\nchallenging and demands a deeper understanding of the dynamics of communication\nchannels. Secondly, scaling exponents serve as a benchmark for different\nvariants of polar codes that helps us select the proper variant for real-life\napplications. Thirdly, the need to optimize for the scaling exponent sheds\nlight on how to reinforce the design of polar code.\n  In this paper, we generalize the binary erasure channel (BEC), the simplest\ncommunication channel and the protagonist of many polar code studies, to the\n\"tetrahedral erasure channel\" (TEC). We then invoke Mori--Tanaka's $2 \\times 2$\nmatrix over GF$(4)$ to construct polar codes over TEC. Our main contribution is\nshowing that the dynamic of TECs converges to an almost--one-parameter family\nof channels, which then leads to an upper bound of $3.328$ on the scaling\nexponent. This is the first non-binary matrix whose scaling exponent is\nupper-bounded. It also polarizes BEC faster than all known binary matrices up\nto $23 \\times 23$ in size. Our result indicates that expanding the alphabet is\na more effective and practical alternative to enlarging the matrix in order to\nachieve faster polarization.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider the convergence theory for dyadic approximation in the\nmiddle-third Cantor set, $K$, for approximation functions of the form\n$\\psi_{\\tau}(n) = n^{-\\tau}$ ($\\tau \\ge 0$). In particular, we show that for\nvalues of $\\tau$ beyond a certain threshold we have that almost no point in $K$\nis dyadically $\\psi_{\\tau}$-well approximable with respect to the natural\nprobability measure on $K$. This refines a previous result in this direction\nobtained by the first, third, and fourth named authors (arXiv, 2020).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Distributed machine learning (DML) over time-varying networks can be an\nenabler for emerging decentralized ML applications such as autonomous driving\nand drone fleeting. However, the commonly used weighted arithmetic mean model\naggregation function in existing DML systems can result in high model loss, low\nmodel accuracy, and slow convergence speed over time-varying networks. To\naddress this issue, in this paper, we propose a novel non-linear class of model\naggregation functions to achieve efficient DML over time-varying networks.\nInstead of taking a linear aggregation of neighboring models as most existing\nstudies do, our mechanism uses a nonlinear aggregation, a weighted power-p mean\n(WPM), as the aggregation function of local models from neighbors. The\nsubsequent optimizing steps are taken using mirror descent defined by a Bregman\ndivergence that maintains convergence to optimality. In this paper, we analyze\nproperties of the WPM and rigorously prove convergence properties of our\naggregation mechanism. Additionally, through extensive experiments, we show\nthat when p > 1, our design significantly improves the convergence speed of the\nmodel and the scalability of DML under time-varying networks compared with\narithmetic mean aggregation functions, with little additional computation\noverhead.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we write exactly solvable generalizations of 1-dimensional\nquantum XY and Ising-like models by using $2^d$-dimensional Gamma ($\\Gamma$)\nmatrices as the degrees of freedom on each site. We show that these models\nresult in quadratic Fermionic Hamiltonians with Jordan-Wigner like\ntransformations. We illustrate the techniques using a specific case of\n4-dimensional $\\Gamma$ matrices and explore the quantum phase transitions\npresent in the model.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  COVID-19 related deaths underestimate the pandemic burden on mortality\nbecause they suffer from completeness and accuracy issues. Excess mortality is\na popular alternative, as it compares observed with expected deaths based on\nthe assumption that the pandemic did not occur. Expected deaths had the\npandemic not occurred depend on population trends, temperature, and\nspatio-temporal patterns. In addition to this, high geographical resolution is\nrequired to examine within country trends and the effectiveness of the\ndifferent public health policies. In this tutorial, we propose a framework\nusing R to estimate and visualise excess mortality at high geographical\nresolution. We show a case study estimating excess deaths during 2020 in Italy.\nThe proposed framework is fast to implement and allows combining different\nmodels and presenting the results in any age, sex, spatial and temporal\naggregation desired. This makes it particularly powerful and appealing for\nonline monitoring of the pandemic burden and timely policy making.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Massively multilingual models are promising for transfer learning across\ntasks and languages. However, existing methods are unable to fully leverage\ntraining data when it is available in different task-language combinations. To\nexploit such heterogeneous supervision, we propose Hyper-X, a single\nhypernetwork that unifies multi-task and multilingual learning with efficient\nadaptation. This model generates weights for adapter modules conditioned on\nboth tasks and language embeddings. By learning to combine task and\nlanguage-specific knowledge, our model enables zero-shot transfer for unseen\nlanguages and task-language combinations. Our experiments on a diverse set of\nlanguages demonstrate that Hyper-X achieves the best or competitive gain when a\nmixture of multiple resources is available, while being on par with strong\nbaselines in the standard scenario. Hyper-X is also considerably more efficient\nin terms of parameters and resources compared to methods that train separate\nadapters. Finally, Hyper-X consistently produces strong results in few-shot\nscenarios for new languages, showing the versatility of our approach beyond\nzero-shot transfer.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A quantum thermal machine is an open quantum system coupled to hot and cold\nthermal baths. Thus, its dynamics can be well understood using the concepts and\ntools from non-Hermitian quantum systems. A hallmark of non-Hermiticity is the\nexistence of exceptional points where the eigenvalues of a non-Hermitian\nHamiltonian or an Liouvillian superoperator and their associated eigenvectors\ncoalesce. Here, we report the experimental realisation of a single-ion heat\nengine and demonstrate the effect of the Liouvillian exceptional points on the\ndynamics and the performance of a quantum heat engine. Our experiments have\nrevealed that operating the engine in the exact- and broken-phases, separated\nby a Liouvillian exceptional point, respectively during the isochoric heating\nand cooling strokes of an Otto cycle produces more work and output power and\nachieves higher efficiency than executing the Otto cycle completely in the\nexact phase where the system has an oscillatory dynamics and higher coherence.\nThis result opens interesting possibilities for the control of quantum heat\nengines and will be of interest to other research areas that are concerned with\nthe role of coherence and exceptional points in quantum processes and in work\nextraction by thermal machines.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A new generation of powerful dark energy experiments will open new vistas for\ncosmology in the next decade. However, these projects cannot reach their utmost\npotential without data from other telescopes. This white paper focuses in\nparticular on the compelling benefits of ground-based spectroscopic and\nphotometric observations to complement the Vera C. Rubin Observatory, as well\nas smaller programs in aid of a DESI-2 experiment and CMB-S4. These additional\ndata sets will both improve dark energy constraints from these flagship\nprojects beyond what would possible on their own and open completely new\nwindows into fundamental physics. For example, additional photometry and\nsingle-object spectroscopy will provide necessary follow-up information for\nsupernova and strong lensing cosmology, while highly-multiplexed spectroscopy\nboth from smaller facilities over wide fields and from larger facilities over\nnarrower regions of sky will yield more accurate photometric redshift estimates\nfor weak lensing and galaxy clustering measurements from the Rubin Observatory,\nprovide critical spectroscopic host galaxy redshifts for supernova Hubble\ndiagrams, provide improved understanding of limiting astrophysical systematic\neffects, and enable new measurements that probe the nature of gravity. A common\nthread is that access to complementary data from a range of\ntelescopes/instruments would have a substantial impact on the rate of advance\nof dark energy science in the coming years.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this study, a Semi-Supervised Learning (SSL) method for improving urban\nchange detection from bi-temporal image pairs was presented. The proposed\nmethod adapted a Dual-Task Siamese Difference network that not only predicts\nchanges with the difference decoder, but also segments buildings for both\nimages with a semantics decoder. First, the architecture was modified to\nproduce a second change prediction derived from the semantics predictions.\nSecond, SSL was adopted to improve supervised change detection. For unlabeled\ndata, we introduced a loss that encourages the network to predict consistent\nchanges across the two change outputs. The proposed method was tested on urban\nchange detection using the SpaceNet7 dataset. SSL achieved improved results\ncompared to three fully supervised benchmarks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Data science pipelines to train and evaluate models with machine learning may\ncontain bugs just like any other code. Leakage between training and test data\ncan lead to overestimating the model's accuracy during offline evaluations,\npossibly leading to deployment of low-quality models in production. Such\nleakage can happen easily by mistake or by following poor practices, but may be\ntedious and challenging to detect manually. We develop a static analysis\napproach to detect common forms of data leakage in data science code. Our\nevaluation shows that our analysis accurately detects data leakage and that\nsuch leakage is pervasive among over 100,000 analyzed public notebooks. We\ndiscuss how our static analysis approach can help both practitioners and\neducators, and how leakage prevention can be designed into the development\nprocess.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  HD 93206 is early-type massive stellar system, composed of components\nresolved by direct imaging (Ab, Ad, B, C, D) as well as a compact sub-system\n(Aa1, Aa2, Ac1, Ac2). Its geometry was already determined on the basis of\nextensive photometric, spectroscopic and interferometric observations. However,\nthe fundamental absolute parameters are still not known precisely enough. We\nuse an advanced N-body model to account for all mutual gravitational\nperturbations among the four close components, and all observational data\ntypes, including: astrometry, radial velocities, eclipse timing variations,\nsquared visibilities, closure phases, triple products, normalized spectra, and\nspectral-energy distribution (SED). The respective model has 38 free\nparameters, namely three sets of orbital elements, component masses, and their\nbasic radiative properties ($T$, $\\log g$, $v_{\\rm rot}$). We revised the\nfundamental parameters of QZ Car as follows. For a model with the nominal\nextinction coefficient $R_V \\equiv A_V/E(B-V) = 3.1$, the best-fit masses are\n$m_1 = 26.1\\,M_{\\rm S}$, $m_2 = 32.3\\,M_{\\rm S}$, $m_3 = 70.3\\,M_{\\rm S}$, $m_4\n= 8.8\\,M_{\\rm S}$, with uncertainties of the order of $2\\,M_{\\rm S}$, and the\nsystem distance $d = (2800\\pm 100)\\,{\\rm pc}$. In an alternative model, where\nwe increased the weights of RV and TTV observations and relaxed the SED\nconstraints, because extinction can be anomalous with $R_V \\sim 3.4$, the\ndistance is smaller, $d = (2450\\pm 100)\\,{\\rm pc}$. This would correspond to\nthat of Collinder 228 cluster. Independently, this is confirmed by dereddening\nof the SED, which is only then consistent with the early-type classification\n(O9.7Ib for Aa1, O8III for Ac1). Future modelling should also account for an\naccretion disk around Ac2 component.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Designing efficient sparse recovery algorithms that could handle noisy\nquantized measurements is important in a variety of applications -- from radar\nto source localization, spectrum sensing and wireless networking. We take\nadvantage of the approximate message passing (AMP) framework to achieve this\ngoal given its high computational efficiency and state-of-the-art performance.\nIn AMP, the signal of interest is assumed to follow certain prior distribution\nwith unknown parameters. Previous works focused on finding the parameters that\nmaximize the measurement likelihood via expectation maximization -- an\nincreasingly difficult problem to solve in cases involving complicated\nprobability models. In this paper, we treat the parameters as unknown variables\nand compute their posteriors via AMP. The parameters and signal of interest can\nthen be jointly recovered. Compared to previous methods, the proposed approach\nleads to a simple and elegant parameter estimation scheme, allowing us to\ndirectly work with 1-bit quantization noise model. We then further extend our\napproach to general multi-bit quantization noise model. Experimental results\nshow that the proposed framework provides significant improvement over\nstate-of-the-art methods across a wide range of sparsity and noise levels.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding the interaction of kilojoule, picosecond laser pulse with\nlong-scale length preplasma or homogeneous near critical density (NCD) plasma\nis crucial for guiding experiments at national short-pulse laser facilities.\nUsing full three-dimensional particle-in-cell simulations, we demonstrate that\nin this regime, cross-filament stochastic acceleration is an important\nmechanism that contributes to the production of superponderomotive, high-flux\nelectron beams. Since the laser power significantly exceeds the threshold of\nthe relativistic self-focusing, multiple filaments are generated and can\npropagate independently over a long distance. Electrons jump across the\nfilaments during the acceleration, and their motion becomes stochastic. We find\nthat the effective temperature of electrons increases with the total\ninteraction time following a scaling like $T_{\\rm eff}\\propto\\tau_{i}^{0.65}$.\nBy irradiating a submillimeter thick NCD target, the space charge of electrons\nwith energy above 2.5 MeV reaches tens of $\\mu$C. Such high-flux electrons with\nsuperponderomotive energies significantly facilitate applications in\nhigh-energy-density science, nuclear science, secondary sources and diagnostic\ntechniques.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we generalize the notion of relative $p$-capacity of $K$ with\nrespect to $\\Omega$, by replacing the Dirichlet boundary condition with a Robin\none. We show that, under volume constraints, our notion of $p$-capacity is\nminimal when $K$ and $\\Omega$ are concentric balls. We use the $H$-function and\na derearrangement technique.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we study the structural, scattering, and wave localization\nproperties of multifractal arrays of electric point dipoles generated from\nmultiplicative random fields with different degrees of multiscale correlations.\nSpecifically, using the rigorous Green's matrix method, we investigate the\nscattering resonances and wave localization behavior of systems with $N=10^{4}$\ndipoles and demonstrate an enhanced localization behavior in highly\ninhomogeneous multifractal structures compared to homogeneous fractals, or\nmonofractals. We show distinctive spectral properties, such as the absence of\nlevel repulsion in the strong multiple scattering regime and power-law\nstatistics of level spacings, which indicate a clear localization transition\nenhanced in non-homogeneous multifractals. Our findings unveil the importance\nof multifractal structural correlations in the multiple scattering regime of\nelectric dipole arrays and provide an efficient model for the design of\nmultiscale nanophotonic systems with enhanced light-matter coupling and\nlocalization phenomena beyond what is possible with traditional fractal\nsystems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A modified weak measurement scheme, rotatory biased weak measurement, is\nproposed to significantly improve the sensitivity and resolution of the\nrefractive index sensor on a total reflection structure. This method introduces\nan additional phase in the post-selected procedure and generates an extinction\npoint in the spectrum distribution. The biased post-selection makes smaller\ncoupling strength available, which leads to an enhancement of phase sensitivity\nand refractive index sensitivity. In rotatory biased weak measurement, we\nachieve an enhanced refractive index sensitivity of 13605 nm/RIU compared to\n1644 nm/RIU in standard weak measurement. The performance of sensors with\ndifferent sensitivity is analyzed, and we find the optimal refractive index\nresolution of sensors increases with sensitivity. In this work, we demonstrate\nan optimal refractive index resolution of $4\\times10^{-7}$ RIU on a total\nreflection structure. The rabbit anti-mouse IgG and mouse IgG binding reaction\nexperiments demonstrate that our system has a high response to the\nconcentration of IgG in a wide range and the limit of detection is 15 ng/mL.\nThe improvements in this work are helpful to the optimizations of other optical\nsensors with weak measurement.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present two contiguous nights of simultaneous time-resolved GTC\nspectroscopy and WHT photometry of the black hole X-ray transient XTE\nJ1859+226, obtained in 2017 July during quiescence. Cross-correlation of the\nindividual spectra against a late K-type spectral template enabled us to\nconstrain the orbital period to $0.276 \\pm 0.003$ d and the radial velocity\nsemi-amplitude of the donor star to $K_2 = 550 \\pm 59$ km s$^{-1}$. An\nellipsoidal modulation is detected in the photometric $r$- and $i$-band light\ncurves, although it is strongly contaminated by flickering activity. By\nexploiting correlations between the properties of the double-peaked H$\\alpha$\nemission-line profile and the binary parameters, we derived an orbital\ninclination of $66.6 \\pm 4.3$ deg, a refined $K_2 = 562 \\pm 40$ km s$^{-1}$ and\nmass ratio $q = M_2/M_1 = 0.07 \\pm 0.01$. From these values we obtained an\nupdated black hole mass of $M_1 = 7.8 \\pm 1.9$ M$_\\odot$. An independent mass\nestimate based on X-ray timing agrees well with our value, which gives further\nsupport for the outburst QPO triplet being explained by the relativistic\nprecession model. We also obtained a companion star mass $M_2 = 0.55 \\pm 0.16$\nM$_\\odot$, which is consistent with its K5-K7 V spectral type.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This work proposes a framework for optimizing machine learning algorithms.\nThe practicality of the framework is illustrated using an important case study\nfrom the healthcare domain, which is predicting the admission status of\nemergency department (ED) patients (e.g., admitted vs. discharged) using\npatient data at the time of triage. The proposed framework can mitigate the\ncrowding problem by proactively planning the patient boarding process. A large\nretrospective dataset of patient records is obtained from the electronic health\nrecord database of all ED visits over three years from three major locations of\na healthcare provider in the Midwest of the US. Three machine learning\nalgorithms are proposed: T-XGB, T-ADAB, and T-MLP. T-XGB integrates extreme\ngradient boosting (XGB) and Tabu Search (TS), T-ADAB integrates Adaboost and\nTS, and T-MLP integrates multi-layer perceptron (MLP) and TS. The proposed\nalgorithms are compared with the traditional algorithms: XGB, ADAB, and MLP, in\nwhich their parameters are tunned using grid search. The three proposed\nalgorithms and the original ones are trained and tested using nine data groups\nthat are obtained from different feature selection methods. In other words, 54\nmodels are developed. Performance was evaluated using five measures: Area under\nthe curve (AUC), sensitivity, specificity, F1, and accuracy. The results show\nthat the newly proposed algorithms resulted in high AUC and outperformed the\ntraditional algorithms. The T-ADAB performs the best among the newly developed\nalgorithms. The AUC, sensitivity, specificity, F1, and accuracy of the best\nmodel are 95.4%, 99.3%, 91.4%, 95.2%, 97.2%, respectively.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hydrophobic interactions have been studied in detail in the past based on\nhydrophobic polymers, such as polystyrene (PS). Because fluorinated materials\nhave relatively low surface energy, they often show both oleophobicity and\nhydrophobicity at the macroscopic level. However, it remains unknown how\nfluorination of hydrophobic polymer influences hydrophobicity at the\nmicroscopic level. In this work, we synthesized PS and fluorine-substituted PS\n(FPS) by reversible addition-fragmentation chain transfer polymerization\nmethod. Contact angle measurements confirmed that FPS is more hydrophobic than\nPS at the macroscopic level due to the introduction of fluorine. However,\nsingle molecule force spectroscopy experiments showed that the forces required\nto unfold the PS and FPS nanoparticles in water are indistinguishable,\nindicating that the strength of the hydrophobic ffect that drives the\nself-assembly of PS and FPS nanoparticles is the same at the microscopic level.\nThe divergence of hydrophobic effect at the macroscopic and microscopic level\nmay hint different underlying mechanisms: the hydrophobicity is dominated by\nthe solvent hydration at the microscopic level and the surface-associated\ninteraction at the macroscopic level.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a novel reinforcement learning (RL) based task allocation and\ndecentralized navigation algorithm for mobile robots in warehouse environments.\nOur approach is designed for scenarios in which multiple robots are used to\nperform various pick up and delivery tasks. We consider the problem of joint\ndecentralized task allocation and navigation and present a two level approach\nto solve it. At the higher level, we solve the task allocation by formulating\nit in terms of Markov Decision Processes and choosing the appropriate rewards\nto minimize the Total Travel Delay (TTD). At the lower level, we use a\ndecentralized navigation scheme based on ORCA that enables each robot to\nperform these tasks in an independent manner, and avoid collisions with other\nrobots and dynamic obstacles. We combine these lower and upper levels by\ndefining rewards for the higher level as the feedback from the lower level\nnavigation algorithm. We perform extensive evaluation in complex warehouse\nlayouts with large number of agents and highlight the benefits over\nstate-of-the-art algorithms based on myopic pickup distance minimization and\nregret-based task selection. We observe improvement up to 14% in terms of task\ncompletion time and up-to 40% improvement in terms of computing collision-free\ntrajectories for the robots.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper tackles the problem of efficient video recognition. In this area,\nvideo transformers have recently dominated the efficiency (top-1 accuracy vs\nFLOPs) spectrum. At the same time, there have been some attempts in the image\ndomain which challenge the necessity of the self-attention operation within the\ntransformer architecture, advocating the use of simpler approaches for token\nmixing. However, there are no results yet for the case of video recognition,\nwhere the self-attention operator has a significantly higher impact (compared\nto the case of images) on efficiency. To address this gap, in this paper, we\nmake the following contributions: (a) we construct a highly efficient \\&\naccurate attention-free block based on the shift operator, coined Affine-Shift\nblock, specifically designed to approximate as closely as possible the\noperations in the MHSA block of a Transformer layer. Based on our Affine-Shift\nblock, we construct our Affine-Shift Transformer and show that it already\noutperforms all existing shift/MLP--based architectures for ImageNet\nclassification. (b) We extend our formulation in the video domain to construct\nVideo Affine-Shift Transformer (VAST), the very first purely attention-free\nshift-based video transformer. (c) We show that VAST significantly outperforms\nrecent state-of-the-art transformers on the most popular action recognition\nbenchmarks for the case of models with low computational and memory footprint.\nCode will be made available.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Data-driven modeling is an imperative tool in various industrial\napplications, including many applications in the sectors of aeronautics and\ncommercial aviation. These models are in charge of providing key insights, such\nas which parameters are important on a specific measured outcome or which\nparameter values we should expect to observe given a set of input parameters.\nAt the same time, however, these models rely heavily on assumptions (e.g.,\nstationarity) or are \"black box\" (e.g., deep neural networks), meaning that\nthey lack interpretability of their internal working and can be viewed only in\nterms of their inputs and outputs. An interpretable alternative to the \"black\nbox\" models and with considerably less assumptions is symbolic regression (SR).\nSR searches for the optimal model structure while simultaneously optimizing the\nmodel's parameters without relying on an a-priori model structure. In this\nwork, we apply SR on real-life exhaust gas temperature (EGT) data, collected at\nhigh frequencies through the entire flight, in order to uncover meaningful\nalgebraic relationships between the EGT and other measurable engine parameters.\nThe experimental results exhibit promising model accuracy, as well as\nexplainability returning an absolute difference of 3{\\deg}C compared to the\nground truth and demonstrating consistency from an engineering perspective.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recapture detection of face and document images is an important forensic\ntask. With deep learning, the performances of face anti-spoofing (FAS) and\nrecaptured document detection have been improved significantly. However, the\nperformances are not yet satisfactory on samples with weak forensic cues. The\namount of forensic cues can be quantified to allow a reliable forensic result.\nIn this work, we propose a forensicability assessment network to quantify the\nforensicability of the questioned samples. The low-forensicability samples are\nrejected before the actual recapturing detection process to improve the\nefficiency of recapturing detection systems. We first extract forensicability\nfeatures related to both image quality assessment and forensic tasks. By\nexploiting domain knowledge of the forensic application in image quality and\nforensic features, we define three task-specific forensicability classes and\nthe initialized locations in the feature space. Based on the extracted features\nand the defined centers, we train the proposed forensic assessment network\n(FANet) with cross-entropy loss and update the centers with a momentum-based\nupdate method. We integrate the trained FANet with practical recapturing\ndetection schemes in face anti-spoofing and recaptured document detection\ntasks. Experimental results show that, for a generic CNN-based FAS scheme,\nFANet reduces the EERs from 33.75% to 19.23% under ROSE to IDIAP protocol by\nrejecting samples with the lowest 30% forensicability scores. The performance\nof FAS schemes is poor in the rejected samples, with EER as high as 56.48%.\nSimilar performances in rejecting low-forensicability samples have been\nobserved for the state-of-the-art approaches in FAS and recaptured document\ndetection tasks. To the best of our knowledge, this is the first work that\nassesses the forensicability of recaptured document images and improves the\nsystem efficiency.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hall-effect thrusters (HETs) are widely used for modern near-earth spacecraft\npropulsion and are vital for future deep-space missions. Methods of modeling\nHETs are developing rapidly. However, such methods are not yet precise enough\nand cannot reliably predict the parameters of a newly designed thruster, mostly\ndue to the enormous computational cost of a HET plasma simulation. Another\napproach is to use scaling techniques based on available experimental data.\nThis paper proposes an approach for scaling HETs using neural networks and\nother modern machine learning methods. The new scaling model was built with\ninformation from an extensive database of HET parameters collected from\npublished papers. Predictions of the new scaling model are valid for the\noperating parameters domain covered by the database. During the design, this\nmodel can help HET developers estimate the performance of a newly-designed\nthruster. At the stage of experimental research, the model can be used to\ncompare the achieved characteristics of the studied thruster with the level\nobtained by other developers. A comparison with the state-of-the-art HET\nscaling model is also presented.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We experimentally investigated the detection performance of highly disordered\nNbxTi1-xN based superconducting nanowire single photon detectors (SNSPDs). The\ndependence on the composition of the transition temperature Tc for NbxTi1-xN\nfilms show a dome-like behavior on the Nb content, with a maximal Tc at\nxNb~0.65 , and the Nb0.65Ti0.35N films also combine relatively large sheet\nresistance and intermediate residual resistivity ratio. Moreover, 60-nm-wide\nand 7-nm-thick Nb0.65Ti0.35N nanowires show a switching current as high as 14.5\nuA, and saturated intrinsic detection efficiency with a plateau of more than 2\nuA at 2.4 K. Finally, the corresponding SNSPDs on an alternative SiO2/Ta2O5\ndielectric mirror showed a system detection efficiency of approximately 92% for\n1550 nm photons, and the timing jitter is around 26 ps. Our results demonstrate\nthat the highly disordered NbxTi1-xN films are promising for fabricating SNSPDs\nfor near- and middle-infrared single photons with high detection efficiency and\nlow timing jitter.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This article introduces a novel structured random matrix composed blockwise\nfrom subsampled randomized Hadamard transforms (SRHTs). The block SRHT is\nexpected to outperform well-known dimension reduction maps, including SRHT and\nGaussian matrices, on distributed architectures with not too many cores\ncompared to the dimension. We prove that a block SRHT with enough rows is an\noblivious subspace embedding, i.e., an approximate isometry for an arbitrary\nlow-dimensional subspace with high probability. Our estimate of the required\nnumber of rows is similar to that of the standard SRHT. This suggests that the\ntwo transforms should provide the same accuracy of approximation in the\nalgorithms. The block SRHT can be readily incorporated into randomized methods,\nfor instance to compute a low-rank approximation of a large-scale matrix. For\ncompleteness, we revisit some common randomized approaches for this problem\nsuch as Randomized Singular Value Decomposition and Nystr\\\"{o}m approximation,\nwith a discussion of their accuracy and implementation on distributed\narchitectures.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider diffeomorphism invariant theories of gravity with arbitrary\nhigher derivative terms in the Lagrangian as corrections to the leading two\nderivative theory of Einstein's general relativity. We construct a proof of the\nzeroth law of black hole thermodynamics in such theories. We assume that a\nstationary black hole solution in an arbitrary higher derivative theory can be\nobtained by starting with the corresponding stationary solution in general\nrelativity and correcting it order by order in a perturbative expansion in the\ncoupling constants of the higher derivative Lagrangian. We prove that surface\ngravity remains constant on its horizon when computed for such stationary black\nholes, which is the zeroth law. We argue that the constancy of surface gravity\non the horizon is related to specific components of the equations of motion in\nsuch theories. We further use a specific boost symmetry of the near horizon\nspace-time of the stationary black hole to constrain the off-shell structure of\nthe equations of motion. Our proof for the zeroth law is valid up to arbitrary\norder in the expansion in the higher derivative couplings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In medical practice, treatments are selected based on the expected causal\neffects on patient outcomes. Here, the gold standard for estimating causal\neffects are randomized controlled trials; however, such trials are costly and\nsometimes even unethical. Instead, medical practice is increasingly interested\nin estimating causal effects among patient subgroups from electronic health\nrecords, that is, observational data. In this paper, we aim at estimating the\naverage causal effect (ACE) from observational data (patient trajectories) that\nare collected over time. For this, we propose DeepACE: an end-to-end deep\nlearning model. DeepACE leverages the iterative G-computation formula to adjust\nfor the bias induced by time-varying confounders. Moreover, we develop a novel\nsequential targeting procedure which ensures that DeepACE has favorable\ntheoretical properties, i.e., is doubly robust and asymptotically efficient. To\nthe best of our knowledge, this is the first work that proposes an end-to-end\ndeep learning model for estimating time-varying ACEs. We compare DeepACE in an\nextensive number of experiments, confirming that it achieves state-of-the-art\nperformance. We further provide a case study for patients suffering from low\nback pain to demonstrate that DeepACE generates important and meaningful\nfindings for clinical practice. Our work enables medical practitioners to\ndevelop effective treatment recommendations tailored to patient subgroups.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a study of scintillation induced by the mid-latitude ionosphere.\nBy implementing methods currently used in Interplanetary Scintillation studies\nto measure amplitude scintillation at low frequencies, we have proven it is\npossible to use the Murchison Widefield Array to study ionospheric\nscintillation in the weak regime, which is sensitive to structures on scales\n$\\sim$300 m at our observing frequency of 154 MHz, where the phase variance on\nthis scale was 0.06 rad$^{2}$ in the most extreme case observed. Analysing over\n1000 individual 2-minute observations, we compared the ionospheric phase\nvariance with that inferred with previous measurements of refractive shifts,\nwhich are most sensitive to scales almost an order of magnitude larger. The two\nmeasurements were found to be highly correlated (Pearson correlation\ncoefficient 0.71). We observed that for an active ionosphere, the relationship\nbetween these two metrics is in line with what would be expected if the\nionosphere's structure is described by Kolmogorov turbulence between the\nrelevant scales of 300m and 2000m. In the most extreme ionospheric conditions,\nthe refractive shifts were sometimes found to underestimate the small-scale\nvariance by a factor of four or more, and it is these ionospheric conditions\nthat could have significant effects on radio astronomy observations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper tackles the problem of how to pre-train a model and make it\ngenerally reusable backbones for downstream task learning. In pre-training, we\npropose a method that builds an agent-environment interaction model by learning\ndomain invariant successor features from the agent's vast experiences covering\nvarious tasks, then discretize them into behavior prototypes which result in an\nembodied set structure. To make the model generally reusable for downstream\ntask learning, we propose (1) embodied feature projection that retains previous\nknowledge by projecting the new task's observation-action pair to the embodied\nset structure and (2) projected Bellman updates which add learning plasticity\nfor the new task setting. We provide preliminary results that show downstream\ntask learning based on a pre-trained embodied set structure can handle unseen\nchanges in task objectives, environmental dynamics and sensor modalities.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Creating novel views from a single image has achieved tremendous strides with\nadvanced autoregressive models. Although recent methods generate high-quality\nnovel views, synthesizing with only one explicit or implicit 3D geometry has a\ntrade-off between two objectives that we call the ``seesaw'' problem: 1)\npreserving reprojected contents and 2) completing realistic out-of-view\nregions. Also, autoregressive models require a considerable computational cost.\nIn this paper, we propose a single-image view synthesis framework for\nmitigating the seesaw problem. The proposed model is an efficient\nnon-autoregressive model with implicit and explicit renderers. Motivated by\ncharacteristics that explicit methods well preserve reprojected pixels and\nimplicit methods complete realistic out-of-view region, we introduce a loss\nfunction to complement two renderers. Our loss function promotes that explicit\nfeatures improve the reprojected area of implicit features and implicit\nfeatures improve the out-of-view area of explicit features. With the proposed\narchitecture and loss function, we can alleviate the seesaw problem,\noutperforming autoregressive-based state-of-the-art methods and generating an\nimage $\\approx$100 times faster. We validate the efficiency and effectiveness\nof our method with experiments on RealEstate10K and ACID datasets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The success of the current wave of artificial intelligence can be partly\nattributed to deep neural networks, which have proven to be very effective in\nlearning complex patterns from large datasets with minimal human intervention.\nHowever, it is difficult to train these models on complex dynamical systems\nfrom data alone due to their low data efficiency and sensitivity to\nhyperparameters and initialisation. This work demonstrates that injection of\npartially known information at an intermediate layer in a DNN can improve model\naccuracy, reduce model uncertainty, and yield improved convergence during the\ntraining. The value of these physics-guided neural networks has been\ndemonstrated by learning the dynamics of a wide variety of nonlinear dynamical\nsystems represented by five well-known equations in nonlinear systems theory:\nthe Lotka-Volterra, Duffing, Van der Pol, Lorenz, and Henon-Heiles systems.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Traveling-wave optical coherence elastography (OCE) is a promising technique\nto measure the stiffness of biological tissues. While OCE has been applied to\nrelatively homogeneous samples, tissues with significantly varying elasticity\nthrough depth pose a challenge, requiring depth-resolved measurement with\nsufficient resolution and accuracy. Here, we develop a broadband Rayleigh-wave\nOCE technique capable of measuring the elastic moduli of the 3 major skin\nlayers (epidermis, dermis, and hypodermis) reliably by analyzing the dispersion\nof leaky Rayleigh surface waves over a wide frequency range of 0.1-10 kHz. We\nshow that a previously unexplored, high frequency range of 4-10 kHz is critical\nto resolve the thin epidermis, while a low frequency range of 0.2-1 kHz is\nadequate to probe the dermis and deeper hypodermis. We develop a dual\nbilayer-based inverse model to determine the elastic moduli in all 3 layers and\nverify its high accuracy with finite element analysis and skin-mimicking\nphantoms. Finally, the technique is applied to measure the forearm skin of\nhealthy volunteers. The Young's modulus of the epidermis (including the stratum\ncorneum) is measured to be ~ 4 MPa at 4-10 kHz, whereas Young's moduli of the\ndermis and hypodermis are about 40 and 15 kPa, respectively, at 0.2-1 kHz.\nBesides dermatologic applications, this method may be useful for the mechanical\nanalysis of various other layered tissues with sub-mm depth resolution.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The purpose of this paper is to provide a first class of explicit sufficient\nconditions for the central limit theorem and related results in the setup of\nnon-uniformly (partially) expanding non iid random transformations, considered\nas stochastic processes together with some random Gibbs measure. More\nprecisely, we prove a central limit theorem (CLT), an almost sure invariance\nprinciple, a moderate deviations principle, Berry-Esseen type estimates and a\nmoderate local central limit theorem for random Birkhoff sums generated by a\nnon-uniformly partially expanding dynamical systems $T_\\omega$ and a random\nGibbs measure $\\mu_\\omega$ corresponding to a random potential $\\phi_\\omega$\nwith a sufficiently regular variation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Quantum error correction is an important ingredient for scalable quantum\ncomputing. Stabilizer codes are one of the most promising and straightforward\nways to correct quantum errors, since they do not require excessive complexity\nof physical qubits, are convenient for logical operations, and improve\nperformance with increasing the involved qubits number. Here, we propose a\nlinear scalable code of the permutative stabilizers for small distances on the\nring architecture, which takes into account the topological features of the\nsuperconducting platform. We present the way to construct the quantum circuit\nof the code and provide numerical simulation that demonstrate the exponential\nlogical error rate suppression.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The notion of neural collapse refers to several emergent phenomena that have\nbeen empirically observed across various canonical classification problems.\nDuring the terminal phase of training a deep neural network, the feature\nembedding of all examples of the same class tend to collapse to a single\nrepresentation, and the features of different classes tend to separate as much\nas possible. Neural collapse is often studied through a simplified model,\ncalled the unconstrained feature representation, in which the model is assumed\nto have \"infinite expressivity\" and can map each data point to any arbitrary\nrepresentation. In this work, we propose a more realistic variant of the\nunconstrained feature representation that takes the limited expressivity of the\nnetwork into account. Empirical evidence suggests that the memorization of\nnoisy data points leads to a degradation (dilation) of the neural collapse.\nUsing a model of the memorization-dilation (M-D) phenomenon, we show one\nmechanism by which different losses lead to different performances of the\ntrained network on noisy data. Our proofs reveal why label smoothing, a\nmodification of cross-entropy empirically observed to produce a regularization\neffect, leads to improved generalization in classification tasks.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Collaborative 3D object detection exploits information exchange among\nmultiple agents to enhance accuracy of object detection in presence of sensor\nimpairments such as occlusion. However, in practice, pose estimation errors due\nto imperfect localization would cause spatial message misalignment and\nsignificantly reduce the performance of collaboration. To alleviate adverse\nimpacts of pose errors, we propose CoAlign, a novel hybrid collaboration\nframework that is robust to unknown pose errors. The proposed solution relies\non a novel agent-object pose graph modeling to enhance pose consistency among\ncollaborating agents. Furthermore, we adopt a multi-scale data fusion strategy\nto aggregate intermediate features at multiple spatial resolutions. Comparing\nwith previous works, which require ground-truth pose for training supervision,\nour proposed CoAlign is more practical since it doesn't require any\nground-truth pose supervision in the training and makes no specific assumptions\non pose errors. Extensive evaluation of the proposed method is carried out on\nmultiple datasets, certifying that CoAlign significantly reduce relative\nlocalization error and achieving the state of art detection performance when\npose errors exist. Code are made available for the use of the research\ncommunity at https://github.com/yifanlu0227/CoAlign.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a new movie and TV show recommendation dataset collected from the\nreal users of MTS Kion video-on-demand platform. In contrast to other popular\nmovie recommendation datasets, such as MovieLens or Netflix, our dataset is\nbased on the implicit interactions registered at the watching time, rather than\non explicit ratings. We also provide rich contextual and side information\nincluding interactions characteristics (such as temporal information, watch\nduration and watch percentage), user demographics and rich movies\nmeta-information. In addition, we describe the MTS Kion Challenge - an online\nrecommender systems challenge that was based on this dataset - and provide an\noverview of the best performing solutions of the winners. We keep the\ncompetition sandbox open, so the researchers are welcome to try their own\nrecommendation algorithms and measure the quality on the private part of the\ndataset.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Hardness of approximation (HA) -- the phenomenon that, assuming P $\\neq$ NP,\none can easily compute an $\\epsilon$-approximation to the solution of a\ndiscrete computational problem for $\\epsilon > \\epsilon_0 > 0$, but for\n$\\epsilon < \\epsilon_0$ it suddenly becomes intractable -- is a core phenomenon\nin the foundations of computations that has transformed computer science. In\nthis paper we study the newly discovered phenomenon in the foundations of\ncomputational mathematics: generalised hardness of approximation (GHA) -- which\nin spirit is close to classical HA in computer science. However, GHA is\ntypically independent of the P vs. NP question in many cases. Thus, it requires\na new mathematical framework that we initiate in this paper. We demonstrate the\nhitherto undiscovered phenomenon that GHA happens when using AI techniques in\norder to train optimal neural networks (NNs). In particular, for any non-zero\nunderdetermined linear problem the following phase transition may occur: One\ncan prove the existence of optimal NNs for solving the problem but they can\nonly be computed to a certain accuracy $\\epsilon_0 > 0$. Below the\napproximation threshold $\\epsilon_0$ -- not only does it become intractable to\ncompute the NN -- it becomes impossible regardless of computing power, and no\nrandomised algorithm can solve the problem with probability better than 1/2. In\nother cases, despite the existence of a stable optimal NN, any attempts of\ncomputing it below the approximation threshold $\\epsilon_0$ will yield an\nunstable NN. Our results use and extend the current mathematical framework of\nthe Solvability Complexity Index (SCI) hierarchy and facilitate a program for\ndetecting the GHA phenomenon throughout computational mathematics and AI.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Anisotropic exchange couplings, such as the Dzyaloshinskii-Moriya interaction\n(DMI), have played a vital role in the formation and dynamics of spin textures.\nThis work predicts an anisotropic conduction electron spin density in metals\nwith heavy magnetic impurities. The polarization of this\n$Dzyaloshinskii$-$Moriya$ $spin$ $density$ (DM-SD) is not collinear to the\nlocalized magnetic moments but rotated by the spin-dependent skew scattering of\nheavy atoms. The DM-SD induces the DMI between magnetic moments in metals and,\ntherefore, it is the anisotropic extension of the\nRutherman-Kittel-Kasuya-Yoshida spin density. Our model consists of two\nlocalized magnetic moments, one with a large spin-orbit coupling (a lanthanide\nor rare earth), in a free electron gas. The lanthanide spin controls the DM-SD\nstrength and polarization, promising a flexible control mechanism for\nanisotropic couplings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We explore the possibility that axion-like-particles (ALPs), which would be\nproduced in the core of magnetars and would then convert in the magnetosphere\ninto photons, can explain magnetar hard X-ray spectra. We remark that this\nscenario would also provide answers to some questions related to magnetar\nheating. Indeed, considering that magnetars have: 1) hard X-ray spectra that\nare difficult to explain with known mechanisms; 2) large photon luminosities\nthat force high core temperatures; 3) high core temperatures that imply large\nneutrino emissivities; 4) and large neutrino emissivities that lead to small\nmagnetar lifetimes in contradiction to observations -- explaining the hard\nX-ray spectra with ALPs could decrease the core temperatures and thus the\nneutrino emissivities, allowing for longer magnetar lifetimes as expected from\nobservations. In this work, we initiate the study of this scenario for three\nmagnetars with extreme luminosities, and conclude that the general idea is\nlikely worth investigating in more detail.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Ramsey's Theorem guarantees for every graph H that any 2-edge-coloring of a\nsufficiently large complete graph contains a monochromatic copy of H. In 1962,\nErdos conjectured that the random 2-edge-coloring minimizes the number of\nmonochromatic copies of K_k, and the conjecture was extended by Burr and Rosta\nto all graphs. In the late 1980s, the conjectures were disproved by Thomason\nand Sidorenko, respectively. A classification of graphs whose number of\nmonochromatic copies is minimized by the random 2-edge-coloring, which are\nreferred to as common graphs, remains a challenging open problem. If\nSidorenko's Conjecture, one of the most significant open problems in extremal\ngraph theory, is true, then every 2-chromatic graph is common, and in fact, no\n2-chromatic common graph unsettled for Sidorenko's Conjecture is known. While\nexamples of 3-chromatic common graphs were known for a long time, the existence\nof a 4-chromatic common graph was open until 2012, and no common graph with a\nlarger chromatic number is known.\n  We construct connected k-chromatic common graphs for every k. This answers a\nquestion posed by Hatami, Hladky, Kral, Norine and Razborov [Combin. Probab.\nComput. 21 (2012), 734-742], and a problem listed by Conlon, Fox and Sudakov\n[London Math. Soc. Lecture Note Ser. 424 (2015), 49-118, Problem 2.28]. This\nalso answers in a stronger form the question raised by Jagger, Stovicek and\nThomason [Combinatorica 16, (1996), 123-131] whether there exists a common\ngraph with chromatic number at least four.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we study the potentials of passive measurements to gain\nadvanced knowledge about QUIC deployments. By analyzing one month backscatter\ntraffic of the /9 CAIDA network telescope, we are able to make the following\nobservations. First, we can identify different off-net deployments of\nhypergiants, using packet features such as QUIC source connection IDs (SCID),\npacket coalescence, and packet lengths. Second, Facebook and Google configure\nsignificantly different retransmission timeouts and maximum number of\nretransmissions. Third, SCIDs allow further insights into load balancer\ndeployments such as number of servers per load balancer. We bolster our results\nby active measurements.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper builds two detailed examples of generalized normal in\nnon-Euclidean spaces, i.e. the hyperbolic and elliptic geometries. In the\nhyperbolic plane we define a n-sided hyperbolic polygon P, which is the\nEuclidean closure of the hyperbolic plane H, bounded by n hyperbolic geodesic\nsegments. The polygon P is built by considering the unique geodesic that\nconnects the n+2 vertices (tilde z),z0,z1,...,z(n-1),z(n). The geodesics that\nlink the vertices are Euclidean semicircles centred on the real axis. The\nvector normal to the geodesic linking two consecutive vertices is evaluated and\nturns out to be discontinuous. Within the framework of elliptic geometry, we\nsolve the geodesic equation and construct a geodesic triangle. Also in this\ncase, we obtain a discontinuous normal vector field. Last, the possible\napplication to two-dimensional Euclidean quantum gravity is outlined.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Based on the ab initio calculations, we study the electronic structure of the\nBiTeI/MnBi2Te4 heterostructure interface composed of the anti-ferromagnetic\ntopological insulator MnBi$_2$Te$_4$ and the polar semiconductor trilayer\nBiTeI. We found significant difference in electronic properties at different\ntypes of contact between substrate and the overlayer. While the case of Te-Te\ninterface forms natural expansion of the substrate, when Dirac cone state\nlocates mostly in the polar overlayer region and undergoes slight exchange\nsplitting, Te-I contact is the source of four-band state contributed by the\nsubstrate Dirac cone and Rashba-type state of the polar trilayer. Owing to\nmagnetic proximity, the pair of Kramers degeneracies for this state are lifted,\nwhat produces Hall response in transport regime. We believe, our findings\nprovide new opportunities to construct novel type spintronic devices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The Quantum Approximate Optimization Algorithm (QAOA) is a promising\ncandidate algorithm for demonstrating quantum advantage in optimization using\nnear-term quantum computers. However, QAOA has high requirements on gate\nfidelity due to the need to encode the objective function in the phase\nseparating operator, requiring a large number of gates that potentially do not\nmatch the hardware connectivity. Using the MaxCut problem as the target, we\ndemonstrate numerically that an easier way to implement an alternative phase\noperator can be used in lieu of the phase operator encoding the objective\nfunction, as long as the ground state is the same. We observe that if the\nground state energy is not preserved, the approximation ratio obtained by QAOA\nwith such phase separating operator is likely to decrease. Moreover, we show\nthat a better alignment of the low energy subspace of the alternative operator\nleads to better performance. Leveraging these observations, we propose a\nsparsification strategy that reduces the resource requirements of QAOA. We also\ncompare our sparsification strategy with some other classical graph\nsparsification methods, and demonstrate the efficacy of our approach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For any integer $m<n$, where $m$ can depend on $n$, we study the rate of\nconvergence of $\\frac{1}{\\sqrt{m}}\\mathrm{Tr} \\mathbf{U}^m$ to its limiting\nGaussian as $n\\to\\infty$ for orthogonal, unitary and symplectic Haar\ndistributed random matrices $\\mathbf{U}$ of size $n$. In the unitary case, we\nprove that the total variation distance is less than $\\Gamma(\\lfloor n/m\n\\rfloor+2)^{-1} m^{- \\lfloor n/m\\rfloor} \\lfloor n/m \\rfloor^{1/4}\\sqrt{\\log\nn}$ times a constant. This result interpolates between the super-exponential\nbound obtained for fixed $m$ and the $1/n$ bound coming from the Berry-Esseen\ntheorem applicable when $m\\ge n$ by a result of Rains. We obtain analogous\nresults for the orthogonal and symplectic groups. In these cases, our total\nvariation upper bound takes the form $\\Gamma(2\\lfloor\nn/m\\rfloor+1)^{-1/2}m^{-\\lfloor n/m\\rfloor +1}(\\log n)^{1/4}$ times a constant\nand the result holds provided $n \\geq 2m$. For $m=1$, we obtain complementary\nlower bounds and precise asymptotics for the $L^2$-distances as $n\\to\\infty$,\nwhich show how sharp our results are.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  String edit distances have been used for decades in applications ranging from\nspelling correction and web search suggestions to DNA analysis. Most string\nedit distances are variations of the Levenshtein distance and consider only\nsingle-character edits. In forensic applications polymorphic genetic markers\nsuch as short tandem repeats (STRs) are used. At these repetitive motifs the\nDNA copying errors consist of more than just single base differences. More\noften the phenomenon of ``stutter'' is observed, where the number of repeated\nunits differs (by whole units) from the template. To adapt the Levenshtein\ndistance to be suitable for forensic applications where DNA sequence similarity\nis of interest, a generalized string edit distance is defined that accommodates\nthe addition or deletion of whole motifs in addition to single-nucleotide\nedits. A dynamic programming implementation is developed for computing this\ndistance between sequences. The novelty of this algorithm is in handling the\ncomplex interactions that arise between multiple- and single-character edits.\nForensic examples illustrate the purpose and use of the Restricted Forensic\nLevenshtein (RFL) distance measure, but applications extend to sequence\nalignment and string similarity in other biological areas, as well as dynamic\nprogramming algorithms more broadly.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The rapid advancement of deep learning has significantly boomed the\ndevelopment of photorealistic style transfer. In this review, we reviewed the\ndevelopment of photorealistic style transfer starting from artistic style\ntransfer and the contribution of traditional image processing techniques on\nphotorealistic style transfer, including some work that had been completed in\nthe Multimedia lab at the University of Alberta. Many techniques were discussed\nin this review. However, our focus is on VGG-based techniques, whitening and\ncoloring transform (WCTs) based techniques, the combination of deep learning\nwith traditional image processing techniques.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The phase transition between galaxies and quasars is often identified with\nthe rare population of hyper-luminous, hot dust-obscured galaxies. Galaxy\nformation models predict these systems to grow via mergers, that can deliver\nlarge amounts of gas toward their centers, induce intense bursts of star\nformation and feed their supermassive black holes. Here we report the detection\nof 24 galaxies emitting Lyman-alpha emission on projected physical scales of\nabout 400 kpc around the hyper-luminous hot dust-obscured galaxy W0410-0913, at\nredshift z = 3.631, using Very Large Telescope observations. While this\nindicates that W0410-0913 evolves in a very dense environment, we do not find\nclear signs of mergers that could sustain its growth. Data suggest that if\nmergers occurred, as models expect, these would involve less massive\nsatellites, with only a moderate impact on the internal interstellar medium of\nW0410-0913, which is sustained by a rotationally-supported fast-rotating\nmolecular disk, as Atacama Large Millimeter Array observations suggest.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recent advances in deep learning have led to a paradigm shift in reversible\nsteganography. A fundamental pillar of reversible steganography is predictive\nmodelling which can be realised via deep neural networks. However, non-trivial\nerrors exist in inferences about some out-of-distribution and noisy data. In\nview of this issue, we propose to consider uncertainty in predictive models\nbased upon a theoretical framework of Bayesian deep learning. Bayesian neural\nnetworks can be regarded as self-aware machinery; that is, a machine that knows\nits own limitations. To quantify uncertainty, we approximate the posterior\npredictive distribution through Monte Carlo sampling with stochastic forward\npasses. We further show that predictive uncertainty can be disentangled into\naleatoric and epistemic uncertainties and these quantities can be learnt in an\nunsupervised manner. Experimental results demonstrate an improvement delivered\nby Bayesian uncertainty analysis upon steganographic capacity-distortion\nperformance.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Any-to-any voice conversion problem aims to convert voices for source and\ntarget speakers, which are out of the training data. Previous works wildly\nutilize the disentangle-based models. The disentangle-based model assumes the\nspeech consists of content and speaker style information and aims to untangle\nthem to change the style information for conversion. Previous works focus on\nreducing the dimension of speech to get the content information. But the size\nis hard to determine to lead to the untangle overlapping problem. We propose\nthe Disentangled Representation Voice Conversion (DRVC) model to address the\nissue. DRVC model is an end-to-end self-supervised model consisting of the\ncontent encoder, timbre encoder, and generator. Instead of the previous work\nfor reducing speech size to get content, we propose a cycle for restricting the\ndisentanglement by the Cycle Reconstruct Loss and Same Loss. The experiments\nshow there is an improvement for converted speech on quality and voice\nsimilarity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we introduce a new framework for generating synthetic vascular\ntrees, based on rigorous model-based mathematical optimization. Our main\ncontribution is the reformulation of finding the optimal global tree geometry\ninto a nonlinear optimization problem (NLP). This rigorous mathematical\nformulation accommodates efficient solution algorithms such as the interior\npoint method and allows us to easily change boundary conditions and constraints\napplied to the tree. Moreover, it creates trifurcations in addition to\nbifurcations. A second contribution is the addition of an optimization stage\nfor the tree topology. Here, we combine constrained constructive optimization\n(CCO) with a heuristic approach to search among possible tree topologies. We\ncombine the NLP formulation and the topology optimization into a single\nalgorithmic approach. Finally, we attempt the validation of our new model-based\noptimization framework using a detailed corrosion cast of a human liver, which\nallows a quantitative comparison of the synthetic tree structure to the tree\nstructure determined experimentally down to the fifth generation. The results\nshow that our new framework is capable of generating asymmetric synthetic trees\nthat match the available physiological corrosion cast data better than trees\ngenerated by the standard CCO approach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This work aims to identify the relevant physical processes in shaping the\nintensity and polarization patterns of the solar K I D lines through spectral\nsyntheses, placing particular emphasis on the D2 line. The theoretical Stokes\nprofiles were obtained by numerically solving the radiative transfer problem\nfor polarized radiation considering one-dimensional semi-empirical models of\nthe solar atmosphere. The calculations account for scattering polarization,\npartial frequency redistribution (PRD) effects, hyperfine structure (HFS), J-\nand F-state interference, multiple isotopes, and magnetic fields of arbitrary\nstrength and orientation. The intensity and circular polarization profiles of\nboth D lines can be suitably modeled while neglecting both J-state interference\nand HFS. The magnetograph formula can be applied to both lines, without\nincluding HFS, to estimate weak longitudinal magnetic fields in the lower\nchromosphere. By contrast, modeling their scattering polarization signals\nrequires the inclusion of HFS. The D2 scattering polarization amplitude is\nstrongly depolarized by HFS, but it remains measurable. An appreciable error is\nincurred in the scattering polarization profile if PRD effects are not taken\ninto account. Collisions during scattering processes also have an appreciable\ndepolarizing effect. Finally, the D2 scattering polarization signal is\nespecially sensitive to magnetic fields with strengths around 10 G and it\nstrongly depends on their orientation. Despite this, its center-to-limb\nvariation relative to the amplitude at the limb is largely insensitive to the\nfield strength and orientation. These findings highlight the value of the K I\nD2 line polarization for diagnostics of the solar magnetism, and show that the\nlinear and circular polarization signals of this line are primarily sensitive\nto magnetic fields in the lower chromosphere and upper photosphere,\nrespectively.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In machine learning, disparity metrics are often defined by measuring the\ndifference in the performance or outcome of a model, across different\nsub-populations (groups) of datapoints. Thus, the inputs to disparity\nquantification consist of a model's predictions $\\hat{y}$, the ground-truth\nlabels for the predictions $y$, and group labels $g$ for the data points.\nPerformance of the model for each group is calculated by comparing $\\hat{y}$\nand $y$ for the datapoints within a specific group, and as a result, disparity\nof performance across the different groups can be calculated. In many real\nworld scenarios however, group labels ($g$) may not be available at scale\nduring training and validation time, or collecting them might not be feasible\nor desirable as they could often be sensitive information. As a result,\nevaluating disparity metrics across categorical groups would not be feasible.\nOn the other hand, in many scenarios noisy groupings may be obtainable using\nsome form of a proxy, which would allow measuring disparity metrics across\nsub-populations. Here we explore performing such analysis on computer vision\nmodels trained on human faces, and on tasks such as face attribute prediction\nand affect estimation. Our experiments indicate that embeddings resulting from\nan off-the-shelf face recognition model, could meaningfully serve as a proxy\nfor such estimation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study dynamic traffic assignment with side-constraints. We first give a\ncounter-example to a key result from the literature regarding the existence of\ndynamic equilibria for volume-constrained traffic models in the classical\nedge-delay model. Our counter-example shows that the feasible flow space need\nnot be convex and it further reveals that classical infinite dimensional\nvariational inequalities are not suited for the definition of side-constrained\ndynamic equilibria. We propose a new framework for side-constrained dynamic\nequilibria based on the concept of feasible $\\varepsilon$-deviations of flow\nparticles in space and time. Under natural assumptions, we characterize the\nresulting equilibria by means of quasi-variational and variational\ninequalities, respectively. Finally, we establish first existence results for\nside-constrained dynamic equilibria for the non-convex setting of\nvolume-constraints.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We show that the regime of strong-light matter coupling with remarkable\nmagnetic properties can be realized in systems based on monolayers of chromium\ntriiodide (CrI3). This two-dimensional material combines the presence of\nferromagnetic ordering with the possibility of forming strongly-bound excitonic\ncomplexes even at room temperature. Using microscopic first-principle\ncalculations we reveal a rich spectrum of optical transitions, corresponding to\nboth Wannier- and Frenkel-type excitons, including those containing electrons\nwith a negative effective mass. We show that excitons of different\npolarizations efficiently hybridize with a photonic mode of a planar\nmicrocavity, and due to the peculiar selection rules polariton modes become\nwell resolved in circular polarizations. The combination of very strong optical\noscillator strength of excitons and cavity confinement leads to large values of\nthe Rabi splitting, reaching 35 meV for a single monolayer, and giant Zeeman\nsplitting between polariton modes of up to 20 meV. This makes CrI3 an excellent\nplatform for magnetopolaritonic applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using $e^{+}e^{-}$ annihilation data corresponding to a total integrated\nluminosity of $6.32$ fb$^{-1}$ collected at the center-of-mass energies between\n4.178 and 4.226 GeV with the BESIII detector, we perform an amplitude analysis\nof the decay $D^+_s \\to K^-K^+\\pi^+\\pi^+\\pi^-$ and determine the relative\nfractions and phases of different intermediate processes. Absolute branching\nfraction of $D^+_s\\to K^-K^+\\pi^+\\pi^+\\pi^-$ decay is measured to be\n($6.60\\pm0.47_{\\rm stat.}\\pm0.35_{\\rm syst.})\\times 10^{-3}$. The dominant\nintermediate process is $D_{s}^{+} \\to a_1(1260)^+\\phi, \\phi\\to K^-K^+,\na_1(1260)^+\\to \\rho\\pi^+, \\rho\\to\\pi^+\\pi^-$, with a branching fraction of\n$(5.16\\pm0.41_{\\rm stat.}\\pm0.27_{\\rm syst.})\\times 10^{-3}$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multi-label classification has attracted much attention in the machine\nlearning community to address the problem of assigning single samples to more\nthan one class at the same time. We propose an evolving multi-label fuzzy\nclassifier (EFC-ML) which is able to self-adapt and self-evolve its structure\nwith new incoming multi-label samples in an incremental, single-pass manner. It\nis based on a multi-output Takagi-Sugeno type architecture, where for each\nclass a separate consequent hyper-plane is defined. The learning procedure\nembeds a locally weighted incremental correlation-based algorithm combined with\n(conventional) recursive fuzzily weighted least squares and Lasso-based\nregularization. The correlation-based part ensures that the interrelations\nbetween class labels, a specific well-known property in multi-label\nclassification for improved performance, are preserved properly; the\nLasso-based regularization reduces the curse of dimensionality effects in the\ncase of a higher number of inputs. Antecedent learning is achieved by\nproduct-space clustering and conducted for all class labels together, which\nyields a single rule base, allowing a compact knowledge view. Furthermore, our\napproach comes with an online active learning (AL) strategy for updating the\nclassifier on just a number of selected samples, which in turn makes the\napproach applicable for scarcely labelled streams in applications, where the\nannotation effort is typically expensive. Our approach was evaluated on several\ndata sets from the MULAN repository and showed significantly improved\nclassification accuracy compared to (evolving) one-versus-rest or classifier\nchaining concepts. A significant result was that, due to the online AL method,\na 90\\% reduction in the number of samples used for classifier updates had\nlittle effect on the accumulated accuracy trend lines compared to a full update\nin most data set cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This letter focuses on integrating rate-splitting multiple-access (RSMA) with\ntime-division-duplex Cell-free Massive MIMO (multiple-input multiple-output)\nfor massive machine-type communications. Due to the large number of devices,\ntheir sporadic access behaviour and limited coherence interval, we assume a\nrandom access strategy with all active devices utilizing the same pilot for\nuplink channel estimation. This gives rise to a highly pilot-contaminated\nscenario, which inevitably deteriorates channel estimates. Motivated by the\nrobustness of RSMA towards imperfect channel state information, we propose a\nnovel RSMA-assisted downlink transmission framework for cell-free massive MIMO.\nOn the basis of the downlink achievable spectral efficiency of the common and\nprivate streams, we devise a heuristic common precoder design and propose a\nnovel max-min power control method for the proposed RSMA-assisted scheme.\nNumerical results show that RSMA effectively mitigates the effect of pilot\ncontamination in the downlink and achieves a significant performance gain over\na conventional cell-free massive MIMO network.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the\nfocus or the perspective that a text expresses in depicting an event. Our tool,\nwhose rationale we also support through a large collection of human judgements,\nis theoretically grounded on frame semantics and cognitive linguistics, and\nimplemented using the LOME frame semantic parser. We describe SOCIOFILLMORE's\ndevelopment and functionalities, show how non-NLP researchers can easily\ninteract with the tool, and present some example case studies which are already\nincorporated in the system, together with the kind of analysis that can be\nvisualised.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Post publication assessment remains necessary to check erroneous or\nfraudulent scientific publications. We present an online platform, the\n'Problematic Paper Screener'\n(https://www.irit.fr/~Guillaume.Cabanac/problematic-paper-screener) that\nleverages both automatic machine detection and human assessment to identify and\nflag already published problematic articles. We provide a new effective tool to\ncurate the scientific literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the application of machine learning to security-critical and sensitive\ndomains, there is a growing need for integrity and privacy in computation using\naccelerators, such as GPUs. Unfortunately, the support for trusted execution on\nGPUs is currently very limited - trusted execution on accelerators is\nparticularly challenging since the attestation mechanism should not reduce\nperformance. Although hardware support for trusted execution on GPUs is\nemerging, we study purely software-based approaches for trusted GPU execution.\nA software-only approach offers distinct advantages: (1) complement\nhardware-based approaches, enhancing security especially when vulnerabilities\nin the hardware implementation degrade security, (2) operate on GPUs without\nhardware support for trusted execution, and (3) achieve security without\nreliance on secrets embedded in the hardware, which can be extracted as history\nhas shown. In this work, we present SAGE, a software-based attestation\nmechanism for GPU execution. SAGE enables secure code execution on NVIDIA GPUs\nof the Ampere architecture (A100), providing properties of code integrity and\nsecrecy, computation integrity, as well as data integrity and secrecy - all in\nthe presence of malicious code running on the GPU and CPU. Our evaluation\ndemonstrates that SAGE is already practical today for executing code in a\ntrustworthy way on GPUs without specific hardware support.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We give an algorithm that takes as input an $n$-vertex graph $G$ and an\ninteger $k$, runs in time $2^{O(k^2)} n^{O(1)}$, and outputs a tree\ndecomposition of $G$ of width at most $k$, if such a decomposition exists. This\nresolves the long-standing open problem of whether there is a $2^{o(k^3)}\nn^{O(1)}$ time algorithm for treewidth. In particular, our algorithm is the\nfirst improvement on the dependency on $k$ in algorithms for treewidth since\nthe $2^{O(k^3)} n^{O(1)}$ time algorithm given by Bodlaender and Kloks [ICALP\n1991] and Lagergren and Arnborg [ICALP 1991].\n  We also give an algorithm that given an $n$-vertex graph $G$, an integer $k$,\nand a rational $\\varepsilon \\in (0,1)$, in time $k^{O(k/\\varepsilon)} n^{O(1)}$\neither outputs a tree decomposition of $G$ of width at most $(1+\\varepsilon)k$\nor determines that the treewidth of $G$ is larger than $k$. Prior to our work,\nno approximation algorithms for treewidth with approximation ratio less than\n$2$, other than the exact algorithms, were known. Both of our algorithms work\nin polynomial space.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we find a necessary and sufficient condition for multi-twisted\nReed-Solomon codes to be MDS. Further, we obtain necessary conditions for the\nexistence of multi-twisted RS codes with zero and one-dimensional hulls.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A proper vertex-colouring of a simple graph $G$ is said to be odd if, for\nevery non-isolated vertex $v$ of $G$, some colour appears an odd number of\ntimes in the neighbourhood of $v$. We show that if $G$ embeds in the torus,\nthen it admits a proper odd vertex-colouring with at most $9$ colours.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The efficient resolution of optimization problems is one of the key issues in\ntoday's industry. This task relies mainly on classical algorithms that present\nscalability problems and processing limitations. Quantum computing has emerged\nto challenge these types of problems. In this paper, we focus on the\nMetropolis-Hastings quantum algorithm that is based on quantum walks. We use\nthis algorithm to build a quantum software tool called Quantum Metropolis\nSolver (QMS). We validate QMS with the N-Queen problem to show a potential\nquantum advantage in an example that can be easily extrapolated to an\nArtificial Intelligence domain. We carry out different simulations to validate\nthe performance of QMS and its configuration.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In lifelong learning, an agent learns throughout its entire life without\nresets, in a constantly changing environment, as we humans do. Consequently,\nlifelong learning comes with a plethora of research problems such as continual\ndomain shifts, which result in non-stationary rewards and environment dynamics.\nThese non-stationarities are difficult to detect and cope with due to their\ncontinuous nature. Therefore, exploration strategies and learning methods are\nrequired that are capable of tracking the steady domain shifts, and adapting to\nthem. We propose Reactive Exploration to track and react to continual domain\nshifts in lifelong reinforcement learning, and to update the policy\ncorrespondingly. To this end, we conduct experiments in order to investigate\ndifferent exploration strategies. We empirically show that representatives of\nthe policy-gradient family are better suited for lifelong learning, as they\nadapt more quickly to distribution shifts than Q-learning. Thereby,\npolicy-gradient methods profit the most from Reactive Exploration and show good\nresults in lifelong learning with continual domain shifts. Our code is\navailable at: https://github.com/ml-jku/reactive-exploration.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The adaptation of a Generative Adversarial Network (GAN) aims to transfer a\npre-trained GAN to a target domain with limited training data. In this paper,\nwe focus on the one-shot case, which is more challenging and rarely explored in\nprevious works. We consider that the adaptation from a source domain to a\ntarget domain can be decoupled into two parts: the transfer of global style\nlike texture and color, and the emergence of new entities that do not belong to\nthe source domain. While previous works mainly focus on style transfer, we\npropose a novel and concise framework to address the \\textit{generalized\none-shot adaptation} task for both style and entity transfer, in which a\nreference image and its binary entity mask are provided. Our core idea is to\nconstrain the gap between the internal distributions of the reference and\nsyntheses by sliced Wasserstein distance. To better achieve it, style fixation\nis used at first to roughly obtain the exemplary style, and an auxiliary\nnetwork is introduced to the generator to disentangle entity and style\ntransfer. Besides, to realize cross-domain correspondence, we propose the\nvariational Laplacian regularization to constrain the smoothness of the adapted\ngenerator. Both quantitative and qualitative experiments demonstrate the\neffectiveness of our method in various scenarios. Code is available at\n\\url{https://github.com/zhangzc21/Generalized-One-shot-GAN-adaptation}.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Learning With Opponent-Learning Awareness (LOLA) (Foerster et al. [2018a]) is\na multi-agent reinforcement learning algorithm that typically learns\nreciprocity-based cooperation in partially competitive environments. However,\nLOLA often fails to learn such behaviour on more complex policy spaces\nparameterized by neural networks, partly because the update rule is sensitive\nto the policy parameterization. This problem is especially pronounced in the\nopponent modeling setting, where the opponent's policy is unknown and must be\ninferred from observations; in such settings, LOLA is ill-specified because\nbehaviorally equivalent opponent policies can result in non-equivalent updates.\nTo address this shortcoming, we reinterpret LOLA as approximating a proximal\noperator, and then derive a new algorithm, proximal LOLA (POLA), which uses the\nproximal formulation directly. Unlike LOLA, the POLA updates are\nparameterization invariant, in the sense that when the proximal objective has a\nunique optimum, behaviorally equivalent policies result in behaviorally\nequivalent updates. We then present practical approximations to the ideal POLA\nupdate, which we evaluate in several partially competitive environments with\nfunction approximation and opponent modeling. This empirically demonstrates\nthat POLA achieves reciprocity-based cooperation more reliably than LOLA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The primary benefit of identifying a valid surrogate marker is the ability to\nuse it in a future trial to test for a treatment effect with shorter follow-up\ntime or less cost. However, previous work has demonstrated potential\nheterogeneity in the utility of a surrogate marker. When such heterogeneity\nexists, existing methods that use the surrogate to test for a treatment effect\nwhile ignoring this heterogeneity may lead to inaccurate conclusions about the\ntreatment effect, particularly when the patient population in the new study has\na different mix of characteristics than the study used to evaluate the utility\nof the surrogate marker. In this paper, we develop a novel test for a treatment\neffect using surrogate marker information that accounts for heterogeneity in\nthe utility of the surrogate. We compare our testing procedure to a test that\nuses primary outcome information (gold standard) and a test that uses surrogate\nmarker information, but ignores heterogeneity. We demonstrate the validity of\nour approach and derive the asymptotic properties of our estimator and variance\nestimates. Simulation studies examine the finite sample properties of our\ntesting procedure and demonstrate when our proposed approach can outperform the\ntesting approach that ignores heterogeneity. We illustrate our methods using\ndata from an AIDS clinical trial to test for a treatment effect using CD4 count\nas a surrogate marker for RNA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper we investigate the next-to-leading power contribution to the\n$B_s \\to \\gamma\\gamma$ and $B_s \\to \\gamma\\ell\\bar{\\ell}$ decays from the\nstrange quark mass effect with the dispersion approach which is QCD inspired\nand more predictive. We have presented the analytic expression of the quark\nmass contribution in the $B_s \\to \\gamma\\gamma$ and $B_s \\to\n\\gamma\\ell\\bar{\\ell}$ decays, together with a new term that is missed in the\nprevious study. The numerical results of the strange quark mass contribution to\nthe $B_s \\to \\gamma\\gamma$ decay is about 6% relative to the total branching\nratio, while it is relatively small in the $B_s \\to \\gamma\\ell\\bar{\\ell}$ decay\ndue to the large resonance contribuiton\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We propose a robust principal component analysis (RPCA) framework to recover\nlow-rank and sparse matrices from temporal observations. We develop an online\nversion of the batch temporal algorithm in order to process larger datasets or\nstreaming data. We empirically compare the proposed approaches with different\nRPCA frameworks and show their effectiveness in practical situations.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the present work, the structural, magnetic, and electronic properties of\nthe two- and one-dimensional honeycomb structures of recently synthesized MnO\n[Zhang et al. Nat. Commun., 20, 1073-1078 (2021)] are investigated by using\nfirst principles calculations. Our calculations show that the single layer 2D\nMnO crystal has a degenerate antiferromagnetic (AFM) ground state and a\nrelatively less favorable ferromagnetic (FM) state. In addition, magnetic\nanisotropy calculations unveil that the easy-axis direction for magnetism\noriginating from unpaired electron states in manganese atoms is normal to the\ncrystal plane. Electronically, while the FM-MnO is a direct semiconductor with\na narrow bandgap, AFM phases display large indirect bandgap semiconducting\nbehavior. Moreover, calculations on nanoribbons of MnO reveal that zigzag edged\nribbons display metallic bahavior, whereas armchair edged nanoribbons are\nsemiconductors. Magnetically, for both zigzag- or armchair-edged nanoribbons,\nAFM order perpendicular to the nanoribbon growth direction is found to be\nfavorable over the other AFM and FM orders. Moreover, depending on the edge\nsymmetry and ribbon width, forbidden band gap values of nanoribbons display\ndistinct family behaviors.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Proton therapy is a modality in fast development. Characterized by a maximum\ndose deposition at the end of the proton trajectory followed by a sharp\nfall-off, proton beams can deliver a highly conformal dose to the tumor while\nsparing organs at risk and surrounding healthy tissues. New treatment planning\nsystems based on spot scanning techniques can now propose multi-field\noptimization. However, in most cases, this optimization only processes the\nfield fluences whereas the choice of ballistics (field geometry) is left to the\noncologist and medical physicist.\n  In this work, we investigate a new optimization framework based on a genetic\napproach. This tool is intended to explore new irradiation schemes and to\nevaluate the potential of actual or future irradiation systems. We propose to\noptimize simultaneously the target points and beam incidence angles in a\ncontinuous manner and with a variable number of beams. No \\textit{a priori}\ntechnological constraints are taken into account, \\textit{i.e.}~the beam energy\nvalues, incidence directions and target points are free parameters.\n  The proposed algorithm is based on a modified version of classical genetic\noperators: mutation, crossover and selection. We use the real coding associated\nwith random perturbations of the parameters to obtain a continuous variation of\nthe potential solutions. We also introduce a perturbation in the exchange\npoints of the crossover to allow variations of the number of beams. These\nvariations are controlled by introducing a beam fluence lower limit.\n  In this paper, we present a complete description of the algorithm and of its\nbehaviour in an elementary test case. The proposed method is finally assessed\nin a clinically-realistic test case.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We have studied the effect of amino acids on electron attachment properties\nof DNA nucleobases, taking cytosine as a model system. The equation of motion\ncoupled cluster theory with an extended basis set has been used to simulate the\nelectron-attached state of the DNA model system. Four selected amino acids,\nArginine, Alanine, Lysine, and Glycine which form a major component of histone\nproteins are considered to investigate their role in electron attachment to DNA\nnucleobase. The electron attachment to cytosine in all the cytosine-amino acid\ndimer complexes follows a doorway mechanism, where the electron gets\ntransferred from initial dipole-bound doorway state to the final\nnucleobase-bound state through the mixing of electronic and nuclear degrees of\nfreedom. In higher amino acid concentration, amino acid-bound state acts as the\ndoorway state, where the initial electron density is localized on the amino\nacid, away from the nucleobase. This leads to the physical shielding of\nnucleobase from the incoming extra electron. At the same time, the presence of\namino acids can increase the stability of nucleobase-bound anionic state, which\ncan suppress the dissociative electron attachment induced sugar-phosphate bond\nbreaking.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper is a cursory study on how topological features are preserved\nwithin the internal representations of neural network layers. Using techniques\nfrom topological data analysis, namely persistent homology, the topological\nfeatures of a simple feedforward neural network's layer representations of a\nmodified torus with a Klein bottle-like twist were computed. The network\nappeared to approximate homeomorphisms in early layers, before significantly\nchanging the topology of the data in deeper layers. The resulting noise\nhampered the ability of persistent homology to compute these features, however\nsimilar topological features seemed to persist longer in a network with a\nbijective activation function.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Multiphoton correlations between entangled quantum images produced through\nparametric downconversion are sensitive to the phase of the bi-photon\nwavefunction, while the 2-photon correlations are insensitive to this phase. To\ndemonstrate this, we propose a simple extension of existing quantum imaging\nexperiments in which the CCD cameras are moved out of focus. We provide\ndetailed analytical predictions for the resulting 4 photon intereferences, and\nsupport these by numerical simulations. The proposed experiment can also be\ninterpreted as entanglement swapping: Bob's photons are initially unentangled,\nbut the joint detection of Alice's photons projects Bob's photons onto an\nentangled state. The general approach proposed here can be extended to other\nquantum optics experiments involving high dimensional entanglement.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present chemical kinetics measurements of the luminol oxydation\nchemiluminescence reaction at the interface between two aqueous solutions,\nusing liquid jet technology. Free-flowing iquid microjets are a relatively\nrecent development that has found its way into a growing number of applications\nin spectroscopy and dynamics. A variant thereof, called flat-jet, is obtained\nwhen two cylindrical jets of a liquid are crossed, leading to a chain of planar\nleaf-shaped structures of the flowing liquid. We here show that in the first\nleaf of this chain the fluids do not exhibit turbulent mixing, providing a\nclean interface between the liquids from the impinging jets. We also show,\nusing the example of the luminol chemiluminescence reaction, how this setup can\nbe used to obtain kinetics information from friction-less flow and by\ncircumventing the requirement for rapid mixing but by intentionally suppressing\nall turbulent mixing and instead relying on diffusion.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Solving general Markov decision processes (MDPs) is a computationally hard\nproblem. Solving finite-horizon MDPs, on the other hand, is highly tractable\nwith well known polynomial-time algorithms. What drives this extreme disparity,\nand do problems exist that lie between these diametrically opposed\ncomplexities? In this paper we identify and analyse a sub-class of stochastic\nshortest path problems (SSPs) for general state-action spaces whose dynamics\nsatisfy a particular drift condition. This construction generalises the\ntraditional, temporal notion of a horizon via decreasing reachability: a\nproperty called reductivity. It is shown that optimal policies can be recovered\nin polynomial-time for reductive SSPs -- via an extension of backwards\ninduction -- with an efficient analogue in reductive MDPs. The practical\nconsiderations of the proposed approach are discussed, and numerical\nverification provided on a canonical optimal liquidation problem.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This paper provides a coalgebraic approach to the language semantics of two\ntypes of non-deterministic automata over nominal sets: non-deterministic\norbit-finite automata (NOFAs) and regular nominal non-deterministic automata\n(RNNAs), which were introduced in previous work. While NOFAs are a\nstraightforward nominal version of non-deterministic automata, RNNAs feature\nordinary as well as name binding transitions. Correspondingly, words accepted\nby RNNAs are strings formed by ordinary letters and name binding letters. Bar\nlanguages are sets of such words modulo $\\alpha$-equivalence, and to every\nstate of an RNNA one associates its accepted bar language. We show that the\nsemantics of NOFAs and RNNAs, respectively, arise both as an instance of the\nKleisli-style coalgebraic trace semantics as well as an instance of the\ncoalgebraic language semantics obtained via generalized determinization. On the\nway we revisit coalgebraic trace semantics in general and give a new compact\nproof for the main result in that theory stating that an initial algebra for a\nfunctor yields the terminal coalgebra for the Kleisli extension of the functor.\nOur proof requires fewer assumptions on the functor than all previous ones.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We develop a symbolic regression framework for extracting the governing\nmathematical expressions from observed data. The evolutionary approach, faiGP,\nis designed to leverage the properties of a function algebra that have been\nencoded into a grammar, providing a theoretical guarantee of universal\napproximation and a way to minimize bloat. In this framework, the choice of\noperators of the grammar may be informed by a physical theory or symmetry\nconsiderations. Since there is currently no theory that can derive the\n'constants of nature', an empirical investigation on extracting these\ncoefficients from an evolutionary process is of methodological interest. We\nquantify the impact of different types of regularizers, including a diversity\nmetric adapted from studies of the transcriptome and a complexity measure, on\nthe performance of the framework. Our implementation, which leverages neural\nnetworks and a genetic programmer, generates non-trivial symbolically\nequivalent expressions (\"Ramanujan expressions\") or approximations with\npotentially interesting numerical applications. To illustrate the framework, a\nmodel of ligand-receptor binding kinetics, including an account of gene\nregulation by transcription factors, and a model of the regulatory range of the\ncistrome from omics data are presented. This study has important implications\non the development of data-driven methodologies for the discovery of governing\nequations in experimental data derived from new sensing systems and\nhigh-throughput screening technologies.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  GRChombo is an open-source code for performing Numerical Relativity time\nevolutions, built on top of the publicly available Chombo software for the\nsolution of PDEs. Whilst GRChombo uses standard techniques in NR, it focusses\non applications in theoretical physics where adaptability, both in terms of\ngrid structure, and in terms of code modification, are key drivers.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We determine the bottom quark mass $\\hat{m}_b$ from QCD sum rules of moments\nof the vector current correlator calculated in perturbative QCD to ${\\cal O}\n(\\hat\\alpha_s^3)$. Our approach is based on the mutual consistency across a set\nof moments where experimental data are required for the resonance contributions\nonly. Additional experimental information from the continuum region can then be\nused for stability tests and to assess the theoretical uncertainty. We find\n$\\hat{m}_b(\\hat{m}_b) = (4180.2 \\pm 7.9)$ MeV for $\\hat\\alpha_s(M_Z) = 0.1182$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The computation of Wasserstein gradient direction is essential for posterior\nsampling problems and scientific computing. The approximation of the\nWasserstein gradient with finite samples requires solving a variational\nproblem. We study the variational problem in the family of two-layer networks\nwith squared-ReLU activations, towards which we derive a semi-definite\nprogramming (SDP) relaxation. This SDP can be viewed as an approximation of the\nWasserstein gradient in a broader function family including two-layer networks.\nBy solving the convex SDP, we obtain the optimal approximation of the\nWasserstein gradient direction in this class of functions. Numerical\nexperiments including PDE-constrained Bayesian inference and parameter\nestimation in COVID-19 modeling demonstrate the effectiveness of the proposed\nmethod.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  For decades, people have been seeking for fishlike flapping motions that can\nrealize underwater propulsion with low energy cost. Complexity of the\nnonstationary flow field around the flapping body makes this problem very\ndifficult. In earlier studies, motion patterns are usually prescribed as\ncertain periodic functions which constrains the following optimization process\nin a small subdomain of the whole motion space. In this work, to avoid this\nmotion constraint, a variational autoencoder (VAE) is designed to compress\nvarious flapping motions into a simple action vector. Then we let a flapping\nairfoil continuously interact with water tunnel environment and adjust its\naction accordingly through a reinforcement learning (RL) framework. By this\nautomatic close-looped experiment, we obtain several motion patterns that can\nresult in high hydrodynamic efficiency comparing to pure harmonic motions with\nthe same thrust level. And we find that, after numerous trials and errors, RL\ntrainings in current experiment always converge to motion patterns that are\nclose to harmonic motions. In other words, current work proves that harmonic\nmotion with appropriate amplitude and frequency is always an optimal choice for\nefficient underwater propulsion. Furthermore, the RL framework proposed here\ncan be also extended to the study of other complex swimming problems, which\nmight pave the way for the creation of a robotic fish that can swim like a real\nfish.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Asteroseismology has revealed small core-to-surface rotation contrasts in\nstars in the whole HR diagram. This is the signature of strong transport of\nangular momentum (AM) in stellar interiors. One of the plausible candidates to\nefficiently carry AM is magnetic fields with various topologies that could be\npresent in stellar radiative zones. Among them, strong axisymmetric azimuthal\nmagnetic fields have received a lot of interest. Indeed, if they are subject to\nthe so-called Tayler instability, the accompanying triggered Maxwell stresses\ncan transport AM efficiently. In addition, the electromotive force induced by\nthe fluctuations of magnetic and velocity fields could potentially sustain a\ndynamo action that leads to the regeneration of the initial strong axisymmetric\nazimuthal magnetic field. The key question we aim to answer is: can we detect\nsignatures of these deep strong azimuthal magnetic fields? The only way to\nanswer this question is asteroseismology and the best laboratories of study are\nintermediate-mass and massive stars. Most of these are rapid rotators during\ntheir main-sequence. Therefore, we have to study stellar pulsations propagating\nin stably stratified, rotating, and potentially strongly magnetised radiative\nzones. We generalise the traditional approximation of rotation by\nsimultaneously taking general axisymmetric differential rotation and azimuthal\nmagnetic fields into account in a non-perturbative way. Using this new\nformalism, we derive the asymptotic properties of magneto-gravito-inertial\n(MGI) waves and their period spacings. We find that toroidal magnetic fields\ninduce a shift in the period spacings of MGI modes. An equatorial azimuthal\nmagnetic field with an amplitude of the order of $10^5\\,\\rm G$ leads to\nsignatures that can be detectable thanks to modern space photometry. More\ncomplex hemispheric configurations are more difficult to observe.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Despite years of research, out-of-domain generalization remains a critical\nweakness of deep networks for semantic segmentation. Previous studies relied on\nthe assumption of a static model, i.e. once the training process is complete,\nmodel parameters remain fixed at test time. In this work, we challenge this\npremise with a self-adaptive approach for semantic segmentation that adjusts\nthe inference process to each input sample. Self-adaptation operates on two\nlevels. First, it employs a self-supervised loss that customizes the parameters\nof convolutional layers in the network to the input image. Second, in Batch\nNormalization layers, self-adaptation approximates the mean and the variance of\nthe entire test data, which is assumed unavailable. It achieves this by\ninterpolating between the training and the reference distribution derived from\na single test sample. To empirically analyze our self-adaptive inference\nstrategy, we develop and follow a rigorous evaluation protocol that addresses\nserious limitations of previous work. Our extensive analysis leads to a\nsurprising conclusion: Using a standard training procedure, self-adaptation\nsignificantly outperforms strong baselines and sets new state-of-the-art\naccuracy on multi-domain benchmarks. Our study suggests that self-adaptive\ninference may complement the established practice of model regularization at\ntraining time for improving deep network generalization to out-of-domain data.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study pathwise regularization by noise for equations on the plane in the\nspirit of the framework outlined by Catellier and Gubinelli in\n\\cite{Catellier2016}. To this end we extend the notion of non-linear Young\nequations to a two dimensional domain and prove existence and uniqueness of\nsuch equations. This concept is then used in order to prove regularization by\nnoise for stochastic equations on the plane. The statement of regularization by\nnoise is formulated in terms of regularity of the local time associated to the\nperturbing stochastic field. As an illustration we prove wellposedness of a 1D\nnon-linear wave equation with a noisy boundary given by two independent\nfractional Brownian motions. A discussion of several open problems and further\ninvestigations is provided.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The condition number of computing the invariant left or right singular\nsubspace corresponding to any selected subset of singular values of matrices is\ndetermined in the Frobenius norm on the input space of matrices and the\nchordal, Frobenius, and Procrustes distances on the Grassmannian output space.\nUp to a small factor, this condition number equals the inverse minimum singular\nvalue gap between the singular values selected by the projector and those not\nselected, including any ghost singular values.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Topological magnetism in low-dimensional systems is of fundamental and\npractical importance in condensed-matter physics and material science. Here,\nusing first-principles and Monte-Carlo simulations, we present that multiple\ntopological magnetism (i.e., skyrmion and bimeron) can survive in van der Waals\nHeterostructure of MnTe2ZrS2. Arising from interlayer coupling, MnTe2ZrS2 can\nharbor a large Dzyaloshinskii-Moriya interaction. This, combined with\nferromagnetic exchange interaction, yields an intriguing skyrmion phase\nconsisting of sub-10 nm magnetic skyrmions under a tiny magnetic field of 75\nmT. Meanwhile, upon harnessing a small electric field, magnetic bimeron can be\nobserved in MnTe2ZrS2 as well, suggesting the existence of multiple topological\nmagnetism. Through interlayer sliding, both topological spin textures can be\nswitched on-off, suggesting their stacking-dependent character. In addition,\nthe impacts of d and Keff on these spin textures are revealed, and a\ndimensionless parameter is utilized to describe their joint effect. These\nexplored phenomena and insights not only are useful for fundamental research in\ntopological magnetism, but also enable novel applications in nanodevices.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a simple and constructive method to find $N$-soliton solutions of\nthe equation suggested by Davydova and Lashkin to describe the dynamics of\nnonlinear ion-cyclotron waves in a plasma and subsequently known (in a more\ngeneral form and as applied to nonlinear optics) as the Fokas-Lenells equation.\nUsing the classical inverse scattering transform approach, we find bright\n$N$-soliton solutions, rational $N$-soliton solutions, and $N$-soliton\nsolutions in the form of a mixture of exponential and rational functions.\nExplicit breather solutions are presented as examples. Unlike purely algebraic\nconstructions of the Hirota or Darboux type, we also give a general expression\nfor arbitrary initial data decaying at infinity, which contains the\ncontribution of the continuous spectrum (radiation).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Recently, due to the poor performance of supervised person re-identification\n(ReID) to an unseen domain, Domain Generalization (DG) person ReID has\nattracted a lot of attention which aims to learn a domain-insensitive model and\ncan resist the influence of domain bias. In this paper, we first verify through\nan experiment that style factors are a vital part of domain bias. Base on this\nconclusion, we propose a Style Variable and Irrelevant Learning (SVIL) method\nto eliminate the effect of style factors on the model. Specifically, we design\na Style Jitter Module (SJM) in SVIL. The SJM module can enrich the style\ndiversity of the specific source domain and reduce the style differences of\nvarious source domains. This leads to the model focusing on identity-relevant\ninformation and being insensitive to the style changes. Besides, we organically\ncombine the SJM module with a meta-learning algorithm, maximizing the benefits\nand further improving the generalization ability of the model. Note that our\nSJM module is plug-and-play and inference cost-free. Extensive experiments\nconfirm the effectiveness of our SVIL and our method outperforms the\nstate-of-the-art methods on DG-ReID benchmarks by a large margin.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Understanding the rich physics of magnetization dynamics in perpendicular\nsynthetic antiferromagnets (p-SAFs) is crucial for developing next-generation\nspintronic devices. In this work, we systematically investigate the\nmagnetization dynamics in p-SAFs combining time-resolved magneto-optical Kerr\neffect (TR-MOKE) measurements with theoretical modeling. These model analyses,\nbased on a Landau-Lifshitz-Gilbert approach incorporating exchange coupling,\nprovide details about the magnetization dynamic characteristics including the\namplitudes, directions, and phases of the precession of p-SAFs under varying\nmagnetic fields. These model-predicted characteristics are in excellent\nquantitative agreement with TR-MOKE measurements on an asymmetric p-SAF. We\nfurther reveal the damping mechanisms of two procession modes co-existing in\nthe p-SAF and successfully identify individual contributions from different\nsources, including Gilbert damping of each ferromagnetic layer, spin pumping,\nand inhomogeneous broadening. Such a comprehensive understanding of\nmagnetization dynamics in p-SAFs, obtained by integrating high-fidelity TR-MOKE\nmeasurements and theoretical modeling, can guide the design of p-SAF-based\narchitectures for spintronic applications.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  It is well known that classical formulations resembling the Horn and Schunck\nmodel are still largely competitive due to the modern implementation practices.\nIn most cases, these models outperform many modern flow estimation methods. In\nview of this, we propose an effective implementation design for an\nedge-preserving $L^1$ regularization approach to optical flow. The mathematical\nwell-posedness of our proposed model is studied in the space of functions of\nbounded variations $BV(\\Omega,\\mathbb{R}^2)$. The implementation scheme is\ndesigned in multiple steps. The flow field is computed using the robust\nChambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro\nand Donoho we extend the heuristic of iterated median filtering to our flow\nestimation. Further, to refine the flow edges we use the weighted median filter\nestablished by Li and Osher as a post-processing step. Our experiments on the\nMiddlebury dataset show that the proposed method achieves the best average\nangular and end-point errors compared to some of the state-of-the-art Horn and\nSchunck based variational methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We address differential privacy for fully distributed aggregative games with\nshared coupling constraints. By co-designing the generalized Nash equilibrium\n(GNE) seeking mechanism and the differential-privacy noise injection mechanism,\nwe propose the first GNE seeking algorithm that can ensure both provable\nconvergence to the GNE and rigorous epsilon-differential privacy, even with the\nnumber of iterations tending to infinity. As a basis of the co-design, we also\npropose a differentially private consensus-tracking algorithm that can achieve\nrigorous epsilon-differential privacy while maintaining accurate tracking\nperformance, which, to our knowledge, has not been achieved before. To\nfacilitate the convergence analysis, we also establish a general convergence\nresult for stochastically-perturbed nonstationary fixed-point iteration\nprocesses, which lie at the core of numerous optimization and variational\nproblems. Numerical simulation results confirm the effectiveness of the\nproposed approach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art\nperformance in generative modeling. Their main strength comes from their unique\nsetup in which a model (the backward diffusion process) is trained to reverse\nthe forward diffusion process, which gradually adds noise to the input signal.\nAlthough DDGMs are well studied, it is still unclear how the small amount of\nnoise is transformed during the backward diffusion process. Here, we focus on\nanalyzing this problem to gain more insight into the behavior of DDGMs and\ntheir denoising and generative capabilities. We observe a fluid transition\npoint that changes the functionality of the backward diffusion process from\ngenerating a (corrupted) image from noise to denoising the corrupted image to\nthe final sample. Based on this observation, we postulate to divide a DDGM into\ntwo parts: a denoiser and a generator. The denoiser could be parameterized by a\ndenoising auto-encoder, while the generator is a diffusion-based model with its\nown set of parameters. We experimentally validate our proposition, showing its\npros and cons.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this work we analyze the finite-size effects on the structural properties\nand on the polarization of the rhombohedral phase of HfO$_2$ subjected to a\nbiaxial compressive strain. We show how the presence of surface charges affects\nthe polarization, leading to a strong reduction with respect to its bulk value.\nThis reduction can be ascribed to two mechanisms: the coupling between\ncompressive strain and the phase-transition order parameter; the changes in the\nferroelectric distortion. We give two alternative explanations of this\nphenomenon: from an atomistic point of view, analyzing the evolution of the\nbond lengths, and from a symmetry-analysis point of view, considering the\nchanges in the amplitude of the symmetry-allowed distortions, when a slab\nconfiguration is considered. These results are independent on the\nslab-thickness in the considered range, suggesting the absence of a critical\nthickness for ferroelectricity in HfO$_2$, in agreement with the proposed\nimproper nature of hafnia ferroelectricity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  As the sequencing costs are decreasing, there is great incentive to perform\nlarge scale association studies to increase power of detecting new variants.\nFederated association testing among different institutions is a viable solution\nfor increasing sample sizes by sharing the intermediate testing statistics that\nare aggregated by a central server. There are, however, standing challenges to\nperforming federated association testing. Association tests are known to be\nconfounded by numerous factors such as population stratification, which can be\nespecially important in multiancestral studies and in admixed populations among\ndifferent sites. Furthermore, disease etiology should be considered via\nflexible models to avoid biases in the significance of the genetic effect. A\nrising challenge for performing large scale association studies is the privacy\nof participants and related ethical concerns of stigmatization and\nmarginalization. Here, we present dMEGA, a flexible and efficient method for\nperforming federated generalized linear mixed model based association testing\namong multiple sites while underlying genotype and phenotype data are not\nexplicitly shared. dMEGA first utilizes a reference projection to estimate\npopulation-based covariates without sharing genotype dataset among sites. Next,\ndMEGA uses Laplacian approximation for the parameter likelihoods and decomposes\nparameter estimation into efficient local-gradient updates among sites. We use\nsimulated and real datasets to demonstrate the accuracy and efficiency of\ndMEGA. Overall, dMEGA's formulation is flexible to integrate fixed and random\neffects in a federated setting.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The modality is important topic for modelling. Using parametric models is an\nefficient way when real data set shows trimodality. In this paper we propose a\nnew class of trimodal probability distributions, that is, probability\ndistributions that have up to three modes. Trimodality itself is achieved by\napplying a proper transformation to density function of certain continuous\nprobability distributions. At first, we obtain preliminary results for an\narbitratry density function $g(x)$ and, next, we focus on the Gaussian case,\nstudying trimodal Gaussian model more deeply. The Gaussian distribution is\napplied to produce the trimodal form of Gaussian known as normal distribution.\nThe tractability of analytical expression of normal distribution, and\nproperties of the trimodal normal distribution are important reasons why we\nchoose normal distribution. Furthermore, the existing distributions should be\nimproved to be capable of modelling efficiently when there exists a trimodal\nform in a data set. After new density function is proposed, estimating its\nparameters is important. Since Mathematica 12.0 software has optimization tools\nand important modelling techniques, computational steps are performed by using\nthis software. The bootstrapped form of real data sets are applied to show the\nmodelling ability of the proposed distribution when real data sets show\ntrimodality.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Although reinforcement learning has seen remarkable progress over the last\nyears, solving robust dexterous object-manipulation tasks in multi-object\nsettings remains a challenge. In this paper, we focus on models that can learn\nmanipulation tasks in fixed multi-object settings and extrapolate this skill\nzero-shot without any drop in performance when the number of objects changes.\nWe consider the generic task of bringing a specific cube out of a set to a goal\nposition. We find that previous approaches, which primarily leverage attention\nand graph neural network-based architectures, do not generalize their skills\nwhen the number of input objects changes while scaling as $K^2$. We propose an\nalternative plug-and-play module based on relational inductive biases to\novercome these limitations. Besides exceeding performances in their training\nenvironment, we show that our approach, which scales linearly in $K$, allows\nagents to extrapolate and generalize zero-shot to any new object number.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  With 84.75 million Filipinos online, the ability for models to process online\ntext is crucial for developing Filipino NLP applications. To this end, spelling\ncorrection is a crucial preprocessing step for downstream processing. However,\nthe lack of data prevents the use of language models for this task. In this\npaper, we propose an N-Gram + Damerau Levenshtein distance model with automatic\nrule extraction. We train the model on 300 samples, and show that despite\nlimited training data, it achieves good performance and outperforms other deep\nlearning approaches in terms of accuracy and edit distance. Moreover, the model\n(1) requires little compute power, (2) trains in little time, thus allowing for\nretraining, and (3) is easily interpretable, allowing for direct\ntroubleshooting, highlighting the success of traditional approaches over more\ncomplex deep learning models in settings where data is unavailable.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The syndrome decoding problem is known to be NP-hard. We use the quantum\napproximate optimization algorithm (QAOA) to solve the syndrome decoding\nproblem with elegantly-designed generator- and check-based cost Hamiltonians\nfor classical and quantum codes. Simulations of the level-4 check-based QAOA\ndecoding of the [7,4,3] Hamming code, as well as the level-4 generator-based\nQAOA decoding of the [[5,1,3]] quantum code, demonstrate decoding performances\nthat match the maximum likelihood decoding. In addition, we show that a\ncombinatorial optimization problem with additional redundant clauses may be\nmore suitable for QAOA, while the number of qubits remains the same.\nFurthermore, we show that the QAOA decoding of a quantum code is inherently\ndegenerate. That is, degenerate errors of comparable weight will be returned by\nQAOA with comparable probability. This is supported by simulations of the\ngenerator-based QAOA decoding of the [[9,1,3]] Shor code.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Privacy regulations and the physical distribution of heterogeneous data are\noften primary concerns for the development of deep learning models in a medical\ncontext. This paper evaluates the feasibility of differentially private\nfederated learning for chest X-ray classification as a defense against data\nprivacy attacks. To the best of our knowledge, we are the first to directly\ncompare the impact of differentially private training on two different neural\nnetwork architectures, DenseNet121 and ResNet50. Extending the federated\nlearning environments previously analyzed in terms of privacy, we simulated a\nheterogeneous and imbalanced federated setting by distributing images from the\npublic CheXpert and Mendeley chest X-ray datasets unevenly among 36 clients.\nBoth non-private baseline models achieved an area under the receiver operating\ncharacteristic curve (AUC) of $0.94$ on the binary classification task of\ndetecting the presence of a medical finding. We demonstrate that both model\narchitectures are vulnerable to privacy violation by applying image\nreconstruction attacks to local model updates from individual clients. The\nattack was particularly successful during later training stages. To mitigate\nthe risk of privacy breach, we integrated R\\'enyi differential privacy with a\nGaussian noise mechanism into local model training. We evaluate model\nperformance and attack vulnerability for privacy budgets $\\epsilon \\in$ {1, 3,\n6, 10}. The DenseNet121 achieved the best utility-privacy trade-off with an AUC\nof $0.94$ for $\\epsilon$ = 6. Model performance deteriorated slightly for\nindividual clients compared to the non-private baseline. The ResNet50 only\nreached an AUC of $0.76$ in the same privacy setting. Its performance was\ninferior to that of the DenseNet121 for all considered privacy constraints,\nsuggesting that the DenseNet121 architecture is more robust to differentially\nprivate training.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In the holomorphic or algebraic setting we consider a vector bundle E on a\nsmooth subvariety X in a smooth variety Y over a field of characteristic zero.\nAssuming E extends to the l-th neighborhood of X in Y, we study cohomological\nobstructions to extending it further to the k-th neighborhood, for k > l.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study global and local geometry of forms on odd symplectic BV\nsupermanifolds, constructed from the total space of the bundle of 1-forms on a\nbase supermanifold. We show that globally 1-forms are an extension of vector\nbundles defined on the base supermanifold. In the holomorphic category, we\nprove that this extension is split if and only if the super Atiyah class of the\nbase supermanifold vanishes. This is equivalent to the existence of a\nholomorphic superconnection: we show how this condition is related to the\ncharacteristic non-split geometry of complex supermanifolds. From a local point\nof view, we prove that the deformed de Rham double complex naturally arises as\na de-quantization of the de Rham/Spencer double complex of the base\nsupermanifold. Following \\v{S}evera, we show that the associated spectral\nsequence yields semidensities on the BV supermanifold, together with their\ndifferential in the form of a super BV Laplacian.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The class of Novikov algebras is a popular object of study among classical\nnonassociative algebras. The generic example of a Novikov algebra may be\nobtained from a differential associative and commutative algebra. We consider a\nmore general class of linear algebras which may be obtained in the same way\nfrom not necessarily commutative associative algebras with a derivation.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Extraterrestrial rovers with a general-purpose robotic arm have many\npotential applications in lunar and planetary exploration. Introducing autonomy\ninto such systems is desirable for increasing the time that rovers can spend\ngathering scientific data and collecting samples. This work investigates the\napplicability of deep reinforcement learning for vision-based robotic grasping\nof objects on the Moon. A novel simulation environment with\nprocedurally-generated datasets is created to train agents under challenging\nconditions in unstructured scenes with uneven terrain and harsh illumination. A\nmodel-free off-policy actor-critic algorithm is then employed for end-to-end\nlearning of a policy that directly maps compact octree observations to\ncontinuous actions in Cartesian space. Experimental evaluation indicates that\n3D data representations enable more effective learning of manipulation skills\nwhen compared to traditionally used image-based observations. Domain\nrandomization improves the generalization of learned policies to novel scenes\nwith previously unseen objects and different illumination conditions. To this\nend, we demonstrate zero-shot sim-to-real transfer by evaluating trained agents\non a real robot in a Moon-analogue facility.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The search for sub-GeV dark matter (DM) particles via electronic transitions\nin underground detectors attracted much theoretical and experimental interest\nin the past few years. A still open question in this field is whether\nexperimental results can in general be interpreted in a framework where the\nresponse of detector materials to an external DM probe is described by a single\nionisation or crystal form factor, as expected for the so-called dark photon\nmodel. Here, ionisation and crystal form factors are examples of material\nresponse functions: interaction-specific integrals of the initial and final\nstate electron wave functions. In this work, we address this question through a\nsystematic classification of the material response functions induced by a wide\nrange of models for spin-0, spin-1/2 and spin-1 DM. We find several examples\nfor which an accurate description of the electronic transition rate at DM\ndirect detection experiments requires material response functions that go\nbeyond those expected for the dark photon model. This concretely illustrates\nthe limitations of a framework that is entirely based on the standard\nionisation and crystal form factors, and points towards the need for the\ngeneral response-function-based formalism we pushed forward recently [1,2]. For\nthe models that require non-standard atomic and crystal response functions, we\nuse the response functions of [1,2] to calculate the DM-induced electronic\ntransition rate in atomic and crystal detectors, and to present 90% confidence\nlevel exclusion limits on the strength of the DM-electron interaction from the\nnull results reported by XENON10, XENON1T, EDELWEISS and SENSEI.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Many real-world networks are inherently decentralized. For example, in social\nnetworks, each user maintains a local view of a social graph, such as a list of\nfriends and her profile. It is typical to collect these local views of social\ngraphs and conduct graph learning tasks. However, learning over graphs can\nraise privacy concerns as these local views often contain sensitive\ninformation.\n  In this paper, we seek to ensure private graph learning on a decentralized\nnetwork graph. Towards this objective, we propose {\\em Solitude}, a new\nprivacy-preserving learning framework based on graph neural networks (GNNs),\nwith formal privacy guarantees based on edge local differential privacy. The\ncrux of {\\em Solitude} is a set of new delicate mechanisms that can calibrate\nthe introduced noise in the decentralized graph collected from the users. The\nprinciple behind the calibration is the intrinsic properties shared by many\nreal-world graphs, such as sparsity. Unlike existing work on locally private\nGNNs, our new framework can simultaneously protect node feature privacy and\nedge privacy, and can seamlessly incorporate with any GNN with privacy-utility\nguarantees. Extensive experiments on benchmarking datasets show that {\\em\nSolitude} can retain the generalization capability of the learned GNN while\npreserving the users' data privacy under given privacy budgets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Let $G$ be a connected, complex reductive Lie group and $G/H$ a spherical\nhomogenous space. Let $(X,L)$ be a polarized $G$-variety which is a spherical\nembedding of $G/H$. In this paper we classify $G$-equivariant normal $\\mathbb\nR$-test configurations of $(X,L)$ via combinatory data. In particular we\nclassify the special ones, and prove a finiteness theorem of central fibres of\n$G$-equivariant special $\\mathbb R$-test configurations. Also, as an\napplication we study the semistable degeneration problem of a $\\mathbb Q$-Fano\nspherical variety.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Upcoming ground and space-based experiments may have sufficient accuracy to\nplace significant constraints upon high-redshift star formation, Reionization,\nand dark matter (DM) using the global 21-cm signal of the intergalactic medium.\nIn the early universe, when the relative abundance of low-mass DM halos is\nimportant, measuring the global signal would place constraints on the damping\nof structure formation caused by DM having a higher relic velocity (warm dark\nmatter, or WDM) than in cold dark matter (CDM). Such damping, however, can be\nmimicked by altering the star formation efficiency (SFE) and difficult to\ndetect because of the presence of Pop III stars with unknown properties. We\nstudy these various cases and their degeneracies with the WDM mass parameter\n$m_X$ using a Fisher matrix analysis. We study the $m_X = 7$ keV case and a\nstar-formation model that parametrizes the SFE as a strong function of halo\nmass and include several variations of this model along with three different\ninput noise levels for the likelihood; we also use a minimum halo virial\ntemperature for collapse near the molecular cooling threshold. We find that\nwhen the likelihood includes only Pop II stars, $m_X$ is constrained to an\nuncertainty of $\\sim 0.4$ keV for all models and noise levels at 68$\\%$ CI.\nWhen the likelihood includes weak Pop III stars, $m_X \\sim 0.3$ keV, and if Pop\nIII star formation is relatively efficient, $m_X \\sim 0.1$ keV uncertainty,\nwith tight Pop III star-formation parameter constraints. Our results show that\nthe global 21-cm signal is a promising test-bed for WDM models, even in the\npresence of strong degeneracies with astrophysical parameters.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Representing words by vectors, or embeddings, enables computational reasoning\nand is foundational to automating natural language tasks. For example, if word\nembeddings of similar words contain similar values, word similarity can be\nreadily assessed, whereas judging that from their spelling is often impossible\n(e.g. cat /feline) and to predetermine and store similarities between all words\nis prohibitively time-consuming, memory intensive and subjective. We focus on\nword embeddings learned from text corpora and knowledge graphs. Several\nwell-known algorithms learn word embeddings from text on an unsupervised basis\nby learning to predict those words that occur around each word, e.g. word2vec\nand GloVe. Parameters of such word embeddings are known to reflect word\nco-occurrence statistics, but how they capture semantic meaning has been\nunclear. Knowledge graph representation models learn representations both of\nentities (words, people, places, etc.) and relations between them, typically by\ntraining a model to predict known facts in a supervised manner. Despite steady\nimprovements in fact prediction accuracy, little is understood of the latent\nstructure that enables this.\n  The limited understanding of how latent semantic structure is encoded in the\ngeometry of word embeddings and knowledge graph representations makes a\nprincipled means of improving their performance, reliability or\ninterpretability unclear. To address this:\n  1. we theoretically justify the empirical observation that particular\ngeometric relationships between word embeddings learned by algorithms such as\nword2vec and GloVe correspond to semantic relations between words; and\n  2. we extend this correspondence between semantics and geometry to the\nentities and relations of knowledge graphs, providing a model for the latent\nstructure of knowledge graph representation linked to that of word embeddings.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we present CAESR, an hybrid learning-based coding approach for\nspatial scalability based on the versatile video coding (VVC) standard. Our\nframework considers a low-resolution signal encoded with VVC intra-mode as a\nbase-layer (BL), and a deep conditional autoencoder with hyperprior (AE-HP) as\nan enhancement-layer (EL) model. The EL encoder takes as inputs both the\nupscaled BL reconstruction and the original image. Our approach relies on\nconditional coding that learns the optimal mixture of the source and the\nupscaled BL image, enabling better performance than residual coding. On the\ndecoder side, a super-resolution (SR) module is used to recover high-resolution\ndetails and invert the conditional coding process. Experimental results have\nshown that our solution is competitive with the VVC full-resolution intra\ncoding while being scalable.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we consider a functional linear regression model, where both\nthe covariate and the response variable are functional random variables. We\naddress the problem of optimal nonparametric estimation of the conditional\nexpectation operator in this model. A collection of projection estimators over\nfinite dimensional subspaces is first introduce. We provide a non-asymptotic\nbias-variance decomposition for the Mean Square Prediction error in the case\nwhere these subspaces are generated by the (empirical) PCA functional basis.\nThe automatic trade-off is realized thanks to a model selection device which\nselects the best projection dimensions: the penalized contrast estimator\nsatisfies an oracle-type inequality and is thus optimal in an adaptive point of\nview. These upper-bounds allow us to derive convergence rates over ellipsoidal\nsmoothness spaces. The rates are shown to be optimal in the minimax sense: they\nmatch with a lower bound of the minimax risk, which is also proved. Finally, we\nconduct a numerical study, over simulated data and over two real-data sets.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We consider bootstrap-based testing for threshold effects in non-linear\nthreshold autoregressive (TAR) models. It is well-known that classic tests\nbased on asymptotic theory tend to be oversized in the case of small, or even\nmoderate sample sizes, or when the estimated parameters indicate\nnon-stationarity, as often witnessed in the analysis of financial or climate\ndata. To address the issue we propose a supremum Lagrange Multiplier test\nstatistic (sLMb), where the null hypothesis specifies a linear autoregressive\n(AR) model against the alternative of a TAR model. We consider a recursive\nbootstrap applied to the sLMb statistic and establish its validity. This result\nis new, and requires the proof of non-standard results for bootstrap analysis\nin time series models; this includes a uniform bootstrap law of large numbers\nand a bootstrap functional central limit theorem. These new results can also be\nused as a general theoretical framework that can be adapted to other\nsituations, such as regime-switching processes with exogenous threshold\nvariables, or testing for structural breaks. The Monte Carlo evidence shows\nthat the bootstrap test has correct empirical size even for small samples, and\nalso no loss of empirical power when compared to the asymptotic test. Moreover,\nits performance is not affected if the order of the autoregression is estimated\nbased on information criteria. Finally, we analyse a panel of short time series\nto assess the effect of warming on population dynamics.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Radio-frequency measurements could satisfy DiVincenzo's readout criterion in\nfuture large-scale solid-state quantum processors, as they allow for high\nbandwidths and frequency multiplexing. However, the scalability potential of\nthis readout technique can only be leveraged if quantum device tuning is\nperformed using exclusively radio-frequency measurements i.e. without resorting\nto current measurements. We demonstrate an algorithm that automatically tunes\ndouble quantum dots using only radio-frequency reflectometry. Exploiting the\nhigh bandwidth of radio-frequency measurements, the tuning was completed within\na few minutes without prior knowledge about the device architecture. Our\nresults show that it is possible to eliminate the need for transport\nmeasurements for quantum dot tuning, paving the way for more scalable device\narchitectures.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  A cap set is a subset of $\\mathbb{F}_3^n$ with no solutions to $x+y+z=0$\nother than when $x=y=z$. In this paper, we provide a new lower bound on the\nsize of a maximal cap set. Building on a construction of Edel, we use improved\ncomputational methods and new theoretical ideas to show that, for large enough\n$n$, there is always a cap set in $\\mathbb{F}_3^n$ of size at least $2.218^n$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We revisit vertex discriminant analysis (VDA) from the perspective of\nproximal distance algorithms. By specifying sparsity sets as constraints that\ndirectly control the number of active features, VDA is able to fit multiclass\nclassifiers with no more than $k$ active features. We combine our sparse VDA\napproach with repeated cross validation to fit classifiers across the full\nrange of model sizes on a given dataset. Our numerical examples demonstrate\nthat grappling with sparsity directly is an attractive approach to model\nbuilding in high-dimensional settings. Applications to kernel-based VDA are\nalso considered.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Cyber-physical systems (CPSs) are usually complex and safety-critical; hence,\nit is difficult and important to guarantee that the system's requirements,\ni.e., specifications, are fulfilled. Simulation-based falsification of CPSs is\na practical testing method that can be used to raise confidence in the\ncorrectness of the system by only requiring that the system under test can be\nsimulated. As each simulation is typically computationally intensive, an\nimportant step is to reduce the number of simulations needed to falsify a\nspecification. We study Bayesian optimization (BO), a sample-efficient method\nthat learns a surrogate model that describes the relationship between the\nparametrization of possible input signals and the evaluation of the\nspecification.\n  In this paper, we improve the falsification using BO by; first adopting two\nprominent BO methods, one fits local surrogate models, and the other exploits\nthe user's prior knowledge. Secondly, the formulation of acquisition functions\nfor falsification is addressed in this paper. Benchmark evaluation shows\nsignificant improvements in using local surrogate models of BO for falsifying\nbenchmark examples that were previously hard to falsify. Using prior knowledge\nin the falsification process is shown to be particularly important when the\nsimulation budget is limited. For some of the benchmark problems, the choice of\nacquisition function clearly affects the number of simulations needed for\nsuccessful falsification.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Inspired by the concept of preconditioning, we propose a novel method to\nincrease adaptation speed for gradient-based meta-learning methods without\nincurring extra parameters. We demonstrate that recasting the optimization\nproblem to a non-linear least-squares formulation provides a principled way to\nactively enforce a $\\textit{well-conditioned}$ parameter space for\nmeta-learning models based on the concepts of the condition number and local\ncurvature. Our comprehensive evaluations show that the proposed method\nsignificantly outperforms its unconstrained counterpart especially during\ninitial adaptation steps, while achieving comparable or better overall results\non several few-shot classification tasks -- creating the possibility of\ndynamically choosing the number of adaptation steps at inference time.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We derive the explicit analytical form for the charge-dipole and\ndipole-dipole interactions in 2D configuration space. We demonstrate that the\nreduction of dimensionality can alter the charge-dipole and dipole-dipole\ninteractions in 2D case. The asymptotics of these interactions at large\ndistances coincide to the charge-dipole and dipole-dipole interactions in 3D\nconfiguration space.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The aim of this paper is to analyze a mixed formulation for the two\ndimensional Stokes eigenvalue problem where the unknowns are the stress and the\nvelocity, whereas the pressure can be recovered with a simple postprocess of\nthe stress. The stress tensor is written in terms of the vorticity of the\nfluid, leading to an alternative mixed formulation that incorporates this\nphysical feature. We propose a mixed numerical method where the stress is\napproximated with suitable N\\'edelec finite elements, whereas the velocity is\napproximated with piecewise polynomials of degree $k\\geq 0$. With the aid of\nthe compact operators theory we derive convergence of the method and spectral\ncorrectness. Moreover, we propose a reliable and efficient a posteriori error\nestimator for our spectral problem. We report numerical tests in different\ndomains, computing the spectrum and convergence orders, together with a\ncomputational analysis for the proposed estimator. In addition, we use the\ncorresponding error estimator to drive an adaptive scheme, and we report the\nresults of a numerical test, that allow us to assess the performance of this\napproach.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Learning in general-sum games is unstable and frequently leads to socially\nundesirable (Pareto-dominated) outcomes. To mitigate this, Learning with\nOpponent-Learning Awareness (LOLA) introduced opponent shaping to this setting,\nby accounting for each agent's influence on their opponents' anticipated\nlearning steps. However, the original LOLA formulation (and follow-up work) is\ninconsistent because LOLA models other agents as naive learners rather than\nLOLA agents. In previous work, this inconsistency was suggested as a cause of\nLOLA's failure to preserve stable fixed points (SFPs). First, we formalize\nconsistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency\nproblem if it converges. Second, we correct a claim made in the literature by\nSch\\\"afer and Anandkumar (2019), proving that Competitive Gradient Descent\n(CGD) does not recover HOLA as a series expansion (and fails to solve the\nconsistency problem). Third, we propose a new method called Consistent LOLA\n(COLA), which learns update functions that are consistent under mutual opponent\nshaping. It requires no more than second-order derivatives and learns\nconsistent update functions even when HOLA fails to converge. However, we also\nprove that even consistent update functions do not preserve SFPs, contradicting\nthe hypothesis that this shortcoming is caused by LOLA's inconsistency.\nFinally, in an empirical evaluation on a set of general-sum games, we find that\nCOLA finds prosocial solutions and that it converges under a wider range of\nlearning rates than HOLA and LOLA. We support the latter finding with a\ntheoretical result for a simple game.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Secret sharing was firstly proposed in 1979 by Shamir and Blakley\nrespectively. To avoid deficiencies of original schemes, researchers presented\nimprovement schemes, among which the multi-secret sharing scheme (MSS) is\nsignificant. There are three categories of MSSs, however, we focus on\nmulti-stage secret sharing scheme (MSSS) recovering secrets with any order in\nthis work. By observing inhomogeneous linear recursions (ILRs) in the\nliterature, we conclude a general formula and divide ILRs into two types\naccording to different variables in them. Utilizing these two kinds of ILRs, we\npropose four verifiable MSSSs with Ajtai's function, which is a lattice-based\nfunction. Our schemes have the following advantages. Firstly, our schemes can\ndetect cheat of the dealer and participants, and are multi-use. Secondly, we\nhave several ways to restore secrets. Thirdly, we can turn our schemes into\nother types of MSSs due to the universality of our method. Fourthly, since we\nutilize a lattice-based function to mask shares, our schemes can resist the\nattack from the quantum computer with computational security. Finally, although\nour schemes need more memory consumption than some known schemes, we need much\nless time consumption, which makes our schemes more suitable facing limited\ncomputing power.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Two-dimensional magnetic materials (2DMM) are significant for studies on the\nnature of 2D long range magnetic order but also for future spintronic devices.\nOf particular interest are 2DMM where spins can be manipulated by electrical\nconduction. Whereas Cr$_2$Si$_2$Te$_6$ exhibits magnetic order in few-layer\ncrystals, its large band gap inhibits electronic conduction. Here we show that\nthe defect-induced short-range crystal order in Cr$_2$Si$_2$Te$_6$ on the\nlength scale below 0.6 nm induces substantially reduced band gap and robust\nsemiconducting behavior down to 2 K that turns to metallic above 10 GPa. Our\nresults will be helpful to design conducting state in 2DMM and call for\nspin-resolved measurement of the electronic structure in exfoliated ultrathin\ncrystals.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study hidden-action principal-agent problems in which a principal commits\nto an outcome-dependent payment scheme (called contract) so as to incentivize\nthe agent to take a costly, unobservable action leading to favorable outcomes.\nIn particular, we focus on Bayesian settings where the agent has private\ninformation. This is collectively encoded by the agent's type, which is unknown\nto the principal, but randomly drawn according to a finitely-supported,\ncommonly-known probability distribution. In Bayesian principal-agent problems,\nthe principal may be better off by committing to a menu of contracts specifying\na contract for each agent's type, rather than committing to a single contract.\nThis induces a two-stage process that resembles interactions studied in\nclassical mechanism design: after the principal has committed to a menu, the\nagent first reports a type to the principal, and, then, the latter puts in\nplace the contract in the menu that corresponds to the reported type. Thus, the\nprincipal's computational problem boils down to designing a menu of contracts\nthat incentivizes the agent to report their true type and maximizes expected\nutility. Previous works showed that computing an optimal menu of contracts is\nAPX-hard. Crucially, previous works focus on menus of deterministic contracts.\nSurprisingly, we show that, if one considers menus of randomized contracts\ndefined as probability distributions over payment vectors, then an\n\"almost-optimal\" menu can be computed in polynomial time. Indeed, the problem\nof computing a principal-optimal menu of randomized contracts may not admit a\nmaximum, but only a supremum. Nevertheless, we show how to design a\npolynomial-time algorithm that guarantees the principal with an expected\nutility arbitrarily close to the supremum. Besides this main result, we also\nclose several gaps in the analysis of menus of deterministic contracts.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We address the problem of ground-to-satellite image geo-localization, that\nis, estimating the camera latitude, longitude and orientation (azimuth angle)\nby matching a query image captured at the ground level against a large-scale\ndatabase with geotagged satellite images. Our prior arts treat the above task\nas pure image retrieval by selecting the most similar satellite reference image\nmatching the ground-level query image. However, such an approach often produces\ncoarse location estimates because the geotag of the retrieved satellite image\nonly corresponds to the image center while the ground camera can be located at\nany point within the image. To further consolidate our prior research findings,\nwe present a novel geometry-aware geo-localization method. Our new method is\nable to achieve the fine-grained location of a query image, up to pixel size\nprecision of the satellite image, once its coarse location and orientation have\nbeen determined. Moreover, we propose a new geometry-aware image retrieval\npipeline to improve the coarse localization accuracy. Apart from a polar\ntransform in our conference work, this new pipeline also maps satellite image\npixels to the ground-level plane in the ground-view via a geometry-constrained\nprojective transform to emphasize informative regions, such as road structures,\nfor cross-view geo-localization. Extensive quantitative and qualitative\nexperiments demonstrate the effectiveness of our newly proposed framework. We\nalso significantly improve the performance of coarse localization results\ncompared to the state-of-the-art in terms of location recalls.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The article is devoted to the investigation of the Noether currents and\nintegrals of motion in the special subclass of theories with higher field\nderivatives -- theories under differential field transformations in action\n(DFTA). Under fairly general assertions, we derive a simple representation for\nthe integrals as sums of \"old\" integrals of motion, terms, that vanishes on the\n\"old\" equations of motion and surface integrals. We show that for some\ncosmological theories of that kind with high order derivatives the last\ncontribution can violate gauge invariance, specifically, diffeomorphisms. Then\nwe investigate ambiguity of the Noether procedure for higher derivative\ntheories and fix it in the way the problem resolves in quite general setting.\nThe obtained results are discussed for the several extensions of the General\nRelativity, which are obtained by DFTA, in particular, for mimetic, disformal\nand Regge-Teitelboim theories of gravity.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Given a tract $F$ in the sense of Baker and Bowler and a matrix $A$ with\nentries in $F$, we define several notions of rank for $A$. In this way, we are\nable to unify and find conceptually satisfying proofs for various results about\nranks of matrices that one finds scattered throughout the literature.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Musculoskeletal and neurological disorders are the most common causes of\nwalking problems among older people, and they often lead to diminished quality\nof life. Analyzing walking motion data manually requires trained professionals\nand the evaluations may not always be objective. To facilitate early diagnosis,\nrecent deep learning-based methods have shown promising results for automated\nanalysis, which can discover patterns that have not been found in traditional\nmachine learning methods. We observe that existing work mostly applies deep\nlearning on individual joint features such as the time series of joint\npositions. Due to the challenge of discovering inter-joint features such as the\ndistance between feet (i.e. the stride width) from generally smaller-scale\nmedical datasets, these methods usually perform sub-optimally. As a result, we\npropose a solution that explicitly takes both individual joint features and\ninter-joint features as input, relieving the system from the need of\ndiscovering more complicated features from small data. Due to the distinctive\nnature of the two types of features, we introduce a two-stream framework, with\none stream learning from the time series of joint position and the other from\nthe time series of relative joint displacement. We further develop a mid-layer\nfusion module to combine the discovered patterns in these two streams for\ndiagnosis, which results in a complementary representation of the data for\nbetter prediction performance. We validate our system with a benchmark dataset\nof 3D skeleton motion that involves 45 patients with musculoskeletal and\nneurological disorders, and achieve a prediction accuracy of 95.56%,\noutperforming state-of-the-art methods.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The operation of industrial facilities is a broad field for optimization.\nIndustrial plants are often a) composed of several components, b) linked using\nnetwork technology, c) physically interconnected and d) complex regarding the\neffect of set-points and operating points in every entity. This leads to the\npossibility of overall optimization but also to a high complexity of the\nemerging optimization problems. The decomposition of complex systems allows the\nmodeling of individual models which can be structured according to the physical\ntopology. A method for energy performance indicators (EnPI) helps to formulate\nan optimization problem. The optimization algorithm OptTopo achieves efficient\nset-points by traversing a graph representation of the overall system.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we continue the investigation of quantum Markov states (QMS)\nand define their mean entropies. Such entropies are explicitly computed under\ncertain conditions. The present work takes a huge leap forward at tackling one\nof the most important open problems in quantum probability which concerns the\ncalculations of mean entropies of quantum Markov fields. Moreover, it opens new\nperspective for the generalization of many interesting results related to the\none dimensional quantum Markov states and chains to multi-dimensional cases.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We study the statistical significances for exclusion and discovery of proton\ndecay at current and future neutrino detectors. Various counterintuitive flaws\nassociated with frequentist and modified frequentist statistical measures of\nsignificance for multi-channel counting experiments are discussed in a general\ncontext and illustrated with examples. We argue in favor of conservative\nBayesian-motivated statistical measures, and as an application we employ these\nmeasures to obtain the current lower limits on proton partial lifetime at\nvarious confidence levels, based on Super-Kamiokande's data, generalizing the\n90\\% CL published limits. Finally, we present projections for exclusion and\ndiscovery reaches for proton partial lifetimes in $p \\rightarrow \\overline \\nu\nK^+$ and $p \\rightarrow e^+ \\pi^0$ decay channels at Hyper-Kamiokande, DUNE,\nJUNO, and THEIA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Open-source software (OSS) licenses dictate the conditions which should be\nfollowed to reuse, distribute, and modify the software. Apart from widely-used\nlicenses such as the MIT License, developers are also allowed to customize\ntheir own licenses (called custom licenses), whose descriptions are more\nflexible. The presence of such various licenses imposes challenges to\nunderstanding licenses and their compatibility. To avoid financial and legal\nrisks, it is essential to ensure license compatibility when integrating\nthird-party packages or reusing code accompanied with licenses. In this work,\nwe propose LiDetector, an effective tool that extracts and interprets OSS\nlicenses (including both official licenses and custom licenses), and detects\nlicense incompatibility among these licenses. Specifically, LiDetector\nintroduces a learning-based method to automatically identify meaningful license\nterms from an arbitrary license and employs Probabilistic Context-Free Grammar\n(PCFG) to infer rights and obligations for incompatibility detection.\nExperiments demonstrate that LiDetector outperforms existing methods with\n93.28% precision for term identification, and 91.09% accuracy for right and\nobligation inference, and can effectively detect incompatibility with a 10.06%\nFP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale\nempirical study on 1,846 projects reveals that 72.91% of the projects are\nsuffering from license incompatibility, including popular ones such as the MIT\nLicense and the Apache License. We highlighted lessons learned from the\nperspectives of different stakeholders and made all related data and the\nreplication package publicly available to facilitate follow-up research.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Spotting refers to the transport of burning pieces of firebrand by wind\nwhich, at the time of landing, may ignite new fires beyond the direct ignition\nzone of the main fire. Spot fires that occur far from the original burn unit\nare rare but have consequential ramifications since their prediction and\ncontrol remains challenging. To facilitate their prediction, we examine three\nmethods for quantifying the landing distribution of firebrands: crude Monte\nCarlo simulations, importance sampling, and large deviation theory (LDT). In\nparticular, we propose an LDT method that accurately and parsimoniously\nquantifies the low probability events at the tail of the landing distribution.\nIn contrast, Monte Carlo and importance sampling methods are most efficient in\nquantifying the high probability landing distances near the mode of the\ndistribution. However, they become computationally intractable for quantifying\nthe tail of the distribution due to the large sample size required. We also\nshow that the most probable landing distance grows linearly with the mean\ncharacteristic velocity of the wind field. Furthermore, defining the relative\nlanded mass as the proportion of mass landed at a given distance from the main\nfire, we derive an explicit formula which allows computing this quantity as a\nfunction of the landing distribution at a negligible computational cost. We\nnumerically demonstrate our findings on two prescribed wind fields.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  False news that spreads on social media has proliferated over the past years\nand has led to multi-aspect threats in the real world. While there are studies\nof false news on specific domains (like politics or health care), little work\nis found comparing false news across domains. In this article, we investigate\nfalse news across nine domains on Weibo, the largest Twitter-like social media\nplatform in China, from 2009 to 2019. The newly collected data comprise 44,728\nposts in the nine domains, published by 40,215 users, and reposted over 3.4\nmillion times. Based on the distributions and spreads of the multi-domain\ndataset, we observe that false news in domains that are close to daily life\nlike health and medicine generated more posts but diffused less effectively\nthan those in other domains like politics, and that political false news had\nthe most effective capacity for diffusion. The widely diffused false news posts\non Weibo were associated strongly with certain types of users -- by gender,\nage, etc. Further, these posts provoked strong emotions in the reposts and\ndiffused further with the active engagement of false-news starters. Our\nfindings have the potential to help design false news detection systems in\nsuspicious news discovery, veracity prediction, and display and explanation.\nThe comparison of the findings on Weibo with those of existing work\ndemonstrates nuanced patterns, suggesting the need for more research on data\nfrom diverse platforms, countries, or languages to tackle the global issue of\nfalse news. The code and new anonymized dataset are available at\nhttps://github.com/ICTMCG/Characterizing-Weibo-Multi-Domain-False-News.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We present a new method for numerically computing generic multi-loop Feynman\nintegrals. The method relies on an iterative application of Feynman's trick for\ncombining two propagators. Each application of Feynman's trick introduces a\nsimplified Feynman integral topology which depends on a Feynman parameter that\nshould be integrated over. For each integral family, we set up a system of\ndifferential equations which we solve in terms of a piecewise collection of\ngeneralized series expansions in the Feynman parameter. These generalized\nseries expansions can be efficiently integrated term by term, and segment by\nsegment. This approach leads to a fully algorithmic method for computing\nFeynman integrals from differential equations, which does not require the\nmanual determination of boundary conditions. Furthermore, the most complicated\ntopology that appears in the method often has less master integrals than the\noriginal one. We illustrate the strength of our method with a five-point\ntwo-loop integral family.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Non-maximum suppression (NMS) is widely used in object detection pipelines\nfor removing duplicated bounding boxes. The inconsistency between the\nconfidence for NMS and the real localization confidence seriously affects\ndetection performance. Prior works propose to predict Intersection-over-Union\n(IoU) between bounding boxes and corresponding ground-truths to improve NMS,\nwhile accurately predicting IoU is still a challenging problem. We argue that\nthe complex definition of IoU and feature misalignment make it difficult to\npredict IoU accurately. In this paper, we propose a novel Decoupled IoU\nRegression (DIR) model to handle these problems. The proposed DIR decouples the\ntraditional localization confidence metric IoU into two new metrics, Purity and\nIntegrity. Purity reflects the proportion of the object area in the detected\nbounding box, and Integrity refers to the completeness of the detected object\narea. Separately predicting Purity and Integrity can divide the complex mapping\nbetween the bounding box and its IoU into two clearer mappings and model them\nindependently. In addition, a simple but effective feature realignment approach\nis also introduced to make the IoU regressor work in a hindsight manner, which\ncan make the target mapping more stable. The proposed DIR can be conveniently\nintegrated with existing two-stage detectors and significantly improve their\nperformance. Through a simple implementation of DIR with HTC, we obtain 51.3%\nAP on MS COCO benchmark, which outperforms previous methods and achieves\nstate-of-the-art.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We argue that the gauge $SL(2N,C)$ theories may point to a possible way where\nthe known elementary forces, including gravity, could be consistently unified.\nRemarkably, while all related gauge fields are presented in the same adjoint\nmultiplet of the $SL(2N,C)$ symmetry group, the tensor field submultiplet\nproviding gravity can be naturally suppressed in the weak-field approach\ndeveloped for accompanying tetrad fields. As a result, the whole theory turns\nout to effectively possess the local $SL(2,C)\\times SU(N)$ symmetry so as to\nnaturally lead to the $SL(2,C)$ gauge gravity, on the one hand, and the $SU(N)$\ngrand unified theory, on the other. Since all states involved in the $SL(2N,C)$\ntheories are additionally classified according to their spin values, many\npossible $SU(N)$ GUTs - including the conventional one-family $SU(5)$ theory -\nappear not to be relevant for the standard $1/2$ spin quarks and leptons.\nMeanwhile, the $SU(8)$ grand unification for all three families of composite\nquarks and leptons that stems from the $SL(16,C)$ theory seems to be of special\ninterest that is studied in some detail.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Transformers have demonstrated a competitive performance across a wide range\nof vision tasks, while it is very expensive to compute the global\nself-attention. Many methods limit the range of attention within a local window\nto reduce computation complexity. However, their approaches cannot save the\nnumber of parameters; meanwhile, the self-attention and inner position bias\n(inside the softmax function) cause each query to focus on similar and close\npatches. Consequently, this paper presents a light self-limited-attention\n(LSLA) consisting of a light self-attention mechanism (LSA) to save the\ncomputation cost and the number of parameters, and a self-limited-attention\nmechanism (SLA) to improve the performance. Firstly, the LSA replaces the K\n(Key) and V (Value) of self-attention with the X(origin input). Applying it in\nvision Transformers which have encoder architecture and self-attention\nmechanism, can simplify the computation. Secondly, the SLA has a positional\ninformation module and a limited-attention module. The former contains a\ndynamic scale and an inner position bias to adjust the distribution of the\nself-attention scores and enhance the positional information. The latter uses\nan outer position bias after the softmax function to limit some large values of\nattention weights. Finally, a hierarchical Vision Transformer with Light\nself-Limited-attention (ViT-LSLA) is presented. The experiments show that\nViT-LSLA achieves 71.6% top-1 accuracy on IP102 (2.4% absolute improvement of\nSwin-T); 87.2% top-1 accuracy on Mini-ImageNet (3.7% absolute improvement of\nSwin-T). Furthermore, it greatly reduces FLOPs (3.5GFLOPs vs. 4.5GFLOPs of\nSwin-T) and parameters (18.9M vs. 27.6M of Swin-T).\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The contribution of the CP violating three-gluon Weinberg operator,\n$\\frac{1}{3!} w f^{abc} \\epsilon^{\\nu \\rho \\alpha \\beta} G^a_{\\mu \\nu}\nG^b_{\\alpha \\beta} G^{c \\mu}_{\\rho}$, to the atomic and nuclear EDMs is\nestimated using QCD sum rules. After calculating the transition matrix element\nbetween the pion and the vacuum through the Weinberg operator, we obtain the\nlong-range CP-odd nuclear force by determining the isovector CP-odd\npion-nucleon vertex, using chiral perturbation theory at NLO. The EDMs of\n$^{199}$Hg, $^{129}$Xe, $^{225}$Ra, $^2$H, and $^3$He are finally given\nincluding comprehensive uncertainty analysis. While the leading contribution of\nthe $^{199}$Hg EDM is given by the intrinsic nucleon EDM, that of $^{129}$Xe\natom may be dominated by the one-pion exchange CP-odd nuclear force generated\nby the Weinberg operator. From current experimental data of the $^{199}$Hg\natomic EDM, we obtain an upper limit on the Weinberg operator magnitude of\n$\\left|w \\right| < 4 \\times 10^{-10} {\\rm GeV}^{-2}$ if we assume that it is\nthe only source of CP violation at the scale $\\mu =1$ TeV.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  In this paper, we present the calculations for $H\\rightarrow f\\bar{f}\\gamma$\nin the $U(1)_{B-L}$ extension for Standard Model. Analytic results for one-loop\nform factors in the decay process are expressed in terms of the scalar one-loop\nPassarino$-$Veltman functions in the conventions of {\\tt LoopTools}. Therefore,\nthe decay rates can be evaluated numerically by using this package. In\nphenomenological results, we show the differential decay rates with respect to\ninvariant mass of fermion pair $m_{ff}$, new neutral gauge mass $M_{Z'}$ and\nthe coupling $g'$ of $U(1)_{B-L}$ gauge group. We find that the contributions\nof the $U(1)_{B-L}$ extension for Standard Model are visible effects and they\nmust be taken into account at future colliders.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Using synthetic Lyman-$\\alpha$ forests from the Dark Energy Spectroscopic\nInstrument (DESI) survey, we present a study of the impact of errors in the\nestimation of quasar redshift on the Lyman-$\\alpha$ correlation functions.\nEstimates of quasar redshift have large uncertainties of a few hundred\n$\\text{km s}^{-1}\\,$ due to the broadness of the emission lines and the\nintrinsic shifts from other emission lines. We inject Gaussian random redshift\nerrors into the mock quasar catalogues, and measure the auto-correlation and\nthe Lyman-$\\alpha$-quasar cross-correlation functions. We find a smearing of\nthe BAO feature in the radial direction, but changes in the peak position are\nnegligible. However, we see a significant unphysical correlation for small\nseparations transverse to the line of sight which increases with the amplitude\nof the redshift errors. We interpret this contamination as a result of the\nbroadening of emission lines in the measured mean continuum, caused by quasar\nredshift errors, combined with the unrealistically strong clustering of the\nsimulated quasars on small scales.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The discovery of interfacial ferroelectricity in two-dimensional rhombohedral\n(3R)-stacked semiconductors opens up a new pathway for achieving ultrathin\ncomputing-in-memory devices. However, exploring ferroelectricity switching in\nnatural 3R crystals is difficult due to lack of co-existing 3R stacking\ndomains. Here, we present that MoS2 homoepitaxial patterns with 3R polytypic\ndomains can manifest switchable ferroelectricity at room-temperature. Based on\nthe diffusion limited aggregation theory, such MoS2 patterns are formed under\nthe low Mo chemical potential and low temperature with respect to common\nchemical vapor deposition synthesis. The alternation of 3R polytypes in the\nMoS2 homoepitaxial patterns, observed by scanning transmission electron\nmicroscopy, accounts for ferroelectricity switching. The MoS2 field-effect\ntransistors with 3R polytypic domains exhibit a repeatable counterclockwise\nhysteresis with gate voltage sweeping, an indication of ferroelectricity\nswitching, and the memory window exceeds those measured for compact-shaped 3R\nbilayer devices. This work provides a direct growth concept for layered\n3R-based ferroelectric memory.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  This article is concerned with the construction and analysis of new time\ndiscretizations for the KdV equation on a torus for low-regularity solutions\nbelow $H^1$. New harmonic analysis tools, including new averaging\napproximations to the exponential phase functions, new frequency decomposition\ntechniques, and new trilinear estimates of the KdV operator, are established\nfor the construction and analysis of time discretizations with higher\nconvergence orders under low-regularity conditions. In addition, new techniques\nare introduced to establish stability estimates of time discretizations under\nlow-regularity conditions without using filters when the energy techniques\nfail. The proposed method is proved to be convergent with order $\\gamma$ (up to\na logarithmic factor) in $L^2$ under the regularity condition $u\\in\nC([0,T];H^\\gamma)$ for $\\gamma\\in(0,1]$.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  Conventional kernel-based machine learning models for ab initio potential\nenergy surfaces, while accurate and convenient in small data regimes, suffer\nimmense computational cost as training set sizes increase. We introduce\nQML-Lightning, a PyTorch package containing GPU-accelerated approximate kernel\nmodels, which reduces the training time by several orders of magnitude,\nyielding trained models within seconds. QML-Lightning includes a cost-efficient\nGPU implementation of FCHL19, which together can yield energy and force\npredictions with competitive accuracy on a microsecond-per-atom timescale.\nUsing modern GPU hardware, we report learning curves of energies and forces as\nwell as timings as numerical evidence for select legacy benchmarks from\natomisitic simulation including QM9, MD-17, and 3BPA.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  The nature of the $P_{c}(4312)$, $P_c(4440)$ and $P_c(4457)$ pentaquarks is a\nfascinating theoretical question. Within the molecular picture their more usual\ninterpretation is that of $I=\\tfrac{1}{2}$ $\\bar{D} \\Sigma_c$ and $\\bar{D}^*\n\\Sigma_c$ bound states. Here we argue in favor of interpreting the $P_c(4457)$\npentaquark as a $I=\\tfrac{3}{2}$ $\\bar{D}^* \\Sigma_c$ bound state (with spin\n$J=\\tfrac{1}{2}$) instead. Owing to isospin symmetry breaking effects, with\nthis identification the partial decay width of the $P_c(4457)^+$ into $J/\\psi\np$ will be of the same order of magnitude as the $P_{c}(4312)^+$ and\n$P_c(4440)^+$, in contrast with the considerably larger partial decay width in\nthe $I=\\tfrac{1}{2}$ scenario. In turn, this leads to a different hidden-charm\nmolecular pentaquark spectrum, in which there are only four or five\n$P_{\\psi}^N$ bound states instead of the usual seven, which might explain why\nthe predicted $J=\\tfrac{1}{2}$ and $\\tfrac{3}{2}$ ($I=\\tfrac{1}{2}$) $\\bar{D}^*\n\\Sigma_c^*$ molecular partners of the $P_c(4312)$ and $P_c(4440)$ have not been\nobserved.\n\n\n###\n\n", "completion": " 22\n"}
{"prompt": "  We introduce the Exel-Pardo $*$-algebra $\\mathrm{EP}_R(G,\\Lambda)$ associated\nto a self-similar $k$-graph $(G,\\Lambda,\\varphi)$. We prove the\n$\\mathbb{Z}^k$-graded and Cuntz-Krieger uniqueness theorems for such algebras\nand investigate their ideal structure. In particular, we modify the graded\nuniqueness theorem for self-similar 1-graphs, and then apply it to present\n$\\mathrm{EP}_R(G,\\Lambda)$ as a Steinberg algebra and to study the ideal\nstructure.\n\n\n###\n\n", "completion": " 22\n"}
