{"prompt": "  We study the problem of selling a divisible item to agents who have concave\nvaluation functions for fractions of the item. This is a fundamental problem\nwith apparent applications to pricing communication bandwidth or cloud\ncomputing services. We focus on simple sequential posted pricing mechanisms\nthat use linear pricing, i.e., a fixed price for the whole item and\nproportional prices for fractions of it. We present results of the following\nform that can be thought of as analogs of the well-known prophet inequality of\nSamuel-Cahn (1984). For $\\rho\\approx 32\\%$, if there is a linear pricing so\nthat sequential posted pricing sells a $\\rho$-fraction of the item, this\nresults in a $\\rho$-approximation of the optimal social welfare. The value of\n$\\rho$ can be improved to approximately $42\\%$ if sequential posted pricing\nconsiders the agents in random order. We also show that the best linear pricing\nyields an expected revenue that is at most $O(\\kappa^2)$ times smaller than the\noptimal one, where $\\kappa$ is a bound on the curvature of the valuation\nfunctions. The proof extends and exploits the approach of Alaei et al. (2019)\nand bounds the revenue gap by the objective value of a mathematical program.\nThe dependence of the revenue gap on $\\kappa$ is unavoidable as a lower bound\nof $\\Omega(\\ln{\\kappa})$ indicates.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Topology control, including topology construction and maintenance phases, is\na vital conception for wireless ad-hoc networks of any kind, expressly the\nwireless sensor networks (WSN). Topology maintenance, the latter phase,\nconcerns several problems, such as optimizing the energy consumption,\nincreasing the data rate, making clusters, and sustaining the connectivity. A\ndisconnected network, among other strategies, can efficiently be connected\nagain using a Movement-based Connectivity Restoration (MCR) method, where a\ncommensurate number of nodes move (or are moved) to the desired positions.\nHowever, finding an optimal route for the nodes to be moved can be a formidable\nproblem. As a matter of fact, this paper presents details regarding a direct\nproof of the NP-Completeness of the MCR Problem by a reduction of the\nwell-studied Steiner Tree Problem using the minimum number of Steiner points\nand the bounded edge length.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Graphics Processing Units (GPUs) employ large register files to accommodate\nall active threads and accelerate context switching. Unfortunately, register\nfiles are a scalability bottleneck for future GPUs due to long access latency,\nhigh power consumption, and large silicon area provisioning. Prior work\nproposes hierarchical register file to reduce the register file power\nconsumption by caching registers in a smaller register file cache.\nUnfortunately, this approach does not improve register access latency due to\nthe low hit rate in the register file cache.\n  In this paper, we propose the Latency-Tolerant Register File (LTRF)\narchitecture to achieve low latency in a two-level hierarchical structure while\nkeeping power consumption low. We observe that compile-time interval analysis\nenables us to divide GPU program execution into intervals with an accurate\nestimate of a warp's aggregate register working-set within each interval. The\nkey idea of LTRF is to prefetch the estimated register working-set from the\nmain register file to the register file cache under software control, at the\nbeginning of each interval, and overlap the prefetch latency with the execution\nof other warps. We observe that register bank conflicts while prefetching the\nregisters could greatly reduce the effectiveness of LTRF. Therefore, we devise\na compile-time register renumbering technique to reduce the likelihood of\nregister bank conflicts. Our experimental results show that LTRF enables\nhigh-capacity yet long-latency main GPU register files, paving the way for\nvarious optimizations. As an example optimization, we implement the main\nregister file with emerging high-density high-latency memory technologies,\nenabling 8X larger capacity and improving overall GPU performance by 34%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An analytical solution for a quantum wave impedance in a case of piesewise\nconstant potential was derived. It is in fact an analytical depiction of a\nwell-known iterative method of a quantum wave impedance determination. The\nexpression for a transmission probability as a function of a particle energy\nfor an arbitrary cascad of constant potentials was obtained. The application of\nobtained results was illustrated on a system of double-well/barrier structures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is well known from the Butcher-Oemler effect that galaxies in dense\nenvironment are mostly red with little star formation and the fraction of blue\ngalaxies in galaxy groups/clusters also declines rapidly with redshifts. A\nrecent work by Hashimoto et al. reported a local 'blue cluster' with high\nfraction of blue galaxies ($\\sim 0.57$), higher than the model predictions.\nThey ascribed this blue cluster to the feeding of gas along a filamentary\nstructure around the cluster. In this work we use group catalog from the Sloan\nDigital Sky Survey Data Release 7 (SDSS DR7) and the state-of-art of\nsemi-analytic model (SAM) to investigate the formation of blue clusters in\nlocal Universe. In total, we find four blue clusters with halo mass $\\sim\n10^{14}M_{\\odot}$ at $0.02 < z < 0.082$, while only the one found by Hashimoto\net al. is in a filamentary structure. The SAM predicts that blue clusters have\nlater formation time and most blue satellite galaxies are recently accreted. We\nconclude that the formation of blue clusters is mainly governed by newly\naccreted blue satellites, rather than the effect of large-scale environment.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Searching for space-time variations of the constants of Nature is a promising\nway to search for new physics beyond General Relativity and the standard model\nmotivated by unification theories and models of dark matter and dark energy. We\npropose a new way to search for a variation of the fine-structure constant\nusing measurements of late-type evolved giant stars from the S-star cluster\norbiting the supermassive black hole in our Galactic Center. A measurement of\nthe difference between distinct absorption lines (with different sensitivity to\nthe fine structure constant) from a star leads to a direct estimate of a\nvariation of the fine structure constant between the star's location and Earth.\nUsing spectroscopic measurements of 5 stars, we obtain a constraint on the\nrelative variation of the fine structure constant below $10^{-5}$. This is the\nfirst time a varying constant of Nature is searched for around a black hole and\nin a high gravitational potential. This analysis shows new ways the monitoring\nof stars in the Galactic Center can be used to probe fundamental physics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many important robotics problems are partially observable in the sense that a\nsingle visual or force-feedback measurement is insufficient to reconstruct the\nstate. Standard approaches involve learning a policy over beliefs or\nobservation-action histories. However, both of these have drawbacks; it is\nexpensive to track the belief online, and it is hard to learn policies directly\nover histories. We propose a method for policy learning under partial\nobservability called the Belief-Grounded Network (BGN) in which an auxiliary\nbelief-reconstruction loss incentivizes a neural network to concisely summarize\nits input history. Since the resulting policy is a function of the history\nrather than the belief, it can be executed easily at runtime. We compare BGN\nagainst several baselines on classic benchmark tasks as well as three novel\nrobotic touch-sensing tasks. BGN outperforms all other tested methods and its\nlearned policies work well when transferred onto a physical robot.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that in the family of degree $d\\geq 2$ rational maps of the Riemann\nsphere, the closure of strictly postcritically finite maps contains a\n(relatively) Baire generic subset of maps displaying maximal non-statistical\nbehavior: for a map $f$ in this generic subset, the set of accumulation points\nof the sequence of empirical measures of almost every point in the phase space\nis the largest possible one that is, the set of all $f$-invariant measures. The\nproofs is based on a transversality argument which allows us to control the\nbehavior of the orbits of critical points for maps close to strictly\npostcritically finite rational maps and also a new concept developed in the\nauthor's PhD thesis, that we call statistical bifurcation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a cross-modality manifold alignment procedure that leverages\ntriplet loss to jointly learn consistent, multi-modal embeddings of\nlanguage-based concepts of real-world items. Our approach learns these\nembeddings by sampling triples of anchor, positive, and negative data points\nfrom RGB-depth images and their natural language descriptions. We show that our\napproach can benefit from, but does not require, post-processing steps such as\nProcrustes analysis, in contrast to some of our baselines which require it for\nreasonable performance. We demonstrate the effectiveness of our approach on two\ndatasets commonly used to develop robotic-based grounded language learning\nsystems, where our approach outperforms four baselines, including a\nstate-of-the-art approach, across five evaluation metrics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Probabilistic word embeddings have shown effectiveness in capturing notions\nof generality and entailment, but there is very little work on doing the\nanalogous type of investigation for sentences. In this paper we define\nprobabilistic models that produce distributions for sentences. Our\nbest-performing model treats each word as a linear transformation operator\napplied to a multivariate Gaussian distribution. We train our models on\nparaphrases and demonstrate that they naturally capture sentence specificity.\nWhile our proposed model achieves the best performance overall, we also show\nthat specificity is represented by simpler architectures via the norm of the\nsentence vectors. Qualitative analysis shows that our probabilistic model\ncaptures sentential entailment and provides ways to analyze the specificity and\npreciseness of individual words.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Using data from the Large European Array for Pulsars (LEAP), and the\nEffelsberg telescope, we study the scintillation parameters of the millisecond\npulsar J0613-0200 over a 7 year timespan. The \"secondary spectrum\" -- the 2D\npower spectrum of scintillation -- presents the scattered power as a function\nof time delay, and contains the relative velocities of the pulsar, observer,\nand scattering material. We detect a persistent parabolic scintillation arc,\nsuggesting scattering is dominated by a thin, anisotropic region. The\nscattering is poorly described by a simple exponential tail, with excess power\nat high delays; we measure significant, detectable scattered power at times out\nto $\\sim 5 \\mu s$, and measure the bulk scattering delay to be between 50 to\n200\\,ns with particularly strong scattering throughout 2013. These delays are\ntoo small to detect a change of the pulse profile shape, yet they would change\nthe times-of-arrival as measured through pulsar timing. The arc curvature\nvaries annually, and is well fit by a one-dimensional scattering screen $\\sim\n40\\%$ of the way towards the pulsar, with a changing orientation during the\nincreased scattering in 2013. Effects of uncorrected scattering will introduce\ntime delays correlated over time in individual pulsars, and may need to be\nconsidered in gravitational wave analyses. Pulsar timing programs would benefit\nfrom simultaneously recording in a way that scintillation can be resolved, in\norder to monitor the variable time delays caused by multipath propagation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present NECI, a state-of-the-art implementation of the Full Configuration\nInteraction Quantum Monte Carlo algorithm, a method based on a stochastic\napplication of the Hamiltonian matrix on a sparse sampling of the wave\nfunction. The program utilizes a very powerful parallelization and scales\nefficiently to more than 24000 CPU cores. In this paper, we describe the core\nfunctionalities of NECI and recent developments. This includes the capabilities\nto calculate ground and excited state energies, properties via the one- and\ntwo-body reduced density matrices, as well as spectral and Green's functions\nfor ab initio and model systems. A number of enhancements of the bare FCIQMC\nalgorithm are available within NECI, allowing to use a partially deterministic\nformulation of the algorithm, working in a spin-adapted basis or supporting\ntranscorrelated Hamiltonians. NECI supports the FCIDUMP file format for\nintegrals, supplying a convenient interface to numerous quantum chemistry\nprograms and it is licensed under GPL-3.0.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Compared with laborious pixel-wise dense labeling, it is much easier to label\ndata by scribbles, which only costs 1$\\sim$2 seconds to label one image.\nHowever, using scribble labels to learn salient object detection has not been\nexplored. In this paper, we propose a weakly-supervised salient object\ndetection model to learn saliency from such annotations. In doing so, we first\nrelabel an existing large-scale salient object detection dataset with\nscribbles, namely S-DUTS dataset. Since object structure and detail information\nis not identified by scribbles, directly training with scribble labels will\nlead to saliency maps of poor boundary localization. To mitigate this problem,\nwe propose an auxiliary edge detection task to localize object edges\nexplicitly, and a gated structure-aware loss to place constraints on the scope\nof structure to be recovered. Moreover, we design a scribble boosting scheme to\niteratively consolidate our scribble annotations, which are then employed as\nsupervision to learn high-quality saliency maps. As existing saliency\nevaluation metrics neglect to measure structure alignment of the predictions,\nthe saliency map ranking metric may not comply with human perception. We\npresent a new metric, termed saliency structure measure, to measure the\nstructure alignment of the predicted saliency maps, which is more consistent\nwith human perception. Extensive experiments on six benchmark datasets\ndemonstrate that our method not only outperforms existing\nweakly-supervised/unsupervised methods, but also is on par with several\nfully-supervised state-of-the-art models. Our code and data is publicly\navailable at https://github.com/JingZhang617/Scribble_Saliency.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Alzheimer's Disease (AD) is nowadays the most common form of dementia, and\nits automatic detection can help to identify symptoms at early stages, so that\npreventive actions can be carried out. Moreover, non-intrusive techniques based\non spoken data are crucial for the development of AD automatic detection\nsystems. In this light, this paper is presented as a contribution to the ADReSS\nChallenge, aiming at improving AD automatic detection from spontaneous speech.\nTo this end, recordings from 108 participants, which are age-, gender-, and AD\ncondition-balanced, have been used as training set to perform two different\ntasks: classification into AD/non-AD conditions, and regression over the\nMini-Mental State Examination (MMSE) scores. Both tasks have been performed\nextracting 28 features from speech -- based on prosody and voice quality -- and\n51 features from the transcriptions -- based on lexical and turn-taking\ninformation. Our results achieved up to 87.5 % of classification accuracy using\na Random Forest classifier, and 4.54 of RMSE using a linear regression with\nstochastic gradient descent over the provided test set. This shows promising\nresults in the automatic detection of Alzheimer's Disease through speech and\nlexical features.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $E$ be a rearrangement invariant (r.i.) function space on $[0,1]$, and\nlet $Z_E$ consist of all measurable functions $f$ on $(0,\\infty)$ such that\n$f^*\\chi_{[0,1]}\\in E$ and $f^*\\chi_{[1,\\infty)}\\in L^2$. We reveal close\nconnections between properties of the generalized Rosenthal's space,\ncorresponding to the space $Z_E$, and the behaviour of independent\nsymmetrically distributed random variables in $E$. The results obtained are\napplied to consider the problem of the existence of isomorphisms between r.i.\\\nspaces on $[0,1]$ and $(0,\\infty)$. Exploiting particular properties of\ndisjoint sequences, we identify a rather wide new class of r.i.\\ spaces on\n$[0,1]$ ``close'' to $L^\\infty$, which fail to be isomorphic to r.i.\\ spaces on\n$(0,\\infty)$. In particular, this property is shared by the Lorentz spaces\n$\\Lambda_2(\\log^{-\\alpha}(e/u))$, with $0<\\alpha\\le 1$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In 2005 a new topological invariant defined in terms of the Brouwer degree of\na determinant map, was introduced by Musso, Pejsachowicz and the first name\nauthor for counting the conjugate points along a semi-Riemannian geodesic. This\ninvariant was defined in terms of a suspension of a complexified family of\nlinear second order Dirichlet boundary value problems.\n  In this paper, starting from this result, we generalize this invariant to a\ngeneral self-adjoint Morse-Sturm system and we prove a new spectral flow\nformula. Finally we discuss the relation between this spectral flow formula and\nthe Hill's determinant formula and we apply this invariant for detecting\ninstability of periodic orbits of a Hamiltonian system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce the package \"GraphicalModelsMLE\" for computing the maximum\nlikelihood estimates (MLEs) of a Gaussian graphical model in the computer\nalgebra system Macaulay2. This package allows the computation of MLEs for the\nclass of loopless mixed graphs. Additional functionality allows the user to\nexplore the underlying algebraic structure of the model, such as its maximum\nlikelihood degree and the ideal of score equations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The heterogenous wireless services and exponentially growing traffic call for\nnovel spectrum- and energy-efficient wireless communication technologies. In\nthis paper, a new technique, called symbiotic radio (SR), is proposed to\nexploit the benefits and address the drawbacks of cognitive radio (CR) and\nambient backscattering communications(AmBC), leading to mutualism spectrum\nsharing and highly reliable backscattering communications. In particular, the\nsecondary transmitter (STx) in SR transmits messages to the secondary receiver\n(SRx) over the RF signals originating from the primary transmitter (PTx) based\non cognitive backscattering communications, thus the secondary system shares\nnot only the radio spectrum, but also the power, and infrastructure with the\nprimary system. In return, the secondary transmission provides beneficial\nmultipath diversity to the primary system, therefore the two systems form\nmutualism spectrum sharing. More importantly, joint decoding is exploited at\nSRx to achieve highly reliable backscattering communications. To exploit the\nfull potential of SR, in this paper, we address three fundamental tasks in SR:\n(1) enhancing the backscattering link via active load; (2) achieving highly\nreliable communications through joint decoding; and (3) capturing PTx's RF\nsignals using reconfigurable intelligent surfaces. Emerging applications,\ndesign challenges and open research problems will also be discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze relative abundances and ionization conditions in a strong\nabsorption system at z=6.84, seen in the spectrum of the z=7.54 background\nquasar ULAS J134208.10+092838.61. Singly ionized C, Si, Fe, Mg, and Al\nmeasurements are consistent with a warm neutral medium that is metal-poor but\nnot chemically pristine. Firm non-detections of C IV and Si IV imply that any\nwarm ionized phase of the IGM or CGM has not yet been enriched past the\nultra-metal-poor regime (<0.001Z_{solar}), unlike lower redshift DLAs where\nthese lines are nearly ubiquitous. Relative abundances of the heavy elements\n794 Myr after the Big Bang resemble those of metal-poor damped Lyman Alpha\nsystems at intermediate redshift and Milky Way halo stars, and show no evidence\nof enhanced [alpha/Fe], [C/Fe] or other signatures of yields dominated by\nmassive stars. A detection of the CII* fine structure line reveals local\nsources of excitation from heating, beyond the level of photo-excitation\nsupplied by the CMB. We estimate the total and [CII] cooling rates, balancing\nagainst ISM heating sources to develop an heuristic two-phase model of the\nneutral medium. The implied heating requires a surface density of star\nformation slightly exceeding that of the Milky Way but not at the level of a\nstrong starburst. For a typical (assumed) NHI=10^{20.6}, an abundance of\n[Fe/H]=-2.2 matches the columns of species in the neutral phase. To remain\nundetected in C IV, a warm ionized phase would either need much lower\n[C/H]<-4.2 over an absorption path of 1 kpc, or else a very small absorption\npath (a few pc). While still speculative, these results suggest a significant\nreduction in heavy element enrichment outside of neutral star forming regions\nof the ISM, as would be expected in early stages of galactic chemical\nevolution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The notion of age of information (AoI) has become an important performance\nmetric in network and control systems. Information freshness, represented by\nAoI, naturally arises in the context of caching. We address optimal scheduling\nof cache updates for a time-slotted system where the contents vary in size.\nThere is limited capacity for the cache and for making content updates. Each\ncontent is associated with a utility function that is monotonically decreasing\nin the AoI. For this combinatorial optimization problem, we present the\nfollowing contributions. First, we provide theoretical results settling the\nboundary of problem tractability. In particular, by a reformulation using\nnetwork flows, we prove the boundary is essentially determined by whether or\nnot the contents are of equal size. Second, we derive an integer linear\nformulation for the problem, of which the optimal solution can be obtained for\nsmall-scale scenarios. Next, via a mathematical reformulation, we derive a\nscalable optimization algorithm using repeated column generation. In addition,\nthe algorithm computes a bound of global optimum, that can be used to assess\nthe performance of any scheduling solution. Performance evaluation of\nlarge-scale scenarios demonstrates the strengths of the algorithm in comparison\nto a greedy schedule. Finally, we extend the applicability of our work to\ncyclic scheduling.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A spectral mixture (SM) kernel is a flexible kernel used to model any\nstationary covariance function. Although it is useful in modeling data, the\nlearning of the SM kernel is generally difficult because optimizing a large\nnumber of parameters for the SM kernel typically induces an over-fitting,\nparticularly when a gradient-based optimization is used. Also, a longer\ntraining time is required. To improve the training, we propose an approximate\nBayesian inference for the SM kernel. Specifically, we employ the variational\ndistribution of the spectral points to approximate SM kernel with a random\nFourier feature. We optimize the variational parameters by applying a\nsampling-based variational inference to the derived evidence lower bound (ELBO)\nestimator constructed from the approximate kernel. To improve the inference, we\nfurther propose two additional strategies: (1) a sampling strategy of spectral\npoints to estimate the ELBO estimator reliably and thus its associated\ngradient, and (2) an approximate natural gradient to accelerate the convergence\nof the parameters. The proposed inference combined with two strategies\naccelerates the convergence of the parameters and leads to better optimal\nparameters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a new application of deep learning to reconstruct the cosmic\nmicrowave background (CMB) temperature maps from the images of microwave sky,\nand to use these reconstructed maps to estimate the masses of galaxy clusters.\nWe use a feed-forward deep learning network, mResUNet, for both steps of the\nanalysis. The first deep learning model, mResUNet-I, is trained to reconstruct\nforeground and noise suppressed CMB maps from a set of simulated images of the\nmicrowave sky that include signals from the cosmic microwave background,\nastrophysical foregrounds like dusty and radio galaxies, instrumental noise as\nwell as the cluster's own thermal Sunyaev Zel'dovich signal. The second deep\nlearning model, mResUNet-II, is trained to estimate cluster masses from the\ngravitational lensing signature in the reconstructed foreground and noise\nsuppressed CMB maps. For SPTpol-like noise levels, the trained mResUNet-II\nmodel recovers the mass for $10^4$ galaxy cluster samples with a 1-$\\sigma$\nuncertainty $\\Delta M_{\\rm 200c}^{\\rm est}/M_{\\rm 200c}^{\\rm est} =$ 0.108 and\n0.016 for input cluster mass $M_{\\rm 200c}^{\\rm true}=10^{14}~\\rm M_{\\odot}$\nand $8\\times 10^{14}~\\rm M_{\\odot}$, respectively. We also test for potential\nbias on recovered masses, finding that for a set of $10^5$ clusters the\nestimator recovers $M_{\\rm 200c}^{\\rm est} = 2.02 \\times 10^{14}~\\rm\nM_{\\odot}$, consistent with the input at 1% level. The 2 $\\sigma$ upper limit\non potential bias is at 3.5% level.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  LiDAR-driven 3D sensing allows new generations of vehicles to achieve\nadvanced levels of situation awareness. However, recent works have demonstrated\nthat physical adversaries can spoof LiDAR return signals and deceive 3D object\ndetectors to erroneously detect \"ghost\" objects. Existing defenses are either\nimpractical or focus only on vehicles. Unfortunately, it is easier to spoof\nsmaller objects such as pedestrians and cyclists, but harder to defend against\nand can have worse safety implications. To address this gap, we introduce\nShadow-Catcher, a set of new techniques embodied in an end-to-end prototype to\ndetect both large and small ghost object attacks on 3D detectors. We\ncharacterize a new semantically meaningful physical invariant (3D shadows)\nwhich Shadow-Catcher leverages for validating objects. Our evaluation on the\nKITTI dataset shows that Shadow-Catcher consistently achieves more than 94%\naccuracy in identifying anomalous shadows for vehicles, pedestrians, and\ncyclists, while it remains robust to a novel class of strong \"invalidation\"\nattacks targeting the defense system. Shadow-Catcher can achieve real-time\ndetection, requiring only between 0.003s-0.021s on average to process an object\nin a 3D point cloud on commodity hardware and achieves a 2.17x speedup compared\nto prior work\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Whereas the original Boltzmann's $H$-theorem applies to elastic collisions,\nits rigorous generalization to the inelastic case is still lacking.\nNonetheless, it has been conjectured in the literature that the relative\nentropy of the velocity distribution function with respect to the homogeneous\ncooling state (HCS) represents an adequate nonequilibrium entropy-like\nfunctional for an isolated freely cooling granular gas. In this work, we\npresent molecular dynamics results reinforcing this conjecture and rejecting\nthe choice of the Maxwellian over the HCS as a reference distribution. These\nresults are qualitatively predicted by a simplified theoretical toy model.\nAdditionally, a Maxwell-demon-like velocity-inversion simulation experiment\nhighlights the microscopic irreversibility of the granular gas dynamics,\nmonitored by the relative entropy, where a short ``anti-kinetic'' transient\nregime appears for nearly elastic collisions only.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the potential of the channel {\\em mono-Higgs + MET} in\nyielding signals of dark mater at the high-luminosity Large Hadron Collider\n(LHC). As illustration, a scalar dark matter in a Higgs portal scenario has\nbeen chosen, whose phenomenological viability has been ensured by postulating\nthe existence of dimension-6 operators that enable cancellation in certain\namplitudes for elastic scattering of dark matter in direct search experiments.\nThese operators are found to have non-negligible contribution to the mono-Higgs\nsignal. Thereafter, we carry out a detailed analysis of this signal, with the\naccompanying MET providing a useful handle in suppressing backgrounds. Signals\nfor the Higgs decaying into both the diphoton and $b{\\bar b}$ channels have\nbeen studied. A cut-based simulation is presented first, followed by a\ndemonstration of how the statistical significance can be improved through\nanalyses based on Boosted Decision Trees and Artificial Neural Network. The\nimprovement is found to be especially noticeable for the $b{\\bar b}$ channel.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the current industrial practices, the exponential growth in terms of\navailability and affordability of sensors, data acquisition systems, and\ncomputer networks forces factories to move toward implementing high integrating\nCyber-Physical Systems (CPS) with production, logistics, and services. This\ntransforms today's factories into Industry 4.0 factories with significant\neconomic potential. Industry 4.0, also known as the fourth Industrial\nRevolution, levers on the integration of cyber technologies, the Internet of\nThings, and Services. This paper proposes an Augmented Reality (AR)-based\nsystem that creates a Cognition Level that integrates existent Manufacturing\nExecution Systems (MES) to CPS. The idea is to highlight the opportunities\noffered by AR technologies to CPS by describing an application scenario. The\nsystem, analyzed in a real factory, shows its capacity to integrate physical\nand digital worlds strongly. Furthermore, the conducted survey (based on the\nSituation Awareness Global Assessment Technique method) reveals significant\nadvantages in terms of production monitoring, progress, and workers' Situation\nAwareness in general.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Adopted by government agencies in Australia, New Zealand and the UK as policy\ninstrument or as embodied into legislation, the 'Five Safes' framework aims to\nmanage risks of releasing data derived from personal information. Despite its\npopularity, the Five Safes has undergone little legal or technical critical\nanalysis. We argue that the Fives Safes is fundamentally flawed: from being\ndisconnected from existing legal protections and appropriation of notions of\nsafety without providing any means to prefer strong technical measures, to\nviewing disclosure risk as static through time and not requiring repeat\nassessment. The Five Safes provides little confidence that resulting data\nsharing is performed using 'safety' best practice or for purposes in service of\npublic interest.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In self-supervised spatio-temporal representation learning, the temporal\nresolution and long-short term characteristics are not yet fully explored,\nwhich limits representation capabilities of learned models. In this paper, we\npropose a novel self-supervised method, referred to as video Playback Rate\nPerception (PRP), to learn spatio-temporal representation in a\nsimple-yet-effective way. PRP roots in a dilated sampling strategy, which\nproduces self-supervision signals about video playback rates for representation\nmodel learning. PRP is implemented with a feature encoder, a classification\nmodule, and a reconstructing decoder, to achieve spatio-temporal semantic\nretention in a collaborative discrimination-generation manner. The\ndiscriminative perception model follows a feature encoder to prefer perceiving\nlow temporal resolution and long-term representation by classifying\nfast-forward rates. The generative perception model acts as a feature decoder\nto focus on comprehending high temporal resolution and short-term\nrepresentation by introducing a motion-attention mechanism. PRP is applied on\ntypical video target tasks including action recognition and video retrieval.\nExperiments show that PRP outperforms state-of-the-art self-supervised models\nwith significant margins. Code is available at github.com/yuanyao366/PRP\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With optical spectroscopy we provide evidence that the insulator-metal\ntransition in Sr$_2$Ir$_{1-x}$Rh$_{x}$O$_{4}$ occurs close to a crossover from\nthe Mott- to the Slater-type. The Mott-gap at $x = 0$ persists to high\ntemperature and evolves without an anomaly across the N\\'{e}el temperature,\n$T_N$. Upon Rh-doping, it collapses rather rapidly and vanishes around $x =\n0.055$. Notably, just as the Mott-gap vanishes yet another gap appears that is\nof the Slater-type and develops right below $T_N$. This Slater-gap is only\npartial and is accompanied by a reduced scattering rate of the remaining free\ncarriers, similar as in the parent compounds of the iron arsenide\nsuperconductors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we show that the pair of classes of locally H\\\"{o}lder\ncontinuous functions (considered on $\\mathbb{R}$ and $\\mathbb{R}^{2}$,\nrespectively) has the double difference property.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Visual imagery is an intuitive brain-computer interface paradigm, referring\nto the emergence of the visual scene. Despite its convenience, analysis of its\nintrinsic characteristics is limited. In this study, we demonstrate the effect\nof time interval and channel selection that affects the decoding performance of\nthe multi-class visual imagery. We divided the epoch into time intervals of 0-1\ns and 1-2 s and performed six-class classification in three different brain\nregions: whole brain, visual cortex, and prefrontal cortex. In the time\ninterval, 0-1 s group showed 24.2 % of average classification accuracy, which\nwas significantly higher than the 1-2 s group in the prefrontal cortex. In the\nthree different regions, the classification accuracy of the prefrontal cortex\nshowed significantly higher performance than the visual cortex in 0-1 s\ninterval group, implying the cognitive arousal during the visual imagery. This\nfinding would provide crucial information in improving the decoding\nperformance.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With the rise of natural user interfaces, immersive analytics applications\noften focus on novel forms of interaction modalities such as mid-air gestures,\ngaze or tangible interaction utilizing input devices such as depth-sensors,\ntouch screens and eye-trackers. At the same time, traditional input devices\nsuch as the physical keyboard and mouse are used to a lesser extent. We argue,\nthat for certain work scenarios, such as conducting analytic tasks at\nstationary desktop settings, it can be valuable to combine the benefits of\nnovel and established input devices as well as input modalities to create\nproductive immersive analytics environments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Today's intelligent traffic light control system is based on the current road\ntraffic conditions for traffic regulation. However, these approaches cannot\nexploit the future traffic information in advance. In this paper, we propose\nGPlight, a deep reinforcement learning (DRL) algorithm integrated with graph\nneural network (GNN) , to relieve the traffic congestion for multi-intersection\nintelligent traffic control system. In GPlight, the graph neural network (GNN)\nis first used to predict the future short-term traffic flow at the\nintersections. Then, the results of traffic flow prediction are used in traffic\nlight control, and the agent combines the predicted results with the observed\ncurrent traffic conditions to dynamically control the phase and duration of the\ntraffic lights at the intersection. Experiments on both synthetic and two\nreal-world data-sets of Hangzhou and New-York verify the effectiveness and\nrationality of the GPlight algorithm.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A graph $G$ is weakly $H$-saturated if the complete graph is obtained by\niteratively completing copies of $H$ minus an edge. We identify the threshold\n$p_c$ at which the Erd\\H{o}s-R\\'enyi graph ${\\mathcal G}_{n,p}$ is likely to be\nweakly $H$-saturated, for all $H$ such that $H\\setminus e$ is 2-balanced for\nevery edge $e\\in H$. The threshold is sharp if this holds strictly. We also\nestablish a general asymptotic lower bound for $p_c$, which holds for all\ngraphs $H$, and is sharp in many cases. Our results apply for instance when\n$H=K_r$, solving a problem of Balogh, Bollob\\'as and Morris.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep neural networks are vulnerable to adversarial examples -- minor\nperturbations added to a model's input which cause the model to output an\nincorrect prediction. We introduce a new method for improving the efficacy of\nadversarial attacks in a black-box setting by undertraining the surrogate model\nwhich the attacks are generated on. Using two datasets and five model\narchitectures, we show that this method transfers well across architectures and\noutperforms state-of-the-art methods by a wide margin. We interpret the\neffectiveness of our approach as a function of reduced surrogate model loss\nfunction curvature and increased universal gradient characteristics, and show\nthat our approach reduces the presence of local loss maxima which hinder\ntransferability. Our results suggest that finding strong single surrogate\nmodels is a highly effective and simple method for generating transferable\nadversarial attacks, and that this method represents a valuable route for\nfuture study in this field.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A new semi-analytical solution to the advection-dispersion-reaction equation\nfor modelling solute transport in layered porous media is derived using the\nLaplace transform. Our solution approach involves introducing unknown functions\nrepresenting the dispersive flux at the interfaces between adjacent layers,\nallowing the multilayer problem to be solved separately on each layer in the\nLaplace domain before being numerically inverted back to the time domain. The\nderived solution is applicable to the most general form of linear\nadvection-dispersion-reaction equation, a finite medium comprising an arbitrary\nnumber of layers, continuity of concentration and dispersive flux at the\ninterfaces between adjacent layers and transient boundary conditions of\narbitrary type at the inlet and outlet. The derived semi-analytical solution\nextends and addresses deficiencies of existing analytical solutions in a\nlayered medium, which consider analogous processes such as diffusion or\nreaction-diffusion only and/or require the solution of complicated nonlinear\ntranscendental equations to evaluate the solution expressions. Code\nimplementing our semi-analytical solution is supplied and applied to a\nselection of test cases, with the reported results in excellent agreement with\na standard numerical solution and other analytical results available in the\nliterature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Le Cam's method, Fano's inequality, and Assouad's lemma are three widely used\ntechniques to prove lower bounds for statistical estimation tasks. We propose\ntheir analogues under central differential privacy. Our results are simple,\neasy to apply and we use them to establish sample complexity bounds in several\nestimation tasks. We establish the optimal sample complexity of discrete\ndistribution estimation under total variation distance and $\\ell_2$ distance.\nWe also provide lower bounds for several other distribution classes, including\nproduct distributions and Gaussian mixtures that are tight up to logarithmic\nfactors. The technical component of our paper relates coupling between\ndistributions to the sample complexity of estimation under differential\nprivacy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present an experiment, designed to investigate and evaluate\nthe scalability and the robustness aspects of mobile manipulation. The\nexperiment involves performing variations of mobile pick and place actions and\nopening/closing environment containers in a human household. The robot is\nexpected to act completely autonomously for extended periods of time. We\ndiscuss the scientific challenges raised by the experiment as well as present\nour robotic system that can address these challenges and successfully perform\nall the tasks of the experiment. We present empirical results and the lessons\nlearned as well as discuss where we hit limitations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Dedicated accelerometers have been developed for the MICROSCOPE mission\ntaking into account the specific range of acceleration to be measured on board\nthe satellite. Considering one micro-g and even less as the full range of the\ninstrument, leads to a customized concept and a high performance electronics\nfor the sensing and servo-actuations of the accelerometer test-masses. In\naddition to a very accurate geometrical sensor core, a high performance\nelectronics architecture provides the measurement of the weak electrostatic\nforces and torques applied to the test-masses. A set of capacitive sensors\ndelivers the position and the attitude of the test-mass with respect to a very\nsteady gold coated cage made in silica. The voltages applied on the electrodes\nsurrounding each test-mass are finely controlled to generate the adequate\nelectrical field and so the electrostatic pressures on the test-mass. This\nfield maintains the test-mass motionless with respect to the instrument\nstructure. Digital control laws are implemented in order to enable instrument\noperation flexibility and a weak position sensor noise. These electronics\nprovide both the scientific data for MICROSCOPE's test of General Relativity\nand the data for the satellite drag-free and attitude control system (DFACS).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we establish pointwise estimates for supersolutions of\nquasilinear elliptic equations with structural conditions involving a\ngeneralized Orlicz growth in terms of a Wolff type potential. As a consequence,\nunder the extra assumption, we obtain that the supersolutions satisfy a Harnack\ninequality and local H\\\"older continuity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In 1924, Satyendra Nath Bose dispatched a manuscript introducing the concept\nnow known as Bose statistics to Albert Einstein. Bose could hardly have\nimagined that the exotic statistics of certain emergent particles of quantum\nmatter would one day suggest a route to fault-tolerant quantum computation.\nThis non-technical Commentary on \"anyons,\" namely particles whose statistics is\nintermediate between Bose and Fermi, aims to convey the underlying concept as\nwell as its experimental manifestations to the uninitiated.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate nef and movable cones of hypersurfaces in Mori dream spaces.\nThe first result is: Let $Z$ be a smooth Mori dream space of dimension at least\nfour whose extremal contractions are of fiber type of relative dimension at\nleast two and let $X$ be a smooth ample divisor in $Z$, then $X$ is a Mori\ndream space as well.\n  The second result is: Let $Z$ be a Fano manifold of dimension at least four\nwhose extremal contractions are of fiber type and let $X$ be a smooth\nanti-canonical hypersurface in $Z$, which is a smooth Calabi--Yau variety, then\nthe unique minimal model of $X$ up to isomorphism is $X$ itself, and moreover,\nthe movable cone conjecture holds for $X$, namely, there exists a rational\npolyhedral cone which is a fundamental domain for the action of birational\nautomorphisms on the effective movable cone of $X$.\n  The third result is: Let $P:= \\mathbb{P}^n \\times \\cdots \\times \\mathbb{P}^n$\nbe the $N$-fold self-product of the $n$-dimensional projective space. Let $X$\nbe a general complete intersection of $n+1$ hypersurfaces of multidegree $(1,\n\\dots, 1)$ in $P$ with $\\dim X \\geq 3$. Then $X$ has only finitely many minimal\nmodels up to isomorphism, and moreover, the movable cone conjecture holds for\n$X$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The giant $\\{ \\mathrm{Mn}_{70} \\}$ and $\\{ \\mathrm{Mn}_{84} \\}$ wheels are\nthe largest nuclearity single-molecule magnets synthesized to date and\nunderstanding their magnetic properties poses a challenge to theory. Starting\nfrom first principles calculations, we explore the magnetic properties and\nexcitations in these wheels using effective spin Hamiltonians. We find that the\nunusual geometry of the superexchange pathways leads to weakly coupled $\\{\n\\mathrm{Mn}_{7} \\}$ subunits carrying an effective $S=2$ spin. The spectrum\nexhibits a hierarchy of energy scales and massive degeneracies, with the lowest\nenergy excitations arising from Heisenberg-ring-like excitations of the $\\{\n\\mathrm{Mn}_{7} \\}$ subunits around the wheel, at energies consistent with the\nobserved temperature dependence of the magnetic susceptibility. We further\nsuggest an important role for weak longer-range couplings in selecting the\nprecise spin ground-state of the $\\mathrm{Mn}$ wheels out of the nearly\ndegenerate ground-state band.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We revisit perturbative RG analysis in the replicated Landau-Ginzburg\ndescription of the Random Field Ising Model near the upper critical dimension\n6. Working in a field basis with manifest vicinity to a weakly-coupled\nParisi-Sourlas supersymmetric fixed point (Cardy, 1985), we look for\ninteractions which may destabilize the SUSY RG flow and lead to the loss of\ndimensional reduction. This problem is reduced to studying the anomalous\ndimensions of \"leaders\" -- lowest dimension parts of $S_n$-invariant\nperturbations in the Cardy basis. Leader operators are classified as\nnon-susy-writable, susy-writable or susy-null depending on their symmetry.\nSusy-writable leaders are additionally classified as belonging to superprimary\nmultiplets transforming in particular $\\textrm{OSp}(d | 2)$ representations. We\nenumerate all leaders up to 6d dimension $\\Delta = 12$, and compute their\nperturbative anomalous dimensions (up to two loops). We thus identify two\nperturbations (with susy-null and non-susy-writable leaders) becoming relevant\nbelow a critical dimension $d_c \\approx 4.2$ - $4.7$. This supports the\nscenario that the SUSY fixed point exists for all $3 < d \\leq 6$, but becomes\nunstable for $d < d_c$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Software development is a very broad activity that captures the entire life\ncycle of a software, which includes designing, programming, maintenance and so\non. In this study, we focus on the maintenance-related concerns of the\npost-deployment of smart contracts. Smart contracts are self-executed programs\nthat run on a blockchain. They cannot be modified once deployed and hence they\nbring unique maintenance challenges compared to conventional software.\nAccording to the definition of ISO/IEC 14764, there are four kinds of software\nmaintenance, i.e., corrective, adaptive, perfective, and preventive\nmaintenance. This study aims to answer (i) What kinds of issues will smart\ncontract developers encounter for corrective, adaptive, perfective, and\npreventive maintenance after they are deployed to the Ethereum? (ii) What are\nthe current maintenance-related methods used for smart contracts? To obtain the\nanswers to these research questions, we first conducted a systematic literature\nreview to analyze 131 smart contract related research papers published from\n2014 to 2020. Since the Ethereum ecosystem is fast-growing, some results from\nprevious publications might be out-of-date and there may be a gap between\nacademia and industry. To address this, we performed an online survey of smart\ncontract developers on Github to validate our findings and received 165 useful\nresponses. Based on the survey feedback and literature review, we present the\nfirst empirical study on smart contract maintenance-related concerns. Our study\ncan help smart contract developers better maintain their smart contract-based\nprojects, and we highlight some key future research directions to improve the\nEthereum ecosystem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove some contact analogs of smooth embedding theorems for closed\n$\\pi$-manifolds. We show that a closed, $k$-connected, $\\pi$-manifold of\ndimension (2n + 1) that bounds a $\\pi$-manifold, contact embeds in the\n$(4n-2k+3)$-dimensional Euclidean space with the standard contact structure. We\nalso prove some isocontact embedding results for $\\pi$-manifolds and\nparallelizable manifolds.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A novel coronavirus disease (COVID-19) is sweeping the world and has taken\naway thousands of lives. As the current epicenter, the United States has the\nlargest number of confirmed and death cases of COVID-19. Meteorological factors\nhave been found associated with many respiratory diseases in the past studies.\nIn order to understand that how and during which period of time do the\nmeteorological factors have the strongest association with the transmission and\nfatality of COVID-19, we analyze the correlation between each meteorological\nfactor during different time periods within the incubation window and the\nconfirmation and fatality rate, and develop statistic models to quantify the\neffects at county level. Results show that meteorological variables except\nmaximum wind speed during the day 13 - 0 before current day shows the most\nsignificant correlation (P < 0.05) with the daily confirmed rate, while\ntemperature during the day 13 - 8 before are most significantly correlated (P <\n0.05) with the daily fatality rate. Temperature is the only meteorological\nfactor showing dramatic positive association nationally, particularly in the\nsoutheast US where the current outbreak most intensive. The influence of\ntemperature is remarkable on the confirmed rate with an increase of over 5 pmp\nin many counties, but not as much on the fatality rate (mostly within 0.01%).\nFindings in this study will help understanding the role of meteorological\nfactors in the spreading of COVID-19 and provide insights for public and\nindividual in fighting against this global epidemic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Thermal Kinetic Inductance Detectors (TKIDs) combine the excellent noise\nperformance of traditional bolometers with a radio frequency multiplexing\narchitecture that enables the large detector counts needed for the next\ngeneration of millimeter-wave instruments. In this paper, we first discuss the\nexpected noise sources in TKIDs and derive the limits where the phonon noise\ncontribution dominates over the other detector noise terms:\ngeneration-recombination, amplifier, and two-level system (TLS) noise. Second,\nwe characterize aluminum TKIDs in a dark environment. We present measurements\nof TKID resonators with quality factors of about $10^5$ at 80 mK. We also\ndiscuss the bolometer thermal conductance, heat capacity, and time constants.\nThese were measured by the use of a resistor on the thermal island to excite\nthe bolometers. These dark aluminum TKIDs demonstrate a noise equivalent power\nNEP = $2 \\times 10^{-17} \\mathrm{W}/\\mathrm{\\sqrt{Hz}} $, with a $1/f$ knee at\n0.1 Hz, which provides background noise limited performance for ground-based\ntelescopes observing at 150 GHz.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we derive the leading quantum gravitational corrections to the\ngeodesics and the equations of motion for a scalar field in the spacetime\ncontaining a constant density star. It is shown that these corrections can be\ncalculated in quantum gravity reliably and in a model independent way.\nFurthermore, we find that quantum gravity gives rise to an additional redshift\nthat results from the gradient instead of the amplitude of the density profile.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Studies show that neural networks, not unlike traditional programs, are\nsubject to bugs, e.g., adversarial samples that cause classification errors and\ndiscriminatory instances that demonstrate the lack of fairness. Given that\nneural networks are increasingly applied in critical applications (e.g.,\nself-driving cars, face recognition systems and personal credit rating\nsystems), it is desirable that systematic methods are developed to analyze\n(e.g., test or verify) neural networks against desirable properties. Recently,\na number of approaches have been developed for analyzing neural networks. These\nefforts are however scattered (i.e., each approach tackles some restricted\nclasses of neural networks against certain particular properties), incomparable\n(i.e., each approach has its own assumptions and input format) and thus hard to\napply, reuse or extend. In this project, we aim to build a unified framework\nfor developing techniques to analyze neural networks. Towards this goal, we\ndevelop a platform called SOCRATES which supports a standardized format for a\nvariety of neural network models, an assertion language for property\nspecification as well as multiple neural network analysis algorithms including\ntwo novel ones for falsifying and probabilistic verification of neural network\nmodels. SOCRATES is extensible and thus existing approaches can be easily\nintegrated. Experiment results show that our platform can handle a wide range\nof networks models and properties. More importantly, it provides a platform for\nsynergistic research on neural network analysis.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce transfer learning for nonlinear dynamics, which enables\nefficient predictions of chaotic dynamics by utilizing a small amount of data.\nFor the Lorenz chaos, by optimizing the transfer rate, we accomplish more\naccurate inference than the conventional method by an order of magnitude.\nMoreover, a surprisingly small amount of learning is enough to infer the energy\ndissipation rate of the Navier-Stokes turbulence because we can, thanks to the\nsmall-scale universality of turbulence, transfer a large amount of the\nknowledge learned from turbulence data at lower Reynolds number.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Federated learning is a distributed learning technique where machine learning\nmodels are trained on client devices in which the local training data resides.\nThe training is coordinated via a central server which is, typically,\ncontrolled by the intended owner of the resulting model. By avoiding the need\nto transport the training data to the central server, federated learning\nimproves privacy and efficiency. But it raises the risk of model theft by\nclients because the resulting model is available on every client device. Even\nif the application software used for local training may attempt to prevent\ndirect access to the model, a malicious client may bypass any such restrictions\nby reverse engineering the application software. Watermarking is a well-known\ndeterrence method against model theft by providing the means for model owners\nto demonstrate ownership of their models. Several recent deep neural network\n(DNN) watermarking techniques use backdooring: training the models with\nadditional mislabeled data. Backdooring requires full access to the training\ndata and control of the training process. This is feasible when a single party\ntrains the model in a centralized manner, but not in a federated learning\nsetting where the training process and training data are distributed among\nseveral client devices. In this paper, we present WAFFLE, the first approach to\nwatermark DNN models trained using federated learning. It introduces a\nretraining step at the server after each aggregation of local models into the\nglobal model. We show that WAFFLE efficiently embeds a resilient watermark into\nmodels incurring only negligible degradation in test accuracy (-0.17%), and\ndoes not require access to training data. We also introduce a novel technique\nto generate the backdoor used as a watermark. It outperforms prior techniques,\nimposing no communication, and low computational (+3.2%) overhead.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An elementary derivation of the Newton \"inverse square law\" from the three\nKepler laws is proposed. Our proof, thought essentially for first-year\nundergraduates, basically rests on Euclidean geometry. It could then be offered\neven to high-school students possessing only the first basics of Calculus.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The rates of strong convergence for various approximation schemes are\ninvestigated for a class of stochastic differential equations (SDEs) which\ninvolve a random time change given by an inverse subordinator. SDEs to be\nconsidered are unique in two different aspects: i) they contain two drift\nterms, one driven by the random time change and the other driven by a regular,\nnon-random time variable; ii) the standard Lipschitz assumption is replaced by\nthat with a time-varying Lipschitz bound. The difficulty imposed by the first\naspect is overcome via an approach that is significantly different from a\nwell-known method based on the so-called duality principle. On the other hand,\nthe second aspect requires the establishment of a criterion for the existence\nof exponential moments of functions of the random time change.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work we analyse five popular commercial password managers for\nsecurity vulnerabilities. Our analysis is twofold. First, we compile a list of\npreviously disclosed vulnerabilities through a comprehensive review of the\nacademic and non-academic sources and test each password manager against all\nthe previously disclosed vulnerabilities. We find a mixed picture of fixed and\npersisting vulnerabilities. Then we carry out systematic functionality tests on\nthe considered password managers and find four new vulnerabilities. Notably,\none of the new vulnerabilities we identified allows a malicious app to\nimpersonate a legitimate app to two out of five widely-used password managers\nwe tested and as a result steal the user's password for the targeted service.\nWe implement a proof-of-concept attack to show the feasibility of this\nvulnerability in a real-life scenario. Finally, we report and reflect on our\nexperience of responsible disclosure of the newly discovered vulnerabilities to\nthe corresponding password manager vendors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Matrix acidization simulation is a challenging task in the study of flows in\nporous media, due to the changing porosity in the procedure. The improved DBF\nframework is one model to do this simulation, and its numerical scheme\ndiscretises the mass and momentum conservation equations together to form a\npressure-velocity linear system. However, this linear system can only be solved\nby direct solvers to solve for pressure and velocity simultaneously, since\nzeros appear in the diagonal of the coefficient matrix. Considering the\nlarge-scale attribute of matrix acidization simulation, the solving time of\ndirect solvers is not intolerant. Thus, a decoupled scheme is proposed in this\nwork to decouple the coupled pressure-velocity linear system into two\nindependent linear systems: one is to solve for pressure, and the other one is\nto solve for velocity. Both of the new linear systems can be solved by parallel\nand iterative solvers, which guarantees the large-scale simulation can be\nfinished in a reasonable time period. A numerical experiment is carried out to\ndemonstrate the correctness of the decoupled scheme and its higher computing\nefficiency.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In $L_2({\\mathbb R}^d; {\\mathbb C}^n)$, we consider a matrix strongly\nelliptic differential operator ${A}_\\varepsilon$ of order $2p$, $p \\geqslant\n2$. The operator ${A}_\\varepsilon$ is given by ${A}_\\varepsilon =\nb(\\mathbf{D})^* g(\\mathbf{x}/\\varepsilon) b(\\mathbf{D})$, $\\varepsilon >0$,\nwhere $g(\\mathbf{x})$ is a periodic, bounded, and positive definite\nmatrix-valued function, and $b(\\mathbf{D})$ is a homogeneous differential\noperator of order $p$. We prove that, for fixed $\\tau \\in {\\mathbb R}$ and\n$\\varepsilon \\to 0$, the operator exponential $e^{-i \\tau {A}_\\varepsilon}$\nconverges to $e^{-i \\tau {A}^0}$ in the norm of operators acting from the\nSobolev space $H^s({\\mathbb R}^d; {\\mathbb C}^n)$ (with a suitable $s$) into\n$L_2({\\mathbb R}^d; {\\mathbb C}^n)$. Here $A^0$ is the effective operator.\nSharp-order error estimate is obtained. The results are applied to\nhomogenization of the Cauchy problem for the Schr\\\"odinger-type equation $i\n\\partial_\\tau {\\mathbf u}_\\varepsilon = {A}_\\varepsilon {\\mathbf u}_\\varepsilon\n+ {\\mathbf F}$, ${\\mathbf u}_\\varepsilon\\vert_{\\tau=0} = \\boldsymbol{\\phi}$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a lattice QCD calculation of the $\\Delta I=1/2$, $K\\to\\pi\\pi$\ndecay amplitude $A_0$ and $\\varepsilon'$, the measure of direct CP-violation in\n$K\\to\\pi\\pi$ decay, improving our 2015 calculation of these quantities. Both\ncalculations were performed with physical kinematics on a $32^3\\times 64$\nlattice with an inverse lattice spacing of $a^{-1}=1.3784(68)$ GeV. However,\nthe current calculation includes nearly four times the statistics and numerous\ntechnical improvements allowing us to more reliably isolate the $\\pi\\pi$\nground-state and more accurately relate the lattice operators to those defined\nin the Standard Model. We find ${\\rm Re}(A_0)=2.99(0.32)(0.59)\\times 10^{-7}$\nGeV and ${\\rm Im}(A_0)=-6.98(0.62)(1.44)\\times 10^{-11}$ GeV, where the errors\nare statistical and systematic, respectively. The former agrees well with the\nexperimental result ${\\rm Re}(A_0)=3.3201(18)\\times 10^{-7}$ GeV. These results\nfor $A_0$ can be combined with our earlier lattice calculation of $A_2$ to\nobtain ${\\rm Re}(\\varepsilon'/\\varepsilon)=21.7(2.6)(6.2)(5.0) \\times 10^{-4}$,\nwhere the third error represents omitted isospin breaking effects, and\nRe$(A_0)$/Re$(A_2) = 19.9(2.3)(4.4)$. The first agrees well with the\nexperimental result of ${\\rm Re}(\\varepsilon'/\\varepsilon)=16.6(2.3)\\times\n10^{-4}$. A comparison of the second with the observed ratio Re$(A_0)/$Re$(A_2)\n= 22.45(6)$, demonstrates the Standard Model origin of this \"$\\Delta I = 1/2$\nrule\" enhancement.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Notable progress has been made in numerous fields of machine learning based\non neural network-driven mutual information (MI) bounds. However, utilizing the\nconventional MI-based losses is often challenging due to their practical and\nmathematical limitations. In this work, we first identify the symptoms behind\ntheir instability: (1) the neural network not converging even after the loss\nseemed to converge, and (2) saturating neural network outputs causing the loss\nto diverge. We mitigate both issues by adding a novel regularization term to\nthe existing losses. We theoretically and experimentally demonstrate that added\nregularization stabilizes training. Finally, we present a novel benchmark that\nevaluates MI-based losses on both the MI estimation power and its capability on\nthe downstream tasks, closely following the pre-existing supervised and\ncontrastive learning settings. We evaluate six different MI-based losses and\ntheir regularized counterparts on multiple benchmarks to show that our approach\nis simple yet effective.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Animals use stereo sampling of odor concentration to localize sources and\nfollow odor trails. We analyze the dynamics of a bilateral model that depends\non the simultaneous comparison between odor concentrations detected by left and\nright sensors. The general model consists of three differential equations for\nthe positions in the plane and the heading. When the odor landscape is an\ninfinite trail, then we reduce the dynamics to a planar system whose dynamics\nhave just two fixed points. Using an integrable approximation (for short\nsensors) we estimate the basin of attraction. In the case of a radially\nsymmetric landscape, we again can reduce the dynamics to a planar system, but\nthe behavior is considerably richer with multi-stability, isolas, and limit\ncycles. As in the linear trail case, there is also an underlying integrable\nsystem when the sensors are short. In odor landscapes that consist of multiple\nspots and trail segments, we find periodic and chaotic dynamics and\ncharacterize the behavior on trails with gaps and that turn corners.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The metal diborides are a class of ceramic materials with crystal structures\nconsisting of hexagonal sheets of boron atoms alternating with planes of metal\natoms held together with mixed character ionic/covalent bonds. Many of the\nmetal diborides are ultrahigh temperature ceramics like HfB$_2$, TaB$_2$, and\nZrB$_2$, which have melting points above 3000$^\\circ$C, high mechanical\nhardness and strength at high temperatures, and high chemical resistance, while\nMgB$_2$ is a superconductor with a transition temperature of 39 K. Here we\ndemonstrate that this diverse family of non-van der Waals materials can be\nprocessed into stable dispersions of two-dimensional (2D) nanosheets using\nultrasonication-assisted exfoliation. We generate 2D nanosheets of the metal\ndiborides AlB$_2$, CrB$_2$, HfB$_2$, MgB$_2$, NbB$_2$, TaB$_2$, TiB$_2$, and\nZrB$_2$, and use electron and scanning probe microscopies to characterize their\nstructures, morphologies, and compositions. The exfoliated layers span up to\nmicrometers in lateral dimension and reach thicknesses down to 2-3 nm, while\nretaining their hexagonal atomic structure and chemical composition. We exploit\nthe convenient solution-phase dispersions of exfoliated CrB$_2$ nanosheets to\nincorporate them directly into polymer composites. In contrast to the hard and\nbrittle bulk CrB$_2$, we find that CrB$_2$ nanocomposites remain very flexible\nand simultaneously provide increases in the elastic modulus and the ultimate\ntensile strength of the polymer. The successful liquid-phase production of 2D\nmetal diborides enables their processing using scalable low-temperature\nsolution-phase methods, extending their use to previously unexplored\napplications, and reveals a new family of non-van der Waals materials that can\nbe efficiently exfoliated into 2D forms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report the first map of large-scale (10 pc in length) emission of\nmillimeter-wavelength hydrogen recombination lines (mm-RRLs) toward the giant H\nII region around the W43-Main young massive star cluster (YMC). Our mm-RRL data\ncome from the IRAM 30 m telescope and are analyzed together with radio\ncontinuum and cm-RRL data from the Karl G. Jansky Very Large Array and\nHCO$^{+}$ 1-0 line emission data from the IRAM 30 m. The mm-RRLs reveal an\nexpanding wind-blown ionized gas shell with an electron density ~70-1500\ncm$^{-3}$ driven by the WR/OB cluster, which produces a total Ly$\\alpha$ photon\nflux of 1.5 x 10$^{50}$ s$^{-1}$. This shell is interacting with the dense\nneutral molecular gas in the W43-Main dense cloud. Combining the high spectral\nand angular resolution mm-RRL and cm-RRL cubes, we derive the two-dimensional\nrelative distributions of dynamical and pressure broadening of the ionized gas\nemission and find that the RRL line shapes are dominated by pressure broadening\n(4-55 km s$^{-1}$) near the YMC and by dynamical broadening (8-36 km s$^{-1}$)\nnear the shell's edge. Ionized gas clumps hosting ultra-compact H II regions\nfound at the edge of the shell suggest that large-scale ionized gas motion\ntriggers the formation of new star generation near the periphery of the shell.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The predict+optimize problem combines machine learning ofproblem coefficients\nwith a combinatorial optimization prob-lem that uses the predicted\ncoefficients. While this problemcan be solved in two separate stages, it is\nbetter to directlyminimize the optimization loss. However, this requires\ndif-ferentiating through a discrete, non-differentiable combina-torial\nfunction. Most existing approaches use some form ofsurrogate gradient.\nDemirovicet alshowed how to directlyexpress the loss of the optimization\nproblem in terms of thepredicted coefficients as a piece-wise linear function.\nHow-ever, their approach is restricted to optimization problemswith a dynamic\nprogramming formulation. In this work wepropose a novel divide and conquer\nalgorithm to tackle op-timization problems without this restriction and predict\nitscoefficients using the optimization loss. We also introduce agreedy version\nof this approach, which achieves similar re-sults with less computation. We\ncompare our approach withother approaches to the predict+optimize problem and\nshowwe can successfully tackle some hard combinatorial problemsbetter than\nother predict+optimize methods.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we present new theoretical results on optimal estimation of\ncertain random quantities based on high frequency observations of a L\\'evy\nprocess. More specifically, we investigate the asymptotic theory for the\nconditional mean and conditional median estimators of the supremum/infimum of a\nlinear Brownian motion and a stable L\\'evy process. Another contribution of our\narticle is the conditional mean estimation of the local time and the occupation\ntime measure of a linear Brownian motion. We demonstrate that the new\nestimators are considerably more efficient compared to the classical\nestimators. Furthermore, we discuss pre-estimation of the parameters of the\nunderlying models, which is required for practical implementation of the\nproposed statistics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, a de-Sitter epoch has been found in the new model of loop quantum\ncosmology which is governed by the scalar constraint with both of Euclidean and\nLorentz terms. The singularity free bounce in the new LQC model and the\nemergent cosmology constant strongly suggest that the effective stress energy\ntensor induced by quantum corrections must violate the standard energy\nconditions. In this paper, we do an explicit calculation to analyze the\nbehaviours of specific representative energy conditions, i.e., average null,\nstrong and dominant energy conditions. It turns out that the averaging null\nenergy condition is violated while the dominant energy condition is violated\nonly at a period around the bounce point. More specifically, the strong energy\ncondition is violated not only at a period around the bounce point, but also\nthe whole period from the bounce point to the classical phase corresponding to\nthe de Sitter period. Our results is expected to shed some lights on\nconstruction of the wormhole and time machine which usually need exotic matters\nviolate energy conditions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spontaneous symmetry breaking constitutes a paradigmatic classification\nscheme of matter. However, broken symmetry also entails domain degeneracy that\noften impedes identification of novel low symmetry states. In quantum matter,\nthis is additionally complicated by competing intertwined symmetry breaking\norders. A prime example is that of unconventional superconductivity and\ndensity-wave orders in doped cuprates in which their respective symmetry\nrelation remains a key question. Using uniaxial pressure as a domain-selective\nstimulus in combination with x-ray diffraction, we unambiguously reveal that\nthe fundamental symmetry of the charge order in the prototypical cuprate\nLa$_{1.88}$Sr$_{0.12}$CuO$_4$ is characterized by uniaxial stripes. We further\ndemonstrate the direct competition of this stripe order with unconventional\nsuperconductivity via magnetic field tuning. The stripy nature of the\ncharge-density-wave state established by our study is a prerequisite for the\nexistence of a superconducting pair-density-wave -- a theoretical proposal that\nclarifies the interrelation of intertwined quantum phases in unconventional\nsuperconductors -- and paves the way for its high-temperature realization.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Vehicles crossing bridge structures respond dynamically to the bridge's\nvibrations. An acceleration signal collected within a moving vehicle contains a\ntrace of the bridge's structural response, but also includes other sources such\nas the vehicle suspension system and surface roughness-induced vibrations. This\npaper introduces two methods for the bridge system identification using data\ncollected by a network of moving vehicles. The contributions of the vehicle\nsuspension system are removed by deconvolving the vehicle response in frequency\ndomain. The first approach utilizes the vehicle transfer function, and the\nsecond uses EEMD method. Next, roughness-induced vibrations are extracted using\nsecond-order blind identification (SOBI) method. After these two processes the\nresulting signal is equivalent to the readings of mobile sensors that scan the\nbridge's dynamic response. Structural modal identification using mobile sensor\ndata has been recently introduced as STRIDEX algorithm. The processed mobile\nsensor data is analyzed using STRIDEX to identify the modal properties of the\nbridge. The performance of the methods is validated on numerical case studies\nof a long single-span bridge monitored via a network of moving vehicles. The\nanalyses consider three road surface roughness patterns. Results show that the\nproposed algorithms are successful in extracting pure bridge vibrations, and\nproduce accurate and comprehensive modal properties of the bridge. The study\nshows that the proposed transfer function method can efficiently deconvolve the\nlinear dynamics of a moving vehicle. EEMD method is able to extract vehicle\ndynamic response without a-priori information about the vehicle. This study is\nthe first proposed methodology for complete bridge modal identification,\nincluding operational natural frequencies, mode shapes and damping ratios using\n\\textit{moving vehicles sensor data}.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A novel evaluation study of the most appropriate round function for\nnearest-neighbor (NN) image interpolation is presented. Evaluated rounding\nfunctions are selected among the five rounding rules defined by the Institute\nof Electrical and Electronics Engineers (IEEE) 754-2008 standard. Both full-\nand no-reference image quality assessment (IQA) metrics are used to study and\nevaluate the influence of rounding functions on NN interpolation image quality.\nThe concept of achieved occurrences over targeted occurrences is used to\ndetermine the percentage of achieved occurrences based on the number of test\nimages used. Inferential statistical analysis is applied to deduce from a small\nnumber of images and draw a conclusion of the behavior of each rounding\nfunction on a bigger number of images. Under the normal distribution and at the\nlevel of confidence equals to 95%, the maximum and minimum achievable\noccurrences by each evaluated rounding function are both provided based on the\ninferential analysis-based experiments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show a near optimal direct-sum theorem for the two-party randomized\ncommunication complexity. Let $f\\subseteq X \\times Y\\times Z$ be a relation,\n$\\varepsilon> 0$ and $k$ be an integer. We show,\n$$\\mathrm{R}^{\\mathrm{pub}}_\\varepsilon(f^k) \\cdot\n\\log(\\mathrm{R}^{\\mathrm{pub}}_\\varepsilon(f^k)) \\ge \\Omega(k \\cdot\n\\mathrm{R}^{\\mathrm{pub}}_\\varepsilon(f)) \\enspace,$$ where $f^k= f \\times\n\\ldots \\times f$ ($k$-times) and $\\mathrm{R}^{\\mathrm{pub}}_\\varepsilon(\\cdot)$\nrepresents the public-coin randomized communication complexity with worst-case\nerror $\\varepsilon$. Given a protocol $\\mathcal{P}$ for $f^k$ with\ncommunication cost $c \\cdot k$ and worst-case error $\\varepsilon$, we exhibit a\nprotocol $\\mathcal{Q}$ for $f$ with external-information-cost $O(c)$ and\nworst-error $\\varepsilon$. We then use a message compression protocol due to\nBarak, Braverman, Chen and Rao [2013] for simulating $\\mathcal{Q}$ with\ncommunication $O(c \\cdot \\log(c\\cdot k))$ to arrive at our result. To show this\nreduction we show some new chain-rules for capacity, the maximum information\nthat can be transmitted by a communication channel. We use the powerful concept\nof Nash-Equilibrium in game-theory, and its existence in suitably defined\ngames, to arrive at the chain-rules for capacity. These chain-rules are of\nindependent interest.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  One believed path to Interstellar Complexes Organic Molecules (iCOMs)\nformation inside the Interstellar Medium (ISM) is through chemical\nrecombination at the surface of amorphous solid water (ASW) mantle covering the\nsilicate-based core of the interstellar grains. The study of these iCOMs\nformation and their binding energy to the ASW, using computational chemistry,\ndepends strongly on the ASW models used, as different models may exhibit sites\nwith different adsorbing features. ASW extended models are rare in the\nliterature because large sizes require very large computational resources when\nquantum mechanical methods based on DFT are used. To circumvent this problem,\nwe propose to use the newly developed GFN-xTB Semi-empirical Quantum Mechanical\n(SQM) methods from the Grimme's group. These methods are, at least, two orders\nof magnitude faster than conventional DFT, only require modest central memory,\nand in this paper we aim to benchmark their accuracy against rigorous and\nresource hungry quantum mechanical methods. We focused on 38 water structures\nstudied by MP2 and CCSD(T) approaches comparing energetic and structures with\nthree levels of GFN-xTB parametrization (GFN0, GFN1, GFN2) methods. The\nextremely good results obtained at the very cheap GFN-xTB level for both water\ncluster structures and energetic paved the way towards the modeling of very\nlarge AWS models of astrochemical interest.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many data mining and analytical tasks rely on the abstraction of networks\n(graphs) to summarize relational structures among individuals (nodes). Since\nrelational data are often sensitive, we aim to seek effective approaches to\ngenerate utility-preserved yet privacy-protected structured data. In this\npaper, we leverage the differential privacy (DP) framework to formulate and\nenforce rigorous privacy constraints on deep graph generation models, with a\nfocus on edge-DP to guarantee individual link privacy. In particular, we\nenforce edge-DP by injecting proper noise to the gradients of a link\nreconstruction-based graph generation model, while ensuring data utility by\nimproving structure learning with structure-oriented graph discrimination.\nExtensive experiments on two real-world network datasets show that our proposed\nDPGGAN model is able to generate graphs with effectively preserved global\nstructure and rigorously protected individual link privacy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The fractional calculus framework will be used to invert the potential energy\nfunction from the classical scattering angle, which will be related to\nRiemann-Liouville fractional integral. Numerical solution of this fractional\norder problem will be applied to the inverse Rutherford scattering and to the\ninverse scattering of Xe--Rn atoms, in which the potential is given by\nLennard-Jones function. Proofs of existence will be presented for more clarity\nand completness of the present work. In the two cases considered, the potential\nenergy function can be retrieved with a desired precision. The present method\ngives a clear understanding of the inverse fractional problem framework.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Robust optimization is a popular paradigm for modeling and solving two- and\nmulti-stage decision-making problems affected by uncertainty. In many\nreal-world applications, the time of information discovery is\ndecision-dependent and the uncertain parameters only become observable after an\noften costly investment. Yet, most of the literature assumes that uncertain\nparameters can be observed for free and that the sequence in which they are\nrevealed is independent of the decision-maker's actions. To fill this gap. we\nconsider two- and multi-stage robust optimization problems in which part of the\ndecision variables control the time of information discovery. Thus, information\ncan be discovered (at least in part) by making strategic exploratory\ninvestments in previous stages. We propose a novel dynamic formulation of the\nproblem and prove its correctness. We leverage our model to provide a solution\nmethod inspired from the K-adaptability approximation. We reformulate the\nproblem as a finite mixed-integer (resp. bilinear) program if none (resp. some\nof the) decision variables are real-valued. This finite program is solvable\nwith off-the-shelf solvers. We generalize our approach to the minimization of\npiecewise linear convex functions. We demonstrate the effectiveness of our\nmethod in terms of interpretability, optimality, and speed on synthetic\ninstances of the Pandora box problem, the preference elicitation problem, the\nbest box problem, and the R&D project portfolio optimization problem. Finally,\nwe evaluate it on an instance of the active preference elicitation problem used\nto recommend kidney allocation policies to policy-makers at the United Network\nfor Organ Sharing.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We describe and characterize an experimental apparatus that has been used to\nstudy interactions between ultracold lithium atoms and ytterbium ions. The\npreparation of ultracold clouds of Li atoms is described as well as their\nsubsequent transport and overlap with Yb$^+$ ions trapped in a Paul trap. We\nshow how the kinetic energy of the ion after interacting with the atoms can be\nobtained by laser spectroscopy. From analyzing the dynamics of the ion in the\nabsence of atoms, we conclude that background heating, due to electric field\nnoise, limits attainable buffer gas cooling temperatures. We suspect that this\neffect can be mitigated by noise reduction and by increasing the density of the\nLi gas, in order to improve its cooling power. Imperfections in the Paul trap\nlead to so-called excess micromotion, which poses another limitation to the\nbuffer gas cooling. We describe in detail how we measure and subsequently\nminimize excess micromotion in our setup. We measure the effect of excess\nmicromotion on attainable ion temperatures after buffer gas cooling and compare\nthis to molecular dynamics simulations which describe the observed data very\nwell.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Observations are studied in toy-models constituting exact cosmological\nsolutions to the Einstein equation which are statistically homogeneous but\nlocally inhomogeneous, without an a priori introduced FLRW background and with\n\"structures\" evolving fairly slowly. The mean redshift-distance relation and\nredshift drift along 500 light rays in each of two models are compared to\nrelations based on spatial averages. The relations based on spatial averages\ngive a good reproduction of the mean redshift-distance relation, although most\nconvincingly in the model where the kinematical backreaction is subpercent. In\nboth models, the mean redshift drift clearly differs from the drift of the mean\nredshift. This indicates that redshift drift could be an important tool for\ntesting the backreaction conjecture as redshift drift appears to distinguish\nbetween local and global effects. The method presented for computing the\nredshift drift is straightforward to generalize and can thus be utilized to\nfairly easily compute this quantity in a general spacetime.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Subclassification and matching are often used in empirical studies to adjust\nfor observed covariates; however, they are largely restricted to relatively\nsimple study designs with a binary treatment and less developed for designs\nwith a continuous exposure. Matching with exposure doses is particularly useful\nin instrumental variable designs and in understanding the dose-response\nrelationships. In this article, we propose two criteria for optimal\nsubclassification based on subclass homogeneity in the context of having a\ncontinuous exposure dose, and propose an efficient polynomial-time algorithm\nthat is guaranteed to find an optimal subclassification with respect to one\ncriterion and serves as a 2-approximation algorithm for the other criterion. We\ndiscuss how to incorporate dose and use appropriate penalties to control the\nnumber of subclasses in the design. Via extensive simulations, we\nsystematically compare our proposed design to optimal non-bipartite pair\nmatching, and demonstrate that combining our proposed subclassification scheme\nwith regression adjustment helps reduce model dependence for parametric causal\ninference with a continuous dose. We apply the new design and associated\nrandomization-based inferential procedure to study the effect of\ntransesophageal echocardiography (TEE) monitoring during coronary artery bypass\ngraft (CABG) surgery on patients' post-surgery clinical outcomes using Medicare\nand Medicaid claims data, and find evidence that TEE monitoring lowers\npatients' all-cause $30$-day mortality rate.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the zero-energy deformations of periodic origami sheets with\ngeneric crease patterns. Using a mapping from the linear folding motions of\nsuch sheets to force-bearing modes in conjunction with the Maxwell-Calladine\nindex theorem we derive a relation between the number of linear folding motions\nand the number of rigid body modes that depends only on the average\ncoordination number of the origami's vertices. This supports the recent result\nby Tachi which shows periodic origami sheets with triangular faces exhibit\ntwo-dimensional spaces of rigidly foldable cylindrical configurations. We also\nfind, through analytical calculation and numerical simulation, branching of\nthis configuration space from the flat state due to geometric compatibility\nconstraints that prohibit finite Gaussian curvature. The same counting argument\nleads to pairing of spatially varying modes at opposite wavenumber in\ntriangulated origami, preventing topological polarization but permitting a\nfamily of zero energy deformations in the bulk that may be used to reconfigure\nthe origami sheet.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the global dynamics of a renewal-type epidemic model with\nvariable susceptibility. We show that in this extended model there exists a\nunique endemic equilibrium and prove that it is globally asymptotically stable\nwhen $R_0 > 1$, i.e. when it exists. We also show that the infection-free\nequilibrium, which exists always, is globally asymptotically stable for $R_0\n\\leq 1$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The problem of objectivity, i.e. how to explain on quantum grounds the\nobjective character of the macroscopic world, is one of the aspects of the\ncelebrated quantum-to-classical transition. Initiated by W. H. Zurek and\ncollaborators, this problem gained some attention recently with several\napproaches being developed. The aim of this work is to compare three of them:\nquantum Darwinism, Spectrum Broadcast Structures, and strong quantum Darwinism.\nThe paper is concentrated on foundations, providing a synthetic analysis of how\nthe three approaches realize the idea of objectivity and how they are related\nto each other. As a byproduct of this analysis, a proof of a generalized\nSpectrum Broadcast Structure theorem is presented. Recent quantum Darwinism\nexperiments are also briefly discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper introduces VESR-Net, a method for video enhancement and\nsuper-resolution (VESR). We design a separate non-local module to explore the\nrelations among video frames and fuse video frames efficiently, and a channel\nattention residual block to capture the relations among feature maps for video\nframe reconstruction in VESR-Net. We conduct experiments to analyze the\neffectiveness of these designs in VESR-Net, which demonstrates the advantages\nof VESR-Net over previous state-of-the-art VESR methods. It is worth to mention\nthat among more than thousands of participants for Youku video enhancement and\nsuper-resolution (Youku-VESR) challenge, our proposed VESR-Net beat other\ncompetitive methods and ranked the first place.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is well known that a Weinberg dimension-5 operator for small neutrino\nmasses can be realized at tree level in three types of renormalizable models:\n(i) the type-I seesaw mediated by fermion singlets, (ii) the type-II seesaw\nmediated by Higgs triplets, (iii) the type-III seesaw mediated by fermion\ntriplets. We here point out such operator can be also induced at tree level by\nvector-like lepton doublets in association with unusual fermion singlets, Higgs\ntriplets or fermion triplets. If these unusual fermion singlets, Higgs triplets\nor fermion triplets are heavy enough, their decays can generate a lepton\nasymmetry to explain the cosmic baryon asymmetry, meanwhile, the vector-like\nlepton doublets can lead to a novel inverse or linear seesaw with rich\nobservable phenomena. We further specify our scenario can be naturally embedded\ninto a grand unification theory without the conventional type-I, type-II or\ntype-III seesaw.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This survey is devoted to the classical and modern problems related to the\nentire function ${\\sigma({\\bf u};\\lambda)}$, defined by a family of nonsingular\nalgebraic curves of genus $2$, where ${\\bf u} = (u_1,u_3)$ and $\\lambda =\n(\\lambda_4, \\lambda_6,\\lambda_8,\\lambda_{10})$. It is an analogue of the\nWeierstrass sigma function $\\sigma(u;g_2,g_3)$ of a family of elliptic curves.\nLogarithmic derivatives of order 2 and higher of the function ${\\sigma({\\bf\nu};\\lambda)}$ generate fields of hyperelliptic functions of ${\\bf u} =\n(u_1,u_3)$ on the Jacobians of curves with a fixed parameter vector $\\lambda$.\nWe consider three Hurwitz series $\\sigma({\\bf u};\\lambda)=\\sum_{m,n\\ge\n0}a_{m,n}(\\lambda)\\frac{u_1^mu_3^n}{m!n!}$, $\\sigma({\\bf u};\\lambda) =\n\\sum_{k\\ge 0}\\xi_k(u_1;\\lambda)\\frac{u_3^k}{k!}$ and $\\sigma({\\bf u};\\lambda) =\n\\sum_{k\\ge 0}\\mu_k(u_3;\\lambda)\\frac{u_1^k}{k!}$. The survey is devoted to the\nnumber-theoretic properties of the functions $a_{m,n}(\\lambda)$,\n$\\xi_k(u_1;\\lambda)$ and $\\mu_k(u_3;\\lambda)$. It includes the latest results,\nwhich proofs use the fundamental fact that the function ${\\sigma ({\\bf\nu};\\lambda)}$ is determined by the system of four heat equations in a\nnonholonomic frame of six-dimensional space.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The paper deals with conditional linear information inequalities valid for\nentropy functions induced by discrete random variables. Specifically, the\nso-called conditional Ingleton inequalities are in the center of interest:\nthese are valid under conditional independence assumptions on the inducing\nrandom variables. We discuss five inequalities of this particular type, four of\nwhich has appeared earlier in the literature. Besides the proof of the new\nfifth inequality, simpler proofs of (some of) former inequalities are\npresented. These five information inequalities are used to characterize all\nconditional independence structures induced by four discrete random variables.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Language model based pre-trained models such as BERT have provided\nsignificant gains across different NLP tasks. In this paper, we study different\ntypes of transformer based pre-trained models such as auto-regressive models\n(GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional\ndata augmentation. We show that prepending the class labels to text sequences\nprovides a simple yet effective way to condition the pre-trained models for\ndata augmentation. Additionally, on three classification benchmarks,\npre-trained Seq2Seq model outperforms other data augmentation methods in a\nlow-resource setting. Further, we explore how different pre-trained model based\ndata augmentation differs in-terms of data diversity, and how well such methods\npreserve the class-label information.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we study connections between Besov spaces of functions on a\ncompact metric space $Z$, equipped with a doubling measure, and the\nNewton--Sobolev space of functions on a uniform domain $X_\\varepsilon$. This\nuniform domain is obtained as a uniformization of a (Gromov) hyperbolic filling\nof $Z$. To do so, we construct a family of hyperbolic fillings in the style of\nthe work of Bonk and Kleiner and the work of Bourdon and Pajot. Then for each\nparameter $\\beta>0$ we construct a lift $\\mu_\\beta$ of the doubling measure\n$\\nu$ on $Z$ to $X_\\varepsilon$, and show that $\\mu_\\beta$ is doubling and\nsupports a $1$-Poincar\\'e inequality. We then show that for each $\\theta$ with\n$0<\\theta<1$ and $p\\ge 1$ there is a choice of $\\beta=p(1-\\theta)\\log\\alpha$\nsuch that the Besov space $B^\\theta_{p,p}(Z)$ is the trace space of the\nNewton--Sobolev space $N^{1,p}(X_\\varepsilon,\\mu_\\beta)$ when\n$\\varepsilon=\\log\\alpha$. Finally, we exploit the tools of potential theory on\n$X_\\varepsilon$ to obtain fine properties of functions in $B^\\theta_{p,p}(Z)$,\nsuch as their quasicontinuity and quasieverywhere existence of $L^q$-Lebesgue\npoints with $q=s_\\nu p/(s_\\nu-p\\theta)$, where $s_\\nu$ is a doubling dimension\nassociated with the measure $\\nu$ on $Z$. Applying this to compact subsets of\nEuclidean spaces improves upon a result of Netrusov in $\\mathbb{R}^n$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a standardized methodology for developing and evaluating use cases\nfor quantum computers and quantum inspired methods. This methodology consists\nof a standardized set of questions which should be asked to determine how and\nindeed if, near term quantum computing can play a role in a given application.\nDeveloping such a set of questions is important because it allows different use\ncases to be evaluated in a fair and objective way, rather than considering each\ncase on an ad hoc basis which could lead to an evaluation which focuses on\npositives of a use case, while ignoring weaknesses. To demonstrate our\nmethodology we apply it to a concrete use case, ambulance dispatch, and find\nthat there are some ways in which near term quantum computing could be deployed\nsensibly, but also demonstrate some cases ways in which its use would not be\nadvised. The purpose of this paper is to initiate a dialogue within the\ncommunity of quantum computing scientists and potential end users on what\nquestions should be asked when developing real world use cases.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent advancements in Neural Architecture Search(NAS) resulted in finding\nnew state-of-the-art Artificial Neural Network (ANN) solutions for tasks like\nimage classification, object detection, or semantic segmentation without\nsubstantial human supervision. In this paper, we focus on exploring NAS for a\ndense prediction task that is image denoising. Due to a costly training\nprocedure, most NAS solutions for image enhancement rely on reinforcement\nlearning or evolutionary algorithm exploration, which usually take weeks (or\neven months) to train. Therefore, we introduce a new efficient implementation\nof various superkernel techniques that enable fast (6-8 RTX2080 GPU hours)\nsingle-shot training of models for dense predictions. We demonstrate the\neffectiveness of our method on the SIDD+ benchmark for image denoising.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum heat transfer through a generic superconducting set-up consisting of\na tunable transmon qubit placed between resonators that are termined by thermal\nreservoirs is explored. Two types of architectures are considered, a sequential\nand a beam splitter setting. Applying the numerical exact hierarchical equation\nof motion (HEOM) approach, steady state properties are revealed, and\nexperimentally relevant parameter sets are identified. Benchmark results are\ncompared with predictions based on approximate treatments to demonstrate their\nfailure in broad ranges of parameter space. These findings may allow to improve\nfuture designs for heat control in superconducting devices.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  There is an increased demand for task automation in robots. Contact-rich\ntasks, wherein multiple contact transitions occur in a series of operations,\nare extensively being studied to realize high accuracy. In this study, we\npropose a methodology that uses reinforcement learning (RL) to achieve high\nperformance in robots for the execution of assembly tasks that require precise\ncontact with objects without causing damage. The proposed method ensures the\nonline generation of stiffness matrices that help improve the performance of\nlocal trajectory optimization. The method has an advantage of rapid response\nowing to short sampling time of the trajectory planning. The effectiveness of\nthe method was verified via experiments involving two contact-rich tasks. The\nresults indicate that the proposed method can be implemented in various\ncontact-rich manipulations. A demonstration video shows the performance.\n(https://youtu.be/gxSCl7Tp4-0)\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The collapse of a gas or vapour bubble near a solid boundary produces a jet\ndirected towards the boundary. High surface pressure and shear stress induced\nby this jet can damage, or clean, the surface. More complex geometries will\nresult in changes in collapse behaviour, in particular the direction of the\njet. The majority of prior research has focused on simple flat boundaries or\nlimited cases with analytic solutions. We numerically and experimentally\ninvestigate how a slot in a flat boundary affects the jet direction for a\nsingle bubble. We use a boundary element model to predict how the jet direction\ndepends on key geometric parameters and show that the results collapse to a\nsingle curve when the parameters are normalised appropriately. We then\nexperimentally validate the predictions using laser-induced cavitation and\ncompare the experimental results to the predicted dependencies. This research\nreveals a tendency for the jet to be directed away from a slot and shows that\nthe jet direction is independent of slot height for slots of sufficient height.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we propose a neural machine translation system for Wolof, a\nlow-resource Niger-Congo language. First we gathered a parallel corpus of 70000\naligned French-Wolof sentences. Then we developped a baseline LSTM based\nencoder-decoder architecture which was further extended to bidirectional LSTMs\nwith attention mechanisms. Our models are trained on a limited amount of\nparallel French-Wolof data of approximately 35000 parallel sentences.\nExperimental results on French-Wolof translation tasks show that our approach\nproduces promising translations in extremely low-resource conditions. The best\nmodel was able to achieve a good performance of 47% BLEU score.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Collisional ionization between two Rydberg atoms in relative motion is\nexamined. A classical trajectory Monte Carlo method is used to determine the\ncross sections associated with Penning ionization. The dependence of the\nionization cross section on the magnitude and the direction of orbital angular\nmomentum of the electrons and the direction of the Laplace-Runge-Lenz vector of\nthe electrons is studied. For a given magnitude of angular momentum, there can\nexist a difference of a factor of up to $\\sim2.5$ in the ionization cross\nsection between the orientation with the highest and the lowest ionization\ncross section. The case of exchange ionization is examined and its dependence\non the magnitude of angular momentum is studied.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We recast the soft $S$-matrices on the celestial sphere as correlation\nfunctions of certain $2$-dimensional models of topological defects. In pointing\nout the double copy structure between the soft photon and soft graviton cases,\nwe arrive at a putative classical double copy between the corresponding\ntopological models and a rederivation of gauge invariance and the equivalence\nprinciple as Ward identities of the $2$-dimensional theories.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The overall cosmological parameter tension between the Atacama Cosmology\nTelescope 2020 (ACT) and Planck 2018 data within the concordance cosmological\nmodel is quantified using the suspiciousness statistic to be 2.6$\\sigma$.\nBetween ACT and the South Pole Telescope (SPT) we find a tension of\n2.4$\\sigma$, and 2.8$\\sigma$ between ACT and Planck+SPT combined. While it is\nunclear whether the tension is caused by statistical fluctuations, systematic\neffects or new physics, caution should be exercised in combining these cosmic\nmicrowave background datasets in the context of the $\\Lambda$CDM standard model\nof the universe.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper develops an easily-implementable version of Page's CUSUM\nquickest-detection test, designed to work in certain composite hypothesis\nscenarios with time-varying data statistics. The decision statistic can be cast\nin a recursive form and is particularly suited for on-line analysis. By\nback-testing our approach on publicly-available COVID-19 data we find reliable\nearly warning of infection flare-ups, in fact sufficiently early that the tool\nmay be of use to decision-makers on the timing of restrictive measures that may\nin the future need to be taken.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Invisible decays of neutral hadrons are evaluated as ordinary-mirror particle\noscillations using the newly developed mirror matter model. Assuming\nequivalence of the $CP$ violation and mirror symmetry breaking scales for\nneutral kaon oscillations, rather precise values of the mirror matter model\nparameters are predicted for such ordinary-mirror particle oscillations. Not\nonly do these parameter values satisfy the cosmological constraints, but they\ncan also be used to precisely determine the oscillation or invisible decay\nrates of neutral hadrons. In particular, invisible decay branching fractions\nfor relatively long-lived hadrons such as $K^0_L$, $K^0_S$, $\\Lambda^0$, and\n$\\Xi^0$ due to such oscillations are calculated to be $9.9\\times 10^{-6}$,\n$1.8\\times 10^{-6}$, $4.4\\times 10^{-7}$, and $3.6\\times 10^{-8}$,\nrespectively. These significant invisible decays are readily detectable at\nexisting accelerator facilities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This survey describes probabilistic algorithms for linear algebra\ncomputations, such as factorizing matrices and solving linear systems. It\nfocuses on techniques that have a proven track record for real-world problem\ninstances. The paper treats both the theoretical foundations of the subject and\nthe practical computational issues.\n  Topics covered include norm estimation; matrix approximation by sampling;\nstructured and unstructured random embeddings; linear regression problems;\nlow-rank approximation; subspace iteration and Krylov methods; error estimation\nand adaptivity; interpolatory and CUR factorizations; Nystr\\\"om approximation\nof positive-semidefinite matrices; single view (\"streaming\") algorithms; full\nrank-revealing factorizations; solvers for linear systems; and approximation of\nkernel matrices that arise in machine learning and in scientific computing.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper is an attempt to extend the recent understanding of the Page curve\nfor evaporating black holes to more general systems coupled to a heat bath.\nAlthough calculating the von Neumann entropy by the replica trick is usually a\nchallenge, we have identified two solvable cases. For the initial section of\nthe Page curve, we sum up the perturbation series in the system-bath coupling\n$\\kappa$; the most interesting contribution is of order $2s$, where $s$ is the\nnumber of replicas. For the saturated regime, we consider the effect of an\nexternal impulse on the entropy at a later time and relate it to OTOCs. A\nsignificant simplification occurs in the maximal chaos case such that the\neffect may be interpreted in terms of an intermediate object, analogous to the\nbranching surface of a replica wormhole.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we consider various graphs, namely: power graph, cyclic graph,\nenhanced power graph and commuting graph, on a finite semigroup $S$. For an\narbitrary pair of these four graphs, we classify finite semigroups such that\nthe graphs in this pair are equal. In this connection, for each of the graph we\nalso give a necessary and sufficient condition on $S$ such that it is complete.\nThe work of this paper generalize the corresponding results obtained for\ngroups.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Air quality has major impact on a country's socio-economic position and\nidentifying major air pollution sources is at the heart of tackling the issue.\nSpatially and temporally distributed air quality data acquisition across a\ncountry as varied as India has been a challenge to such analysis. The launch of\nthe Sentinel-5P satellite has helped in the observation of a wider variety of\nair pollutants than measured before at a global scale on a daily basis. In this\nchapter, spatio-temporal multi pollutant data retrieved from Sentinel-5P\nsatellite is used to cluster states as well as districts in India and\nassociated average monthly pollution signature and trends depicted by each of\nthe clusters are derived and presented.The clustering signatures can be used to\nidentify states and districts based on the types of pollutants emitted by\nvarious pollution sources.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The thermophotovoltaic cells which convert the low temperature radiation into\nelectricity are of significance due to their potential applications in many\nfields. In this work, Bi2Te3/Si thermophotovoltaic cells which work under the\nradiation from the blackbody with the temperature of 300 K-480 K are presented.\nThe experimental results show that the cells can output electricity even under\nthe radiation temperature of 300 K. The band structure of Bi2Te3/Si\nheterojunctions and the defects in Bi2Te3 thin films lower the conversion\nefficiency of the cells. It is also demonstrated that the resistivity of Si and\nthe thickness of Bi2Te3 thin films have important effects on Bi2Te3/Si\nthermophotovoltaic cells. Although the cells' output power is small, this work\nprovides a possible way to utilize the low temperature radiation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a notion of iterating functions $f:X^{k}\\rightarrow X$ in a way\nthat represents recurrence relations of the form\n$a_{n+k}=f(a_{n},a_{n+1},...,a_{n+k-1})$. We define a function as\n$n$-involutory when its $n$th iterate is the identity map, and discuss\nelementary group-theoretic properties of such functions along with their\nrelation to cycles of their corresponding recurrence relations. Further, it is\nshown that a function $f:X^{k}\\rightarrow X$ that is 2-involutory in each of\nits $k$ arguments (holding others fixed) is $(k+1)$-involutory.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Fossil groups (FGs) have been discovered twenty-five years ago, and are now\ndefined as galaxy groups with an X-ray luminosity higher than $10^{42}\\\nh_{50}^{-2}$ erg s$^{-1}$ and a brightest group galaxy brighter than the other\ngroup members by at least 2 magnitudes. However, the scenario of their\nformation remains controversial. We propose here a probabilistic analysis of\nFGs, extracted from the large catalogue of candidate groups and clusters\ndetected by Sarron et al. (2018) in the CFHTLS survey, based on photometric\nredshifts, to investigate their position in the cosmic web and probe their\nenvironment. Based on spectroscopic and photometric redshifts, we estimate the\nprobability of galaxies to belong to a galaxy structure, and by imposing the\ncondition that the brightest group galaxy is at least brighter than the others\nby 2 magnitudes, we compute the probability for a given galaxy structure to be\na FG. We analyse the mass distribution of these candidate FGs, and estimate\ntheir distance to the filaments and nodes of the cosmic web in which they are\nembedded. We find that the structures with masses lower than $2.4\\times\n10^{14}$ M$_\\odot$ have the highest probabilities of being fossil groups (PFG).\nOverall, structures with PFG$\\geq$50% are located close to the cosmic web\nfilaments (87% are located at less than 1 Mpc from their nearest filament).\nThey are preferentially four times more distant from their nearest node than\nfrom their nearest filament. We confirm that FGs have small masses and are\nrare. They seem to reside closeby cosmic filaments and do not survive in nodes.\nBeing in a poor environment could therefore be the driver of FG formation, the\nnumber of nearby galaxies not being sufficient to compensate for the\ncannibalism of the central group galaxy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many ultra-diffuse galaxies (UDGs) have been discovered in the Coma cluster,\nand there is evidence that some, notably Dragonfly 44, have Milky Way-like\ndynamical masses despite dwarf-like stellar masses. We used X-ray, UV, and\noptical data to investigate the star formation and nuclear activity in the Coma\nUDGs, and we obtained deep UV and X-ray data (Swift and XMM-Newton) for\nDragonfly 44 to search for low-level star formation, hot circumgalactic gas,\nand the integrated emission from X-ray binaries. Among the Coma UDGs, we find\nUV luminosities consistent with quiescence but NUV$-r$ colors indicating star\nformation in the past Gyr. This indicates that the UDGs were recently quenched.\nThe $r$-band luminosity declines with projected distance from the Coma core.\nThe Dragonfly 44 UV luminosity is also consistent with quiescence, with\nSFR$<6\\times 10^{-4} M_{\\odot}$ yr$^{-1}$, and no X-rays are detected down to a\nsensitivity of $10^{38}$ erg s$^{-1}$. This rules out a hot corona with a $M >\n10^8 M_{\\odot}$ within the virial radius, which would normally be expected for\na dynamically massive galaxy. The absence of bright, low mass X-ray binaries is\nconsistent with the expectation from the galaxy total stellar mass, but it is\nunlikely if most low-mass X-ray binaries form in globular clusters, as\nDragonfly 44 has a very large population. Based on the UV and X-ray analysis,\nthe Coma UDGs are consistent with quenched dwarf galaxies, although we cannot\nrule out a dynamically massive population.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Aims. We investigate the physical and chemical conditions of molecular gas in\nthe circumnuclear disk (CND) region of NGC 1068. Methods. We carried out a\nspectral line survey with the IRAM 30m telescope toward the center of NGC 1068\nand mainly focused on the 2 mm band with a frequency coverage of 160.7-168.6\nGHz and 176.5-184.3 GHz. Results. Fifteen lines are detected in NGC 1068, eight\nof which are new detections for this galaxy. We derive the rotation\ntemperatures and column densities of fourteen molecular species. Conclusions.\nBased on the [HCO+ (2 - 1)]/[HOC+ (2 - 1)] ratio, we obtain a high ionization\ndegree in the CND of NGC 1068. It is found that HC3N is concentrated in the\neast knot, while 13CCH, CH3CN, SO, HOC+, CS, CH3CCH, and H2CO are concentrated\nin the west knot. Compared to the star-forming galaxies M 82 and NGC 253, the\nchemistry of NGC 1068 might be less strongly affected by the UV radiation\nfield, and its kinetic temperature might be lower.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Structural engineering of van der Waals heterostructures via stacking and\ntwisting has recently been used to create moir\\'e superlattices, enabling the\nrealization of new optical and electronic properties in solid-state systems. In\nparticular, moir\\'e lattices in twisted bilayers of transition metal\ndichalcogenides (TMDs) have been shown to lead to exciton trapping, host Mott\ninsulating and superconducting states, and act as unique Hubbard systems whose\ncorrelated electronic states can be detected and manipulated optically.\nStructurally, these twisted heterostructures also feature atomic reconstruction\nand domain formation. Unfortunately, due to the nanoscale sizes (~10 nm) of\ntypical moir\\'e domains, the effects of atomic reconstruction on the electronic\nand excitonic properties of these heterostructures could not be investigated\nsystematically and have often been ignored. Here, we use near-0$^o$ twist angle\nMoSe$_2$/MoSe$_2$ bilayers with large rhombohedral AB/BA domains to directly\nprobe excitonic properties of individual domains with far-field optics. We show\nthat this system features broken mirror/inversion symmetry, with the AB and BA\ndomains supporting interlayer excitons with out-of-plane (z) electric dipole\nmoments in opposite directions. The dipole orientation of ground-state\n$\\Gamma$-K interlayer excitons (X$_{I,1}$) can be flipped with electric fields,\nwhile higher-energy K-K interlayer excitons (X$_{I,2}$) undergo\nfield-asymmetric hybridization with intralayer K-K excitons (X$_0$). Our study\nreveals the profound impacts of crystal symmetry on TMD excitons and points to\nnew avenues for realizing topologically nontrivial systems, exotic\nmetasurfaces, collective excitonic phases, and quantum emitter arrays via\ndomain-pattern engineering.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the possibility of completing financial markets in a model\nwith no exogenous probability measure and market imperfections. A necessary and\nsufficient condition is obtained for such extension to be possible.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study a noisy oscillator with pulse delayed feedback, theoretically and in\nan electronic experimental implementation. Without noise, this system has\nmultiple stable periodic regimes. We consider two types of noise: i) phase\nnoise acting on the oscillator state variable and ii) stochastic fluctuations\nof the coupling delay. For both types of stochastic perturbations the system\nhops between the deterministic regimes, but it shows dramatically different\nscaling properties for different types of noise. The robustness to conventional\nphase noise increases with coupling strength. However for stochastic variations\nin the coupling delay, the lifetimes decrease exponentially with the coupling\nstrength. We provide an analytic explanation for these scaling properties in a\nlinearised model. Our findings thus indicate that the robustness of a system to\nstochastic perturbations strongly depends on the nature of these perturbations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper develops a mechanical tool as well as its manipulation policies\nfor 2-finger parallel robotic grippers. It primarily focuses on a mechanism\nthat converts the gripping motion of 2-finger parallel grippers into a\ncontinuous rotation to realize tasks like fastening screws. The essential\nstructure of the tool comprises a Scissor-Like Element (SLE) mechanism and a\ndouble-ratchet mechanism. They together convert repeated linear motion into\ncontinuous rotating motion. At the joints of the SLE mechanism, elastic\nelements are attached to provide resisting force for holding the tool as well\nas for producing torque output when a gripper releases the tool. The tool is\nentirely mechanical, allowing robots to use the tool without any peripherals\nand power supply. The paper presents the details of the tool design, optimizes\nits dimensions and effective stroke lengths, and studies the contacts and\nforces to achieve stable grasping and screwing. Besides the design, the paper\ndevelops manipulation policies for the tool. The policies include visual\nrecognition, picking-up and manipulation, and exchanging tooltips. The\ndeveloped tool produces clockwise rotation at the front end and\ncounter-clockwise rotation at the back end. Various tooltips can be installed\nat both two ends. Robots may employ the developed manipulation policies to\nexchange the tooltips and rotating directions following the needs of specific\nfastening or loosening tasks. Robots can also reorient the tool using\npick-and-place or handover, and move the tool to work poses using the policies.\nThe designed tool, together with the developed manipulation policies, are\nanalyzed and verified in several real-world applications. The tool is small,\ncordless, convenient, and has good robustness and adaptability.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The use of photo-activated fluorescent molecules to create long sequences of\nlow emitter-density diffraction-limited images enables high-precision emitter\nlocalization, but at the cost of low temporal resolution. We suggest combining\nSPARCOM, a recent high-performing classical method, with model-based deep\nlearning, using the algorithm unfolding approach, to design a compact neural\nnetwork incorporating domain knowledge. Our results show that we can obtain\nsuper-resolution imaging from a small number of high emitter density frames\nwithout knowledge of the optical system and across different test sets using\nthe proposed learned SPARCOM (LSPARCOM) network. We believe LSPARCOM can pave\nthe way to interpretable, efficient live-cell imaging in many settings, and\nfind broad use in single-molecule localization microscopy of biological\nstructures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper proposes a new reactive temporal logic planning algorithm for\nmultiple robots that operate in environments with unknown geometry modeled\nusing occupancy grid maps. The robots are equipped with individual sensors that\nallow them to continuously learn a grid map of the unknown environment using\nexisting Simultaneous Localization and Mapping (SLAM) methods. The goal of the\nrobots is to accomplish complex collaborative tasks, captured by global Linear\nTemporal Logic (LTL) formulas. The majority of existing LTL planning approaches\nrely on discrete abstractions of the robot dynamics operating in known\nenvironments and, as a result, they cannot be applied to the more realistic\nscenarios where the environment is initially unknown. In this paper, we address\nthis novel challenge by proposing the first reactive, abstraction-free, and\ndistributed LTL planning algorithm that can be applied for complex mission\nplanning of multiple robots operating in unknown environments. The proposed\nalgorithm is reactive, i.e., planning is adapting to the updated environmental\nmap and abstraction-free as it does not rely on designing abstractions of the\nrobot dynamics. Also, our algorithm is distributed in the sense that the global\nLTL task is decomposed into single-agent reachability problems constructed\nonline based on the continuously learned map. The proposed algorithm is\ncomplete under mild assumptions on the structure of the environment and the\nsensor models. We provide extensive numerical simulations and hardware\nexperiments that illustrate the theoretical analysis and show that the proposed\nalgorithm can address complex planning tasks for large-scale multi-robot\nsystems in unknown environments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Autofocus (AF) methods are extensively used in biomicroscopy, for example to\nacquire timelapses, where the imaged objects tend to drift out of focus. AD\nalgorithms determine an optimal distance by which to move the sample back into\nthe focal plane. Current hardware-based methods require modifying the\nmicroscope and image-based algorithms either rely on many images to converge to\nthe sharpest position or need training data and models specific to each\ninstrument and imaging configuration. Here we propose DeepFocus, an AF method\nwe implemented as a Micro-Manager plugin, and characterize its Convolutional\nneural network-based sharpness function, which we observed to be depth\nco-variant and sample-invariant. Sample invariance allows our AF algorithm to\nconverge to an optimal axial position within as few as three iterations using a\nmodel trained once for use with a wide range of optical microscopes and a\nsingle instrument-dependent calibration stack acquisition of a flat (but\narbitrary) textured object. From experiments carried out both on synthetic and\nexperimental data, we observed an average precision, given 3 measured images,\nof 0.30 +- 0.16 micrometers with a 10x, NA 0.3 objective. We foresee that this\nperformance and low image number will help limit photodamage during\nacquisitions with light-sensitive samples.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gate-induced modulation of the spin-orbit interaction (SOI) in a 1.5 nm-thick\nPd thin film grown on a ferrimagnetic insulator was investigated. Efficient\ncharge accumulation by ionic gating enables a substantial upshift in the Fermi\nlevel of the Pd film, which was corroborated by suppression of the resistivity\nin the Pd. Electromotive forces arising from the inverse spin Hall effect in Pd\nunder spin pumping were substantially modulated by the gating, in consequence\nof the modulation of the spin Hall conductivity of Pd as in an ultrathin Pt\nfilm. The same experiment using a thin Cu film, for which the band structure is\nlargely different from Pd and Pt and its SOI is quite small, provides further\nresults supporting our claim. The results obtained help in developing a\nholistic understanding of the gate-tunable SOI in solids and confirm a previous\nexplanation of the significant modulation of the spin Hall conductivity in an\nultrathin Pt film by gating.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Ehrhart quasipolynomial of a rational polytope $P$ encodes the number of\ninteger lattice points in dilates of $P$, and the $h^*$-polynomial of $P$ is\nthe numerator of the accompanying generating function. We provide two\ndecomposition formulas for the $h^*$-polynomial of a rational polytope. The\nfirst decomposition generalizes a theorem of Betke and McMullen for lattice\npolytopes. We use our rational Betke--McMullen formula to provide a novel proof\nof Stanley's Monotonicity Theorem for the $h^*$-polynomial of a rational\npolytope. The second decomposition generalizes a result of Stapledon, which we\nuse to provide rational extensions of the Stanley and Hibi inequalities\nsatisfied by the coefficients of the $h^*$-polynomial for lattice polytopes.\nLastly, we apply our results to rational polytopes containing the origin whose\nduals are lattice polytopes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This work has two main purposes. On the one side we investigate in this work\na question of H. Esnault on congruence formula in a construction of H. Esnault\nand C. Xu for the number of rational points on the closed fiber of a singular\nmodel of the projective plane over a local field. From the viewpoint of\nasymptotic analysis, the question is quite familiar with a question of N.\nKoblitz, which in turn has some meaningful applications in cryptography. We\ndon't try to solve those questions in this work, but rather concentrate on\nstudying asymptotic behaviours with very elementary techniques of generating\nfunctions. On the other side we extend the discussion on generating functions\nto the global situation, which inherit hybrid-properties from the Riemann zeta\nfunction and the $L$-function of elliptic curves. At the end we will look at an\nexample of modular forms, where we are able to prove an analytic result, which\nis similar to a result of P\\'olya for the Riemann $\\xi$-function.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that correlations between the phases of the galaxy density field in\nredshift space provide additional information about the growth rate of\nlarge-scale structure that is complementary to the power spectrum multipoles.\nIn particular, we consider the multipoles of the line correlation function\n(LCF), which correlates phases between three collinear points, and use the\nFisher forecasting method to show that the LCF multipoles can break the\ndegeneracy between the measurement of the growth rate of structure $f$ and the\namplitude of perturbations $\\sigma_8$ that is present in the power spectrum\nmultipoles at large scales. This leads to an improvement in the measurement of\n$f$ and $\\sigma_8$ by up to 220 per cent for $k_{\\rm max} = 0.15 \\,\nh\\mathrm{Mpc}^{-1}$ and up to 50 per cent for $k_{\\rm max} = 0.30 \\,\nh\\mathrm{Mpc}^{-1}$ at redshift $z=0.25$, with respect to power spectrum\nmeasurements alone for the upcoming generation of galaxy surveys like DESI and\nEuclid. The average improvements in the constraints on $f$ and $\\sigma_8$ for\n$k_{\\rm max} = 0.15 \\, h\\mathrm{Mpc}^{-1}$ are $\\sim 90$ per cent for the DESI\nBGS sample with mean redshift $\\overline{z}=0.25$, $\\sim 40$ per cent for the\nDESI ELG sample with $\\overline{z}=1.25$, and $\\sim 40$ per cent for the Euclid\nH$\\alpha$ galaxies with $\\overline{z}=1.3$. For $k_{\\rm max} = 0.30 \\,\nh\\mathrm{Mpc}^{-1}$, the average improvements are $\\sim 40$ per cent for the\nDESI BGS sample and $\\sim 20$ per cent for both the DESI ELG and Euclid\nH$\\alpha$ galaxies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The contact process with diffusion (PCPD) defined by the binary reactions 2 B\n-> 3 B, 2 B -> 0 and diffusive particle spreading exhibits an unusual active to\nabsorbing phase transition whose universality class has long been disputed.\nMultiple studies have indicated that an explicit account of particle pair\ndegrees of freedom may be required to properly capture this system's effective\nlong-time, large-scale behavior. We introduce a two-species representation in\nwhich single particles B and pairs A are coupled according to the stochastic\nreactions B + B -> A, A -> A + B, A -> 0, and A -> B + B. Mean-field analysis\nreveals that the phase transition is driven by competition and balance between\nboth species. We employ Monte Carlo simulations to demonstrate that this model\ncaptures the pertinent PCPD features. In the inactive phase, A particles\nrapidly go extinct, leaving the B species to undergo pure pair annihilation\nkinetics. At criticality, both A and B densities decay with the same exponents\nas the PCPD order parameters, and display mean-field scaling above the critical\ndimension 2. In one dimension, the critical exponents for the B species\nobtained from seed simulations agree well with previously reported exponent\nvalues. We demonstrate that the scaling properties of consecutive particle\npairs in the PCPD are identical with that of the A species in the coupled\nmodel. This two-species picture resolves the conceptual difficulty for seed\nsimulations in the original PCPD and naturally introduces multiple length and\ntime scales, which cause strong corrections to scaling. The extracted moment\nratios from our simulations indicate that our model displays the same temporal\ncrossover behavior as the PCPD, which further corroborates its full dynamical\nequivalence with our coupled model.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We theoretically investigate the piezo-optic effect of high-harmonic\ngeneration (HHG) in shear-strained semiconductors. By focusing on a typical\nsemiconductor, GaAs, we show that there is optical activity, meaning different\nresponses to right-handed and left-handed elliptically polarized electric\nfields. We also show that this optical activity is more pronounced for higher\nharmonics whose perturbative order exceeds the band-gap energy. These findings\npoint to a useful pathway for strain engineering of nonlinear optics to control\nthe reciprocity of HHG.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove new results on upper and lower limits of real-valued functions by\nmeans of $\\psi$-densities introduced by P. D. Barry in 1962. This allows us to\nimprove several existing results on the growth of non-decreasing and unbounded\nreal-valued functions in sets of positive density. The $\\psi$-densities are\nalso used to introduce a new concept of a limit for real-valued functions. The\nresults in this paper are of interest in real analysis as well as in the theory\nof meromorphic functions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We formulate a statistical wave-mechanical approach to describe dissipation\nand instabilities in two-dimensional turbulent flows of magnetized plasmas and\natmospheric fluids, such as drift and Rossby waves. This is made possible by\nthe existence of Hilbert space, associated with the electric potential of\nplasma or stream function of atmospheric fluid. We therefore regard such\nturbulent flows as macroscopic wave-mechanical phenomena, driven by the\nnon-Hermitian Hamiltonian operator we derive, whose anti-Hermitian component is\nattributed to an effect of the environment. Introducing a wave-mechanical\ndensity operator for the statistical ensembles of waves, we formulate master\nequations and define observables: such as the enstrophy and energy of both the\nwaves and zonal flow as statistical averages. We establish that our open system\ncan generally follow two types of time evolution, depending on whether the\nenvironment hinders or assists the system's stability and integrity. We also\nconsider a phase-space formulation of the theory, including the\ngeometrical-optic limit and beyond, and study the conservation laws of physical\nobservables. It is thus shown that the approach predicts various mechanisms of\nenergy and enstrophy exchange between drift waves and zonal flow, which were\nhitherto overlooked in models based on wave kinetic equations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Underwater wireless optical communication is one of the critical technologies\nfor buoy-based high-speed cross-sea surface communication, where the\ncommunication nodes are vertically deployed. Due to the vertically\ninhomogeneous nature of the underwater environment, seawater is usually\nvertically divided into multiple layers with different parameters that reflect\nthe real environment. In this work, we consider a generalized UWOC channel\nmodel that contains$N$ layers. To capture the effects of air bubbles and\ntemperature gradients on channel statistics, we model each layer by a mixture\nExponential-Generalized Gamma(EGG) distribution. We derive the PDF and CDF of\nthe end-to-end SNR in exact closed-form. Then, unified BER and outage\nexpressions using OOK and BPSK are also derived. The performance and behavior\nof common vertical underwater optical communication scenarios are thoroughly\nanalyzed through the appropriate selection of parameters. All the derived\nexpressions are verified via Monte Carlo simulations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $m\\ge 1$ be an integer and $G$ be a graph with $m$ edges. We say that $G$\nhas an antimagic orientation if $G$ has an orientation $D$ and a bijection\n$\\tau:A(D)\\rightarrow \\{1,2,\\cdots,m\\}$ such that no two vertices in $D$ have\nthe same vertex-sum under $\\tau$, where the vertex-sum of a vertex $u$ in $D$\nunder $\\tau$ is the sum of labels of all arcs entering $u$ minus the sum of\nlabels of all arcs leaving $u$. Hefetz, M\\\"{u}tze and Schwartz [J. Graph\nTheory, 64: 219-232, 2010] conjectured that every connected graph admits an\nantimagic orientation. The conjecture was confirmed for certain classes of\ngraphs such as dense graphs, regular graphs, and trees including caterpillars\nand $k$-ary trees. In this note, we prove that every lobster admits an\nantimagic orientation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  40 days after the start of the international monitoring of COVID-19, we\nsearch for the effect of official announcements regarding new cases of\ninfection and death ratio on the financial markets volatility index (VIX).\nWhereas the new cases reported in China and outside China have a mixed effect\non financial volatility, the death ratio positively influences VIX, that\noutside China triggering a more important impact. In addition, the higher the\nnumber of affected countries, the higher the financial volatility is.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An experimental procedure for studying soliton gases in shallow water is\ndevised. Nonlinear waves propagate at constant depth in a 34\\,m-long wave\nflume. At one end of the flume, the waves are generated by a piston-type\nwave-maker. The opposite end is a vertical wall. Wave interactions are recorded\nwith a video system using seven side-looking cameras with a pixel resolution of\n1\\,mm, covering 14\\,m of the flume. The accuracy in the detection of the water\nsurface elevation is shown to be better than 0.1 mm. A continuous monochromatic\nforcing can lead to a random state such as a soliton gas. The measured wave\nfield is separated into right- and left-propagating waves in the Radon space\nand solitary pulses are identified as solitons of KdV or Rayleigh types. Both\nweak and strong interactions of solitons are detected. These interactions\ninduce phase shifts that constitute the seminal mechanism for disorganization\nand soliton gas formation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we present a quantum secret sharing scheme based on Bell state\nentanglement and sequential projection measurements. The protocol verifies the\n$n$ out of $n$ scheme and supports the aborting of the protocol in case all the\nparties do not divulge in their valid measurement outcomes. The operator-qubit\npair forms an integral part of the scheme determining the classical secret to\nbe shared. The protocol is robust enough to neutralize any eavesdropping on a\nparticular qubit of the dealer. The experimental demonstration of the scheme is\ndone on IBM-QE cloud platform with backends \\texttt{IBMQ\\_16\\_Melbourne} and\n\\texttt{IBMQ\\_QASM\\_SIMULATOR\\_V0.1.547} simulator. The security analysis\nperformed on the scheme and the comparative analysis supports our claim of a\nstringent and an efficient scheme as compared to some recent quantum and\nsemi-quantum techniques of secret sharing.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The availability of shared software models provides opportunities for\nreusing, adapting and learning from them. Public models are typically stored in\na variety of locations, including model repositories, regular source code\nrepositories, web pages, etc. To profit from them developers need effective\nsearch mechanisms to locate the models relevant for their tasks. However, to\ndate, there has been little success in creating a generic and efficient search\nengine specially tailored to the modelling domain.\n  In this paper we present MAR, a search engine for models. MAR is generic in\nthe sense that it can index any type of model if its meta-model is known. MAR\nuses a query-by-example approach, that is, it uses example models as queries.\nThe search takes the model structure into account using the notion of bag of\npaths, which encodes the structure of a model using paths between model\nelements and is a representation amenable for indexing. MAR is built over HBase\nusing a specific design to deal with large repositories. Our benchmarks show\nthat the engine is efficient and has fast response times in most cases. We have\nalso evaluated the precision of the search engine by creating model mutants\nwhich simulate user queries. A REST API is available to perform queries and an\nEclipse plug-in allows end users to connect to the search engine from model\neditors. We have currently indexed more than 50.000 models of different kinds,\nincluding Ecore meta-models, BPMN diagrams and UML models. MAR is available at\nhttp://mar-search.org.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study online learning in repeated first-price auctions with censored\nfeedback, where a bidder, only observing the winning bid at the end of each\nauction, learns to adaptively bid in order to maximize her cumulative payoff.\nTo achieve this goal, the bidder faces a challenging dilemma: if she wins the\nbid--the only way to achieve positive payoffs--then she is not able to observe\nthe highest bid of the other bidders, which we assume is iid drawn from an\nunknown distribution. This dilemma, despite being reminiscent of the\nexploration-exploitation trade-off in contextual bandits, cannot directly be\naddressed by the existing UCB or Thompson sampling algorithms.\n  In this paper, by exploiting the structural properties of first-price\nauctions, we develop the first learning algorithm that achieves\n$O(\\sqrt{T}\\log^{2.5} T)$ regret bound, which is minimax optimal up to $\\log$\nfactors, when the bidder's private values are stochastically generated. We do\nso by providing an algorithm on a general class of problems, called the\npartially ordered contextual bandits, which combine the graph feedback across\nactions, the cross learning across contexts, and a partial order over the\ncontexts. We establish both strengths and weaknesses of this framework, by\nshowing a curious separation that a regret nearly independent of the\naction/context sizes is possible under stochastic contexts, but is impossible\nunder adversarial contexts. Despite the limitation of this general framework,\nwe further exploit the structure of first-price auctions and develop a learning\nalgorithm that operates sample-efficiently (and computationally efficiently) in\nthe presence of adversarially generated private values. We establish an\n$O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm, hence providing a\ncomplete characterization of optimal learning guarantees for first-price\nauctions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In 2010, Olson \\& Robinson [Transactions of the American Mathematical\nSociety, 362(1), 145-168] introduced the notion of an almost homogeneous metric\nspace and showed that if $X$ is a subset of a Hilbert space such that $X-X$ is\nalmost homogeneous, then $X$ admits almost bi--Lipschitz embeddings into\nEuclidean spaces. In this paper, we extend this result and we show that if $X$\nis a subset of a Banach space such that $X-X$ is almost homogeneous at the\norigin, then $X$ can be embedded in a Euclidean space in an almost\nbi--Lipschitz way.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a string theory construction which allows us to study properties\nof the potential of two heavy quarks coupled to a light quark. In such a case,\nthe potential is a function of separation between the heavy quarks. The results\nshow the universality of the string tension and factorization at small\nseparations expected from heavy quark-diquark symmetry. In addition, we make an\nestimate of the string breaking distance. With the parameter values we use,\nthis distance is found to be almost the same as that for the heavy\nquark-antiquark potential. We also discuss the heavy quark-quark potential and\nits relation to Lipkin rule.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In a level-5 autonomous driving system, the autonomous driving vehicles (AVs)\nare expected to sense the surroundings via analyzing a large amount of data\ncaptured by a variety of onboard sensors in near-real-time. As a result,\nenormous computing costs will be introduced to the AVs for processing the tasks\nwith the deployed machine learning (ML) model, while the inference accuracy may\nnot be guaranteed. In this context, the advent of edge intelligence (EI) and\nsixth-generation (6G) wireless networking are expected to pave the way to more\nreliable and safer autonomous driving by providing multi-access edge computing\n(MEC) together with ML to AVs in close proximity. To realize this goal, we\npropose a two-tier EI-empowered autonomous driving framework. In the\nautonomous-vehicles tier, the autonomous vehicles are deployed with the shallow\nlayers by splitting the trained deep neural network model. In the\nedge-intelligence tier, an edge server is implemented with the remaining layers\n(also deep layers) and an appropriately trained multi-task learning (MTL)\nmodel. In particular, obtaining the optimal offloading strategy (including the\nbinary offloading decision and the computational resources allocation) can be\nformulated as a mixed-integer nonlinear programming (MINLP) problem, which is\nsolved via MTL in near-real-time with high accuracy. On another note, an\nedge-vehicle joint inference is proposed through neural network segmentation to\nachieve efficient online inference with data privacy-preserving and less\ncommunication delay. Experiments demonstrate the effectiveness of the proposed\nframework, and open research topics are finally listed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A new collaborative learning, called split learning, was recently introduced,\naiming to protect user data privacy without revealing raw input data to a\nserver. It collaboratively runs a deep neural network model where the model is\nsplit into two parts, one for the client and the other for the server.\nTherefore, the server has no direct access to raw data processed at the client.\nUntil now, the split learning is believed to be a promising approach to protect\nthe client's raw data; for example, the client's data was protected in\nhealthcare image applications using 2D convolutional neural network (CNN)\nmodels. However, it is still unclear whether the split learning can be applied\nto other deep learning models, in particular, 1D CNN.\n  In this paper, we examine whether split learning can be used to perform\nprivacy-preserving training for 1D CNN models. To answer this, we first design\nand implement an 1D CNN model under split learning and validate its efficacy in\ndetecting heart abnormalities using medical ECG data. We observed that the 1D\nCNN model under split learning can achieve the same accuracy of 98.9\\% like the\noriginal (non-split) model. However, our evaluation demonstrates that split\nlearning may fail to protect the raw data privacy on 1D CNN models. To address\nthe observed privacy leakage in split learning, we adopt two privacy leakage\nmitigation techniques: 1) adding more hidden layers to the client side and 2)\napplying differential privacy. Although those mitigation techniques are helpful\nin reducing privacy leakage, they have a significant impact on model accuracy.\nHence, based on those results, we conclude that split learning alone would not\nbe sufficient to maintain the confidentiality of raw sequential data in 1D CNN\nmodels.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we investigate the performance of a mixed\nradio-frequency-underwater wireless optical communication (RF-UWOC) system\nwhere an unmanned aerial vehicle (UAV), as a low-altitude mobile aerial base\nstation, transmits information to an autonomous underwater vehicle (AUV)\nthrough a fixed-gain amplify-and-forward (AF) or decode-and-forward (DF) relay.\nOur analysis accounts for the main factors that affect the system performance,\nsuch as the UAV height, air bubbles, temperature gradient, water salinity\nvariations, and detection techniques. Employing fixed-gain AF relaying and DF\nrelaying, we derive closed-form expressions for some key performance metrics,\ne.g., outage probability (OP), average bit error rate (ABER), and average\nchannel capacity (ACC). In addition, in order to get further insights,\nasymptotic analyses for the OP and ABER are also carried out. Furthermore,\nassuming DF relaying, we derive analytical expressions for the optimal UAV\naltitude that minimizes the OP. Simulation results show that the UAV altitude\ninfluences the system performance and there is an optimal altitude which\nensures a minimum OP. Moreover, based on the asymptotic results, it is\ndemonstrated that the diversity order of fixed-gain AF relaying and DF relaying\nare respectively determined by the RF link and by the detection techniques of\nthe UWOC link.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Given pointed $CW$-complexes $X$ and $Y$, $\\rmph(X, Y)$ denotes the set of\nhomotopy classes of phantom maps from $X$ to $Y$ and $\\rmsph(X, Y)$ denotes the\nsubset of $\\rmph(X, Y)$ consisting of homotopy classes of special phantom maps.\nIn a preceding paper, we gave a sufficient condition such that $\\rmph(X, Y)$\nand $\\rmsph(X, Y)$ have natural group structures and established a formula for\ncalculating the groups $\\rmph(X, Y)$ and $\\rmsph(X, Y)$ in many cases where the\ngroups $[X,\\Omega \\widehat{Y}]$ are nontrivial. In this paper, we establish a\ndual version of the formula, in which the target is the total space of a\nfibration, to calculate the groups $\\rmph(X, Y)$ and $\\rmsph(X, Y)$ for pairs\n$(X,Y)$ to which the formula or existing methods do not apply. In particular,\nwe calculate the groups $\\rmph(X,Y)$ and $\\rmsph(X,Y)$ for pairs $(X,Y)$ such\nthat $X$ is the classifying space $BG$ of a compact Lie group $G$ and $Y$ is a\nhighly connected cover $Y' \\langle n \\rangle$ of a nilpotent finite complex\n$Y'$ or the quotient $\\gbb / H$ of $\\gbb = U, O$ by a compact Lie group $H$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The effects of resonant magnetic perturbations on the turbulent transport of\nfast ions in tokamak devices are investigated using a theoretical transport\nmodel of test-particle type. The direct numerical simulation method is used to\ncompute, via the transport model, the diffusion coefficients. The numerical\nresults are in good agreement with other, analytically derived, estimations. It\nis found that finite Larmor radius effects decrease algebraically the\ntransport, while the amplitude of magnetic perturbations has an opposite\neffect. In the presence of stochastic dynamics, the asymmetric toroidal\nmagnetic field induces a small, radial, outward pinch. A synergistic mechanism\nof non-linear coupling between turbulence and magnetic perturbations enhances\nthe radial diffusion. General scaling laws are proposed for the transport\ncoefficients.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A search for direct pair production of scalar partners of the top quark (top\nsquarks or scalar third-generation up-type leptoquarks) in the all-hadronic\n$t\\bar{t}$ plus missing transverse momentum final state is presented. The\nanalysis of 139 fb$^{-1}$ of ${\\sqrt{s}=13}$ TeV proton-proton collision data\ncollected using the ATLAS detector at the LHC yields no significant excess over\nthe Standard Model background expectation. To interpret the results, a\nsupersymmetric model is used where the top squark decays via $\\tilde{t} \\to\nt^{(*)} \\tilde{\\chi}^0_1$, with $t^{(*)}$ denoting an on-shell (off-shell) top\nquark and $\\tilde{\\chi}^0_1$ the lightest neutralino. Three specific event\nselections are optimised for the following scenarios. In the scenario where\n$m_{\\tilde{t}}> m_t+m_{\\tilde{\\chi}^0_1}$, top squark masses are excluded in\nthe range 400-1250 GeV for $\\tilde{\\chi}^0_1$ masses below $200$ GeV at 95 %\nconfidence level. In the situation where $m_{\\tilde{t}}\\sim\nm_t+m_{\\tilde{\\chi}^0_1}$, top squark masses in the range 300-630 GeV are\nexcluded, while in the case where $m_{\\tilde{t}}< m_W+m_b+m_{\\tilde{\\chi}^0_1}$\n(with $m_{\\tilde{t}}-m_{\\tilde{\\chi}^0_1}\\ge 5$ GeV), considered for the first\ntime in an ATLAS all-hadronic search, top squark masses in the range 300-660\nGeV are excluded. Limits are also set for scalar third-generation up-type\nleptoquarks, excluding leptoquarks with masses below $1240$ GeV when\nconsidering only leptoquark decays into a top quark and a neutrino.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Systems using 5G are expected to be used in various cases of Society 5.0 and\nIndustrie 4.0 such as smart cities, smart factories, and also critical\ninfrastructures. These systems are essential for our life, thus cyberattacks\nagainst the system must be prevented. In this paper, we tackle two problems\nposed by 5G features: system construction using multi-vendor devices and\nsoftwarized functions. Specifically, there are supply-chain risks that\nmalicious devices are used in the construction phase. Moreover, the softwarized\nnetwork functions are easy to be attacked compared to hardware. To cope with\nthese problems, we propose a concept of architecture comprising a blockchain to\nrecord security events including supply-chain information and a tamper\ndetection engine to ensure the integrity of software components in 5G system.\nWe implement the initial prototype of the architecture and show its\nfeasibility.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Identifying similar protein sequences is a core step in many computational\nbiology pipelines such as detection of homologous protein sequences, generation\nof similarity protein graphs for downstream analysis, functional annotation and\ngene location. Performance and scalability of protein similarity searches have\nproven to be a bottleneck in many bioinformatics pipelines due to increases in\ncheap and abundant sequencing data. This work presents a new distributed-memory\nsoftware, PASTIS. PASTIS relies on sparse matrix computations for efficient\nidentification of possibly similar proteins. We use distributed sparse matrices\nfor scalability and show that the sparse matrix infrastructure is a great fit\nfor protein similarity searches when coupled with a fully-distributed\ndictionary of sequences that allows remote sequence requests to be fulfilled.\nOur algorithm incorporates the unique bias in amino acid sequence substitution\nin searches without altering the basic sparse matrix model, and in turn,\nachieves ideal scaling up to millions of protein sequences.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With the widespread adoption of the quantified self movement, an increasing\nnumber of users rely on mobile applications to monitor their physical activity\nthrough their smartphones. Granting to applications a direct access to sensor\ndata expose users to privacy risks. Indeed, usually these motion sensor data\nare transmitted to analytics applications hosted on the cloud leveraging\nmachine learning models to provide feedback on their health to users. However,\nnothing prevents the service provider to infer private and sensitive\ninformation about a user such as health or demographic attributes.In this\npaper, we present DySan, a privacy-preserving framework to sanitize motion\nsensor data against unwanted sensitive inferences (i.e., improving privacy)\nwhile limiting the loss of accuracy on the physical activity monitoring (i.e.,\nmaintaining data utility). To ensure a good trade-off between utility and\nprivacy, DySan leverages on the framework of Generative Adversarial Network\n(GAN) to sanitize the sensor data. More precisely, by learning in a competitive\nmanner several networks, DySan is able to build models that sanitize motion\ndata against inferences on a specified sensitive attribute (e.g., gender) while\nmaintaining a high accuracy on activity recognition. In addition, DySan\ndynamically selects the sanitizing model which maximize the privacy according\nto the incoming data. Experiments conducted on real datasets demonstrate that\nDySan can drasticallylimit the gender inference to 47% while only reducing the\naccuracy of activity recognition by 3%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  While low-luminosity galaxies dominate number counts at all redshifts, their\ncontribution to cosmic Reionization is poorly understood due to a lack of\nknowledge of their physical properties. We isolate a sample of 35 z~4-5\ncontinuum-faint Lyman-alpha emitters from deep VLT/MUSE spectroscopy and\ndirectly measure their Halpha emission using stacked Spitzer/IRAC Ch. 1\nphotometry. Based on Hubble Space Telescope imaging, we determine that the\naverage UV continuum magnitude is fainter than -16 (~0.01 L_star), implying a\nmedian Lyman-alpha equivalent width of 249 Angstroms. By combining the Halpha\nmeasurement with the UV magnitude we determine the ionizing photon production\nefficiency, xi_ion, a first for such faint galaxies. The measurement of log\n(xi_ion [Hz/erg]) = 26.28 (+0.28; -0.40) is in excess of literature\nmeasurements of both continuum- and emission line-selected samples, implying a\nmore efficient production of ionizing photons in these lower-luminosity,\nLyman-alpha-selected systems. We conclude that this elevated efficiency can be\nexplained by stellar populations with metallicities between 4e-4 and 0.008,\nwith light-weighted ages less than 3 Myr.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We perform a complete study of the low-energy phenomenology of $S_1$ and\n$S_3$ lepto-quarks, aimed at addressing the observed deviations in $B$-meson\ndecays and the muon magnetic dipole moment. Leptoquark contributions to\nobservables are computed at one-loop accuracy in an effective field theory\napproach, using the recently published complete one-loop matching of these\nleptoquarks to the Standard Model effective field theory. We present several\nscenarios, discussing in each case the preferred parameter space and the most\nrelevant observables.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Field-induced excitation gaps in quantum spin chains are an interesting\nphenomenon related to confinements of topological excitations. In this paper, I\npresent a novel type of this phenomenon. I show that an effective magnetic\nfield with a fourfold screw symmetry induces the excitation gap accompanied by\ndimer orders. The gap and dimer orders induced so exhibit characteristic\npower-law dependence on the fourfold screw-symmetric field. Moreover, the\nfield-induced dimer order and the field-induced N\\'eel order coexist when the\nexternal uniform magnetic field, the fourfold screw-symmetric field, and the\ntwofold staggered field are applied. This situation is in close connection with\na compound [Cu(pym)(H$_2$O)$_4$]SiF$_6$ [J. Liu et al., Phys. Rev. Lett. 122,\n057207 (2019)]. In this paper, I discuss a mechanism of field-induced dimer\norders by using a density-matrix renormalization group method, a perturbation\ntheory, and quantum field theories.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Global Positioning System (GPS) and inertial measurement unit (IMU) sensors\nare commonly integrated using the extended Kalman filter (EKF), for achieving\nbetter navigation performance. However, because of nonlinearity, the\nperformance of the EKF is affected by the initial state estimation errors, and\nthe navigation solutions, including the attitude, diverge rapidly as the\ninitial errors increase. This paper analyzes the data obtained from an outdoor\nexperiment, and investigates the effect of the initial errors on the attitude\nestimation performance using EKF, which is used in loosely coupled low-cost\nsmartphone GPS/IMU sensors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword\ntokenization process of language models as it provides multiple benefits.\nHowever, this process is solely based on pre-training data statistics, making\nit hard for the tokenizer to handle infrequent spellings. On the other hand,\nthough robust to misspellings, pure character-level models often lead to\nunreasonably long sequences and make it harder for the model to learn\nmeaningful words. To alleviate these challenges, we propose a character-based\nsubword module (char2subword) that learns the subword embedding table in\npre-trained models like BERT. Our char2subword module builds representations\nfrom characters out of the subword vocabulary, and it can be used as a drop-in\nreplacement of the subword embedding table. The module is robust to\ncharacter-level alterations such as misspellings, word inflection, casing, and\npunctuation. We integrate it further with BERT through pre-training while\nkeeping BERT transformer parameters fixed--and thus, providing a practical\nmethod. Finally, we show that incorporating our module to mBERT significantly\nimproves the performance on the social media linguistic code-switching\nevaluation (LinCE) benchmark.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, the search for an axion insulator state in the ferromagnet-3D\ntopological insulator (TI) heterostructure and $\\mathrm{MnBi_2Te_4}$ has\nattracted intensive interest. However, its detection remains difficult in\nexperiments. We systematically investigate the disorder-induced phase\ntransition of the axion insulator state in a 3D TI with antiparallel\nmagnetization alignment surfaces. It is found that there exists a 2D\ndisorder-induced phase transition which shares the same universality class with\nthe quantum Hall plateau to plateau transition. Then, we provide a\nphenomenological theory which maps the random mass Dirac Hamiltonian of the\naxion insulator state into the Chalker-Coddington network model. Therefore, we\npropose to probe the axion insulator state by investigating the universal\nsignature of such a phase transition in the ferromagnet-3D TI heterostructure\nand $\\mathrm{MnBi_2Te_4}$. Our findings not only show a global phase diagram of\nthe axion insulator state, but also provide a new experimental routine to probe\nit.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gas and vapour explosions have been involved in industrial accidents since\nthe beginnings of industry. A century ago, at 11:55 am on Friday 24th September\n1920, the petroleum barge Warwick exploded in London's docklands and seven men\nwere killed. Understanding what happened when it blew up as it was being\nrefurbished, and how to prevent similar explosions, involves fluid mechanics\nand thermodynamics plus chemistry. I recount the 1920 accident as an example,\ntogether with the history of thermo-kinetic explosions prior to 1920 and up to\nthe present day, and I review the history and the actual state of the science\nof explosion and the roles of fluid mechanics, thermodynamics, and chemistry in\nthat science. The science of explosions has been aware of its societal\nimplications from the beginning, but, despite advances in health and safety\nover the past century, is there still work to do?\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove through Monte Carlo analysis that the covariant euclidean scalar\nfield theory, $\\varphi^r_n$, where $r$ denotes the power of the interaction\nterm and $n = s + 1$ where $s$ is the spatial dimension and $1$ adds imaginary\ntime, such that $r = 12, n = 3$ can be acceptably quantized using scaled affine\nquantization and the resulting theory is nontrivial, unlike what happens using\ncanonical quantization when the system is plagued by asymptotic freedom.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Antibodies recognizing complexes of the chemokine platelet factor 4\n(PF4-CXCL4) and polyanions (P) opsonize PF4-coated bacteria hereby mediating\nbacterial host defense. A subset of these antibodies may activate platelets\nafter binding to PF4-heparin complexes, causing the prothrombotic adverse drug\nreaction heparin-induced thrombocytopenia (HIT). In autoimmune-HIT,\nanti-PF4-P-antibodies activate platelets in the absence of heparin. Here we\nshow that antibodies with binding forces of approximately 60-100 pN activate\nplatelets in the presence of polyanions, while a subset of antibodies from\nautoimmune-HIT patients with binding forces greater than 100 pN binds to PF4\nalone in the absence of polyanions. These antibodies with high binding forces\ncluster PF4-molecules forming antigenic complexes which allow binding of\npolyanion-dependent anti-PF4-P-antibodies. The resulting immunocomplexes induce\nmassive platelet activation in the absence of heparin. Antibody-mediated\nchanges in endogenous proteins that trigger binding of otherwise non-pathogenic\n(or cofactor-dependent) antibodies may also be relevant in other\nantibody-mediated autoimmune disorders.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present TDNet, a temporally distributed network designed for fast and\naccurate video semantic segmentation. We observe that features extracted from a\ncertain high-level layer of a deep CNN can be approximated by composing\nfeatures extracted from several shallower sub-networks. Leveraging the inherent\ntemporal continuity in videos, we distribute these sub-networks over sequential\nframes. Therefore, at each time step, we only need to perform a lightweight\ncomputation to extract a sub-features group from a single sub-network. The full\nfeatures used for segmentation are then recomposed by application of a novel\nattention propagation module that compensates for geometry deformation between\nframes. A grouped knowledge distillation loss is also introduced to further\nimprove the representation power at both full and sub-feature levels.\nExperiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method\nachieves state-of-the-art accuracy with significantly faster speed and lower\nlatency.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n  Compared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a proposal of a faster Wasserstein $k$-means algorithm\nfor histogram data by reducing Wasserstein distance computations and exploiting\nsparse simplex projection. We shrink data samples, centroids, and the ground\ncost matrix, which leads to considerable reduction of the computations used to\nsolve optimal transport problems without loss of clustering quality.\nFurthermore, we dynamically reduced the computational complexity by removing\nlower-valued data samples and harnessing sparse simplex projection while\nkeeping the degradation of clustering quality lower. We designate this proposed\nalgorithm as sparse simplex projection based Wasserstein $k$-means, or SSPW\n$k$-means. Numerical evaluations conducted with comparison to results obtained\nusing Wasserstein $k$-means algorithm demonstrate the effectiveness of the\nproposed SSPW $k$-means for real-world datasets\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we provide a guideline for using the Neural Network\nDependability Kit (NNDK) during the development process of NN models, and show\nhow the algorithm is applied in two image classification use cases. The case\nstudies demonstrate the usage of the dependability kit to obtain insights about\nthe NN model and how they informed the development process of the neural\nnetwork model. After interpreting neural networks via the different metrics\navailable in the NNDK, the developers were able to increase the NNs' accuracy,\ntrust the developed networks, and make them more robust. In addition, we\nobtained a novel application-oriented technique to provide supporting evidence\nfor an NN's classification result to the user. In the medical image\nclassification use case, it was used to retrieve case images from the training\ndataset that were similar to the current patient's image and could therefore\nact as a support for the NN model's decision and aid doctors in interpreting\nthe results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Charge transport processes at interfaces which are governed by complex\ninterfacial electronic structure play a crucial role in catalytic reactions,\nenergy storage, photovoltaics, and many biological processes. Here, the first\nsoft X-ray second harmonic generation (SXR-SHG) interfacial spectrum of a\nburied interface (boron/Parylene-N) is reported. SXR-SHG shows distinct\nspectral features that are not observed in X-ray absorption spectra,\ndemonstrating its extraordinary interfacial sensitivity. Comparison to\nelectronic structure calculations indicates a boron-organic separation distance\nof 1.9 {\\AA}, wherein changes as small as 0.1 {\\AA} result in easily detectable\nSXR-SHG spectral shifts (ca. 100s of meV). As SXR-SHG is inherently ultrafast\nand sensitive to individual atomic layers, it creates the possibility to study\na variety of interfacial processes, e.g. catalysis, with ultrafast time\nresolution and bond specificity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate partial functions and computability theory from within a\nconstructive, univalent type theory. The focus is on placing computability into\na larger mathematical context, rather than on a complete development of\ncomputability theory. We begin with a treatment of partial functions, using the\nnotion of dominance, which is used in synthetic domain theory to discuss\nclasses of partial maps. We relate this and other ideas from synthetic domain\ntheory to other approaches to partiality in type theory. We show that the\nnotion of dominance is difficult to apply in our setting: the set of\n$\\Sigma_0^1$ propositions investigated by Rosolini form a dominance precisely\nif a weak, but nevertheless unprovable, choice principle holds. To get around\nthis problem, we suggest an alternative notion of partial function we call\ndisciplined maps. In the presence of countable choice, this notion coincides\nwith Rosolini's.\n  Using a general notion of partial function, we take the first steps in\nconstructive computability theory. We do this both with computability as\nstructure, where we have direct access to programs; and with computability as\nproperty, where we must work in a program-invariant way. We demonstrate the\ndifference between these two approaches by showing how these approaches relate\nto facts about computability theory arising from topos-theoretic and\ntype-theoretic concerns. Finally, we tie the two threads together: assuming\ncountable choice and that all total functions $\\mathbb{N}\\to\\mathbb{N}$ are\ncomputable (both of which hold in the effective topos), the Rosolini partial\nfunctions, the disciplined maps, and the computable partial functions all\ncoincide. We observe, however, that the class of all partial functions includes\nnon-computable partial functions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  One of the most challenging problems in solid state systems is the\nmicroscopic analysis of electronic correlations. A paramount minimal model that\nencodes correlation effects is the Hubbard Hamiltonian, which -- albeit its\nsimplicity -- is exactly solvable only in a few limiting cases and approximate\nmany-body methods are required for its solution. In this review we present an\noverview on the non-perturbative Two-Particle Self-Consistent method (TPSC)\nwhich was originally introduced to describe the electronic properties of the\nsingle-band Hubbard model. We introduce here a detailed derivation of the\nmulti-orbital generalization of TPSC and discuss particular features of the\nmethod on exemplary interacting models in comparison to dynamical mean-field\ntheory results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Animals exhibit an innate ability to learn regularities of the world through\ninteraction. By performing experiments in their environment, they are able to\ndiscern the causal factors of variation and infer how they affect the world's\ndynamics. Inspired by this, we attempt to equip reinforcement learning agents\nwith the ability to perform experiments that facilitate a categorization of the\nrolled-out trajectories, and to subsequently infer the causal factors of the\nenvironment in a hierarchical manner. We introduce {\\em causal curiosity}, a\nnovel intrinsic reward, and show that it allows our agents to learn optimal\nsequences of actions and discover causal factors in the dynamics of the\nenvironment. The learned behavior allows the agents to infer a binary quantized\nrepresentation for the ground-truth causal factors in every environment.\nAdditionally, we find that these experimental behaviors are semantically\nmeaningful (e.g., our agents learn to lift blocks to categorize them by\nweight), and are learnt in a self-supervised manner with approximately 2.5\ntimes less data than conventional supervised planners. We show that these\nbehaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or\nother downstream tasks). Finally, we show that the knowledge of causal factor\nrepresentations aids zero-shot learning for more complex tasks. Visit\nhttps://sites.google.com/usc.edu/causal-curiosity/home for website.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove convergence of multiple interfaces in the critical planar q = 2\nrandom cluster model, and provide an explicit description of the scaling limit.\nRemarkably, the expression for the partition function of the resulting multiple\nSLE(16/3) coincides with the bulk spin correlation in the critical Ising model\nin the half-plane, after formally replacing a position of each spin and its\ncomplex conjugate with a pair of points on the real line. As a corollary, we\nrecover Belavin-Polyakov-Zamolodchikov equations for the spin correlations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An important experimental design problem in early-stage drug discovery is how\nto prioritize available compounds for testing when very little is known about\nthe target protein. Informer based ranking (IBR) methods address the\nprioritization problem when the compounds have provided bioactivity data on\nother potentially relevant targets. An IBR method selects an informer set of\ncompounds, and then prioritizes the remaining compounds on the basis of new\nbioactivity experiments performed with the informer set on the target. We\nformalize the problem as a two-stage decision problem and introduce the Bayes\nOptimal Informer SEt (BOISE) method for its solution. BOISE leverages a\nflexible model of the initial bioactivity data, a relevant loss function, and\neffective computational schemes to resolve the two-step design problem. We\nevaluate BOISE and compare it to other IBR strategies in two retrospective\nstudies, one on protein-kinase inhibition and the other on anti-cancer drug\nsensitivity. In both empirical settings BOISE exhibits better predictive\nperformance than available methods. It also behaves well with missing data,\nwhere methods that use matrix completion show worse predictive performance. We\nprovide an R implementation of BOISE at\nhttps://github.com/wiscstatman/esdd/BOISE\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In crowd counting, each training image contains multiple people, where each\nperson is annotated by a dot. Existing crowd counting methods need to use a\nGaussian to smooth each annotated dot or to estimate the likelihood of every\npixel given the annotated point. In this paper, we show that imposing Gaussians\nto annotations hurts generalization performance. Instead, we propose to use\nDistribution Matching for crowd COUNTing (DM-Count). In DM-Count, we use\nOptimal Transport (OT) to measure the similarity between the normalized\npredicted density map and the normalized ground truth density map. To stabilize\nOT computation, we include a Total Variation loss in our model. We show that\nthe generalization error bound of DM-Count is tighter than that of the Gaussian\nsmoothed methods. In terms of Mean Absolute Error, DM-Count outperforms the\nprevious state-of-the-art methods by a large margin on two large-scale counting\ndatasets, UCF-QNRF and NWPU, and achieves the state-of-the-art results on the\nShanghaiTech and UCF-CC50 datasets. DM-Count reduced the error of the\nstate-of-the-art published result by approximately 16%. Code is available at\nhttps://github.com/cvlab-stonybrook/DM-Count.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The use of multiple drugs accounts for almost 30% of all hospital admission\nand is the 5th leading cause of death in America. Since over 30% of all adverse\ndrug events (ADEs) are thought to be caused by drug-drug interactions (DDI),\nbetter identification and prediction of administration of known DDIs in primary\nand secondary care could reduce the number of patients seeking urgent care in\nhospitals, resulting in substantial savings for health systems worldwide along\nwith better public health. However, current DDI prediction models are prone to\nconfounding biases along with either inaccurate or a lack of access to\nlongitudinal data from Electronic Health Records (EHR) and other drug\ninformation such as FDA Adverse Event Reporting System (FAERS) which continue\nto be the main barriers in measuring the prevalence of DDI and characterizing\nthe phenomenon in medical care. In this review, analytical models including\nLabel Propagation using drug side effect data and Supervised Learning DDI\nPrediction model using Drug-Gene interactions (DGIs) data are discussed.\nImproved identification of DDIs in both of these models compared to previous\nversions are highlighted while limitations that include bias, inaccuracy, and\ninsufficient data are also assessed. A case study of Psoriasis DDI prediction\nby DGI data using Random Forest Classifier was studied. Transfer Matrix\nRecurrent Neural Networks (TM-RNN) that address the above limitations are\ndiscussed in future works.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we investigate the existence of an elementary abelian closure\nin characteristic not $2$ for biquadratic extensions. We discover that it\nexists for any non-cyclic extension. We make use of it to obtain a\nclassification for this class of extensions up to isomorphism via descent. This\npermits us to describe the geometry of this moduli space in group theoretic\nterms. We also provide two families of polynomials with two parameters that can\ndescribe any quartic extensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Acoustic shadow moir\\'e has unique properties to be used for many potential\napplications in medical diagnostics, manufacturing, and material\ncharacterization. In this paper, numerical analysis, using Comsol, is used to\ninvestigate the principles of acoustic moir\\'e interference phenomenon. The\nstudy confirmed that the expected fringe images of shadow interference can be\ncreated at Talbot distances. The study confirms the experimental results\nreported by this group [1] and proves without any doubt that acoustic moir\\'e\ninterference phenomenon exists.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The locally modified finite element method, which is introduced in [Frei,\nRichter: SINUM 52(2014), p. 2315-2334] is a simple fitted finite element method\nthat is able to resolve weak discontinuities in interface problems. The method\nis based on a fixed structured coarse mesh, which is then refined into\nsub-elements to resolve an interior interface. In this work, we extend the\nlocally modified finite element method to second order using an isoparametric\napproach in the interface elements. Thereby we need to take care that the\nresulting curved edges do not lead to degenerate sub-elements. We prove optimal\na priori error estimates in the $L^2$-norm and in a modified energy norm, as\nwell as a reduced convergence order of ${\\cal O}(h^{3/2})$ in the standard\n$H^1$-norm. Finally, we present numerical examples to substantiate the\ntheoretical findings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze the convergence properties of operator product expansions (OPE)\nfor Lorentzian CFT four-point functions of scalar operators. We give a complete\nclassification of Lorentzian four-point configurations. All configurations in\neach class have the same OPE convergence properties in s-, t- and u-channels.\nWe give tables including the information of OPE convergence for all classes.\nOur work justifies that in a subset of the configuration space, Lorentzian CFT\nfour-point functions are genuine analytic functions. Our results are valid for\nunitary CFTs in $d\\geq2$. Our work also provides some Lorentzian regions where\none can do bootstrap analysis in the sense of functions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The nonlinear convection terms in the governing equations of compressible\nfluid flows are hyperbolic in nature and are nontrivial for modelling and\nnumerical simulation. Many numerical methods have been developed in the last\nfew decades for this purpose and are typically based on Riemann solvers, which\nare strongly dependent on the underlying eigen-structure of the governing\nequations. Objective of the present work is to develop simple algorithms which\nare not dependent on the eigen-structure and yet can tackle easily the\nhyperbolic parts. Central schemes with smart diffusion mechanisms are apt for\nthis purpose. For fixing the numerical diffusion, the basic ideas of satisfying\nthe Rankine-Hugoniot (RH) conditions along with generalized Riemann invariants\nare proposed. Two such interesting algorithms are presented, which capture\ngrid-aligned steady contact discontinuities exactly and yet have sufficient\nnumerical diffusion to avoid numerical shock instabilities. Both the algorithms\npresented are robust in avoiding shock instabilities, apart from being accurate\nin capturing contact discontinuities, do not need wave speed corrections and\nare independent of eigen-strutures of the underlying hyperbolic parts of the\nsystems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Behavior Trees (BTs) got the robotics society attention not least thanks to\ntheir modularity and reusability. The subtrees of BTs could be treated as\nseparate behaviors and therefore reused. We address the following research\nquestion: do we exploit the full power of BT on these properties? We suggest to\ngeneralise the idea of subtree reuse to \"node templates\" concept, which allows\nto represent an arbitrary nodes collection. In addition, previously hardcoded\nbehaviors such as Node* and many Decorator nodes could be implemented in a\nmemory-based BT by node templates.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Multi-messenger astronomy has experienced an explosive development in the\npast few years. While not being a particularly young field, it has recently\nattracted a lot of attention by several major discoveries and unprecedented\nobservation campaigns covering the entity of the electromagnetic spectrum as\nwell as observations of cosmic rays, neutrinos, and gravitational waves. The\nexploration of synergies is in full steam and requires close cooperation\nbetween different instruments. Here I give an overview over the subject of\nmulti-messenger astronomy and its virtues compared to classical \"single\nmessenger\" observations, present the recent break throughs of the field, and\ndiscuss some of its organisational and technical challenges.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Several works in the past decade have used the ratio between total (rest\n8-1000$\\mu$m) infrared and radio (rest 1.4~GHz) luminosity in star-forming\ngalaxies (q$_{IR}$), often referred to as the \"infrared-radio correlation\"\n(IRRC), to calibrate radio emission as a star formation rate (SFR) indicator.\nPrevious studies constrained the evolution of q$_{IR}$ with redshift, finding a\nmild but significant decline, that is yet to be understood. For the first time,\nwe calibrate q$_{IR}$ as a function of \\textit{both} stellar mass (M$_{\\star}$)\nand redshift, starting from an M$_{\\star}$-selected sample of $>$400,000\nstar-forming galaxies in the COSMOS field, identified via (NUV-r)/(r-J)\ncolours, at redshifts 0.1$<$z$<$4.5. Within each (M$_{\\star}$,z) bin, we stack\nthe deepest available infrared/sub-mm and radio images. We fit the stacked IR\nspectral energy distributions with typical star-forming galaxy and IR-AGN\ntemplates, and carefully remove radio AGN candidates via a recursive approach.\nWe find that the IRRC evolves primarily with M$_{\\star}$, with more massive\ngalaxies displaying systematically lower q$_{IR}$. A secondary, weaker\ndependence on redshift is also observed. The best-fit analytical expression is\nthe following:\nq$_{IR}$(M$_{\\star}$,z)=(2.646$\\pm$0.024)$\\times$(1+z)$^{(-0.023\\pm0.008)}$-(0.148$\\pm$0.013)$\\times$($\\log~M_{\\star}$/M$_{\\odot}$-10).\nThe lower IR/radio ratios seen in more massive galaxies are well described by\ntheir higher observed SFR surface densities. Our findings highlight that using\nradio-synchrotron emission as a proxy for SFR requires novel\nM$_{\\star}$-dependent recipes, that will enable us to convert detections from\nfuture ultra deep radio surveys into accurate SFR measurements down to low-SFR,\nlow-M$_{\\star}$ galaxies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Amenable category is a variant of the Lusternik-Schnirelman category, based\non covers by amenable open subsets. We study the monotonicity problem for\ndegree-one maps and amenable category and the relation between amenable\ncategory and topological complexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The recently observed diversity of Type Ia supernovae (SNe Ia) has motivated\nus to conduct the theoretical modeling of SNe Ia for a wide parameter range. In\nparticular, the origin of Type Iax supernovae (SNe Iax) has been obscure.\nFollowing our earlier work on the parameter dependence of SN Ia models, we\nfocus on SNe Iax in the present study. For a model of SNe Iax, we adopt the\ncurrently leading model of pure turbulent deflagration (PTD) of\nnear-Chandrasekhar mass C+O white dwarfs (WDs). We carry out 2-dimensional\nhydrodynamical simulations of the propagation of deflagration wave, which\nleaves a small WD remnant behind and eject nucleosynthesis materials. We show\nhow the explosion properties, such as nucleosynthesis and explosion energy,\ndepend on the model parameters such as central densities and compositions of\nthe WDs (including the hybrid WDs), and turbulent flame prescription and\ninitial flame geometry. We extract the associated observables in our models,\nand compare with the recently discovered low-mass WDs with unusual surface\nabundance patterns and the abundance patterns of some SN remnants. We provide\nthe nucleosynthesis yield tables for applications to stellar archaeology and\ngalactic chemical evolution. Our results are compared with the representative\nmodels in the literature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The anisotropy in resonant tunneling transport through an electrostatic\nbarrier in mono layer black phosphorus either in presence or in absence of an\noscillating potential is studied. Non-perturbative Floquet theory is applied to\nsolve the time dependent problem and the results obtained are discussed\nthoroughly. The resonance spectra in field free transmission are Lorentzian in\nnature although the width of the resonance for the barrier along the zigzag\ndirection is too thinner than that for the armchair one. Resonant transmission\nis suppressed for both the cases by the application of oscillating potential\nthat produces small oscillations in the transmission around the resonant energy\nparticularly at low frequency range. Sharp asymmetric Fano resonances are noted\nin the transmission spectrum along the armchair direction while a distinct line\nshape resonance is noted for the zigzag direction at higher frequency of the\noscillating potential. Even after the angular average, the conductance along\nthe armchair direction retains the characteristic Fano features that could be\nobserved experimentally. The present results are supposed to suggest that the\nphosphorene electrostatic barrier could be used successfully as switching\ndevices and nano detectors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Every philosophy has holes, and it is the responsibility of proponents of a\nphilosophy to point out these problems. Here are a few holes in Bayesian data\nanalysis: (1) the usual rules of conditional probability fail in the quantum\nrealm, (2) flat or weak priors lead to terrible inferences about things we care\nabout, (3) subjective priors are incoherent, (4) Bayesian decision picks the\nwrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6)\nfor Cantorian reasons we need to check our models, but this destroys the\ncoherence of Bayesian inference. Some of the problems of Bayesian statistics\narise from people trying to do things they shouldn't be trying to do, but other\nholes are not so easily patched. In particular, it may be a good idea to avoid\nflat, weak, or conventional priors, but such advice, if followed, would go\nagainst the vast majority of Bayesian practice and requires us to confront the\nfundamental incoherence of Bayesian inference. This does not mean that we think\nBayesian inference is a bad idea, but it does mean that there is a tension\nbetween Bayesian logic and Bayesian workflow which we believe can only be\nresolved by considering Bayesian logic as a tool, a way of revealing inevitable\nmisfits and incoherences in our model assumptions, rather than as an end in\nitself.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The manipulation of neutral atoms by light is at the heart of countless\nscientific discoveries in the field of quantum physics in the last three\ndecades. The level of control that has been achieved at the single particle\nlevel within arrays of optical traps, while preserving the fundamental\nproperties of quantum matter (coherence, entanglement, superposition), makes\nthese technologies prime candidates to implement disruptive computation\nparadigms. In this paper, we review the main characteristics of these devices\nfrom atoms / qubits to application interfaces, and propose a classification of\na wide variety of tasks that can already be addressed in a computationally\nefficient manner in the Noisy Intermediate Scale Quantum era we are in. We\nillustrate how applications ranging from optimization challenges to simulation\nof quantum systems can be explored either at the digital level (programming\ngate-based circuits) or at the analog level (programming Hamiltonian\nsequences). We give evidence of the intrinsic scalability of neutral atom\nquantum processors in the 100-1,000 qubits range and introduce prospects for\nuniversal fault tolerant quantum computing and applications beyond quantum\ncomputing.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Internet has been weaponized to carry out cybercriminal activities at an\nunprecedented pace. The rising concerns for preserving the privacy of personal\ndata while availing modern tools and technologies is alarming. End-to-end\nencrypted solutions are in demand for almost all commercial platforms. On one\nside, it seems imperative to provide such solutions and give people trust to\nreliably use these platforms. On the other side, this creates a huge\nopportunity to carry out unchecked cybercrimes. This paper proposes a robust\nvideo hashing technique, scalable and efficient in chalking out matches from an\nenormous bulk of videos floating on these commercial platforms. The video hash\nis validated to be robust to common manipulations like scaling, corruptions by\nnoise, compression, and contrast changes that are most probable to happen\nduring transmission. It can also be transformed into the encrypted domain and\nwork on top of encrypted videos without deciphering. Thus, it can serve as a\npotential forensic tool that can trace the illegal sharing of videos without\nknowing the underlying content. Hence, it can help preserve privacy and combat\ncybercrimes such as revenge porn, hateful content, child abuse, or illegal\nmaterial propagated in a video.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Data of long-lived and high profile projects is valuable for research on\nsuccessful software engineering in the wild. Having a dataset with different\nlinked software repositories of such projects, enables deeper diving\ninvestigations. This paper presents 20-MAD, a dataset linking the commit and\nissue data of Mozilla and Apache projects. It includes over 20 years of\ninformation about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue\ncomments, and its compressed size is over 6 GB. The data contains all the\ntypical information about source code commits (e.g., lines added and removed,\nmessage and commit time) and issues (status, severity, votes, and summary). The\nissue comments have been pre-processed for natural language processing and\nsentiment analysis. This includes emoticons and valence and arousal scores.\nLinking code repository and issue tracker information, allows studying\nindividuals in two types of repositories and provide more accurate time zone\ninformation for issue trackers as well. To our knowledge, this the largest\nlinked dataset in size and in project lifetime that is not based on GitHub.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The kernel relation $K$ on the lattice $\\mathcal{L}(\\mathcal{CR})$ of\nvarieties of completely regular semigroups has been a central component in many\ninvestigations into the structure of $\\mathcal{L}(\\mathcal{CR})$. However,\napart from the $K$-class of the trivial variety, which is just the lattice of\nvarieties of bands, the detailed structure of kernel classes has remained a\nmystery until recently. Kad'ourek [RK2] has shown that for two large classes of\nsubvarieties of $\\mathcal{CR}$ their kernel classes are singletons. Elsewhere\n(see [RK1], [RK2], [RK3]) we have provided a detailed analysis of the kernel\nclasses of varieties of abelian groups. Here we study more general kernel\nclasses. We begin with a careful development of the concept of duality in the\nlattice of varieties of completely regular semigroups and then show that the\nkernel classes of many varieties, including many self-dual varieties, of\ncompletely regular semigroups contain multiple copies of the lattice of\nvarieties of bands as sublattices.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It has been shown that the $\\alpha-z$ R{\\'e}nyi relative entropy satisfies\nthe Data Processing Inequality (DPI) for a certain range of $\\alpha$'s and\n$z$'s. Moreover, the range is completely characterized by Zhang in `20. We\nprove necessary and algebraically sufficient conditions to saturate the DPI for\nthe $\\alpha-z$ R{\\'e}nyi relative entropy whenever $1<\\alpha\\leq 2$ and\n$\\frac{\\alpha}{2}\\leq z\\leq\\alpha$. Moreover, these conditions coincide\nwhenever $\\alpha=z$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Terrestrial communication networks mainly focus on users in urban areas but\nhave poor coverage performance in harsh environments, such as mountains,\ndeserts, and oceans. Satellites can be exploited to extend the coverage of\nterrestrial fifth-generation (5G) networks. However, satellites are restricted\nby their high latency and relatively low data rate. Consequently, the\nintegration of terrestrial and satellite components has been widely studied, to\ntake advantage of both sides and enable seamless broadband coverage. Due to the\nsignificant differences between satellite communications (SatComs) and\nterrestrial communications (TerComs) in terms of channel fading, transmission\ndelay, mobility, and coverage performance, the establishment of an efficient\nhybrid satellite-terrestrial network (HSTN) still faces many challenges. In\ngeneral, it is difficult to decompose a HSTN into a sum of separate satellite\nand terrestrial links due to the complicated coupling relationships therein. To\nuncover the complete picture of HSTNs, we regard the HSTN as a combination of\nbasic cooperative models that contain the main traits of satellite-terrestrial\nintegration but are much simpler and thus more tractable than the large-scale\nheterogeneous HSTNs. In particular, we present three basic cooperative models,\ni.e., model X, model L, and model V, and provide a survey of the\nstate-of-the-art technologies for each of them. We discuss future research\ndirections towards establishing a cell-free, hierarchical, decoupled HSTN. We\nalso outline open issues to envision an agile, smart, and secure HSTN for the\nsixth-generation (6G) ubiquitous Internet of Things (IoT).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  State estimation aims at approximately reconstructing the solution $u$ to a\nparametrized partial differential equation from $m$ linear measurements, when\nthe parameter vector $y$ is unknown. Fast numerical recovery methods have been\nproposed based on reduced models which are linear spaces of moderate dimension\n$n$ which are tailored to approximate the solution manifold $\\mathcal{M}$ where\nthe solution sits. These methods can be viewed as deterministic counterparts to\nBayesian estimation approaches, and are proved to be optimal when the prior is\nexpressed by approximability of the solution with respect to the reduced model.\nHowever, they are inherently limited by their linear nature, which bounds from\nbelow their best possible performance by the Kolmogorov width\n$d_m(\\mathcal{M})$ of the solution manifold. In this paper we propose to break\nthis barrier by using simple nonlinear reduced models that consist of a finite\nunion of linear spaces $V_k$, each having dimension at most $m$ and leading to\ndifferent estimators $u_k^*$. A model selection mechanism based on minimizing\nthe PDE residual over the parameter space is used to select from this\ncollection the final estimator $u^*$. Our analysis shows that $u^*$ meets\noptimal recovery benchmarks that are inherent to the solution manifold and not\ntied to its Kolmogorov width. The residual minimization procedure is\ncomputationally simple in the relevant case of affine parameter dependence in\nthe PDE. In addition, it results in an estimator $y^*$ for the unknown\nparameter vector. In this setting, we also discuss an alternating minimization\n(coordinate descent) algorithm for joint state and parameter estimation, that\npotentially improves the quality of both estimators.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent theoretical studies have suggested that the suddenly recoiled atom\nstruck by dark matter (DM) particle is much more likely to excite or lose its\nelectrons than expected. Such Migdal effect provides a new avenue for exploring\nthe sub-GeV DM particles. There have been various attempts to describe the\nMigdal effect in liquids and semiconductor targets. In this paper we\nincorporate the treatment of the bremsstrahlung process and the electronic\nmany-body effects to give a full description of the Migdal effect in bulk\nsemiconductor targets diamond and silicon. Compared with the results obtained\nwith the atom-centered localized Wannier functions (WFs) under the framework of\nthe tight-binding (TB) approximation, the method proposed in this study yields\nmuch larger event rates in the low energy regime, due to a $\\omega^{-4}$\nscaling. We also find that the effect of the bremsstrahlung photon mediating\nthe Coulomb interaction between recoiled ion and the electron-hole pair is\nequivalent to that of the exchange of a single phonon.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum thermodynamics has emerged as a separate sub-discipline, revising the\nconcepts and laws of thermodynamics, at the quantum scale. In particular, there\nhas been a disruptive shift in the way thermometry, and thermometers are\nperceived and designed. Currently, we face two major challenges in quantum\nthermometry. First, all of the existing optimally precise temperature probes\nare local, meaning their operation is optimal only for a narrow range of\ntemperatures. Second, aforesaid optimal local probes mandate complex energy\nspectrum with immense degeneracy, rendering them impractical. Here, we address\nthese challenges by formalizing the notion of global thermometry leading to the\ndevelopment of optimal temperature sensors over a wide range of temperatures.\nWe observe the emergence of different phases for such optimal probes as the\ntemperature interval is increased. In addition, we show how the best\napproximation of optimal global probes can be realized in spin chains,\nimplementable in ion traps and quantum dots.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In a binary black hole merger, it is known that the inspiral portion of the\nwaveform corresponds to two distinct horizons orbiting each other, and the\nmerger and ringdown signals correspond to the final horizon being formed and\nsettling down to equilibrium. However, we still lack a detailed understanding\nof the relation between the horizon geometry in these three regimes and the\nobserved waveform. Here we show that the well known inspiral chirp waveform has\na clear counterpart on black hole horizons, namely, the shear of the outgoing\nnull rays at the horizon. We demonstrate that the shear behaves very much like\na compact binary coalescence waveform with increasing frequency and amplitude.\nFurthermore, the parameters of the system estimated from the horizon agree with\nthose estimated from the waveform. This implies that even though black hole\nhorizons are causally disconnected from us, assuming general relativity to be\ntrue, we can potentially infer some of their detailed properties from\ngravitational wave observations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  One very common type of fake news is satire which comes in a form of a news\nwebsite or an online platform that parodies reputable real news agencies to\ncreate a sarcastic version of reality. This type of fake news is often\ndisseminated by individuals on their online platforms as it has a much stronger\neffect in delivering criticism than through a straightforward message. However,\nwhen the satirical text is disseminated via social media without mention of its\nsource, it can be mistaken for real news. This study conducts several\nexploratory analyses to identify the linguistic properties of Arabic fake news\nwith satirical content. We exploit these features to build a number of machine\nlearning models capable of identifying satirical fake news with an accuracy of\nup to 98.6%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Satisfying the high computation demand of modern deep learning architectures\nis challenging for achieving low inference latency. The current approaches in\ndecreasing latency only increase parallelism within a layer. This is because\narchitectures typically capture a single-chain dependency pattern that prevents\nefficient distribution with a higher concurrency (i.e., simultaneous execution\nof one inference among devices). Such single-chain dependencies are so\nwidespread that even implicitly biases recent neural architecture search (NAS)\nstudies. In this visionary paper, we draw attention to an entirely new space of\nNAS that relaxes the single-chain dependency to provide higher concurrency and\ndistribution opportunities. To quantitatively compare these architectures, we\npropose a score that encapsulates crucial metrics such as communication,\nconcurrency, and load balancing. Additionally, we propose a new generator and\ntransformation block that consistently deliver superior architectures compared\nto current state-of-the-art methods. Finally, our preliminary results show that\nthese new architectures reduce the inference latency and deserve more\nattention.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The magnetotelluric (MT) responses of the Earth are biased by spatially\nheterogeneous source fields. Recently, such biases have been reported even in\nmid-latitude areas, where localized source currents rarely flow. This study\nfocuses on shifts in the MT responses arising from variations in the focus\nlatitude of the solar quiet (Sq) current. The MT responses at 60 s were\ncalculated by changing the center of the Sq current. Slight variations in the\nfocus latitude cause large shifts in the apparent resistivity and phase. During\nperiods of quiet geomagnetic activity, the center varies within a wide range of\n20-45 N, whereas this range narrows when there are disruptions. Even though\nthis study considers only a limited case, the results demonstrate the\ninstability of the impedances in periods of quiet geomagnetic activity and a\ncorrelation of the MT responses with the magnitude of the geomagnetic activity.\nAs a consequence, even when a station is located at mid-latitudes, the\nimpedances need to be treated carefully for time-lapse MT soundings, for\nexample, by checking the ionospheric current conditions and their dependence on\nthe geomagnetic activity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We fabricate large-area atomically thin MoS$_2$ layers through the direct\ntransformation of crystalline molybdenum MoS$_2$ (MoO$_3$) by sulfurization at\nrelatively low temperatures. The obtained MoS2 sheets are polycrystalline\n(~10-20 nm single-crystal domain size) with areas of up to 300x300 um$^2$ with\n2-4 layers in thickness and show a marked p-type behaviour. The synthesized\nfilms are characterized by a combination of complementary techniques: Raman\nspectroscopy, X-ray diffraction, transmission electron microscopy and\nelectronic transport measurements.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Treating the cosmological constant as a dynamical variable, we investigate\nthe thermodynamics and weak cosmic censorship conjecture (WCCC) of a charged\nAdS black hole (BH) in the Rastall gravity. We determine the energy momentum\nrelation of charged fermion at the horizon of the BH by using the Dirac\nequation. Based on this relation, we show that the first law of thermodynamics\n(FLT) still holds as a fermion is absorbed by the BH. However, the entropy of\nboth the extremal and near-extremal BH decreases in the irreversible process,\nwhich means that the second law of thermodynamics (SLT) is violated.\nFurthermore, we verify the validity of the WCCC by the minimum values of the\nmetric function h(r) at its final state. For the extremal charged AdS BH in the\nRastall gravity, we find that the WCCC is valid always since the BH is extreme.\nWhile for the case of near-extremal BH, we find the WCCC could be violable in\nthe extended phase space (EPS), depending on the value of the parameters of the\nBH and their variations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, a growing interest has been seen in deep learning-based semantic\nsegmentation. UNet, which is one of deep learning networks with an\nencoder-decoder architecture, is widely used in medical image segmentation.\nCombining multi-scale features is one of important factors for accurate\nsegmentation. UNet++ was developed as a modified Unet by designing an\narchitecture with nested and dense skip connections. However, it does not\nexplore sufficient information from full scales and there is still a large room\nfor improvement. In this paper, we propose a novel UNet 3+, which takes\nadvantage of full-scale skip connections and deep supervisions. The full-scale\nskip connections incorporate low-level details with high-level semantics from\nfeature maps in different scales; while the deep supervision learns\nhierarchical representations from the full-scale aggregated feature maps. The\nproposed method is especially benefiting for organs that appear at varying\nscales. In addition to accuracy improvements, the proposed UNet 3+ can reduce\nthe network parameters to improve the computation efficiency. We further\npropose a hybrid loss function and devise a classification-guided module to\nenhance the organ boundary and reduce the over-segmentation in a non-organ\nimage, yielding more accurate segmentation results. The effectiveness of the\nproposed method is demonstrated on two datasets. The code is available at:\ngithub.com/ZJUGiveLab/UNet-Version\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we introduce a new type of exponential map in semi-simple\ncompact Lie groups, which is related to the sub-Riemannian geometry generated\nby the orthogonal complement of a Cartan subalgebra in a similar way to how the\ngroup exponential map is related to the Riemannian geometry.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A human-swarm cooperative system, which mixes multiple robots and a human\nsupervisor to form a heterogeneous team, is widely used for emergent scenarios\nsuch as criminal tracking in social security and victim assistance in a natural\ndisaster. These emergent scenarios require a cooperative team to quickly\nterminate the current task and transit the system to a new task, bringing\ndifficulty in motion planning. Moreover, due to the immediate task transitions,\nuncertainty from both physical systems and prior tasks is accumulated to\ndecrease swarm performance, causing robot failures and influencing the\ncooperation effectiveness between the human and the robot swarm. Therefore,\ngiven the quick-transition requirements and the introduced uncertainty, it is\nchallenging for a human-swarm system to respond to emergent tasks, compared\nwith executing normal tasks where a gradual transition between tasks is\nallowed. Human trust reveals the behavior expectations of others and is used to\nadjust unsatisfactory behaviors for better cooperation. Inspired by human\ntrust, in this paper, a trust-aware reflective control (Trust-R) is developed\nto dynamically calibrate human-swarm cooperation. Trust-R, based on a weighted\nmean subsequence reduced algorithm (WMSR) and human trust modeling, helps a\nswarm to self-reflect its performance from the perspective of human trust; then\nproactively correct its faulty behaviors in an early stage before a human\nintervenes. One typical task scenario {emergency response} was designed in the\nreal-gravity simulation environment, and a human user study with 145 volunteers\nwas conducted. Trust-R's effectiveness in correcting faulty behaviors in\nemergency response was validated by the improved swarm performance and\nincreased trust scores.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Model-based reinforcement learning (RL) has emerged as a promising tool for\ndeveloping controllers for real world systems (e.g., robotics, autonomous\ndriving, etc.). However, real systems often have constraints imposed on their\nstate space which must be satisfied to ensure the safety of the system and its\nenvironment. Developing a verification tool for RL algorithms is challenging\nbecause the non-linear structure of neural networks impedes analytical\nverification of such models or controllers. To this end, we present a novel\nsafety verification framework for model-based RL controllers using reachable\nset analysis. The proposed frame-work can efficiently handle models and\ncontrollers which are represented using neural networks. Additionally, if a\ncontroller fails to satisfy the safety constraints in general, the proposed\nframework can also be used to identify the subset of initial states from which\nthe controller can be safely executed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we study the smooth convex-concave saddle point problem.\nSpecifically, we analyze the last iterate convergence properties of the\nExtragradient (EG) algorithm. It is well known that the ergodic (averaged)\niterates of EG converge at a rate of $O(1/T)$ (Nemirovski, 2004). In this\npaper, we show that the last iterate of EG converges at a rate of\n$O(1/\\sqrt{T})$. To the best of our knowledge, this is the first paper to\nprovide a convergence rate guarantee for the last iterate of EG for the smooth\nconvex-concave saddle point problem. Moreover, we show that this rate is tight\nby proving a lower bound of $\\Omega(1/\\sqrt{T})$ for the last iterate. This\nlower bound therefore shows a quadratic separation of the convergence rates of\nergodic and last iterates in smooth convex-concave saddle point problems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Universal quantum computation requires the implementation of a logical\nnon-Clifford gate. In this paper, we characterize all stabilizer codes whose\ncode subspaces are preserved under physical $T$ and $T^{-1}$ gates. For\nexample, this could enable magic state distillation with non-CSS codes and,\nthus, provide better parameters than CSS-based protocols. However, among\nnon-degenerate stabilizer codes that support transversal $T$, we prove that CSS\ncodes are optimal. We also show that triorthogonal codes are, essentially, the\nonly family of CSS codes that realize logical transversal $T$ via physical\ntransversal $T$. Using our algebraic approach, we reveal new purely-classical\ncoding problems that are intimately related to the realization of logical\noperations via transversal $T$. Decreasing monomial codes are also used to\nconstruct a code that realizes logical CCZ. Finally, we use Ax's theorem to\ncharacterize the logical operation realized on a family of quantum Reed-Muller\ncodes. This result is generalized to finer angle $Z$-rotations in\narXiv:1910.09333.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The miniaturization of infrared gas sensors is largely hindered by expensive\nand bulky laser sources as well as the use of optical filters. In this work, we\npropose a dual-band, directional thermal emitter based on compact W-Si-Cu\nmetasurfaces to address this issue. This metasurface emitter is designed to\nsupport two nondispersive magnetic polariton modes that exhibit distinct\ndirectional thermal emission profiles, thus enabling dual-band detection\nwithout the need of optical filters. Specifically, we evaluate the feasibility\nof such dual-band filterless detection by adapting the metasurface emitter to\nCO2 sensing. The model of sensing system shows a selective relative sensitivity\nof CO2 which is 3.2 times higher than that using a blackbody emitter, and a\nrelative sensitivity of temperature of emitter about 1.32%/K.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a novel technique for solving the problem of safe control for a\ngeneral class of nonlinear, control-affine systems subject to parametric model\nuncertainty. Invoking Lyapunov analysis and the notion of fixed-time stability\n(FxTS), we introduce a parameter adaptation law which guarantees convergence of\nthe estimates of unknown parameters in the system dynamics to their true values\nwithin a fixed-time independent of the initial parameter estimation error. We\nthen synthesize the adaptation law with a robust, adaptive control barrier\nfunction (RaCBF) based quadratic program to compute safe control inputs despite\nthe considered model uncertainty. To corroborate our results, we undertake a\ncomparative case study on the efficacy of this result versus other recent\napproaches in the literature to safe control under uncertainty, and close by\nhighlighting the value of our method in the context of an automobile overtake\nscenario.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Robotic manipulation of deformable 1D objects such as ropes, cables, and\nhoses is challenging due to the lack of high-fidelity analytic models and large\nconfiguration spaces. Furthermore, learning end-to-end manipulation policies\ndirectly from images and physical interaction requires significant time on a\nrobot and can fail to generalize across tasks. We address these challenges\nusing interpretable deep visual representations for rope, extending recent work\non dense object descriptors for robot manipulation. This facilitates the design\nof interpretable and transferable geometric policies built on top of the\nlearned representations, decoupling visual reasoning and control. We present an\napproach that learns point-pair correspondences between initial and goal rope\nconfigurations, which implicitly encodes geometric structure, entirely in\nsimulation from synthetic depth images. We demonstrate that the learned\nrepresentation -- dense depth object descriptors (DDODs) -- can be used to\nmanipulate a real rope into a variety of different arrangements either by\nlearning from demonstrations or using interpretable geometric policies. In 50\ntrials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66%\nknot-tying success rate from previously unseen configurations. See\nhttps://tinyurl.com/rope-learning for supplementary material and videos.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  GPU accelerators have become an important backbone for scientific high\nperformance computing, and the performance advances obtained from adopting new\nGPU hardware are significant. In this paper we take a first look at NVIDIA's\nnewest server line GPU, the A100 architecture part of the Ampere generation.\nSpecifically, we assess its performance for sparse linear algebra operations\nthat form the backbone of many scientific applications and assess the\nperformance improvements over its predecessor.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We provide the proof of a previously announced result that resolves the\nfollowing problem posed by A.~A.~Kirillov. Let $T$ be a presentation of a group\n$\\mathcal{G}$ by bounded linear operators in a Banach space $G$ and $E\\subset\nG$ be a closed invariant subspace. Then $T$ generates in the natural way\npresentations $T_1$ in $E$ and $T_2$ in $F:=G/E$. What additional information\nis required besides $T_1, T_2$ to recover the presentation $T$? In\nfinite-dimensional (and even in infinite dimensional Hilbert) case the solution\nis well known: one needs to supply a group cohomology class $h\\in\nH^1(\\mathcal{G},Hom(F,E))$. The same holds in the Banach case, if the subspace\n$E$ is complemented in $G$. However, every Banach space that is not isomorphic\nto a Hilbert one has non-complemented subspaces, which aggravates the problem\nsignificantly and makes it non-trivial even in the case of a trivial group\naction, where it boils down to what is known as the three-space problem. This\nexplains the title we have chosen. A solution of the problem stated above has\nbeen announced by the author in 1976, but the complete proof, for\nnon-mathematical reasons, has not been made available. This article contains\nthe proof, as well as some related considerations of the functor $Ext^1$ in the\ncategory \\textbf{Ban} of Banach spaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With entanglement-assisted (EA) formalism, arbitrary classical linear codes\nare allowed to transform into EAQECCs by using pre-shared entanglement between\nthe sender and the receiver. In this paper, based on classical cyclic MDS codes\nby exploiting pre-shared maximally entangled states, we construct two families\nof $q$-ary entanglement-assisted quantum MDS codes\n$[[\\frac{q^{2}+1}{a},\\frac{q^{2}+1}{a}-2(d-1)+c,d;c]]$, where q is a prime\npower in the form of $am+l$, and $a=(l^2+1)$ or $a=\\frac{(l^2+1)}{5}$. We show\nthat all of $q$-ary EAQMDS have minimum distance upper limit much larger than\nthe known quantum MDS (QMDS) codes of the same length. Most of these $q$-ary\nEAQMDS codes are new in the sense that their parameters are not covered by the\ncodes available in the literature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The self-learning Metropolis-Hastings algorithm is a powerful Monte Carlo\nmethod that, with the help of machine learning, adaptively generates an\neasy-to-sample probability distribution for approximating a given\nhard-to-sample distribution. This paper provides a new self-learning Monte\nCarlo method that utilizes a quantum computer to output a proposal\ndistribution. In particular, we show a novel subclass of this general scheme\nbased on the quantum Fourier transform circuit; this sampler is classically\nsimulable while having a certain advantage over conventional methods. The\nperformance of this \"quantum inspired\" algorithm is demonstrated by some\nnumerical simulations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Optical spectra of semiconductor quantum wells driven by an off-resonant\noscillating field are studied theoretically. Due to the dynamical stabilization\neffect, the field induces the quasi-stationary electron states confined at\nrepulsive scatterers and immersed into the continuum of states of conduction\nelectrons. As a result, the Fano resonances in the spectra of interband optical\ntransitions appear near the energies of the quasi-stationary states.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  To efficiently execute dynamically typed languages, many language\nimplementations have adopted a two-tier architecture. The first tier aims for\nlow-latency startup times and collects dynamic profiles, such as the dynamic\ntypes of variables. The second tier provides high-throughput using an\noptimizing compiler that specializes code to the recorded type information. If\nthe program behavior changes to the point that not previously seen types occur\nin specialized code, that specialized code becomes invalid, it is deoptimized,\nand control is transferred back to the first tier execution engine which will\nstart specializing anew. However, if the program behavior becomes more\nspecific, for instance, if a polymorphic variable becomes monomorphic, nothing\nchanges. Once the program is running optimized code, there are no means to\nnotice that an opportunity for optimization has been missed.\n  We propose to employ a sampling-based profiler to monitor native code without\nany instrumentation. The absence of instrumentation means that when the\nprofiler is not active, no overhead is incurred. We present an implementation\nis in the context of the \\v{R} just-in-time, optimizing compiler for the R\nlanguage. Based on the sampled profiles, we are able to detect when the native\ncode produced by \\v{R} is specialized for stale type feedback and recompile it\nto more type-specific code. We show that sampling adds an overhead of less than\n3% in most cases and up to 9% in few cases and that it reliably detects stale\ntype feedback within milliseconds.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper concerns the minimax center of a collection of linear subspaces.\nWhen the subspaces are $k$-dimensional subspaces of $\\mathbb{R}^n$, this can be\ncast as finding the center of a minimum enclosing ball on a Grassmann manifold,\nGr$(k,n)$. For subspaces of different dimension, the setting becomes a disjoint\nunion of Grassmannians rather than a single manifold, and the problem is no\nlonger well-defined. However, natural geometric maps exist between these\nmanifolds with a well-defined notion of distance for the images of the\nsubspaces under the mappings. Solving the initial problem in this context leads\nto a candidate minimax center on each of the constituent manifolds, but does\nnot inherently provide intuition about which candidate is the best\nrepresentation of the data. Additionally, the solutions of different rank are\ngenerally not nested so a deflationary approach will not suffice, and the\nproblem must be solved independently on each manifold. We propose and solve an\noptimization problem parametrized by the rank of the minimax center. The\nsolution is computed using a subgradient algorithm on the dual. By scaling the\nobjective and penalizing the information lost by the rank-$k$ minimax center,\nwe jointly recover an optimal dimension, $k^*$, and a central subspace, $U^*\n\\in$ Gr$(k^*,n)$ at the center of the minimum enclosing ball, that best\nrepresents the data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Privacy-preserving recommendations are recently gaining momentum, since the\ndecentralized user data is increasingly harder to collect, by recommendation\nservice providers, due to the serious concerns over user privacy and data\nsecurity. This situation is further exacerbated by the strict government\nregulations such as Europe's General Data Privacy Regulations(GDPR). Federated\nLearning(FL) is a newly developed privacy-preserving machine learning paradigm\nto bridge data repositories without compromising data security and privacy.\nThus many federated recommendation(FedRec) algorithms have been proposed to\nrealize personalized privacy-preserving recommendations. However, existing\nFedRec algorithms, mostly extended from traditional collaborative filtering(CF)\nmethod, cannot address cold-start problem well. In addition, their performance\noverhead w.r.t. model accuracy, trained in a federated setting, is often\nnon-negligible comparing to centralized recommendations. This paper studies\nthis issue and presents FL-MV-DSSM, a generic content-based federated\nmulti-view recommendation framework that not only addresses the cold-start\nproblem, but also significantly boosts the recommendation performance by\nlearning a federated model from multiple data source for capturing richer\nuser-level features. The new federated multi-view setting, proposed by\nFL-MV-DSSM, opens new usage models and brings in new security challenges to FL\nin recommendation scenarios. We prove the security guarantees of \\xxx, and\nempirical evaluations on FL-MV-DSSM and its variations with public datasets\ndemonstrate its effectiveness. Our codes will be released if this paper is\naccepted.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Palabos-npFEM is a computational framework for the simulation of blood flow\nwith fully resolved constituents. The software resolves the trajectories and\ndeformed state of blood cells, such as red blood cells and platelets, and the\ncomplex interaction between them. The tool combines the lattice Boltzmann\nsolver Palabos for the simulation of blood plasma (fluid phase), a finite\nelement method (FEM) solver for the resolution of blood cells (solid phase),\nand an immersed boundary method (IBM) for the coupling of the two phases.\nPalabos-npFEM provides, on top of a CPU-only version, the option to simulate\nthe deformable bodies on GPUs, thus the code is tailored for the fastest\nsupercomputers. The software is integrated in the Palabos core library, and is\navailable on the Git repository https://gitlab.com/unigespc/palabos. It offers\nthe possibility to simulate various setups, e.g. several geometries and blood\nparameters, and due to its modular design, it allows external solvers to\nreadily replace the provided ones.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  $\\gamma$ Doradus is the prototype star for the eponymous class of pulsating\nstars that consists of late A-early F main-sequence stars oscillating in\nlow-frequency gravito-inertial modes. Being among the brightest stars of its\nkind (V = 4.2), $\\gamma$ Dor benefits from a large set of observational data\nthat has been recently completed by high-quality space photometry from the TESS\nmission. With these new data, we propose to study $\\gamma$ Dor as an example of\npossibilities offered by synergies between multi-technical ground and\nspace-based observations. Here, we present the preliminary results of our\ninvestigations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Trilayered, Ising, spin-1/2, ferrimagnets are an interesting subject for\nsimulational studies for they show compensation effect. A Monte Carlo study on\nsuch a system with sublayers on triangular lattice is performed in the current\nwork. Three layers, making up the bulk, is formed completely by either A or B\ntype of atoms. The interactions between like atoms (A-A; B-B) are ferromagnetic\nand between unlike ones (A-B) are anti-ferromagnetic. Thus the system has three\ncoupling constants and manifests into two distinct trilayer compositions: AAB\nand ABA. Metropolis single spin flip algorithm is employed for the simulation\nand the location of the critical points (sublattice magnetisations vanish,\nleading to zero bulk magnetisation) and the compensation points (bulk\nmagnetisation vanishes but nonzero sublattice magnetisations exist) are\nestimated. Close range simulations with variable lattice sizes for compensation\npoint and Binder's cumulant crossing technique for critical points are employed\nfor analysis and conditions for the existence of compensation points are\ndetermined. Comprehensive phase diagrams are obtained in the Hamiltonian\nparameter space and morphological studies at critical and compensation\ntemperatures for both the configurations are also reported. The alternative\ndescription in terms of Inverse absolute of reduced residual magnetisation and\nTemperature interval between Critical and Compensation temperatures is also\nproposed and compared with traditional simulational results. Such simulational\nstudies and the proposed systematics of compensation effect are useful in\ndesigning materials for specific technological applications.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Iron loss determination in the magnetic core of an electrical machine, such\nas a motor or a transformer, is formulated as an inverse heat source problem.\nThe sensor positions inside the object are optimized in order to minimize the\nuncertainty in the reconstruction in the sense of the A-optimality of Bayesian\nexperimental design. This paper focuses on the problem formulation and an\nefficient numerical solution of the discretized sensor optimization and source\nreconstruction problems. A semirealistic linear model is discretized by finite\nelements and studied numerically.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This work presents a novel training technique for deep neural networks that\nmakes use of additional data from a distribution that is different from that of\nthe original input data. This technique aims to reduce overfitting and improve\nthe generalization performance of the network. Our proposed technique, namely\nPassive Batch Injection Training Technique (PBITT), even reduces the level of\noverfitting in networks that already use the standard techniques for reducing\noverfitting such as $L_2$ regularization and batch normalization, resulting in\nsignificant accuracy improvements. Passive Batch Injection Training Technique\n(PBITT) introduces a few passive mini-batches into the training process that\ncontain data from a distribution that is different from the input data\ndistribution. This technique does not increase the number of parameters in the\nfinal model and also does not increase the inference (test) time but still\nimproves the performance of deep CNNs. To the best of our knowledge, this is\nthe first work that makes use of different data distribution to aid the\ntraining of convolutional neural networks (CNNs). We thoroughly evaluate the\nproposed approach on standard architectures: VGG, ResNet, and WideResNet, and\non several popular datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. We\nobserve consistent accuracy improvement by using the proposed technique. We\nalso show experimentally that the model trained by our technique generalizes\nwell to other tasks such as object detection on the MS-COCO dataset using\nFaster R-CNN. We present extensive ablations to validate the proposed approach.\nOur approach improves the accuracy of VGG-16 by a significant margin of 2.1%\nover the CIFAR-100 dataset.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In panel experiments, we randomly assign units to different interventions,\nmeasuring their outcomes, and repeating the procedure in several periods. Using\nthe potential outcomes framework, we define finite population dynamic causal\neffects that capture the relative effectiveness of alternative treatment paths.\nFor a rich class of dynamic causal effects, we provide a nonparametric\nestimator that is unbiased over the randomization distribution and derive its\nfinite population limiting distribution as either the sample size or the\nduration of the experiment increases. We develop two methods for inference: a\nconservative test for weak null hypotheses and an exact randomization test for\nsharp null hypotheses. We further analyze the finite population probability\nlimit of linear fixed effects estimators. These commonly-used estimators do not\nrecover a causally interpretable estimand if there are dynamic causal effects\nand serial correlation in the assignments, highlighting the value of our\nproposed estimator.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Global gauge anomalies in $6d$ associated with non-trivial homotopy groups\n$\\pi_6(G)$ for $G=SU(2)$, $SU(3)$, and $G_2$ were computed and utilized in the\npast. In the modern bordism point of view of anomalies, however, they come from\nthe bordism groups $\\Omega^\\text{spin}_7(BG)$, which are in fact trivial and\ntherefore preclude their existence. Instead, it was noticed that a proper\ntreatment of the $6d$ Green-Schwarz mechanism reproduces the same anomaly\ncancellation conditions derived from $\\pi_6(G)$. In this paper, we revisit and\nclarify the relation between these two different approaches.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently there has been increasing interest in alternate methods to compute\nquantum tunneling in field theory. Of particular interest is a stochastic\napproach which involves (i) sampling from the free theory Gaussian\napproximation to the Wigner distribution in order to obtain stochastic initial\nconditions for the field and momentum conjugate, then (ii) evolving under the\nclassical field equations of motion, which leads to random bubble formation.\nPrevious work showed parametric agreement between the logarithm of the\ntunneling rate in this stochastic approach and the usual instanton\napproximation. However, recent work [1] claimed excellent agreement between\nthese methods. Here we show that this approach does not in fact match\nprecisely; the stochastic method tends to overpredict the instanton tunneling\nrate. To quantify this, we parameterize the standard deviations in the initial\nstochastic fluctuations by $\\epsilon \\sigma$, where $\\sigma$ is the actual\nstandard deviation of the Gaussian distribution and $\\epsilon$ is a fudge\nfactor; $\\epsilon = 1$ is the physical value. We numerically implement the\nstochastic approach to obtain the bubble formation rate for a range of\npotentials in 1+1-dimensions, finding that $\\epsilon$ always needs to be\nsomewhat smaller than unity to suppress the otherwise much larger stochastic\nrates towards the instanton rates; for example, in the potential of [1] one\nneeds $\\epsilon \\approx 1/2$. We find that a mismatch in predictions also\noccurs when sampling from other Wigner distributions, and in single particle\nquantum mechanics even when the initial quantum system is prepared in an exact\nGaussian state. If the goal is to obtain agreement between the two methods, our\nresults show that the stochastic approach would be useful if a prescription to\nspecify optimal fudge factors for fluctuations can be developed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We initiate quantitative studies of complexity in (1+1)-dimensional conformal\nfield theories with a view that they provide the simplest setting to find a\ngravity dual to complexity. Our work pursues a geometric understanding of\ncomplexity of conformal transformations and embeds Fubini-Study state\ncomplexity and direct counting of stress tensor insertion in the relevant\ncircuits in a unified mathematical language. In the former case, we iteratively\nsolve the emerging integro-differential equation for sample optimal circuits\nand discuss the sectional curvature of the underlying geometry. In the latter\ncase, we recognize that optimal circuits are governed by Euler-Arnold type\nequations and discuss relevant results for three well-known equations of this\ntype in the context of complexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Knowledge about deep-ocean turbulent mixing and flow circulation above\nabyssal hilly plains is important to quantify processes for the modelling of\nresuspension and dispersal of sediments in areas where turbulence sources are\nexpected to be relatively weak. Turbulence may disperse sediments from\nartificial deep-sea mining activities over large distances. To quantify\nturbulent mixing above the deep-ocean floor around 4000 m depth,\nhigh-resolution moored temperature sensor observations have been obtained from\nthe near-equatorial southeast Pacific (7{\\deg}S, 88{\\deg}W). Models demonstrate\nlow activity of equatorial flow dynamics, internal tides and surface\nnear-inertial motions in the area. The present observations demonstrate a\nConservative Temperature difference of about 0.012{\\deg}C between 7 and 406\nmeter above the bottom (hereafter, mab, for short), which is a quarter of the\nadiabatic lapse rate. The very weakly stratified waters with buoyancy periods\nbetween about six hours and one day allow for slowly varying mixing. The\ncalculated turbulence dissipation rate values are half to one order of\nmagnitude larger than those from open-ocean turbulent exchange well away from\nbottom topography and surface boundaries. In the deep, turbulent overturns\nextend up to 100 m tall, in the ocean-interior, and also reach the lowest\nsensor. The overturns are governed by internal-wave-shear and -convection. The\nturbulence inertial subrange is observed to extend into the internal wave\nfrequency band. The associated mixing is not related to bottom friction\nprocesses but to internal wave breaking and near-inertial shear. The mixing\nfacilitates long (hours to day) and high (exceeding 100 mab) dispersal of\nsuspended sediments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Kuramoto model is a paradigm for studying oscillator networks with\ninterplay between coupling tending towards synchronization, and heterogeneity\nin the oscillator population driving away from synchrony. In continuum versions\nof this model an oscillator population is represented by a probability density\non the circle. Ott and Antonsen identified a special class of densities which\nis invariant under the dynamics and on which the dynamics are low-dimensional\nand analytically tractable. The reduction to the OA manifold has been used to\nanalyze the dynamics of many variants of the Kuramoto model. To address the\nfundamental question of whether the OA manifold is attracting, we develop a\nsystematic technique using weighted averages of Poisson measures for analyzing\ndynamics off the OA manifold. We show that for models with a finite number of\npopulations, the OA manifold is {\\it not} attracting in any sense; moreover,\nthe dynamics off the OA manifold is often more complex than on the OA manifold,\neven at the level of macroscopic order parameters. The OA manifold consists of\nPoisson densities $\\rho_\\omega$. A simple extension of the OA manifold consists\nof averages of pairs of Poisson densities; then the hyperbolic distance between\nthe centroids of each Poisson pair is a dynamical invariant (for each\n$\\omega$). These conserved quantities, defined on the double Poisson manifold,\nare a measure of the distance to the OA manifold. This invariance implies that\nchimera states, which have some but not all populations in sync, can never be\nstable in the full state space, even if stable in the OA manifold. More\nbroadly, our framework facilitates the analysis of multi-population continuum\nKuramoto networks beyond the restrictions of the OA manifold, and has the\npotential to reveal more intricate dynamical behavior than has previously been\nobserved for these networks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We revisit the topic of invisible neutrino decay in the precision\ncosmological context, via a first-principles approach to understanding the\ncosmic microwave background and large-scale structure phenomenology of such a\nnon-standard physics scenario. Assuming an effective Lagrangian in which a\nheavier standard-model neutrino $\\nu_H$ couples to a lighter one $\\nu_l$ and a\nmassless scalar particle $\\phi$ via a Yukawa interaction, we derive from first\nprinciples the complete set of Boltzmann equations, at both the spatially\nhomogeneous and the first-order inhomogeneous levels, for the phase space\ndensities of $\\nu_H$, $\\nu_l$, and $\\phi$ in the presence of the relevant decay\nand inverse decay processes. With this set of equations in hand, we perform a\ncritical survey of recent works on cosmological invisible neutrino decay in\nboth limits of decay while $\\nu_H$ is ultra-relativistic and non-relativistic.\nOur two main findings are: (i) in the non-relativistic limit, the effective\nequations of motion used to describe perturbations in the neutrino--scalar\nsystem in the existing literature formally violate momentum conservation and\ngauge invariance, and (ii) in the ultra-relativistic limit, exponential damping\nof the anisotropic stress does not occur at the commonly-used rate $\\Gamma_{\\rm\nT} =(1/\\tau_0) (m_{\\nu H}/E_{\\nu H})^3$, but at a rate $\\sim (1/\\tau_0) (m_{\\nu\nH}/E_{\\nu H})^5$. Both results are model-independent. The impact of the former\nfinding on the cosmology of invisible neutrino decay is likely small. The\nlatter, however, implies a significant revision of the cosmological limit on\nthe neutrino lifetime $\\tau_0$ from $\\tau_0^{\\rm old} \\gtrsim 1.2 \\times 10^9\\,\n{\\rm s}\\, (m_{\\nu H}/50\\, {\\rm meV})^3$ to $\\tau_0 \\gtrsim (4 \\times 10^5 \\to 4\n\\times 10^6)\\, {\\rm s}\\, (m_{\\nu H}/50 \\, {\\rm meV})^5$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Social Networks (SNs) have been gradually applied by utility companies as an\naddition to smart grid and are proved to be helpful in smoothing load curves\nand reducing energy usage. However, SNs also bring in new threats to smart\ngrid: misinformation in SNs may cause smart grid users to alter their demand,\nresulting in transmission line overloading and in turn leading to catastrophic\nimpact to the grid. In this paper, we discuss the interdependency in the social\nnetwork coupled smart grid and focus on its vulnerability. That is, how much\ncan the smart grid be damaged when misinformation related to it diffuses in\nSNs? To analytically study the problem, we propose the Misinformation Attack\nProblem in Social-Smart Grid (MAPSS) that identifies the top critical nodes in\nthe SN, such that the smart grid can be greatly damaged when misinformation\npropagates from those nodes. This problem is challenging as we have to\nincorporate the complexity of the two networks concurrently. Nevertheless, we\npropose a technique that can explicitly take into account information diffusion\nin SN, power flow balance and cascading failure in smart grid integratedly when\nevaluating node criticality, based on which we propose various strategies in\nselecting the most critical nodes. Also, we introduce controlled load shedding\nas a protection strategy to reduce the impact of cascading failure. The\neffectiveness of our algorithms are demonstrated by experiments on IEEE bus\ntest cases as well as the Pegase data set.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  MPTCP is a new transport protocol that enables mobile devices to use multiple\nphysical paths simultaneously through several network interfaces, such as WiFi\nand Cellular. However, wireless path capacities change frequently in the mobile\nenvironments, causing challenges for path selection. For example, WiFi\nassociated paths often become poor as devices walk away, since WiFi has\nintermittent connectivity caused by the short signal coverage and stochastic\ninterference. MPTCP's native decision based on hysteretic TCP-layer estimation\nwill miss the real switching point of wireless quality, which may cumulate\npackets on the broken path and causes serious packets reinjection. Through\nanalyzing a unique dataset in the wild, we quantitatively study the impact of\nMAC-layer factors on the aggregated performance of MPTCP. We then propose a\ndecision tree approach for cross-layer path selection that decides which path\nto carry the incoming packets dynamically according to the prior learned\nschemes. A prototype of the path selection system named SmartPS, which\nproactively probes the wireless environments, is realized and deployed in Linux\nand Android. Evaluation results demonstrate that our SmartPS can efficiently\nutilize the faster path, with goodput improvements of up to 29%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider stochastic optimization of a smooth non-convex loss function with\na convex non-smooth regularizer. In the online setting, where a single sample\nof the stochastic gradient of the loss is available at every iteration, the\nproblem can be solved using the proximal stochastic gradient descent (SGD)\nalgorithm and its variants. However in many problems, especially those arising\nin communications and signal processing, information beyond the stochastic\ngradient may be available thanks to the structure of the loss function. Such\nextra-gradient information is not used by SGD, but has been shown to be useful,\nfor instance in the context of stochastic expectation-maximization, stochastic\nmajorization-minimization, and stochastic successive convex approximation (SCA)\napproaches. By constructing a stochastic strongly convex surrogates of the loss\nfunction at every iteration, the stochastic SCA algorithms can exploit the\nstructural properties of the loss function and achieve superior empirical\nperformance as compared to the SGD.\n  In this work, we take a closer look at the stochastic SCA algorithm and\ndevelop its asynchronous variant which can be used for resource allocation in\nwireless networks. While the stochastic SCA algorithm is known to converge\nasymptotically, its iteration complexity has not been well-studied, and is the\nfocus of the current work. The insights obtained from the non-asymptotic\nanalysis allow us to develop a more practical asynchronous variant of the\nstochastic SCA algorithm which allows the use of surrogates calculated in\nearlier iterations. We characterize precise bound on the maximum delay the\nalgorithm can tolerate, while still achieving the same convergence rate. We\napply the algorithm to the problem of linear precoding in wireless sensor\nnetworks, where it can be implemented at low complexity but is shown to perform\nwell in practice.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  RX J1301.9+2747 is an ultrasoft active galactic nucleus (AGN) with unusual\nX-ray variability that is characterized by a long quiescent state and a\nshort-lived flare state. The X-ray flares are found to recur quasi-periodically\non a timescale of 13-20 ks. Here, we report the analysis of the light curve in\nthe quiescent state from two XMM observations spanning 18.5 years, along with\nthe discovery of a possible quasi-periodic X-ray oscillation (QPO) with a\nperiod of ~1500s. The QPO is detected at the same frequency in the two\nindependent observations, with a combined significance of >99.89%. The QPO is\nin agreement with the relation between frequency and black hole mass (M_BH)\nthat has been reported in previous works for AGNs and Galactic black hole X-ray\nbinaries (XRBs). The QPO frequency is stable over almost two decades,\nsuggesting that it may correspond to the high-frequency type found in XRBs and\noriginates, perhaps, from a certain disk resonance mode. In the 3:2\ntwin-frequency resonance model, our best estimate on the M_BH range implies\nthat a maximal black hole spin can be ruled out. We find that all ultrasoft\nAGNs reported so far display quasi-periodicities in the X-ray emission,\nsuggesting a possible link on the part of the extreme variability phenomenon to\nthe ultrasoft X-ray component. This indicates that ultrasoft AGNs could be the\nmost promising candidates in future searches for X-ray periodicities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although blockchain-based smart contracts promise a ``trustless'' way of\nenforcing agreements even with monetary consequences, they suffer from multiple\nsecurity issues. Many of these issues could be mitigated via an effective\naccess control system, however, its realization is challenging due to the\nproperties of current blockchain platforms (like lack of privacy, costly\non-chain resources, or latency). To address this problem, we propose the SMACS\nframework, where updatable and sophisticated Access Control Rules (ACRs)} for\nsmart contracts can be realized with low cost. SMACS shifts the burden of\nexpensive ACRs validation and management operations to an off-chain\ninfrastructure, while implementing on-chain only lightweight token-based access\ncontrol. SMACS is flexible and in addition to simple access control lists can\neasily implement rules enhancing the runtime security of smart contracts. With\ndedicated ACRs backed by vulnerability-detection tools, SMACS can protect\nvulnerable contracts after deployment. We fully implement SMACS and evaluate\nit.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We extend the approach of [Smith et al. 2019] to derive analytical\nexpressions for the eigenvalues and eigenmatrices of an isotropic membrane\nenergy density function $\\psi : \\mathbb{R}^{3x2} \\to \\mathbb{R}$. Clamping the\neigenvalue expressions to be positive for each quadrature point of a finite\nelement membrane simulation guarantees a positive semi-definite Hessian for the\nfull discrete membrane energy, enabling an efficient Newton-type simulation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the distribution of the generalized gcd and lcm functions on\naverage. The generalized gcd function, denoted by $(m,n)_b$, is the largest\n$b$-th power divisor common to $m$ and $n$. Likewise, the generalized lcm\nfunction, denoted by $[m,n]_b$, is the smallest $b$-th power multiple common to\n$m$ and $n$. We derive asymptotic formulas for the average order of the\narithmetic, geometric, and harmonic means of $(m,n)_b$. Additionally, we also\ndeduce asymptotic formulas with error terms for the means of $(n_1,n_2,\\cdots,\nn_k)_b$, and $[n_1,n_2,\\cdots, n_k]_b$ over a set of lattice points, thereby\ngeneralizing some of the previous work on gcd and lcm-sum estimates.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present some partial results for the m-step solvable\nGrothendieck conjecture in anabelian geometry. Among other things, we prove the\n3-step solvable Grothendieck conjecture for genus 0 curves over fields finitely\ngenerated over the prime field of arbitrary characteristic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Multi-task learns multiple tasks, while sharing knowledge and computation\namong them. However, it suffers from catastrophic forgetting of previous\nknowledge when learned incrementally without access to the old data. Most\nexisting object detectors are domain-specific and static, while some are\nlearned incrementally but only within a single domain. Training an object\ndetector incrementally across various domains has rarely been explored. In this\nwork, we propose three incremental learning scenarios across various domains\nand categories for object detection. To mitigate catastrophic forgetting,\nattentive feature distillation is proposed to leverages both bottom-up and\ntop-down attentions to extract important information for distillation. We then\nsystematically analyze the proposed distillation method in different scenarios.\nWe find out that, contrary to common understanding, domain gaps have smaller\nnegative impact on incremental detection, while category differences are\nproblematic. For the difficult cases, where the domain gaps and especially\ncategory differences are large, we explore three different exemplar sampling\nmethods and show the proposed adaptive sampling method is effective to select\ndiverse and informative samples from entire datasets, to further prevent\nforgetting. Experimental results show that we achieve the significant\nimprovement in three different scenarios across seven object detection\nbenchmark datasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Cloud computing is one of the highly flexible, confidential and easily\naccessible medium of platforms and provides powerful service for sharing\ninformation over the Internet. Cloud security has become an emerging issue as\nnetwork manager eventually encounter its data protection, vulnerability during\ninformation exchange on the cloud system. We can protect our data from unwanted\naccess on a hybrid cloud through controlling the respective firewall of the\nnetwork. But, the firewall has already proved its weakness as it is unable to\nensure multi-layered, secured accessibility of the cloud network. Efficient\npacket utilization sometimes causes high response time in accessing hybrid\ncloud. In this paper, a Cloud Model with Hybrid functionality and a secure\nFuzzy Integrated Firewall for that Hybrid Cloud is proposed and thereby\nevaluated for the performance in traffic response. Experimental result\nillustrated that having a fuzzified firewall gives high point-to-point packet\nutilization decreasing the response time than a conventional firewall. Results\nfrom this research work will highly be implemented in transplanting artificial\nintelligence in future Internet of Things (IoT).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Research in oncology has changed the focus from histological properties of\ntumors in a specific organ to a specific genomic aberration potentially shared\nby multiple cancer types. This motivates the basket trial, which assesses the\nefficacy of treatment simultaneously on multiple cancer types that have a\ncommon aberration. Although the assumption of homogeneous treatment effects\nseems reasonable given the shared aberration, in reality, the treatment effect\nmay vary by cancer type, and potentially only a subgroup of the cancer types\nrespond to the treatment. Various approaches have been proposed to increase the\ntrial power by borrowing information across cancer types, which, however, tend\nto inflate the type I error rate. In this paper, we review some representative\nBayesian information borrowing methods for the analysis of early-phase basket\ntrials. We then propose a novel method called the Bayesian hierarchical model\nwith a correlated prior (CBHM), which conducts more flexible borrowing across\ncancer types according to sample similarity. We did simulation studies to\ncompare CBHM with independent analysis and three information borrowing\napproaches: the conventional Bayesian hierarchical model, the EXNEX approach\nand Liu's two-stage approach. Simulation results show that all information\nborrowing approaches substantially improve the power of independent analysis if\na large proportion of the cancer types truly respond to the treatment. Our\nproposed CBHM approach shows an advantage over the existing information\nborrowing approaches, with a power similar to that of EXNEX or Liu's approach,\nbut the potential to provide substantially better control of type I error rate.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We search for RR Lyrae stars in 27 nearby ($<100$ kpc) ultra-faint dwarf\nsatellite galaxies using the Gaia DR2 catalog of RR Lyrae stars. Based on\nproper motions, magnitudes and location on the sky, we associate 47 Gaia RR\nLyrae stars to 14 different satellites. Distances based on RR Lyrae stars are\nprovided for those galaxies. We have identified RR Lyrae stars for the first\ntime in the Tucana II dwarf galaxy, and find additional members in Ursa Major\nII, Coma Berenices, Hydrus I, Bootes I and Bootes III. In addition we have\nidentified candidate extra-tidal RR Lyrae stars in six galaxies which suggest\nthey may be undergoing tidal disruption. We found 10 galaxies have no RR Lyrae\nstars neither in Gaia nor in the literature. However, given the known\ncompleteness of Gaia DR2 we cannot conclude these galaxies indeed lack variable\nstars of this type.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Bayesian neural network models (BNN) have re-surged in recent years due to\nthe advancement of scalable computations and its utility in solving complex\nprediction problems in a wide variety of applications. Despite the popularity\nand usefulness of BNN, the conventional Markov Chain Monte Carlo based\nimplementation suffers from high computational cost, limiting the use of this\npowerful technique in large scale studies. The variational Bayes inference has\nbecome a viable alternative to circumvent some of the computational issues.\nAlthough the approach is popular in machine learning, its application in\nstatistics is somewhat limited. This paper develops a variational Bayesian\nneural network estimation methodology and related statistical theory. The\nnumerical algorithms and their implementational are discussed in detail. The\ntheory for posterior consistency, a desirable property in nonparametric\nBayesian statistics, is also developed. This theory provides an assessment of\nprediction accuracy and guidelines for characterizing the prior distributions\nand variational family. The loss of using a variational posterior over the true\nposterior has also been quantified. The development is motivated by an\nimportant biomedical engineering application, namely building predictive tools\nfor the transition from mild cognitive impairment to Alzheimer's disease. The\npredictors are multi-modal and may involve complex interactive relations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Sentiment analysis is crucial for the advancement of artificial intelligence\n(AI). Sentiment understanding can help AI to replicate human language and\ndiscourse. Studying the formation and response of sentiment state from\nwell-trained Customer Service Representatives (CSRs) can help make the\ninteraction between humans and AI more intelligent. In this paper, a sentiment\nanalysis pipeline is first carried out with respect to real-world multi-party\nconversations - that is, service calls. Based on the acoustic and linguistic\nfeatures extracted from the source information, a novel aggregated method for\nvoice sentiment recognition framework is built. Each party's sentiment pattern\nduring the communication is investigated along with the interaction sentiment\npattern between all parties.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we consider Schr\\\"{o}dinger operators on $M \\times\n\\mathbb{Z}^{d_2}$, with $M=\\{M_{1}, \\ldots, M_{2}\\}^{d_1}$ (`quantum wave\nguides') with a `$\\Gamma$-trimmed' random potential, namely a potential which\nvanishes outside a subset $\\Gamma$ which is periodic with respect to a sub\nlattice. We prove that (under appropriate assumptions) for strong disorder\nthese operators have \\emph{pure point spectrum } outside the set\n$\\Sigma_{0}=\\sigma(H_{0,\\Gamma^{c}})$ where $H_{0,\\Gamma^{c}} $ is the free\n(discrete) Laplacian on the complement $\\Gamma^{c} $ of $\\Gamma $. We also\nprove that the operators have some \\emph{absolutely continuous spectrum} in an\nenergy region $\\mathcal{E}\\subset\\Sigma_{0}$. Consequently, there is a mobility\nedge for such models. We also consider the case $-M_{1}=M_{2}=\\infty$, i.~e.~\n$\\Gamma $-trimmed operators on\n$\\mathbb{Z}^{d}=\\mathbb{Z}^{d_1}\\times\\mathbb{Z}^{d_2}$. Again, we prove\nlocalisation outside $\\Sigma_{0} $ by showing exponential decay of the Green\nfunction $G_{E+i\\eta}(x,y) $ uniformly in $\\eta>0 $. For \\emph{all} energies\n$E\\in\\mathcal{E}$ we prove that the Green's function $G_{E+i\\eta} $ is\n\\emph{not} (uniformly) in $\\ell^{1}$ as $\\eta$ approaches $0$. This implies\nthat neither the fractional moment method nor multi scale analysis \\emph{can}\nbe applied here.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We explore the relationship between the classical constructions of cumulants\nand Koszul brackets, showing that the former are an expontial version of the\nlatter. Moreover, under some additional technical assumptions, we prove that\nboth constructions are compatible with standard homological perturbation theory\nin an appropriate sense. As an application of these results, we provide new\nproofs for the homotopy transfer Theorem for $L_\\infty$ and $IBL_\\infty$\nalgebras based on the symmetrized tensor trick and the standard perturbation\nLemma, as in the usual approach for $A_\\infty$ algebras. Moreover, we prove a\nhomotopy transfer Theorem for commutative $BV_\\infty$ algebras in the sense of\nKravchenko which appears to be new. Along the way, we introduce a new\ndefinition of morphism between commutative $BV_\\infty$ algebras.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ne atoms with energies up to 3 keV are diffracted under grazing angles of\nincidence from a LiF(001) surface. For a small momentum component of the\nincident beam perpendicular to the surface, we observe an increase of the\nelastic rainbow angle together with a broadening of the inelastic scattering\nprofile. We interpret these two effects as the refraction of the atomic wave in\nthe attractive part of the surface potential. We use a fast, rigorous dynamical\ndiffraction calculation to find a projectile-surface potential model that\nenables a quantitative reproduction of the experimental data for up to ten\ndiffraction orders. This allows us to extract an attractive potential well\ndepth of 10.4 meV. Our results set a benchmark for more refined surface\npotential models which include the weak Van der Waals region, a long-standing\nchallenge in the study of atom-surface interactions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  All computing devices, including quantum computers, must exhibit that for a\ngiven input, an output is produced in accordance with the program. The outputs\ngenerated by quantum computers that fulfill these requirements are not\ntemporally correlated, however. In a quantum-computing device comprising\nsolid-state qubits such as superconducting qubits, any operation to rest the\nqubits to their initial state faces a practical problem. We applied a\nstatistical analysis to a collection of random numbers output from a 20-qubit\nsuperconducting-qubit cloud quantum computer using the simplest random number\ngeneration scheme. The analysis indicates temporal correlation in the output of\nsome sequences obtained from the 20 qubits. This temporal correlation is not\nrelated to the relaxation time of each qubit. We conclude that the correlation\ncould be a result of a systematic error.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Longitudinal and transverse sound velocities of Lennard-Jones systems are\ncalculated at the liquid-solid coexistence using the additivity principle. The\nresults are shown to agree well with the ``exact'' values obtained from their\nrelations to excess energy and pressure. Some consequences, in particular, in\nthe context of the Lindemann's melting rule and Stokes-Einstein relation\nbetween the self-diffusion and viscosity coefficients are discussed. Comparison\nwith available experimental data on the sound velocities of solid argon at\nmelting conditions is provided.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Preparation of entangled states of photons are useful for quantum computing\nand communication. In this paper, we present a simplistic protocol of\nentanglement generation using beam splitters with suitable reflectivity. The\nphotons in an initial state with fully classical probability distribution pass\nthrough an optical network, made up of sequential beam splitters and are\nprepared in maximally entangled states. We also present the detailed\ntheoretical analysis of entangled state generation, for an arbitrary number of\nphotons, fed through the input ports of the beam splitters with equal\nprobability.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The recent discovery by LIGO/Virgo of a merging binary having a $\\sim 23\nM_\\odot$ black hole and a $\\sim 2.6 M_\\odot$ compact companion has triggered a\ndebate regarding the nature of the secondary, which falls into the so-called\nmass gap. Here we explore some consequences of the assumption that the\nsecondary was a neutron star (NS). We show with concrete examples of heretofore\nviable equations of state (EOSs) that rapid uniform rotation may neither be\nnecessary for some EOSs nor sufficient for others to explain the presence of a\nNS. Absolute upper limits for the maximum mass of a spherical NS derived from\nGW170817 already suggest that this unknown compact companion might be a slowly\nor even a nonrotating NS. However several soft NS EOSs favored by GW170817 with\nmaximum spherical masses $\\lesssim 2.1 M_\\odot$ cannot be invoked to explain\nthis object, even allowing for maximum uniform rotation. By contrast,\nsufficiently stiff EOSs that yield $2.6 M_\\odot$ NSs which are slowly rotating\nor, in some cases, nonrotating, and are compatible with GW170817 and the\nresults of NICER, can account for the black hole companion.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We construct families of commutative (super) algebra objects in the category\nof weight modules for the unrolled restricted quantum group\n$\\overline{U}_q^H(\\mfg)$ of a simple Lie algebra $\\mfg$ at roots of unity, and\nstudy their categories of local modules. We determine their simple modules and\nderive conditions for these categories being finite, non-degenerate, and\nribbon. Motivated by numerous examples in the $\\mfg=\\mathfrak{sl}_2$ case, we\nexpect some of these categories to compare nicely to categories of modules for\nvertex operator algebras. We focus in particular on examples expected to\ncorrespond to the higher rank triplet vertex algebra $W_Q(r)$ of Feigin and\nTipunin \\cite{FT} and the $B_Q(r)$ algebras of \\cite{C1}.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Crowdsourcing has attracted much attention for its convenience to collect\nlabels from non-expert workers instead of experts. However, due to the high\nlevel of noise from the non-experts, an aggregation model that learns the true\nlabel by incorporating the source credibility is required. In this paper, we\npropose a novel framework based on graph neural networks for aggregating crowd\nlabels. We construct a heterogeneous graph between workers and tasks and derive\na new graph neural network to learn the representations of nodes and the true\nlabels. Besides, we exploit the unknown latent interaction between the same\ntype of nodes (workers or tasks) by adding a homogeneous attention layer in the\ngraph neural networks. Experimental results on 13 real-world datasets show\nsuperior performance over state-of-the-art models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An independent set of size $k$ in a finite undirected graph $G$ is a set of\n$k$ vertices of the graph, no two of which are connected by an edge. Let\n$x_{k}(G)$ be the number of independent sets of size $k$ in the graph $G$ and\nlet $\\alpha(G)=\\max\\{k\\geq0\\colon x_{k}(G)\\neq0\\}$. In 1987, Alavi, Malde,\nSchwenk and Erd\\\"{o}s asked if the independent set sequence\n$x_{0}(G),x_{1}(G),\\ldots,x_{\\alpha(G)}(G)$ of a tree is unimodal (the sequence\ngoes up and then down). This problem is still open. In 2006, Levit and\nMandrescu showed that the last third of the independent set sequence of a tree\nis decreasing. We show that the first 46.8\\% of the independent set sequence of\na random tree is increasing with (exponentially) high probability as the number\nof vertices goes to infinity. So, the question of Alavi, Malde, Schwenk and\nErd\\\"{o}s is ``four-fifths true'', with high probability.\n  We also show unimodality of the independent set sequence of Erd\\\"{o}s-Renyi\nrandom graphs, when the expected degree of a single vertex is large (with\n(exponentially) high probability as the number of vertices in the graph goes to\ninfinity, except for a small region near the mode). A weaker result is shown\nfor random regular graphs.\n  The structure of independent sets of size $k$ as $k$ varies is of interest in\nprobability, statistical physics, combinatorics, and computer science.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  As the COVID-19 pandemic is disrupting life worldwide, related online\ncommunities are popping up. In particular, two \"new\" communities, /r/China flu\nand /r/Coronavirus, emerged on Reddit and have been dedicated to COVID- related\ndiscussions from the very beginning of this pandemic. With /r/Coronavirus\npromoted as the official community on Reddit, it remains an open question how\nusers choose between these two highly-related communities.\n  In this paper, we characterize user trajectories in these two communities\nfrom the beginning of COVID-19 to the end of September 2020. We show that new\nusers of /r/China flu and /r/Coronavirus were similar from January to March.\nAfter that, their differences steadily increase, evidenced by both language\ndistance and membership prediction, as the pandemic continues to unfold.\nFurthermore, users who started at /r/China flu from January to March were more\nlikely to leave, while those who started in later months tend to remain highly\n\"loyal\". To understand this difference, we develop a movement analysis\nframework to understand membership changes in these two communities and\nidentify a significant proportion of /r/China flu members (around 50%) that\nmoved to /r/Coronavirus in February. This movement turns out to be highly\npredictable based on other subreddits that users were previously active in. Our\nwork demonstrates how two highly-related communities emerge and develop their\nown identity in a crisis, and highlights the important role of existing\ncommunities in understanding such an emergence.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The rise of flexible electronics calls for cost-effective and scalable\nbatteries with good mechanical and electrochemical performance. In this work,\nwe developed printable, polymer-based AgO-Zn batteries that feature\nflexibility, rechargeability, high areal capacity, and low impedance. Using\nelastomeric substrate and binders, the current collectors, electrodes, and\nseparators can be easily screen-printed layer-by-layer and vacuum-sealed in a\nstacked configuration. The batteries are customizable in sizes and capacities,\nwith the highest obtained areal capacity of 54 mAh/cm2 for primary\napplications. Advanced micro-CT and EIS were used to characterize the battery,\nwhose mechanical stability was tested with repeated twisting and bending. The\nbatteries were used to power a flexible E-ink display system that requires a\nhigh-current drain and exhibited superior performance than commercial coin-cell\nbatteries. The developed battery presents a practical solution for powering a\nwide range of electronics and holds major implications for the future\ndevelopment of practical and high-performance flexible batteries.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Strong gravitational lensing is a powerful tool to measure cosmological\nparameters and to study galaxy evolution mechanisms. However, quantitative\nstrong lensing studies often require mock observations. To capture the full\ncomplexity of galaxies, the lensing galaxy is often drawn from high resolution,\ndark matter only or hydro-dynamical simulations. These have their own\nlimitations, but the way we use them to emulate mock lensed systems may also\nintroduce significant artefacts. In this work we identify and explore the\nspecific impact of mass truncation on simulations of strong lenses by applying\ndifferent truncation schemes to a fiducial density profile with conformal\nisodensity contours. Our main finding is that improper mass truncation can\nintroduce undesired artificial shear. The amplitude of the spurious shear\ndepends on the shape and size of the truncation area as well as on the slope\nand ellipticity of the lens density profile. Due to this effect, the value of\nH0 or the shear amplitude inferred by modelling those systems may be biased by\nseveral percents. However, we show that the effect becomes negligible provided\nthat the lens projected map extends over at least 50 times the Einstein radius.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Using recently acquired HST NIR observations (J, Pa$\\beta$ and H bands) of\nthe nearby galaxy NGC1313, we investigate the timescales required by a young\nstar cluster to emerge from its natal cloud. We search for extincted star\nclusters, potentially embedded in their natal cloud as either: 1. compact\nsources in regions with high H$\\alpha$/Pa$\\beta$ extinctions; 2. compact HII\nregions that appear as point-like sources in the Pa$\\beta$ emission map. The\nNUV--optical--NIR photometry of the candidate clusters is used to derive their\nages, masses and extinctions via a least$-\\chi^2$ SED broad and narrow--band\nfitting process. The 100 clusters in the final samples have masses in the range\n$\\rm \\log_{10}(M/M_\\odot)=2.5-3.5$ and moderate extinctions, $\\rm\nE(B-V)\\lesssim1.0$ mag.\n  Focusing on the young clusters ($0-6$ Myr) we derive a weak correlation\nbetween extinction and age of the clusters. Almost half of the clusters have\nlow extinctions, $\\rm E(B-V)<0.25$ mag, already at very young ages ($\\le3$\nMyr), suggesting that dust is quickly removed from clusters. A stronger\ncorrelation is found between the morphology of the nebular emission (compact,\npartial or absent, both in H$\\alpha$ and Pa$\\beta$) and cluster age.\n  Relative fractions of clusters associated with a specific nebular morphology\nis used to estimate the typical timescales for clearing the natal gas cloud,\nresulting between 3 and 5 Myr, $\\sim1$ Myr older than what estimated from\nNUV--optical--based cluster studies. This difference hints to a bias for\noptically--only based studies, which JWST will address in the coming years.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the influence of chiral symmetry which varies along the\nspace-time evolution of the system by considering the chiral phase transition\nin an non-equilibrium expanding quark-antiquark system. The chiral symmetry is\ndescribed by the mean field order parameter, whose values is the solution of a\nself-consistent equation, and affects the space-time evolution of the system\nthrough the force term in the Vlasov equation. The Vlasov equation and the gap\nequation are solved concurrently and continuously for a longitudinal\nboost-invariant and transversely rotation-invariant system. This numerical\nframework enables us to carefully investigate how the phase transition and\ncollision affect the evolution of the system. It is observed that the chiral\nphase transition gives rise to a kink in the flow velocity, which is caused by\nthe force term in the Vlasov equation. The kink is enhanced by larger\nsusceptibility and tends to be smoothed out by non-equilibrium effect. The\nspatial phase boundary appears as a \"wall\" for the quarks, as the quarks with\nlow momentum are bounced back, while those with high momentum go through the\nwall but are slowed down.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Lexical semantic typology has identified important cross-linguistic\ngeneralizations about the variation and commonalities in polysemy\npatterns---how languages package up meanings into words. Recent computational\nresearch has enabled investigation of lexical semantics at a much larger scale,\nbut little work has explored lexical typology across semantic domains, nor the\nfactors that influence cross-linguistic similarities. We present a novel\ncomputational framework that quantifies semantic affinity, the cross-linguistic\nsimilarity of lexical semantics for a concept. Our approach defines a common\nmultilingual semantic space that enables a direct comparison of the lexical\nexpression of concepts across languages. We validate our framework against\nempirical findings on lexical semantic typology at both the concept and domain\nlevels. Our results reveal an intricate interaction between semantic domains\nand extra-linguistic factors, beyond language phylogeny, that co-shape the\ntypology of polysemy across languages.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Large machine learning models based on Convolutional Neural Networks (CNNs)\nwith rapidly increasing number of parameters, trained with massive amounts of\ndata, are being deployed in a wide array of computer vision tasks from\nself-driving cars to medical imaging. The insatiable demand for computing\nresources required to train these models is fast outpacing the advancement of\nclassical computing hardware, and new frameworks including Optical Neural\nNetworks (ONNs) and quantum computing are being explored as future\nalternatives.\n  In this work, we report a novel quantum computing based deep learning model,\nthe Quantum Optical Convolutional Neural Network (QOCNN), to alleviate the\ncomputational bottleneck in future computer vision applications. Using the\npopular MNIST dataset, we have benchmarked this new architecture against a\ntraditional CNN based on the seminal LeNet model. We have also compared the\nperformance with previously reported ONNs, namely the GridNet and ComplexNet,\nas well as a Quantum Optical Neural Network (QONN) that we built by combining\nthe ComplexNet with quantum based sinusoidal nonlinearities. In essence, our\nwork extends the prior research on QONN by adding quantum convolution and\npooling layers preceding it.\n  We have evaluated all the models by determining their accuracies, confusion\nmatrices, Receiver Operating Characteristic (ROC) curves, and Matthews\nCorrelation Coefficients. The performance of the models were similar overall,\nand the ROC curves indicated that the new QOCNN model is robust. Finally, we\nestimated the gains in computational efficiencies from executing this novel\nframework on a quantum computer. We conclude that switching to a quantum\ncomputing based approach to deep learning may result in comparable accuracies\nto classical models, while achieving unprecedented boosts in computational\nperformances and drastic reduction in power consumption.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Polycrystalline members of the GaV$_4$S$_{8-y}$Se$_y$ family of materials\nwith small levels of substitution between $0 \\leq y \\leq 0.5$ and $7.5 \\leq\ny\\leq 8$ have been synthesized in order to investigate their magnetic and\nstructural properties. Substitutions to the skyrmion hosting parent compounds\nGaV$_4$S$_8$ and GaV$_4$Se$_8$, are found to suppress the temperature of the\ncubic to rhombohedral structural phase transition that occurs in both end\ncompounds and to create a temperature region around the transition where there\nis a coexistence of these two phases. Similarly, the magnitude of the\nmagnetization and temperature of the magnetic transition are both suppressed in\nall substituted compounds until a glassy-like magnetic state is realized. There\nis evidence from the $ac$ susceptibility data that skyrmion lattices with\nsimilar dynamics to those in GaV$_4$S$_8$ and GaV$_4$Se$_8$ are present in\ncompounds with very low levels of substitution, $0 < y< 0.2$ and $7.8 < y < 8$,\nhowever, these states vanish at higher levels of substitution. The magnetic\nproperties of these substituted materials are affected by the substitution\naltering exchange pathways and resulting in the creation of increasingly\ndisordered magnetic states.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the current work we study a stochastic parabolic problem. The underlying\nproblem is actually motivated by the study of an idealized electrically\nactuated MEMS (Micro-Electro-Mechanical System) device in the case of random\nfluctuations of the potential difference controlling the device.\n  We first present the mathematical model and then we deduce some local\nexistence results. Next for some particular versions of the model, regarding\nits boundary conditions, we derive quenching results as well as estimations of\nthe probability for such singularity to occur. Additional numerical study of\nthe problem in one dimension follows, investigating the problem further with\nrespect to its quenching behaviour.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Thermal frequency fluctuations in optical cavities limit the sensitivity of\nprecision experiments ranging from gravitational wave observatories to optical\natomic clocks. Conventional modeling of these noises assumes a linear response\nof the optical field to the fluctuations of cavity frequency. Fundamentally,\nhowever, this response is nonlinear. Here we show that nonlinearly transduced\nthermal fluctuations of cavity frequency can dominate the broadband noise in\nphotodetection, even when the magnitude of fluctuations is much smaller than\nthe cavity linewidth. We term this noise \"thermal intermodulation noise\" and\nshow that for a resonant laser probe it manifests as intensity fluctuations. We\nreport and characterize thermal intermodulation noise in an optomechanical\ncavity, where the frequency fluctuations are caused by mechanical Brownian\nmotion, and find excellent agreement with our developed theoretical model. We\ndemonstrate that the effect is particularly relevant to quantum optomechanics:\nusing a phononic crystal $Si_3N_4$ membrane with a low mass, soft-clamped\nmechanical mode we are able to operate in the regime where measurement quantum\nbackaction contributes as much force noise as the thermal environment does.\nHowever, in the presence of intermodulation noise, quantum signatures of\nmeasurement are not revealed in direct photodetectors. The reported noise\nmechanism, while studied for an optomechanical system, can exist in any optical\ncavity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Interpretation and improvement of deep neural networks relies on better\nunderstanding of their underlying mechanisms. In particular, gradients of\nclasses or concepts with respect to the input features (e.g., pixels in images)\nare often used as importance scores or estimators, which are visualized in\nsaliency maps. Thus, a family of saliency methods provide an intuitive way to\nidentify input features with substantial influences on classifications or\nlatent concepts. Several modifications to conventional saliency maps, such as\nRectified Gradients and Layer-wise Relevance Propagation (LRP), have been\nintroduced to allegedly denoise and improve interpretability. While visually\ncoherent in certain cases, Rectified Gradients and other modified saliency maps\nintroduce a strong input bias (e.g., brightness in the RGB space) because of\ninappropriate uses of the input features. We demonstrate that dark areas of an\ninput image are not highlighted by a saliency map using Rectified Gradients,\neven if it is relevant for the class or concept. Even in the scaled images, the\ninput bias exists around an artificial point in color spectrum. Our\nmodification, which simply eliminates multiplication with input features,\nremoves this bias. This showcases how a visual criteria may not align with true\nexplainability of deep learning models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Minimally invasive medical procedures, such as endovascular catheterization,\nhave drastically reduced procedure time and associated complications. However,\nmany regions inside the body, such as in the brain vasculature, still remain\ninaccessible due to the lack of appropriate guidance technologies. Here,\nexperimentally and through numerical simulations, we show that tethered\nultra-flexible endovascular microscopic probes can be transported through\ntortuous vascular networks almost effortlessly by harnessing hydrokinetic\nenergy. Dynamic steering at bifurcations is performed by deformation of the\nprobe head using magnetic actuation. We developed an endovascular microrobotic\ntoolkit with a cross-sectional area that is approximately three orders of\nmagnitude smaller than the smallest microcatheter currently available. Our\ntechnology has the potential to revolutionize the state-of-the-art practices as\nit enhances the reachability, reduces the risk of iatrogenic damage,\nsignificantly increases the speed of robot-assisted interventions, and enables\nthe deployment of multiple leads simultaneously through a standard needle\ninjection.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The signal processing firmware that has been developed for the Low Frequency\nAperture Array component of the Square Kilometre Array is described. The\nfirmware is implemented on a dual FPGA board, that is capable of processing the\nstreams from 16 dual polarization antennas. Data processing includes\nchannelization of the sampled data for each antenna, correction for\ninstrumental response and for geometric delays and formation of one or more\nbeams by combining the aligned streams. The channelizer uses an oversampling\npolyphase filterbank architecture, allowing a frequency continuous processing\nof the input signal without discontinuities between spectral channels. Each\nboard processes the streams from 16 antennas, as part of larger beamforming\nsystem, linked by standard Ethernet interconnections. There are envisaged to be\n8192 of these signal processing platforms in the first phase of the Square\nKilometre array so particular attention has been devoted to ensure the design\nis low cost and low power.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce a new general framework for the approximation of evolution\nequations at low regularity and develop a new class of schemes for a wide range\nof equations under lower regularity assumptions than classical methods require.\nIn contrast to previous works, our new framework allows a unified practical\nformulation and the construction of the new schemes does not rely on any\nFourier based expansions. This allows us for the first time to overcome the\nsevere restriction to periodic boundary conditions, to embed in the same\nframework parabolic and dispersive equations and to handle nonlinearities that\nare not polynomial. In particular, as our new formalism does no longer require\nperiodicity of the problem, one may couple the new time discretisation\ntechnique not only with spectral methods, but rather with various spatial\ndiscretisations. We apply our general theory to the time discretization of\nvarious concrete PDEs, such as the nonlinear heat equation, the nonlinear\nSchr\\\"odinger equation, the complex Ginzburg-Landau equation, the half wave and\nKlein--Gordon equations, set in $\\Omega \\subset \\mathbb{R}^d$, $d \\leq 3$ with\nsuitable boundary conditions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider an embedded $n$-dimensional compact complex manifold in $n+d$\ndimensional complex manifolds. We are interested in the holomorphic\nclassification of neighborhoods as part of Grauert's formal principle program.\nWe will give conditions ensuring that a neighborhood of $C_n$ in $M_{n+d}$ is\nbiholomorphic to a neighborhood of the zero section of its normal bundle. This\nextends Arnold's result about neighborhoods of a complex torus in a surface. We\nalso prove the existence of a holomorphic foliation in $M_{n+d }$ having $C_n$\nas a compact leaf, extending Ueda's theory to the high codimension case. Both\nproblems appear as a kind linearization problem involving small divisors\ncondition arising from solutions to their cohomological equations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, authors propose a new procedure to provide a tool for\nmonitoring critical infrastructures. Particularly, through the analysis of\nCOSMO-SkyMed satellite data, a detailed and updated survey is provided, for\nmonitoring the accelerating destabilization process of the Mosul dam, that\nrepresents the largest hydraulic facility of Iraq and is located on the Tigris\nriver. The destructive potential of the wave that would be generated, in the\nevent of the dam destruction, could have serious consequences. If the concern\nfor human lives comes first, the concern for cultural heritage protection is\nnot negligible, since several archaeological sites are located around the Mosul\ndam. The proposed procedure is an in-depth modal assessment based on the\nmicro-motion estimation, through a Doppler sub-apertures tracking and a\nMulti-Chromatic Analysis (MCA). The method is based initially on the Persistent\nScatterers Interferometry (PSI) that is also discussed for completeness and\nvalidation. The modal analysis has detected the presence of several areas of\nresonance that could mean the presence of cracks, and the results have shown\nthat the dam is still in a strong destabilization. Moreover, the dam appears to\nbe divided into two parts: the northern part is accelerating rapidly while the\nsouthern part is decelerating and a main crack in this north-south junction is\nfound. The estimated velocities through the PS-InSAR technique show a good\nagreement with the GNSS in-situ measurements, resulting in a very high\ncorrelation coefficient and showing how the proposed procedure works\nefficiently.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The problem whether a given pair of functions can be used as the kernels of a\ngeneralized fractional derivative and the associated generalized fractional\nintegral is reduced to the problem of existence of a solution to the Sonine\nequation. It is shown for some selected classes of functions that a necessary\ncondition for a function to be the kernel of a fractional derivative is an\nintegrable singularity at 0. It is shown that locally integrable completely\nmonotone functions satisfy the Sonine equation if and only if they are singular\nat 0.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Existing methods of level generation using latent variable models such as\nVAEs and GANs do so in segments and produce the final level by stitching these\nseparately generated segments together. In this paper, we build on these\nmethods by training VAEs to learn a sequential model of segment generation such\nthat generated segments logically follow from prior segments. By further\ncombining the VAE with a classifier that determines whether to place the\ngenerated segment to the top, bottom, left or right of the previous segment, we\nobtain a pipeline that enables the generation of arbitrarily long levels that\nprogress in any of these four directions and are composed of segments that\nlogically follow one another. In addition to generating more coherent levels of\nnon-fixed length, this method also enables implicit blending of levels from\nseparate games that do not have similar orientation. We demonstrate our\napproach using levels from Super Mario Bros., Kid Icarus and Mega Man, showing\nthat our method produces levels that are more coherent than previous latent\nvariable-based approaches and are capable of blending levels across games.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Supercontinuum (SC) light source has advanced ultrafast laser spectroscopy in\ncondensed matter science, biology, physics, and chemistry. Compared to the\nfrequently used photonic crystal fibers and bulk materials, femtosecond laser\nfilamentation in gases is damage-immune for supercontinuum generation. A\nbottleneck problem is the strong jitters from filament induced self-heating at\nkHz repetition rate level. We demonstrate stable kHz supercontinuum generation\ndirectly in air with multiple mJ level pulse energy. This is achieved by\napplying an external DC electric field to the air plasma filament through the\neffects of plasma wave guiding and Coulomb interaction. Both pointing and\nintensity jitters of 1 kHz air filament induced SC light are reduced by more\nthan 2 fold. This offers the opportunities for stable intense SC generation and\nother laser filament based applications in air.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Using a sample of nearly 140,000 primary red clump stars selected from the\nLAMOST and $Gaia$ surveys, we have identified a large sample of \"young\"\n[$\\alpha$/Fe]-enhanced stars with stellar ages younger than 6.0 Gyr and\n[$\\alpha$/Fe] ratios greater than 0.15 dex. The stellar ages and [$\\alpha$/Fe]\nratios are measured from LAMOST spectra, using a machine learning method\ntrained with common stars in the LAMOST-APOGEE fields (for [$\\alpha$/Fe]) and\nin the LAMOST-$Kepler$ fields (for stellar age). The existence of these \"young\"\n[$\\alpha$/Fe]-enhanced stars is not expected from the classical Galactic\nchemical evolution models. To explore their possible origins, we have analyzed\nthe spatial distribution, and the chemical and kinematic properties of those\nstars and compared the results with those of the chemically thin and thick disk\npopulations. We find that those \"young\" [$\\alpha$/Fe]-enhanced stars have\ndistributions in number density, metallicity, [C/N] abundance ratio, velocity\ndispersion and orbital eccentricity that are essentially the same as those of\nthe chemically thick disk population. Our results clearly show those so-called\n\"young\" [$\\alpha$/Fe]-enhanced stars are not really young but $genuinely$\n$old$. Although other alternative explanations can not be fully ruled out, our\nresults suggest that the most possible origin of these old stars is the result\nof stellar mergers or mass transfer.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a recent Chandra observation of the quiescent low-mass X-ray\nbinary containing a neutron star, located in the globular cluster M30. We fit\nthe thermal emission from the neutron star to extract its mass and radius. We\nfind no evidence of flux variability between the two observations taken in 2001\nand 2017, nor between individual 2017 observations, so we analyse them together\nto increase the signal to noise. We perform simultaneous spectral fits using\nstandard light-element composition atmosphere models (hydrogen or helium),\nincluding absorption by the interstellar medium, correction for pile-up of\nX-ray photons on the detector, and a power-law for count excesses at high\nphoton energy. Using a Markov-chain Monte Carlo approach, we extract mass and\nradius credible intervals for both chemical compositions of the atmosphere:\n$R_{\\textrm{NS}}=7.94^{+0.76}_{-1.21}$ km and $M_{\\textrm{NS}}<1.19$\nM$_{\\odot}$ assuming pure hydrogen, and $R_{\\textrm{NS}}=10.50^{+2.88}_{-2.03}$\nkm and $M_{\\textrm{NS}}<1.78$ M$_{\\odot}$ for helium, where the uncertainties\nrepresent the 90% credible regions. For H, the small radius is difficult to\nreconcile with most current nuclear physics models (especially for nucleonic\nequations of state) and with other measurements of neutron star radii, with\nrecent preferred values generally in the 11-14 km range. Whereas for He, the\nmeasured radius is consistent with this range. We discuss possible sources of\nsystematic uncertainty that may result in an underestimation of the radius,\nidentifying the presence of surface temperature inhomogeneities as the most\nrelevant bias. According to this, we conclude that either the atmosphere is\ncomposed of He, or it is a H atmosphere with a significant contribution of hot\nspots to the observed radiation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The rapid development and wide utilization of object detection techniques\nhave aroused attention on both accuracy and speed of object detectors. However,\nthe current state-of-the-art object detection works are either\naccuracy-oriented using a large model but leading to high latency or\nspeed-oriented using a lightweight model but sacrificing accuracy. In this\nwork, we propose YOLObile framework, a real-time object detection on mobile\ndevices via compression-compilation co-design. A novel block-punched pruning\nscheme is proposed for any kernel size. To improve computational efficiency on\nmobile devices, a GPU-CPU collaborative scheme is adopted along with advanced\ncompiler-assisted optimizations. Experimental results indicate that our pruning\nscheme achieves 14$\\times$ compression rate of YOLOv4 with 49.0 mAP. Under our\nYOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung\nGalaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the\ninference speed is increased to 19.1 FPS, and outperforms the original YOLOv4\nby 5$\\times$ speedup. Source code is at:\n\\url{https://github.com/nightsnack/YOLObile}.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The purpose of this paper is extend recent results of Bonder-Groisman and\nFoondun-Nualart to the stochastic wave equation. In particular, a suitable\nintegrability condition for non-existence of global solutions is derived.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that the Kounterterms for pure AdS gravity in arbitrary even\ndimensions coincide with the boundary counterterms obtained through holographic\nrenormalization if and only if the boundary Weyl tensor vanishes. In\nparticular, the Kounterterms lead to a well posed variational problem for\ngeneric asymptotically locally AdS manifolds only in four dimensions. We\ndetermine the exact form of the counterterms for conformally flat boundaries\nand demonstrate that, in even dimensions, the Kounterterms take exactly the\nsame form. This agreement can be understood as a consequence of Anderson's\ntheorem for the renormalized volume of conformally compact Einstein 4-manifolds\nand its higher dimensional generalizations by Albin and Chang, Qing and Yang.\nFor odd dimensional asymptotically locally AdS manifolds with a conformally\nflat boundary, the Kounterterms coincide with the boundary counterterms except\nfor the logarithmic divergence associated with the holographic conformal\nanomaly, and finite local terms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The visionary work of Veselago had inspired intensive research efforts over\nthe last decade, towards the realization of man-made structures with\nunprecedented electromagnetic (EM) properties. These structures, known as\nmetamaterials, are typically periodic metallic-based resonant structures\ndemonstrating effective constitutive parameters beyond the possibilities of\nnatural material. For example they can exhibit optical magnetism or\nsimultaneously negative effective permeability and permittivity which implies\nthe existence of a negative refractive index. However, also periodic dielectric\nand polar material, known as photonic crystals, can exhibit EM capabilities\nbeyond natural materials. This paper reviews the conditions and manifestations\nof metamaterial capabilities of photonic crystal systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Money laundering is the crucial mechanism utilized by criminals to inject\nproceeds of crime to the financial system. The primary responsibility of the\ndetection of suspicious activity related to money laundering is with the\nfinancial institutions. Most of the current systems in these institutions are\nrule-based and ineffective. The available data science-based anti-money\nlaundering (AML) models in order to replace the existing rule-based systems\nwork on customer relationship management (CRM) features and time\ncharacteristics of transaction behaviour. However, there is still a challenge\non accuracy and problems around feature engineering due to thousands of\npossible features.\n  Aiming to improve the detection performance of suspicious transaction\nmonitoring systems for AML systems, in this article, we introduce a novel\nfeature set based on time-frequency analysis, that makes use of 2-D\nrepresentations of financial transactions. Random forest is utilized as a\nmachine learning method, and simulated annealing is adopted for hyperparameter\ntuning. The designed algorithm is tested on real banking data, proving the\nefficacy of the results in practically relevant environments. It is shown that\nthe time-frequency characteristics of suspicious and non-suspicious entities\ndifferentiate significantly, which would substantially improve the precision of\ndata science-based transaction monitoring systems looking at only time-series\ntransaction and CRM features.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove the existence of non-trivial phase transitions for the intersection\nof two independent random interlacements and the complement of the\nintersection. Some asymptotic results about the phase curves are also obtained.\nMoreover, we show that at least one of these two sets percolates in high\ndimensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent advances in noiseless non-adaptive group testing have led to a precise\nasymptotic characterization of the number of tests required for\nhigh-probability recovery in the sublinear regime $k = n^{\\theta}$ (with\n$\\theta \\in (0,1)$), with $n$ individuals among which $k$ are infected.\nHowever, the required number of tests may increase substantially under\nreal-world practical constraints, notably including bounds on the maximum\nnumber $\\Delta$ of tests an individual can be placed in, or the maximum number\n$\\Gamma$ of individuals in a given test. While previous works have given\nrecovery guarantees for these settings, significant gaps remain between the\nachievability and converse bounds. In this paper, we substantially or\ncompletely close several of the most prominent gaps. In the case of\n$\\Delta$-divisible items, we show that the definite defectives (DD) algorithm\ncoupled with a random regular design is asymptotically optimal in dense scaling\nregimes, and optimal to within a factor of $\\eul$ more generally; we establish\nthis by strengthening both the best known achievability and converse bounds. In\nthe case of $\\Gamma$-sized tests, we provide a comprehensive analysis of the\nregime $\\Gamma = \\Theta(1)$, and again establish a precise threshold proving\nthe asymptotic optimality of SCOMP (a slight refinement of DD) equipped with a\ntailored pooling scheme. Finally, for each of these two settings, we provide\nnear-optimal adaptive algorithms based on sequential splitting, and provably\ndemonstrate gaps between the performance of optimal adaptive and non-adaptive\nalgorithms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  User-generated content platforms curate their vast repositories into thematic\ncompilations that facilitate the discovery of high-quality material. Platforms\nthat seek tight editorial control employ people to do this curation, but this\nprocess involves time-consuming routine tasks, such as sifting through\nthousands of videos. We introduce Sifter, a system that improves the curation\nprocess by combining automated techniques with a human-powered pipeline that\nbrowses, selects, and reaches an agreement on what videos to include in a\ncompilation. We evaluated Sifter by creating 12 compilations from over 34,000\nuser-generated videos. Sifter was more than three times faster than dedicated\ncurators, and its output was of comparable quality. We reflect on the\nchallenges and opportunities introduced by Sifter to inform the design of\ncontent curation systems that need subjective human judgments of videos at\nscale.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The planet Mercury possesses an anomalously large iron core, and a\ncorrespondingly high bulk density. Numerous hypotheses have been proposed in\norder to explain such a large iron content. A long-standing idea holds that\nMercury once possessed a larger silicate mantle which was removed by a giant\nimpact early in the the Solar system's history. A central problem with this\nidea has been that material ejected from Mercury is typically re-accreted onto\nthe planet after a short (~Myr) timescale. Here, we show that the primordial\nSolar wind would have provided sufficient drag upon ejected debris to remove\nthem from Mercury-crossing trajectories before re-impacting the planet's\nsurface. Specifically, the young Sun likely possessed a stronger wind, fast\nrotation and strong magnetic field. Depending upon the time of the giant\nimpact, the ram pressure associated with this wind would push particles outward\ninto the Solar system, or inward toward the Sun, on sub-Myr timescales,\ndepending upon the size of ejected debris. Accordingly, the giant impact\nhypothesis remains a viable pathway toward the removal of planetary mantles,\nboth on Mercury and extrasolar planets, particularly those close to young stars\nwith strong winds.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Random lasing occurs as the result of a coherent optical feedback from\nmultiple scattering centers. Here, we demonstrate that plasmonic gold nanostars\nare efficient light scattering centers, exhibiting strong field enhancement at\ntheir nanotips, which assists a very narrow bandwidth and highly amplified\ncoherent random lasing with a low lasing threshold. First, by embedding\nplasmonic gold nanostars in a rhodamine 6G dye gain medium, we observe a series\nof very narrow random lasing peaks with full-width at half-maximum ~ 0.8 nm. In\ncontrast, free rhodamine 6G dye molecules exhibit only a single amplified\nspontaneous emission peak with a broader linewidth of 6 nm. The lasing\nthreshold for the dye with gold nanostars is two times lower than that for a\nfree dye. Furthermore, by coating the tip of a single-mode optical fiber with\ngold nanostars, we demonstrate a collection of random lasing signal through the\nfiber that can be easily guided and analyzed. Time-resolved measurements show a\nsignificant increase in the emission rate above the lasing threshold,\nindicating a stimulated emission process. Our study provides a method for\ngenerating random lasing in the nanoscale with low threshold values that can be\neasily collected and guided, which promise a range of potential applications in\nremote sensing, information processing, and on-chip coherent light sources.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With the increasing interest in converter-fed islanded microgrids,\nparticularly for resilience, it is becoming more critical to understand the\ndynamical behavior of these systems. This paper takes a holistic view of\ngrid-forming converters and considers control approaches for both modeling and\nregulating the DC-link voltage when the DC-source is a battery energy storage\nsystem. We are specifically interested in understanding the performance of\nthese controllers, subject to large load changes, for decreasing values of the\nDC-side capacitance. We consider a fourth, second, and zero-order model of the\nbattery; and establish that the zero-order model captures the dynamics of\ninterest for the timescales considered for disturbances examined. Additionally,\nwe adapt a grid search for optimizing the controller parameters of the DC/DC\ncontroller and show how the inclusion of AC side measurements into the DC/DC\ncontroller can improve its dynamic performance. This improvement in performance\noffers the opportunity to reduce the DC-side capacitance given an admissible DC\nvoltage transient deviation, thereby, potentially allowing for more reliable\ncapacitor technology to be deployed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  M-quantile regression is a general form of quantile-like regression which\nusually utilises the Huber influence function and corresponding tuning\nconstant. Estimation requires a nuisance scale parameter to ensure the\nM-quantile estimates are scale invariant, with several scale estimators having\npreviously been proposed. In this paper we assess these scale estimators and\nevaluate their suitability, as well as proposing a new scale estimator based on\nthe method of moments. Further, we present two approaches for estimating\ndata-driven tuning constant selection for M-quantile regression. The tuning\nconstants are obtained by i) minimising the estimated asymptotic variance of\nthe regression parameters and ii) utilising an inverse M-quantile function to\nreduce the effect of outlying observations. We investigate whether data-driven\ntuning constants, as opposed to the usual fixed constant, for instance, at\nc=1.345, can improve the efficiency of the estimators of M-quantile regression\nparameters. The performance of the data-driven tuning constant is investigated\nin different scenarios using model-based simulations. Finally, we illustrate\nthe proposed methods using a European Union Statistics on Income and Living\nConditions data set.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  By using low-dimensional chaos maps, the power law relationship established\nbetween the sample mean and variance called Taylor's Law (TL) is studied. In\nparticular, we aim to clarify the relationship between TL from the spatial\nensemble (STL) and the temporal ensemble (TTL). Since the spatial ensemble\ncorresponds to independent sampling from a stationary distribution, we confirm\nthat STL is explained by the skewness of the distribution. The difference\nbetween TTL and STL is shown to be originated in the temporal correlation of a\ndynamics. In case of logistic and tent maps, the quadratic relationship in the\nmean and variance, called Bartlett's law, is found analytically. On the other\nhand, TTL in the Hassell model can be well explained by the chunk structure of\nthe trajectory, whereas the TTL of the Ricker model have a different mechanism\noriginated from the specific form of the map.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Stochastic simulation algorithms (SSAs) are widely used to numerically\ninvestigate the properties of stochastic, discrete-state models. The Gillespie\nDirect Method is the pre-eminent SSA, and is widely used to generate sample\npaths of so-called agent-based or individual-based models. However, the\nsimplicity of the Gillespie Direct Method often renders it impractical where\nlarge-scale models are to be analysed in detail. In this work, we carefully\nmodify the Gillespie Direct Method so that it uses a customised binary decision\ntree to trace out sample paths of the model of interest. We show that a\ndecision tree can be constructed to exploit the specific features of the chosen\nmodel. Specifically, the events that underpin the model are placed in\ncarefully-chosen leaves of the decision tree in order to minimise the work\nrequired to keep the tree up-to-date. The computational efficencies that we\nrealise can provide the apparatus necessary for the investigation of\nlarge-scale, discrete-state models that would otherwise be intractable. Two\ncase studies are presented to demonstrate the efficiency of the method.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An optical monitoring survey in the nearby dwarf galaxies was carried out\nwith the 2.5-m Isaac Newton Telescope (INT). 55 dwarf galaxies and four\nisolated globular clusters in the Local Group (LG) were observed with the Wide\nField Camera (WFC). The main aims of this survey are to identify the most\nevolved asymptotic giant branch (AGB) stars and red supergiants at the\nend-point of their evolution based on their pulsational instability, use their\ndistribution over luminosity to reconstruct the star formation history,\nquantify the dust production and mass loss from modelling the multi-wavelength\nspectral energy distributions, and relate this to luminosity and radius\nvariations. In this first of a series of papers, we present the methodology of\nthe variability survey and describe the photometric catalogue of Andromeda I\n(And I) dwarf galaxy as an example of the survey, and discuss the identified\nlong period variable (LPV) stars. We detected 5581 stars and identified 59 LPV\ncandidates within two half-light radii of the centre of And I. The amplitudes\nof these candidates range from 0.2 to 3 mag in the $i$-band. 75 % of detected\nsources and 98 % of LPV candidates are detected at mid-infrared wavelengths. We\nshow evidence for the presence of dust-producing AGB stars in this galaxy\nincluding five extreme AGB (x-AGB) stars, and model some of their spectral\nenergy distributions. A distance modulus of 24.41 mag for And I was determined\nbased on the tip of the red giant branch (RGB). Also, a half-light radius of\n3.2 arcmin is calculated.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We characterize the subsets $E \\subset \\mathbb{R}$ for which there exists a\ncontinuous real valued function $f: \\mathbb{R}\\to\\mathbb{R}$ such that lip $f$\nis finite everywhere and Lip $f$ is infinite exactly on $E$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present an ion-lattice quantum processor based on a two-dimensional\narrangement of linear surface traps. Our design features a tunable coupling\nbetween ions in adjacent lattice sites and a configurable ion-lattice\nconnectivity, allowing one, e.g., to realize rectangular and triangular\nlattices with the same trap chip. We present detailed trap simulations of a\nsimplest-instance ion array with $2\\times9$ trapping sites and report on the\nfabrication of a prototype device in an industrial facility. The design and the\nemployed fabrication processes are scalable to larger array sizes. We\ndemonstrate trapping of ions in rectangular and triangular lattices and\ndemonstrate transport of a $2\\times2$ ion-lattice over one lattice period.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent work by Clark et al. (2020) shows that transformers can act as 'soft\ntheorem provers' by answering questions over explicitly provided knowledge in\nnatural language. In our work, we take a step closer to emulating formal\ntheorem provers, by proposing PROVER, an interpretable transformer-based model\nthat jointly answers binary questions over rule-bases and generates the\ncorresponding proofs. Our model learns to predict nodes and edges corresponding\nto proof graphs in an efficient constrained training paradigm. During\ninference, a valid proof, satisfying a set of global constraints is generated.\nWe conduct experiments on synthetic, hand-authored, and human-paraphrased\nrule-bases to show promising results for QA and proof generation, with strong\ngeneralization performance. First, PROVER generates proofs with an accuracy of\n87%, while retaining or improving performance on the QA task, compared to\nRuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained\non questions requiring lower depths of reasoning, it generalizes significantly\nbetter to higher depths (up to 15% improvement). Third, PROVER obtains near\nperfect QA accuracy of 98% using only 40% of the training data. However,\ngenerating proofs for questions requiring higher depths of reasoning becomes\nchallenging, and the accuracy drops to 65% for 'depth 5', indicating\nsignificant scope for future work. Our code and models are publicly available\nat https://github.com/swarnaHub/PRover\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Viscosity is an important property of out-of-equilibrium systems such as\nactive biological materials and driven non-Newtonian fluids, and for fields\nranging from biomaterials to geology, energy technologies and medicine.\nHowever, noninvasive viscosity measurements typically require integration times\nof seconds. Here we demonstrate a four orders-of-magnitude improvement in\nspeed, down to twenty microseconds, with uncertainty dominated by fundamental\nthermal noise for the first time. We achieve this using the instantaneous\nvelocity of a trapped particle in an optical tweezer. To resolve the\ninstantaneous velocity we develop a structured-light detection system that\nallows particle tracking with megahertz bandwidths. Our results translate\nviscosity from a static averaged property, to one that may be dynamically\ntracked on the timescales of active dynamics. This opens a pathway to new\ndiscoveries in out-of-equilibrium systems, from the fast dynamics of phase\ntransitions, to energy dissipation in motor molecule stepping, to violations of\nfluctuation laws of equilibrium thermodynamics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We construct for every connected surface $S$ of finite negative Euler\ncharacteristic and every $H \\in [0,1)$, a hyperbolic 3-manifold $N(S,H)$ of\nfinite volume and a proper, two-sided, totally umbilic embedding $f\\colon S\\to\nN(S,H)$ with mean curvature $H$. Conversely, we prove that a complete, totally\numbilic surface with mean curvature $H \\in [0,1)$ embedded in a hyperbolic\n3-manifold of finite volume must be proper and have finite negative Euler\ncharacteristic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Optical dielectric metasurfaces composed of arrayed nanostructures are\nexpected to enable arbitrary spatial control of incident wavefronts with\nsubwavelength spatial resolution. For phase modulation, one often resorts to\ntwo physical effects to implement a 2\\pi-phase excursion. The first effect\nrelies on guidance by tall nanoscale pillars and the second one exploits\nresonant confinement by nanoresonators with two degenerate Mie-resonances. The\nfirst approach requires high aspect ratios, while the second one, known as\nHuygens metasurfaces, is much flatter, and thus easier to manufacture. We\ncompare the two approaches, more focusing on conceptual rather than\ntechnological issues, and identify fundamental limitations with the latter. We\nexplain the origin of the limitations based on general arguments, such as\nreciprocity, multimode/monomode operation and symmetry breaking.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a detailed study of the magnetic and electronic properties of\nU$_2$Rh$_3$Si$_5$, a material that has been demonstrated to exhibit a first\norder antiferromagnetic phase transition. From a high magnetic field study,\ntogether with extensive experiments in moderate fields, we establish the\nmagnetic phase diagrams for all crystallographic directions. The possibility of\nan electronic phase in a narrow interval above the N\\'eel temperature as a\nprecursor of a magnetic phase is discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is known from the algebraic graph theory that if $L$ is the Laplacian\nmatrix of some tree $G$ with a vertex degree sequence $\\mathbf{d}=(d_1, ...,\nd_n)^\\top$ and $D$ is its distance matrix, then\n$LD+2I=(2\\cdot\\mathbf{1}-\\mathbf{d})\\mathbf{1}^\\top$, where $\\mathbf{1}$ is an\nall-ones column vector. We prove that if this matrix identity holds for the\nLaplacian matrix of some graph $G$ with a degree sequence $\\mathbf{d}$ and for\nsome matrix $D$, then $G$ is essentially a tree, and $D$ is its distance\nmatrix. This result immediately generalizes to weighted graphs. If the matrix\n$D$ is symmetric, the lower triangular part of this matrix identity is\nredundant and can be omitted. Therefore, the above bilinear matrix equation in\n$L$, $D$, and $\\mathbf{d}$ characterizes trees in terms of their Laplacian and\ndistance matrices. Applications to the extremal graph theory (especially, to\ntopological index optimization and to optimal tree problems) and to road\ntopology design are discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we propose the Sparse Multi-Family Deep Scattering Network\n(SMF-DSN), a novel architecture exploiting the interpretability of the Deep\nScattering Network (DSN) and improving its expressive power. The DSN extracts\nsalient and interpretable features in signals by cascading wavelet transforms,\ncomplex modulus and extract the representation of the data via a\ntranslation-invariant operator. First, leveraging the development of highly\nspecialized wavelet filters over the last decades, we propose a multi-family\napproach to DSN. In particular, we propose to cross multiple wavelet transforms\nat each layer of the network, thus increasing the feature diversity and\nremoving the need for an expert to select the appropriate filter. Secondly, we\ndevelop an optimal thresholding strategy adequate for the DSN that regularizes\nthe network and controls possible instabilities induced by the signals, such as\nnon-stationary noise. Our systematic and principled solution sparsifies the\nnetwork's latent representation by acting as a local mask distinguishing\nbetween activity and noise. The SMF-DSN enhances the DSN by (i) increasing the\ndiversity of the scattering coefficients and (ii) improves its robustness with\nrespect to non-stationary noise.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Generalized Canonical Correlation Analysis (GCCA) is an important tool that\nfinds numerous applications in data mining, machine learning, and artificial\nintelligence. It aims at finding `common' random variables that are strongly\ncorrelated across multiple feature representations (views) of the same set of\nentities. CCA and to a lesser extent GCCA have been studied from the\nstatistical and algorithmic points of view, but not as much from the standpoint\nof linear algebra. This paper offers a fresh algebraic perspective of GCCA\nbased on a (bi-)linear generative model that naturally captures its essence. It\nis shown that from a linear algebra point of view, GCCA is tantamount to\nsubspace intersection; and conditions under which the common subspace of the\ndifferent views is identifiable are provided. A novel GCCA algorithm is\nproposed based on subspace intersection, which scales up to handle large GCCA\ntasks. Synthetic as well as real data experiments are provided to showcase the\neffectiveness of the proposed approach.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $A(G)$ and $D(G)$ be the adjacency matrix and the degree diagonal matrix\nof a graph $G$, respectively. Then $L(G)=D(G)-A(G)$ is called Laplacian matrix\nof the graph $G$. Let $G$ be a graph with $n$ vertices and $m$ edges. Then the\n$LI$-matrix of $G$ are defined as $LI(G)=L(G)-\\frac{2m}{n}I_n$, where $I_n$ is\nthe identity matrix. In this paper, we are interested in extremal properties of\nthe Ky Fan $k$-norm of the $LI$-matrix of graphs, which is closely related to\nthe well known problems and results in spectral graph theory, such as the\nLaplacian spectral radius, the Laplacian spread, the sum of the $k$ largest\nLaplacian eigenvalues, the Laplacian energy, and other parameters. Some bounds\non the Ky Fan $k$-norm of the $LI$-matrix of graphs are given, and the extremal\ngraphs are partly characterized. In addition, upper and lower bounds on the Ky\nFan $k$-norm of $LI$-matrix of trees, unicyclic graphs and bicyclic graphs are\ndetermined, and the corresponding extremal graphs are characterized.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we develop conformal bootstrap for Galilean conformal field\ntheory (GCFT). In a GCFT, the Hilbert space could be decomposed into\nquasiprimary states and its global descendants. Different from the usual\nconformal field theory, the quasi-primary states in a GCFT constitute\nmultiplets, which are block-diagonized under the Galilean boost operator. More\nimportantly the multiplets include the states of negative norms, indicating the\ntheory is not unitary. We compute global blocks of the multiplets, and discuss\nthe expansion of four-point functions in terms of the global blocks of the\nmultiplets. Furthermore we do the harmonic analysis for the Galilean conformal\nsymmetry and obtain an inversion formula. As the first step to apply the\nGalilean conformal bootstrap, we construct generalized Galilean free theory\n(GGFT) explicitly. We read the data of GGFT by using Taylor series expansion of\nfour-point function and the inversion formula independently, and find exact\nagreement. We discuss some novel features in the Galilean conformal bootstrap,\ndue to the non-semisimpleness of the Galilean conformal algebra and the\nnon-unitarity of the GCFTs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We demonstrate a method to solve a general class of random matrix ensembles\nnumerically. The method is suitable for solving log-gas models with\nbiorthogonal type two-body interactions and arbitrary potentials. We reproduce\nstandard results for a variety of well-known ensembles and show some new\nresults for the Muttalib-Borodin ensembles and recently introduced\n$\\gamma$-ensemble for which analytic results are not yet available.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze the problem of how to optimally bid for ad spaces in online ad\nauctions. For this we consider the general case of multiple ad campaigns with\noverlapping targeting criteria. In our analysis we first characterize the\nstructure of an optimal bidding strategy. In particular, we show that an\noptimal bidding strategies decomposes the problem into disjoint sets of\ncampaigns and targeting groups. In addition, we show that pure bidding\nstrategies that use only a single bid value for each campaign are not optimal\nwhen the supply curves are not continuous. For this case, we derive a\nlower-bound on the optimal cost of any bidding strategy, as well as mixed\nbidding strategies that either achieve the lower-bound, or can get arbitrarily\nclose to it.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We model incompressible flows with an adaptive stabilized finite element\nmethod Stokes flows, which solves a discretely stable saddle-point problem to\napproximate the velocity-pressure pair. Additionally, this saddle-point problem\ndelivers a robust error estimator to guide mesh adaptivity. We analyze the\naccuracy of different discrete velocity-pressure pairs of continuous finite\nelement spaces, which do not necessarily satisfy the discrete inf-sup\ncondition. We validate the framework's performance with numerical examples.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent progress in nanophotonics is driven by the desire to engineer\nlight-matter interaction in two-dimensional (2D) materials using high-quality\nresonances in plasmonic and dielectric structures. Here, we demonstrate a link\nbetween the radiation control at critical coupling and the metasurface-based\nbound states in the continuum (BIC) physics, and develop a generalized theory\nto engineer light absorption of 2D materials in coupling resonance\nmetasurfaces. In a typical example of hybrid graphene-dielectric metasurfaces,\nwe present the manipulation of absorption bandwidth by more than one order of\nmagnitude by simultaneously adjusting the asymmetry parameter of silicon\nresonators governed by BIC and the graphene surface conductivity while the\nabsorption efficiency maintains maximum. This work reveals the generalized role\nof BIC in the radiation control at critical coupling and provides promising\nstrategies in engineering light absorption of 2D materials for high-efficiency\noptoelectronics device applications, e.g., light emission, detection and\nmodulation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  While the prompt J/$\\psi$ cross section and polarization have been measured\nwith good precision as a function of transverse momentum, $p_{\\rm T}$, those of\nthe directly produced J/$\\psi$ are practically unknown, given that the cross\nsections and polarizations of the $\\chi_{c1}$ and $\\chi_{c2}$ mesons, large\nindirect contributors to J/$\\psi$ production, are only known with rather poor\naccuracy. The lack of precise measurements of the $\\chi_{cJ}$ polarizations\ninduces large uncertainties in the level of their feed-down contributions to\nthe prompt J/$\\psi$ yield, because of the polarization-dependent acceptance\ncorrections. The experimental panorama of charmonium production can be\nsignificantly improved through a consistent and model-independent global\nanalysis of existing measurements of J/$\\psi$, $\\psi$(2S) and $\\chi_{c}$ cross\nsections and polarizations, faithfully respecting all the correlations and\nuncertainties. In particular, it is seen that the $\\chi_{cJ}$ polarizations and\nfeed-down fractions to J/$\\psi$ production have a negligible dependence on the\nJ/$\\psi$ $p_{\\rm T}$, with average values $\\lambda_\\vartheta^{\\chi_{c1}} = 0.55\n\\pm 0.23$, $\\lambda_\\vartheta^{\\chi_{c2}} = -0.39 \\pm 0.22$, $R^{\\chi_{c1}} =\n(18.8 \\pm 1.4)\\%$ and $R^{\\chi_{c2}} = (6.5 \\pm 0.5)\\%$. The analysis also\nshows that $(67.2 \\pm 1.9)\\%$ of the prompt J/$\\psi$ yield is due to\ndirectly-produced mesons, of polarization constrained to remarkably small\nvalues, $\\lambda_\\vartheta^{{\\rm J}/\\psi} = 0.04 \\pm 0.06$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gabidulin codes over fields of characteristic zero were recently constructed\nby Augot et al., whenever the Galois group of the underlying field extension is\ncyclic. In parallel, the interest in sparse generator matrices of Reed-Solomon\nand Gabidulin codes has increased lately, due to applications in distributed\ncomputations. In particular, a certain condition pertaining the intersection of\nzero entries at different rows, was shown to be necessary and sufficient for\nthe existence of the sparsest possible generator matrix of Gabidulin codes over\nfinite fields. In this paper we complete the picture by showing that the same\ncondition is also necessary and sufficient for Gabidulin codes over fields of\ncharacteristic zero. Our proof builds upon and extends tools from the finite\nfield case, combines them with a variant of the Schwartz-Zippel lemma over\nautomorphisms, and provides a simple randomized construction algorithm whose\nprobability of success can be arbitrarily close to one. In addition, potential\napplications for low-rank matrix recovery are discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Visualization guidelines, if defined properly, are invaluable to both\npractical applications and the theoretical foundation of visualization. In this\npaper, we present a collection of research activities for studying\nvisualization guidelines according to Grounded Theory (GT). We used the\ndiscourses at VisGuides, which is an online discussion forum for visualization\nguidelines, as the main data source for enabling data-driven research processes\nas advocated by the grounded theory methodology. We devised a categorization\nscheme focusing on observing how visualization guidelines were featured in\ndifferent threads and posts at VisGuides, and coded all 248 posts between\nSeptember 27, 2017 (when VisGuides was first launched) and March 13, 2019. To\ncomplement manual categorization and coding, we used text analysis and\nvisualization to help reveal patterns that may have been missed by the manual\neffort and summary statistics. To facilitate theoretical sampling and negative\ncase analysis, we made an in-depth analysis of the 148 posts (with both\nquestions and replies) related to a student assignment of a visualization\ncourse. Inspired by two discussion threads at VisGuides, we conducted two\ncontrolled empirical studies to collect further data to validate specific\nvisualization guidelines. Through these activities guided by grounded theory,\nwe have obtained some new findings about visualization guidelines.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In contract analysis and contract automation, a knowledge base (KB) of legal\nentities is fundamental for performing tasks such as contract verification,\ncontract generation and contract analytic. However, such a KB does not always\nexist nor can be produced in a short time. In this paper, we propose a\nclustering-based approach to automatically generate a reliable knowledge base\nof legal entities from given contracts without any supplemental references. The\nproposed method is robust to different types of errors brought by\npre-processing such as Optical Character Recognition (OCR) and Named Entity\nRecognition (NER), as well as editing errors such as typos. We evaluate our\nmethod on a dataset that consists of 800 real contracts with various qualities\nfrom 15 clients. Compared to the collected ground-truth data, our method is\nable to recall 84\\% of the knowledge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose entropic order parameters that capture the physics of generalized\nsymmetries and phases in QFT's. We do it through an analysis of simple\nproperties (additivity and Haag duality) of the net of operator algebras\nattached to space-time regions. We observe that different types of symmetries\nare associated with the breaking of these properties in regions of different\nnon-trivial topologies. When such topologies are connected, we show that the\nnon locally generated operators generate an Abelian symmetry group, and their\ncommutation relations are fixed. The existence of order parameters with area\nlaw, like the Wilson loop for the confinement phase, or the 't Hooft loop for\nthe dual Higgs phase, is shown to imply the existence of more than one possible\nchoice of algebras for the same underlying theory. A natural entropic order\nparameter arises by this non-uniqueness. We display aspects of the phases of\ntheories with generalized symmetries in terms of these entropic order\nparameters. In particular, the connection between constant and area laws for\ndual order and disorder parameters is transparent in this approach, new\nconstraints arising from conformal symmetry are revealed, and the algebraic\norigin of the Dirac quantization condition (and generalizations thereof) is\ndescribed. A novel tool in this approach is the entropic certainty relation\nsatisfied by dual relative entropies associated with complementary regions,\nwhich quantitatively relates the statistics of order and disorder parameters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that, when an approximation used in this prior work is removed, the\nresulting improved calculation yields an alternative derivation, in the\nparticular case studied, of the accidental curvature constraint of Hellmann and\nKaminski. The result is at the same time extended to apply to almost all\nnon-degenerate Regge-like boundary data and a broad class of face amplitudes.\nThis resolves a tension in the literature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We have studied microwave response of a high-mobility two-dimensional\nelectron system (2DES) contacted by two side electrodes. Using kinetic\ninductance of the 2DES and inter-electrode capacitance, we have constructed a\nsubwavelength 2D plasmonic resonator. We have shown that the resonant frequency\nof this circuit can be controlled by 2D electron density, external magnetic\nfield, or size of the electrodes. This opens up possibilities for using arrays\nof plasmonic circuits as tunable components in different frequency ranges.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a new method for score-based adversarial attack, where the\nattacker queries the loss-oracle of the target model. Our method employs a\nparameterized search space with a structure that captures the relationship of\nthe gradient of the loss function. We show that searching over the structured\nspace can be approximated by a time-varying contextual bandits problem, where\nthe attacker takes feature of the associated arm to make modifications of the\ninput, and receives an immediate reward as the reduction of the loss function.\nThe time-varying contextual bandits problem can then be solved by a Bayesian\noptimization procedure, which can take advantage of the features of the\nstructured action space. The experiments on ImageNet and the Google Cloud\nVision API demonstrate that the proposed method achieves the state of the art\nsuccess rates and query efficiencies for both undefended and defended models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The bipolar organization of the microtubule-based mitotic spindle is\nessential for the faithful segregation of chromosomes in cell division. Despite\nour extensive knowledge of genes and proteins, the physical mechanism of how\nthe ensemble of microtubules can assemble into a proper bipolar shape remains\nelusive. Here, we study the pathways of spindle self-organization using\ncell-free Xenopus egg extracts and computer-based automated shape analysis. Our\nmicroscopy assay allows us to simultaneously record the growth of hundreds of\nspindles in the bulk cytoplasm and systematically analyze the shape of each\nstructure over the course of self-organization. We find that spindles that are\nmaturing into a bipolar shape take a route that is distinct from those ending\nup with faulty structures, such as those of a tripolar shape. Moreover, matured\nstructures are highly stable with little occasions of transformation between\ndifferent shape phenotypes. Visualizing the movement of microtubules further\nreveals a fraction of microtubules that assemble between extra poles and push\nthe poles apart, suggesting the presence of active extensile force that\nprevents pole coalescence. Together, we propose that a proper control over the\nmagnitude and location of the extensile, pole-pushing force is crucial for\nestablishing spindle bipolarity while preventing multipolarity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Motivated from studies on anomalous diffusion, we show that the memory\nfunction $M(t)$ of complex materials, that their creep compliance follows a\npower law, $J(t)\\sim t^q$ with $q\\in \\mathbb{R}^+$, is the fractional\nderivative of the Dirac delta function,\n$\\frac{\\mathrm{d}^q\\delta(t-0)}{\\mathrm{d}t^q}$ with $q\\in \\mathbb{R}^+$. This\nleads to the finding that the inverse Laplace transform of $s^q$ for any $q\\in\n\\mathbb{R}^+$ is the fractional derivative of the Dirac delta function,\n$\\frac{\\mathrm{d}^q\\delta(t-0)}{\\mathrm{d}t^q}$. This result, in association\nwith the convolution theorem, makes possible the calculation of the inverse\nLaplace transform of $\\frac{s^q}{s^{\\alpha}\\mp\\lambda}$ where\n$\\alpha<q\\in\\mathbb{R}^+$ which is the fractional derivative of order $q$ of\nthe Rabotnov function $\\varepsilon_{\\alpha-1}(\\pm\\lambda,\nt)=t^{\\alpha-1}E_{\\alpha, \\alpha}(\\pm\\lambda t^{\\alpha})$. The fractional\nderivative of order $q\\in \\mathbb{R}^+$ of the Rabotnov function,\n$\\varepsilon_{\\alpha-1}(\\pm\\lambda, t)$ produces singularities which are\nextracted with a finite number of fractional derivatives of the Dirac delta\nfunction depending on the strength of $q$ in association with the recurrence\nformula of the two-parameter Mittag-Leffler function.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Reflection of X-ray emission on molecular clouds in the inner $\\sim$ 100 pc\nof our Galaxy reveals that, despite being extremely quiet at the moment, our\nsupermassive black hole Sgr A* should have experienced bright flares of X-ray\nemission in the recent past. Thanks to the improving characterisation of the\nreflection signal, we are able to infer parameters of the most recent flare(s)\n(age, duration and luminosity) and relative line-of-sight disposition of the\nbrightest individual molecular complexes. We show that combining these data\nwith measurements of polarisation in the reflected X-ray continuum will not\nonly justify Sgr A* as the primary source but also allow deriving intrinsic\npolarisation properties of the flare emission. This will help to identify\nradiation mechanisms and underlying astrophysical phenomena behind them. For\nthe currently brightest reflecting molecular complex, Sgr A, the required level\nof sensitivity might be already accessible with upcoming X-ray polarimeters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We derive continuity equation and exact expression for flow probability\ndensity in a space with arbitrary deformed algebra leading to minimal length.\nIn coordinate representation the flow probability density is presented as\ninfinite series over parameter of deformation which in momentum representation\ncan be casted into exact closed form determined by deformed kinetic energy. The\nflow probability density is calculated explicitly for plane wave and for\nsuperposition of two plane waves.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The quantum channels with memory, known as non-Markovian channels, are of\ncrucial importance for a realistic description of a variety of physical\nsystems, and pave ways for new methods of decoherence control by manipulating\nthe properties of environment such as its frequency spectrum. In this work, the\nreduced dynamics of coin in a discrete-time quantum walk is characterized as a\nnon-Markovian quantum channel. A general formalism is sketched to extract the\nKraus operators for a $t$-step quantum walk. Non-Markovianity, in the sense of\nP-indivisibility of the reduced coin dynamics, is inferred from the\nnon-monotonous behavior of distinguishably of two orthogonal states subjected\nto it. Further, we study various quantum information theoretic quantities of a\nqubit under the action of this channel, putting in perspective, the role such\nchannels can play in various quantum information processing tasks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum state smoothing is a technique to construct an estimate of the\nquantum state at a particular time, conditioned on a measurement record from\nboth before and after that time. The technique assumes that an observer, Alice,\nmonitors part of the environment of a quantum system and that the remaining\npart of the environment, unobserved by Alice, is measured by a secondary\nobserver, Bob, who may have a choice in how he monitors it. The effect of Bob's\nmeasurement choice on the effectiveness of Alice's smoothing has been studied\nin a number of recent papers. Here we expand upon the Letter which introduced\nlinear Gaussian quantum (LGQ) state smoothing [Phys. Rev. Lett., 122, 190402\n(2019)]. In the current paper we provide a more detailed derivation of the LGQ\nsmoothing equations and address an open question about Bob's optimal\nmeasurement strategy. Specifically, we develop a simple hypothesis that allows\none to approximate the optimal measurement choice for Bob given Alice's\nmeasurement choice. By 'optimal choice' we mean the choice for Bob that will\nmaximize the purity improvement of Alice's smoothed state compared to her\nfiltered state (an estimated state based only on Alice's past measurement\nrecord). The hypothesis, that Bob should choose his measurement so that he\nobserves the back-action on the system from Alice's measurement, seems contrary\nto one's intuition about quantum state smoothing. Nevertheless we show that it\nworks even beyond a linear Gaussian setting.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  For the GBAR (Gravitational Behaviour of Antihydrogen at Rest) experiment at\nCERN's Antiproton Decelerator (AD) facility we have constructed a source of\nslow positrons, which uses a low-energy electron linear accelerator (linac).\nThe driver linac produces electrons of 9 MeV kinetic energy that create\npositrons from bremsstrahlung-induced pair production. Staying below 10 MeV\nensures no persistent radioactive activation in the target zone and that the\nradiation level outside the biological shield is safe for public access. An\nannealed tungsten-mesh assembly placed directly behind the target acts as a\npositron moderator. The system produces $5\\times10^7$ slow positrons per\nsecond, a performance demonstrating that a low-energy electron linac is a\nsuperior choice over positron-emitting radioactive sources for high positron\nflux.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Surface brightness-colour relations (SBCRs) are used to derive the stellar\nangular diameters from photometric observations. They have various\nastrophysical applications, such as the distance determination of eclipsing\nbinaries or the determination of exoplanet parameters. However, strong\ndiscrepancies between the SBCRs still exist in the literature, in particular\nfor early and late-type stars. We aim to calibrate new SBCRs as a function of\nthe spectral type and the luminosity class of the stars. Our goal is also to\napply homogeneous criteria to the selection of the reference stars and in view\nof compiling an exhaustive and up-to-date list of interferometric late-type\ntargets. We implemented criteria to select measurements in the JMMC Measured\nDiameters Catalog (JMDC). We then applied additional criteria on the\nphotometric measurements used to build the SBCRs, together with stellar\ncharacteristics diagnostics. We built SBCRs for F5/K7-II/III, F5/K7-IV/V,\nM-II/III and M-V stars, with respective RMS of $\\sigma_{F_{V}} = 0.0022$ mag,\n$\\sigma_{F_{V}} = 0.0044$ mag, $\\sigma_{F_{V}} = 0.0046$ mag, and\n$\\sigma_{F_{V}} = 0.0038$ mag. This results in a precision on the angular\ndiameter of 1.0\\%, 2.0\\%, 2.1\\%, and 1.7\\%, respectively. These relations cover\na large $V-K$ colour range of magnitude, from 1 to 7.5. Our work demonstrates\nthat SBCRs are significantly dependent on the spectral type and the luminosity\nclass of the star. Through a new set of interferometric measurements, we\ndemonstrate the critical importance of the selection criteria proposed for the\ncalibration of SBCR. Finally, using the Gaia photometry for our samples, we\nobtained (G-K) SBCRs with a precision on the angular diameter between 1.1\\% and\n2.4\\%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We extend the notion of quasibounded harmonic functions to the\nplurisubharmonic setting. As an application, using the theory of Jensen\nmeasures, we show that certain generalized Dirichlet problems with unbounded\nboundary data admit unique solutions, and that these solutions are continuous\noutside a pluripolar set.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The relativistic six-quark equations are found in the framework of the\ndispersion relation technique. The strange baryonia are constructed without the\nmixing of the quarks and antiquarks. The relativistic six-quark amplitudes of\nthe strange baryonia with the open and hidden strange are calculated. The poles\nof these amplitudes determine the masses of strange baryonia. 17 masses of\nbaryonia are predicted.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The inductive bias of a neural network is largely determined by the\narchitecture and the training algorithm. To achieve good generalization, how to\neffectively train a neural network is of great importance. We propose a novel\northogonal over-parameterized training (OPT) framework that can provably\nminimize the hyperspherical energy which characterizes the diversity of neurons\non a hypersphere. By maintaining the minimum hyperspherical energy during\ntraining, OPT can greatly improve the empirical generalization. Specifically,\nOPT fixes the randomly initialized weights of the neurons and learns an\northogonal transformation that applies to these neurons. We consider multiple\nways to learn such an orthogonal transformation, including unrolling\northogonalization algorithms, applying orthogonal parameterization, and\ndesigning orthogonality-preserving gradient descent. For better scalability, we\npropose the stochastic OPT which performs orthogonal transformation\nstochastically for partial dimensions of neurons. Interestingly, OPT reveals\nthat learning a proper coordinate system for neurons is crucial to\ngeneralization. We provide some insights on why OPT yields better\ngeneralization. Extensive experiments validate the superiority of OPT over the\nstandard training.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A common approach for compressing NLP networks is to encode the embedding\nlayer as a matrix $A\\in\\mathbb{R}^{n\\times d}$, compute its rank-$j$\napproximation $A_j$ via SVD, and then factor $A_j$ into a pair of matrices that\ncorrespond to smaller fully-connected layers to replace the original embedding\nlayer. Geometrically, the rows of $A$ represent points in $\\mathbb{R}^d$, and\nthe rows of $A_j$ represent their projections onto the $j$-dimensional subspace\nthat minimizes the sum of squared distances (\"errors\") to the points. In\npractice, these rows of $A$ may be spread around $k>1$ subspaces, so factoring\n$A$ based on a single subspace may lead to large errors that turn into large\ndrops in accuracy.\n  Inspired by \\emph{projective clustering} from computational geometry, we\nsuggest replacing this subspace by a set of $k$ subspaces, each of dimension\n$j$, that minimizes the sum of squared distances over every point (row in $A$)\nto its \\emph{closest} subspace. Based on this approach, we provide a novel\narchitecture that replaces the original embedding layer by a set of $k$ small\nlayers that operate in parallel and are then recombined with a single\nfully-connected layer.\n  Extensive experimental results on the GLUE benchmark yield networks that are\nboth more accurate and smaller compared to the standard matrix factorization\n(SVD). For example, we further compress DistilBERT by reducing the size of the\nembedding layer by $40\\%$ while incurring only a $0.5\\%$ average drop in\naccuracy over all nine GLUE tasks, compared to a $2.8\\%$ drop using the\nexisting SVD approach. On RoBERTa we achieve $43\\%$ compression of the\nembedding layer with less than a $0.8\\%$ average drop in accuracy as compared\nto a $3\\%$ drop previously. Open code for reproducing and extending our results\nis provided.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the privacy of interactions between individuals in a network. For\nmany networks, while nodes are anonymous to outside observers, the existence of\na link between individuals implies the possibility of one node revealing\nidentifying information about its neighbor. Moreover, while the identities of\nthe accounts are likely hidden to an observer, the network of interaction\nbetween two anonymous accounts is often available. For example, in blockchain\ncryptocurrencies, transactions between two anonymous accounts are published\nopenly. Here we consider what happens if one (or more) parties in such a\nnetwork are deanonymized by an outside identity. These compromised individuals\ncould leak information about others with whom they interacted, which could then\ncascade to more and more nodes' information being revealed. We use a\npercolation framework to analyze the scenario outlined above and show for\ndifferent likelihoods of individuals possessing information on their\ncounter-parties, the fraction of accounts that can be identified and the\nidealized minimum number of steps from a deanonymized node to an anonymous node\n(a measure of the effort required to deanonymize that individual). We further\ndevelop a greedy algorithm to estimate the \\emph{actual} number of steps that\nwill be needed to identify a particular node based on the noisy information\navailable to the attacker. We apply our framework to three real-world networks:\n(1) a blockchain transaction network, (2) a network of interactions on the dark\nweb, and (3) a political conspiracy network. We find that in all three\nnetworks, beginning from one compromised individual, it is possible to\ndeanonymize a significant fraction of the network ($>50$%) within less than 5\nsteps. Overall these results provide guidelines for investigators seeking to\nidentify actors in anonymous networks, as well as for users seeking to maintain\ntheir privacy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The combination of Monte-Carlo Tree Search (MCTS) and deep reinforcement\nlearning is state-of-the-art in two-player perfect-information games. In this\npaper, we describe a search algorithm that uses a variant of MCTS which we\nenhanced by 1) a novel action value normalization mechanism for games with\npotentially unbounded rewards (which is the case in many optimization\nproblems), 2) defining a virtual loss function that enables effective search\nparallelization, and 3) a policy network, trained by generations of self-play,\nto guide the search. We gauge the effectiveness of our method in \"SameGame\"---a\npopular single-player test domain. Our experimental results indicate that our\nmethod outperforms baseline algorithms on several board sizes. Additionally, it\nis competitive with state-of-the-art search algorithms on a public set of\npositions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Knock Codes are a knowledge-based unlock authentication scheme used on LG\nsmartphones where a user enters a code by tapping or \"knocking\" a sequence on a\n2x2 grid. While a lesser used authentication method, as compared to PINs or\nAndroid patterns, there is likely a large number of Knock Code users; we\nestimate, 700,000--2,500,000 in the US alone. In this paper, we studied Knock\nCodes security asking participants to select codes on mobile devices in three\nsettings: a control treatment, a blocklist treatment, and a treatment with a\nlarger, 2x3 grid. We find that Knock Codes are significantly weaker than other\ndeployed authentication, e.g., PINs or Android patterns. In a simulated\nattacker setting, 2x3 grids offered no additional security, but blocklisting\nwas more beneficial, making Knock Codes' security similar to Android patterns.\nParticipants expressed positive perceptions of Knock Codes, but usability was\nchallenged. SUS values were \"marginal\" or \"ok\" across treatments. Based on\nthese findings, we recommend deploying blacklists for selecting a Knock Code\nbecause it improves security but has limited impact on usability perceptions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Detector backaction can be completely evaded when the state of a one-electron\nquantum cyclotron is detected, but it nonetheless significantly broadens the\nquantum-jump resonance lineshapes from which the cyclotron frequency can be\ndeduced. This limits the accuracy with which the electron magnetic moment can\nbe determined to test the standard model's most precise prediction. A steady\nstate solution to a master equation, the first quantum calculation for the open\nquantum cyclotron system, illustrates a method to circumvent the detection\nbackaction upon the measured frequency.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent work in NLP shows that LSTM language models capture hierarchical\nstructure in language data. In contrast to existing work, we consider the\n\\textit{learning} process that leads to their compositional behavior. For a\ncloser look at how an LSTM's sequential representations are composed\nhierarchically, we present a related measure of Decompositional Interdependence\n(DI) between word meanings in an LSTM, based on their gate interactions. We\nconnect this measure to syntax with experiments on English language data, where\nDI is higher on pairs of words with lower syntactic distance. To explore the\ninductive biases that cause these compositional representations to arise during\ntraining, we conduct simple experiments on synthetic data. These synthetic\nexperiments support a specific hypothesis about how hierarchical structures are\ndiscovered over the course of training: that LSTM constituent representations\nare learned bottom-up, relying on effective representations of their shorter\nchildren, rather than learning the longer-range relations independently from\nchildren.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Detailed magnetic field structure of the dense core SL42 (CrA-E) in the\nCorona Australis molecular cloud complex was investigated based on\nnear-infrared polarimetric observations of background stars to measure\ndichroically polarized light produced by magnetically aligned dust grains. The\nmagnetic fields in and around SL42 were mapped using 206 stars and curved\nmagnetic fields were identified. On the basis of simple hourglass (parabolic)\nmagnetic field modeling, the magnetic axis of the core on the plane of sky was\nestimated to be $40^{\\circ} \\pm 3^{\\circ}$. The plane-of-sky magnetic field\nstrength of SL42 was found to be $22.4 \\pm 13.9$ $\\mu$G. Taking into account\nthe effects of thermal/turbulent pressure and the plane-of-sky magnetic field\ncomponent, the critical mass of SL42 was obtained to be $M_{\\rm cr} = 21.2 \\pm\n6.6$ M$_{\\odot}$, which is close to the observed core mass of $M_{\\rm core}\n\\approx 20$ M$_{\\odot}$. We thus conclude that SL42 is in a condition close to\nthe critical state if the magnetic fields lie near the plane of the sky. Since\nthere is a very low luminosity object (VeLLO) toward the center of SL42, it is\nunlikely this core is in a highly subcritical condition (i.e., magnetic\ninclination angle significantly deviated from the plane of sky). The core\nprobably started to collapse from a nearly kinematically critical state. In\naddition to the hourglass magnetic field modeling, the Inoue \\& Fukui (2013)\nmechanism may explain the origin of the curved magnetic fields in the SL42\nregion.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the eternal domination game, an attacker attacks a vertex at each turn and\na team of guards must move a guard to the attacked vertex to defend it. The\nguards may only move to adjacent vertices and no more than one guard may occupy\na vertex. The goal is to determine the eternal domination number of a graph\nwhich is the minimum number of guards required to defend the graph against an\ninfinite sequence of attacks. In this paper, we continue the study of the\neternal domination game on strong grids. Cartesian grids have been vastly\nstudied with tight bounds for small grids such as $2\\times n$, $3\\times n$,\n$4\\times n$, and $5\\times n$ grids, and recently it was proven in [Lamprou et\nal., CIAC 2017, 393-404] that the eternal domination number of these grids in\ngeneral is within $O(m+n)$ of their domination number which lower bounds the\neternal domination number. Recently, Finbow et al. proved that the eternal\ndomination number of strong grids is upper bounded by $\\frac{mn}{6}+O(m+n)$. We\nadapt the techniques of [Lamprou et al., CIAC 2017, 393-404] to prove that the\neternal domination number of strong grids is upper bounded by\n$\\frac{mn}{7}+O(m+n)$. While this does not improve upon a recently announced\nbound of $\\lceil\\frac{m}{3}\\rceil \\lceil\\frac{n}{3}\\rceil+O(m\\sqrt{n})$ [Mc\nInerney, Nisse, P\\'erennes, CIAC 2019] in the general case, we show that our\nbound is an improvement in the case where the smaller of the two dimensions is\nat most $6179$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Due to accessible big data collections from consumers, products, and stores,\nadvanced sales forecasting capabilities have drawn great attention from many\ncompanies especially in the retail business because of its importance in\ndecision making. Improvement of the forecasting accuracy, even by a small\npercentage, may have a substantial impact on companies' production and\nfinancial planning, marketing strategies, inventory controls, supply chain\nmanagement, and eventually stock prices. Specifically, our research goal is to\nforecast the sales of each product in each store in the near future. Motivated\nby tensor factorization methodologies for personalized context-aware\nrecommender systems, we propose a novel approach called the Advanced Temporal\nLatent-factor Approach to Sales forecasting (ATLAS), which achieves accurate\nand individualized prediction for sales by building a single\ntensor-factorization model across multiple stores and products. Our\ncontribution is a combination of: tensor framework (to leverage information\nacross stores and products), a new regularization function (to incorporate\ndemand dynamics), and extrapolation of tensor into future time periods using\nstate-of-the-art statistical (seasonal auto-regressive integrated\nmoving-average models) and machine-learning (recurrent neural networks) models.\nThe advantages of ATLAS are demonstrated on eight product category datasets\ncollected by the Information Resource, Inc., where a total of 165 million\nweekly sales transactions from more than 1,500 grocery stores over 15,560\nproducts are analyzed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we propose a dynamical model to describe the transmission of\nCOVID-19, which is spreading in China and many other countries. To avoid a\nlarger outbreak in the worldwide, Chinese government carried out a series of\nstrong strategies to prevent the situation from deteriorating. Home quarantine\nis the most important one to prevent the spread of COVID-19. In order to\nestimate the effect of population quarantine, we divide the population into\nseven categories for simulation. Based on a Least-Squares procedure and\nofficially published data, the estimation of parameters for the proposed model\nis given. Numerical simulations show that the proposed model can describe the\ntransmission of COVID-19 accurately, the corresponding prediction of the trend\nof the disease is given. The home quarantine strategy plays an important role\nin controlling the disease spread and speeding up the decline of COVID-19. The\ncontrol reproduction number of most provinces in China are analyzed and\ndiscussed adequately. We should pay attention to that, though the epidemic is\nin decline in China, the disease still has high risk of human-to-human\ntransmission continuously. Once the control strategy is removed, COVID-19 may\nbecome a normal epidemic disease just like flu. Further control for the disease\nis still necessary, we focus on the relationship between the spread rate of the\nvirus and the meteorological conditions. A comprehensive meteorological index\nis introduced to represent the impact of meteorological factors on both high\nand low migration groups. As the progress on the new vaccine, we design detail\nvaccination strategies for COVID-19 in different control phases and show the\neffectiveness of efficient vaccination. Once the vaccine comes into use, the\nnumerical simulation provide a promptly prospective research.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Understanding the fluctuations by which phenomenological evolution equations\nwith thermodynamic structure can be enhanced is the key to a general framework\nof nonequilibrium statistical mechanics. These fluctuations provide an\nidealized representation of microscopic details. We consider\nfluctuation-enhanced equations associated with Markov processes and elaborate\nthe general recipes for evaluating dynamic material properties, which\ncharacterize force-flux constitutive laws, by statistical mechanics. Markov\nprocesses with continuous trajectories are conveniently characterized by\nstochastic differential equations and lead to Green-Kubo-type formulas for\ndynamic material properties. Markov processes with discontinuous jumps include\ntransitions over energy barriers with the rates calculated by Kramers. We\ndescribe a unified approach to Markovian fluctuations and demonstrate how the\nappropriate type of fluctuations (continuous versus discontinuous) is reflected\nin the mathematical structure of the phenomenological equations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Decidability and synthesis of inductive invariants ranging in a given domain\nplay an important role in many software and hardware verification systems. We\nconsider here inductive invariants belonging to an abstract domain $A$ as\ndefined in abstract interpretation, namely, ensuring the existence of the best\napproximation in $A$ of any system property. In this setting, we study the\ndecidability of the existence of abstract inductive invariants in $A$ of\ntransition systems and their corresponding algorithmic synthesis. Our model\nrelies on some general results which relate the existence of abstract inductive\ninvariants with least fixed points of best correct approximations in $A$ of the\ntransfer functions of transition systems and their completeness properties.\nThis approach allows us to derive decidability and synthesis results for\nabstract inductive invariants which are applied to the well-known Kildall's\nconstant propagation and Karr's affine equalities abstract domains. Moreover,\nwe show that a recent general algorithm for synthesizing inductive invariants\nin domains of logical formulae can be systematically derived from our results\nand generalized to a range of algorithms for computing abstract inductive\ninvariants.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Rearranging the six-dimensional phase space of particles in plasma can\nrelease energy. The rearrangement may happen through the application of\nelectric and magnetic fields, subject to various constraints. The maximum\nenergy that can be released through a rearrangement of a distribution of\nparticles can be called its available or free energy. Rearrangement subject to\nphase space volume conservation leads to the classic Gardner free energy. Less\nfree energy is available when constraints are applied, such as respecting\nconserved quantities. Also, less energy is available if particles can only be\ndiffused in phase-space from regions of high phase-space density to regions of\nlower phase-space density. The least amount of free energy is available if\nparticles can only be diffused in phase space, while conserved quantities still\nneed to be respected.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An efficient finite-difference time-domain (FDTD) algorithm is built to solve\nthe transverse electric 2D Maxwell's equations with inhomogeneous dielectric\nmedia where the electric fields are discontinuous across the dielectric\ninterface. The new algorithm is derived based upon the integral version of the\nMaxwell's equations as well as the relationship between the electric fields\nacross the interface. It is an improvement over the contour-path\neffective-permittivity algorithm by including some extra terms in the formulas.\nThe scheme is validated in solving the scattering of a dielectric cylinder with\nexact solution from Mie theory and is then compared with the above contour-path\nmethod, the usual staircase and the volume-average method. The numerical\nresults demonstrate that the new algorithm has achieved significant improvement\nin accuracy over the other methods. Furthermore, the algorithm has a simple\nstructure and can be merged into any existing FDTD software package very\neasily.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A di-sk tree is a rooted binary tree whose nodes are labeled by $\\oplus$ or\n$\\ominus$, and no node has the same label as its right child. The di-sk trees\nare in natural bijection with separable permutations. We construct a\ncombinatorial bijection on di-sk trees proving the two quintuples\n$(\\LMAX,\\LMIN,\\DESB,\\iar,\\comp)$ and $(\\LMAX,\\LMIN,\\DESB,\\comp,\\iar)$ have the\nsame distribution over separable permutations. Here for a permutation $\\pi$,\n$\\LMAX(\\pi)/\\LMIN(\\pi)$ is the set of values of the left-to-right maxima/minima\nof $\\pi$ and $\\DESB(\\pi)$ is the set of descent bottoms of $\\pi$, while\n$\\comp(\\pi)$ and $\\iar(\\pi)$ are respectively the number of components of $\\pi$\nand the length of initial ascending run of $\\pi$.\n  Interestingly, our bijection specializes to a bijection on $312$-avoiding\npermutations, which provides (up to the classical {\\em Knuth--Richards\nbijection}) an alternative approach to a result of Rubey (2016) that asserts\nthe two triples $(\\LMAX,\\iar,\\comp)$ and $(\\LMAX,\\comp,\\iar)$ are\nequidistributed on $321$-avoiding permutations. Rubey's result is a symmetric\nextension of an equidistribution due to Adin--Bagno--Roichman, which implies\nthe class of $321$-avoiding permutations with a prescribed number of components\nis Schur positive.\n  Some equidistribution results for various statistics concerning tree\ntraversal are presented in the end.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We establish exponential inequalities for a class of V-statistics under\nstrong mixing conditions. Our theory is developed via a novel kernel expansion\nbased on random Fourier features and the use of a probabilistic method. This\ntype of expansion is new and useful for handling many notorious classes of\nkernels.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In-vehicle human object identification plays an important role in\nvision-based automated vehicle driving systems while objects such as\npedestrians and vehicles on roads or streets are the primary targets to protect\nfrom driverless vehicles. A challenge is the difficulty to detect objects in\nmoving under the wild conditions, while illumination and image quality could\ndrastically vary. In this work, to address this challenge, we exploit Deep\nConvolutional Generative Adversarial Networks (DCGANs) with Single Shot\nDetector (SSD) to handle with the wild conditions. In our work, a GAN was\ntrained with low-quality images to handle with the challenges arising from the\nwild conditions in smart cities, while a cascaded SSD is employed as the object\ndetector to perform with the GAN. We used tested our approach under wild\nconditions using taxi driver videos on London street in both daylight and night\ntimes, and the tests from in-vehicle videos demonstrate that this strategy can\ndrastically achieve a better detection rate under the wild conditions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The functionals of double phase type \\[ \\mathcal{H} (u):= \\int \\left(|Du|^{p}\n+ a(x)|Du|^{q} \\right) dx, ( q > p > 1, a(x)\\geq 0) \\] are introduced in the\nepoch-making paper by Colombo-Mingione for constants $p$ and $q$, and\ninvestigated by them and Baroni. They obtained sharp regularity results for\nminimizers of such functionals. In this paper we treat the case that the\nexponents are functions of $x$ and partly generalize their regularity results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A fundamental challenge in estimations of daily streamflow time series at\nsites with incomplete records is how to effectively and efficiently select\nreference or donor gauges from an existing gauge network to infer the missing\ndata. While research on estimating missing streamflow time series is not new,\nthe existing approaches either use a single reference streamflow gauge or\nemploy a set of \"ad-hoc\" reference gauges, leaving a systematic selection of\nreference gauges as a long-standing open question. In this work, a novel method\nis introduced that facilitates systematical selection of multiple reference\ngauges from any given streamflow network. The idea is to mathematically\ncharacterize the network-wise correlation structure of a streamflow network via\ngraphical Markov modeling, and further transforms a dense network into a\nsparsely connected one. The resulted underlying sparse graph from the graphical\nmodel encodes conditional independence conditions among all reference gauges\nfrom the streamflow network, allowing determination of an optimum subset of the\ndonor gauges. The sparsity is discovered by using the Graphical Lasso algorithm\nwith an L1-norm regularization parameter and a thresholding parameter. These\ntwo parameters are determined by a multi-objective optimization process.\nFurthermore, the graphical modeling approach is employed to solve another open\nproblem in gauge removal planning decision (e.g., due to operation budget\nconstraints): which gauges to remove would statistically guarantee the least\nloss of information by estimations from the remaining gauges? Our graphical\nmodel-based method is demonstrated with daily streamflow data from a network of\n34 gauges over the Ohio River basin.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The aim of this short note is to extend the recent variational proof of\npartial regularity for optimal transport maps to the case of continuous\ndensities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the effect of dark matter (DM) being encapsulated in primordial\nblack holes (PBHs) on the power spectrum of density fluctuations $P(k)$; we\nalso look at its effect on the abundance of haloes and their clustering. We\nallow the growth of Poisson fluctuations since matter and radiation equality\nand study both monochromatic and extended PBH mass distributions. We present\nupdated monochromatic black hole mass constraints by demanding $<10\\%$\ndeviations from the $\\Lambda$ cold dark matter power spectrum at a scale of\n$k=1$hMpc$^{-1}$. Our results show that PBHs with masses $>10^4$h$^{-1}M_\\odot$\nare excluded from conforming all of the DM in the Universe. We also apply this\ncondition to our extended Press-Schechter (PS) mass functions, and find that\nthe Poisson power is scale dependent even before applying evolution. We find\nthat characteristic masses $M^*\\leq10^2 $h$^{-1}M_\\odot$ are allowed, {leaving\nonly two characteristic PBH mass windows of PS mass functions when combining\nwith previous constraints, at $M^*\\sim10^2$h$^{-1}M_\\odot$ and\n$\\sim10^{-8}$h$^{-1}M_\\odot$ where all of the DM can be in PBHs. The resulting\nDM halo mass functions within these windows are similar} to those resulting\nfrom cold dark matter made of fundamental particles. However, as soon as the\nparameters produce unrealistic $P(k)$, the resulting halo mass functions and\ntheir bias as a function of halo mass deviate strongly from the behaviour\nmeasured in the real Universe.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ontology is the explicit and formal representation of the concepts in a\ndomain and relations among them. Transportation science is a wide domain\ndealing with mobility over various complex and interconnected transportation\nsystems, such as land, aviation, and maritime transport, and can take\nconsiderable advantage from ontology development. While several studies can be\nfound in the recent literature, there exists a large potential to improve and\ndevelop a comprehensive smart mobility ontology. The current chapter aims to\npresent different aspects of ontology development in general, such as ontology\ndevelopment methods, languages, tools, and software. Subsequently, it presents\nthe currently available mobility-related ontologies developed across different\ndomains, such as transportation, smart cities, goods mobility, sensors. Current\ngaps in the available ontologies are identified, and future directions\nregarding ontology development are proposed that can incorporate the\nforthcoming autonomous and connected vehicles, mobility as a service (MaaS),\nand other disruptive transportation technologies and services.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a mathematical model for describing radially propagating spin\nwaves emitted from the core region in a magnetic patch with n vertices in a\nmagnetic vortex state. The azimuthal anisotropic propagation of surface spin\nwaves (SSW) into the domain, and confined spin waves (or Winter's Magnons, WM)\nin domain walls increases the complexity of the magnonic landscape. In order to\nunderstand the spin wave propagation in these systems, we first use an approach\nbased on geometrical curves called 'hippopedes', however it provides no insight\ninto the underlying physics. Analytical models rely on generalized expressions\nfrom the dispersion relation of SSW with an arbitrary angle between\nmagnetization M and wavenumber k. The derived algebraic expression for the\nazimuthal dispersion is found to be equivalent to that of the 'hippopede'\ncurves. The fitting curves from the model yield a spin wave wavelength for any\ngiven azimuthal direction, number of patch vertices and excitation frequency,\nshowing a connection with fundamental physics of exchange dominated surface\nspin waves. Analytical results show good agreement with micromagnetic\nsimulations and can be easily extrapolated to any n-corner patch geometry.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Randomized experiments is a key part of product development in the tech\nindustry. It is often necessary to run programs of exclusive experiments, i.e.,\nexperiments that cannot be run on the same units during the same time. These\nprograms implies restriction on the random sampling, as units that are\ncurrently in an experiment cannot be sampled into a new one. Moreover, to\ntechnically enable this type of coordination with large populations, the units\nin the population are often grouped into 'buckets' and sampling is then\nperformed on the bucket level. This paper investigates some statistical\nimplications of both the restricted sampling and the bucket-level sampling. The\ncontribution of this paper is threefold: First, bucket sampling is connected to\nthe existing literature on randomized experiments in complex sampling designs\nwhich enables establishing properties of the difference-in-means estimator of\nthe average treatment effect. These properties are needed for inference to the\npopulation under random sampling of buckets. Second, the bias introduced by\nrestricting the sampling as imposed by programs of exclusive experiments, is\nderived. Finally, simulation results supporting the theoretical findings are\npresented together with recommendations on how to empirically evaluate and\nhandle this bias.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We develop a hydrodynamic field theory of the three-dimensional fractional\nquantum Hall effect, which was recently proposed to exist in magnetic Weyl\nsemimetals, when the Weyl nodes are gapped by strong repulsive interactions.\nThis theory takes the form of a BF theory, which contains both one-form and\ntwo-form gauge fields, coupling to quasiparticle and loop excitations\ncorrespondingly. It may be regarded as a generalization of the Chern-Simons\ntheory of two-dimensional fractional quantum Hall liquids to three dimensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We examine the static structure configurations and radial stability of\ncompact stars within the context of $f(R, T)$ gravity, with $R$ and $T$\nstanding for the Ricci scalar and trace of the energy-momentum tensor,\nrespectively. Considering the $f(R, T)=R+2\\beta T$ functional form, with\n$\\beta$ being a constant, we derive the corresponding hydrostatic equilibrium\nequation and the modified Chandrasekhar's pulsation equation. The mass-radius\nrelations and radial mode frequencies are obtained for some realistic equations\nof state. Our results show that the traditional stellar stability criteria,\nnamely, the necessary condition $dM/d\\rho_c >0$ and sufficient condition\n$\\omega^2 >0$, still hold in this theory of gravity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The two-dimensional ordering of bacteriorhodopsins in a lipid bilayer was\nstudied using a binary hard-disk model. The phase diagrams were calculated,\ntaking into account the lateral depletion effects. The critical concentrations\nof the protein ordering for the monomers and the trimers were obtained from the\nphase diagrams. The critical concentration ratio agreed well with the\nexperiment when the repulsive core interaction between the depletants, namely\nthe lipids, was taken into account. The results suggest that the depletion\neffect plays an important role in the association behaviors of transmembrane\nproteins.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have\nproven very useful for a broad range of applications including enabling real\nrobots to recover quickly from joint damage, solving strongly deceptive maze\ntasks or evolving robot morphologies to discover new gaits. However, present\nimplementations of MAP-Elites and other QD algorithms seem to be limited to\nlow-dimensional controllers with far fewer parameters than modern deep neural\nnetwork models. In this paper, we propose to leverage the efficiency of\nEvolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers\nparameterized by large neural networks. We design and evaluate a new hybrid\nalgorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage\nrecovery in a difficult high-dimensional control task where traditional ME\nfails. Additionally, we show that ME-ES performs efficient exploration, on par\nwith state-of-the-art exploration algorithms in high-dimensional control tasks\nwith strongly deceptive rewards.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose the position-based scaled gradient (PSG) that scales the gradient\ndepending on the position of a weight vector to make it more\ncompression-friendly. First, we theoretically show that applying PSG to the\nstandard gradient descent (GD), which is called PSGD, is equivalent to the GD\nin the warped weight space, a space made by warping the original weight space\nvia an appropriately designed invertible function. Second, we empirically show\nthat PSG acting as a regularizer to a weight vector is favorable for model\ncompression domains such as quantization and pruning. PSG reduces the gap\nbetween the weight distributions of a full-precision model and its compressed\ncounterpart. This enables the versatile deployment of a model either as an\nuncompressed mode or as a compressed mode depending on the availability of\nresources. The experimental results on CIFAR-10/100 and ImageNet datasets show\nthe effectiveness of the proposed PSG in both domains of pruning and\nquantization even for extremely low bits. The code is released in Github.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This two-part review examines how automation has contributed to different\naspects of discovery in the chemical sciences. In this second part, we reflect\non a selection of exemplary studies. It is increasingly important to articulate\nwhat the role of automation and computation has been in the scientific process\nand how that has or has not accelerated discovery. One can argue that even the\nbest automated systems have yet to ``discover'' despite being incredibly useful\nas laboratory assistants. We must carefully consider how they have been and can\nbe applied to future problems of chemical discovery in order to effectively\ndesign and interact with future autonomous platforms.\n  The majority of this article defines a large set of open research directions,\nincluding improving our ability to work with complex data, build empirical\nmodels, automate both physical and computational experiments for validation,\nselect experiments, and evaluate whether we are making progress toward the\nultimate goal of autonomous discovery. Addressing these practical and\nmethodological challenges will greatly advance the extent to which autonomous\nsystems can make meaningful discoveries.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  For a finite-dimensional Lie algebra $\\mathfrak{L}$ over $\\mathbb{C}$ with a\nfixed Levi decomposition $\\mathfrak{L} = \\mathfrak{g} \\oplus \\mathfrak{r}$\nwhere $\\mathfrak{g}$ is semi-simple, we investigate $\\mathfrak{L}$-modules\nwhich decompose, as $\\mathfrak{g}$-modules, into a direct sum of simple\nfinite-dimensional $\\mathfrak{g}$-modules with finite multiplicities. We call\nsuch modules $\\mathfrak{g}$-Harish-Chandra modules. We give a complete\nclassification of simple $\\mathfrak{g}$-Harish-Chandra modules for the Takiff\nLie algebra associated to $\\mathfrak{g} = \\mathfrak{sl}_2$, and for the\nSchr\\\"{o}dinger Lie algebra, and obtain some partial results in other cases. An\nadapted version of Enright's and Arkhipov's completion functors plays a crucial\nrole in our arguments. Moreover, we calculate the first extension groups of\ninfinite-dimensional simple $\\mathfrak{g}$-Harish-Chandra modules and their\nannihilators in the universal enveloping algebra, for the Takiff\n$\\mathfrak{sl}_2$ and the Schr\\\"{o}dinger Lie algebra. In the general case, we\ngive a sufficient condition for the existence of infinite-dimensional simple\n$\\mathfrak{g}$-Harish-Chandra modules.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The analyses of sunspot observations revealed a fundamental magnetic property\nof the umbral boundary, the invariance of the vertical component of the\nmagnetic field. We aim to analyse the magnetic properties of the umbra-penumbra\nboundary in simulated sunspots and thus assess their similarity to observed\nsunspots. Also, we aim to investigate the role of plasma $\\beta$ and the ratio\nof kinetic to magnetic energy in simulated sunspots on the convective motions.\nWe use a set of non-grey simulation runs of sunspots with the MURaM code. These\ndata are used to synthesise the Stokes profiles that are then degraded to the\nHinode spectropolarimeter-like observations. Then, the data are treated like\nreal Hinode observations of a sunspot and magnetic properties at the umbral\nboundaries are determined. Simulations with potential field extrapolation\nproduce a realistic magnetic field configuration on their umbral boundaries.\nTwo simulations with potential field upper boundary, but different subsurface\nmagnetic field structures, differ significantly in the extent of their\npenumbrae. Increasing the penumbra width by forcing more horizontal magnetic\nfields at the upper boundary results in magnetic properties that are not\nconsistent with observations. This implies that the size of the penumbra is\ngiven by the subsurface structure of the magnetic field. None of the sunspot\nsimulations is consistent with observed properties of the magnetic field and\ndirection of the Evershed flow at the same time. Strong outward directed\nEvershed flows are only found in setups with artificially enhanced horizontal\ncomponent of the magnetic field at the top boundary that are not consistent\nwith the observed magnetic field properties at the UP boundary. We want to\nstress out that the `photospheric' boundary of simulated sunspots is defined by\na magnetic field strength of equipartition field value.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In order to elucidate the plateau phenomena caused by vanishing gradient, we\nherein analyse stability of stochastic gradient descent near degenerated\nsubspaces in a multi-layer perceptron. In stochastic gradient descent for\nFukumizu-Amari model, which is the minimal multi-layer perceptron showing\nnon-trivial plateau phenomena, we show that (1) attracting regions exist in\nmultiply degenerated subspaces, (2) a strong plateau phenomenon emerges as a\nnoise-induced synchronisation, which is not observed in deterministic gradient\ndescent, (3) an optimal fluctuation exists to minimise the escape time from the\ndegenerated subspace. The noise-induced degeneration observed herein is\nexpected to be found in a broad class of machine learning via neural networks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that every ridge unfolding of an $n$-cube is without self-overlap,\nyielding a valid net. The results are obtained by developing machinery that\ntranslates cube unfolding into combinatorial frameworks. Moreover, the geometry\nof the bounding boxes of these cube nets are classified using integer\npartitions, as well as the combinatorics of path unfoldings seen through the\nlens of chord diagrams.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study a specialization of the problem of broadcasting on directed acyclic\ngraphs, namely, broadcasting on 2D regular grids. Consider a 2D regular grid\nwith source vertex $X$ at layer $0$ and $k+1$ vertices at layer $k\\geq 1$,\nwhich are at distance $k$ from $X$. Every vertex of the 2D regular grid has\noutdegree $2$, the vertices at the boundary have indegree $1$, and all other\nvertices have indegree $2$. At time $0$, $X$ is given a random bit. At time\n$k\\geq 1$, each vertex in layer $k$ receives transmitted bits from its parents\nin layer $k-1$, where the bits pass through binary symmetric channels with\nnoise level $\\delta\\in(0,1/2)$. Then, each vertex combines its received bits\nusing a common Boolean processing function to produce an output bit. The\nobjective is to recover $X$ with probability of error better than $1/2$ from\nall vertices at layer $k$ as $k \\rightarrow \\infty$. Besides their natural\ninterpretation in communication networks, such broadcasting processes can be\nconstrued as 1D probabilistic cellular automata (PCA) with boundary conditions\nthat limit the number of sites at each time $k$ to $k+1$. We conjecture that it\nis impossible to propagate information in a 2D regular grid regardless of the\nnoise level and the choice of processing function. In this paper, we make\nprogress towards establishing this conjecture, and prove using ideas from\npercolation and coding theory that recovery of $X$ is impossible for any\n$\\delta$ provided that all vertices use either AND or XOR processing functions.\nFurthermore, we propose a martingale-based approach that establishes the\nimpossibility of recovering $X$ for any $\\delta$ when all NAND processing\nfunctions are used if certain supermartingales can be rigorously constructed.\nWe also provide numerical evidence for the existence of these supermartingales\nby computing explicit examples for different values of $\\delta$ via linear\nprogramming.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Massive galaxy-scale outflows of gas are one of the most commonly-invoked\nmechanisms to regulate the growth and evolution of galaxies throughout the\nuniverse. While the gas in outflows spans a large range of temperatures and\ndensities, the cold molecular phase is of particular interest because molecular\noutflows may be capable of suppressing star formation in galaxies by removing\nthe star-forming gas. We have conducted the first survey of molecular outflows\nat z > 4, targeting 11 strongly-lensed dusty, star-forming galaxies (DSFGs)\nwith high-resolution Atacama Large Millimeter Array (ALMA) observations of OH\n119um absorption as an outflow tracer. In this first paper, we give an overview\nof the survey, focusing on the detection rate and structure of molecular\noutflows. We find unambiguous evidence for outflows in 8/11 (73%) galaxies,\nmore than tripling the number known at z > 4. This implies that molecular winds\nin z > 4 DSFGs must have both a near-unity occurrence rate and large opening\nangles to be detectable in absorption. Lensing reconstructions reveal that\n500pc-scale clumpy structures in the outflows are common. The individual clumps\nare not directly resolved, but from optical depth arguments we expect that\nfuture observations will require 50-200pc spatial resolution to do so. We do\nnot detect high-velocity [CII] wings in any of the sources with clear OH\noutflows, indicating that [CII] is not a reliable tracer of molecular outflows.\nOur results represent a first step toward characterizing molecular outflows at\nz > 4 at the population level, demonstrating that large-scale outflows are\nubiquitous among early massive, dusty galaxies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The extraction of phenotypic traits is often very time and labour intensive.\nEspecially the investigation in viticulture is restricted to an on-site\nanalysis due to the perennial nature of grapevine. Traditionally skilled\nexperts examine small samples and extrapolate the results to a whole plot.\nThereby different grapevine varieties and training systems, e.g. vertical shoot\npositioning (VSP) and semi minimal pruned hedges (SMPH) pose different\nchallenges. In this paper we present an objective framework based on automatic\nimage analysis which works on two different training systems. The images are\ncollected semi automatic by a camera system which is installed in a modified\ngrape harvester. The system produces overlapping images from the sides of the\nplants. Our framework uses a convolutional neural network to detect single\nberries in images by performing a semantic segmentation. Each berry is then\ncounted with a connected component algorithm. We compare our results with the\nMask-RCNN, a state-of-the-art network for instance segmentation and with a\nregression approach for counting. The experiments presented in this paper show\nthat we are able to detect green berries in images despite of different\ntraining systems. We achieve an accuracy for the berry detection of 94.0% in\nthe VSP and 85.6% in the SMPH.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a crowd monitoring system based on the passive detection\nof probe requests. The system meets strict privacy requirements and is suited\nto monitoring events or buildings with a least a few hundreds of attendees. We\npresent our counting process and an associated mathematical model. From this\nmodel, we derive a concentration inequality that highlights the accuracy of our\ncrowd count estimator. Then, we describe our system. We present and discuss our\nsensor hardware, our computing system architecture, and an efficient\nimplementation of our counting algorithm -- as well as its space and time\ncomplexity. We also show how our system ensures the privacy of people in the\nmonitored area. Finally, we validate our system using nine weeks of data from a\npublic library endowed with a camera-based counting system, which generates\ncounts against which we compare those of our counting system. This comparison\nempirically quantifies the accuracy of our counting system, thereby showing it\nto be suitable for monitoring public areas. Similarly, the concentration\ninequality provides a theoretical validation of the system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Due to increasing amounts of data and compute resources, deep learning\nachieves many successes in various domains. The application of deep learning on\nthe mobile and embedded devices is taken more and more attentions, benchmarking\nand ranking the AI abilities of mobile and embedded devices becomes an urgent\nproblem to be solved. Considering the model diversity and framework diversity,\nwe propose a benchmark suite, AIoTBench, which focuses on the evaluation of the\ninference abilities of mobile and embedded devices. AIoTBench covers three\ntypical heavy-weight networks: ResNet50, InceptionV3, DenseNet121, as well as\nthree light-weight networks: SqueezeNet, MobileNetV2, MnasNet. Each network is\nimplemented by three frameworks which are designed for mobile and embedded\ndevices: Tensorflow Lite, Caffe2, Pytorch Mobile. To compare and rank the AI\ncapabilities of the devices, we propose two unified metrics as the AI scores:\nValid Images Per Second (VIPS) and Valid FLOPs Per Second (VOPS). Currently, we\nhave compared and ranked 5 mobile devices using our benchmark. This list will\nbe extended and updated soon after.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We explore the relationship between symmetrisation and entanglement through\nmeasurements on few-particle systems in a multi-well potential. In particular,\nconsidering two or three trapped atoms, we measure and distinguish correlations\narising from two different physical origins: antisymmetrisation of the\nfermionic wavefunction and interaction between particles. We quantify this\nthrough the entanglement negativity of states, and the introduction of an\nantisymmetric negativity, which allows us to understand the role that\nsymmetrisation plays in the measured entanglement properties. We apply this\nconcept both to pure theoretical states and to experimentally reconstructed\ndensity matrices of two or three mobile particles in an array of optical\ntweezers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove that the $L^2$-Betti numbers of a rigid $C^*$-tensor category vanish\nin the presence of an almost-normal subcategory with vanishing $L^2$-Betti\nnumbers, generalising a result of Bader, Furman and Sauer. We apply this\ncriterion to show that the categories constructed from totally disconnected\ngroups by Arano and Vaes have vanishing $L^2$-Betti numbers. Given an\nalmost-normal inclusion of discrete groups $\\Lambda<\\Gamma$, with $\\Gamma$\nacting on a type $\\mathrm{II}_1$ factor $P$ by outer automorphisms, we relate\nthe cohomology theory of the quasi-regular inclusion $P\\rtimes\\Lambda\\subset\nP\\rtimes\\Gamma$ to that of the Schlichting completion $G$ of $\\Lambda<\\Gamma$.\nIf $\\Lambda<\\Gamma$ is unimodular, this correspondence allows us to prove that\nthe $L^2$-Betti numbers of $P\\rtimes\\Lambda\\subset P\\rtimes\\Gamma$ are equal to\nthose of $G$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we study the problem of actively classifying the attributes of\ndynamical systems characterized as a finite set of Markov decision process\n(MDP) models. We are interested in finding strategies that actively interact\nwith the dynamical system and observe its reactions so that the attribute of\ninterest is classified efficiently with high confidence. We present a\ndecision-theoretic framework based on partially observable Markov decision\nprocesses (POMDPs). The proposed framework relies on assigning a classification\nbelief (a probability distribution) to the attributes of interest. Given an\ninitial belief, confidence level over which a classification decision can be\nmade, a cost bound, safe belief sets, and a finite time horizon, we compute\nPOMDP strategies leading to classification decisions. We present two different\nalgorithms to compute such strategies. The first algorithm computes the optimal\nstrategy exactly by value iteration. To overcome the computational complexity\nof computing the exact solutions, we propose a second algorithm is based on\nadaptive sampling to approximate the optimal probability of reaching a\nclassification decision. We illustrate the proposed methodology using examples\nfrom medical diagnosis and privacy-preserving advertising.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The problem of identifying a probabilistic context free grammar has two\naspects: the first is determining the grammar's topology (the rules of the\ngrammar) and the second is estimating probabilistic weights for each rule.\nGiven the hardness results for learning context-free grammars in general, and\nprobabilistic grammars in particular, most of the literature has concentrated\non the second problem. In this work we address the first problem. We restrict\nattention to structurally unambiguous weighted context-free grammars (SUWCFG)\nand provide a query learning algorithm for structurally unambiguous\nprobabilistic context-free grammars (SUPCFG). We show that SUWCFG can be\nrepresented using co-linear multiplicity tree automata (CMTA), and provide a\npolynomial learning algorithm that learns CMTAs. We show that the learned CMTA\ncan be converted into a probabilistic grammar, thus providing a complete\nalgorithm for learning a structurally unambiguous probabilistic context free\ngrammar (both the grammar topology and the probabilistic weights) using\nstructured membership queries and structured equivalence queries. We\ndemonstrate the usefulness of our algorithm in learning PCFGs over genomic\ndata.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Cascade decays of new scalars into final states with multiple photons and\npossibly quarks may lead to distinctive experimental signatures at high-energy\ncolliders. Such signals are even more striking if the scalars are highly\nboosted, as when produced from the decay of a much heavier resonance. We study\nthis type of events within the framework of the minimal stealth boson model, an\nanomaly-free $\\text{U}(1)_{Y'}$ extension of the Standard Model with two\ncomplex scalar singlets. It is shown that, while those signals may have cross\nsections that might render them observable with LHC Run 2 data, they have\nlittle experimental coverage. We also establish a connection with a CMS excess\nobserved in searches for new scalars decaying into diphoton final states near\n96 GeV. In particular, we conclude that the predicted multiphoton signatures\nare compatible with such excess.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Consider an essentially nonbranching metric measure space with the measure\ncontraction property of Ohta and Sturm, or with a Ricci curvature lower bound\nin the sense of Lott, Sturm and Villani. We prove a sharp upper bound on the\ninscribed radius of any subset whose boundary has a suitably signed lower bound\non its generalized mean curvature. This provides a nonsmooth analog to a result\nof Kasue (1983) and Li (2014). We prove a stability statement concerning such\nbounds and - in the Riemannian curvature-dimension (RCD) setting - characterize\nthe cases of equality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Meta-analysis combines pertinent information from existing studies to provide\nan overall estimate of population parameters/effect sizes, as well as to\nquantify and explain the differences between studies. However, testing the\nbetween-study heterogeneity is one of the most troublesome topics in\nmeta-analysis research. Additionally, no methods have been proposed to test\nwhether the size of the heterogeneity is larger than a specific level. The\nexisting methods, such as the Q test and likelihood ratio (LR) tests, are\ncriticized for their failure to control the Type I error rate and/or failure to\nattain enough statistical power. Although better reference distribution\napproximations have been proposed in the literature, the expression is\ncomplicated and the application is limited. In this article, we propose\nbootstrap based heterogeneity tests combining the restricted maximum likelihood\n(REML) ratio test or Q test with bootstrap procedures, denoted as B-REML-LRT\nand B-Q respectively. Simulation studies were conducted to examine and compare\nthe performance of the proposed methods with the regular LR tests, the regular\nQ test, and the improved Q test in both the random-effects meta-analysis and\nmixed-effects meta-analysis. Based on the results of Type I error rates and\nstatistical power, B-Q is recommended. An R package \\mathtt{boot.heterogeneity}\nis provided to facilitate the implementation of the proposed method.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Hartree-Fock problem provides the conceptual and mathematical\nunderpinning of a large portion of quantum chemistry. As efforts in quantum\ntechnology aim to enhance computational chemistry algorithms, the fundamental\nHartree-Fock problem is a natural target. While quantum computers and quantum\nsimulation offer many prospects for the future of modern chemistry, the\nHartree-Fock problem is not a likely candidate. We highlight this fact from a\nnumber of perspectives including computational complexity, practical examples,\nand the full characterization of the energy landscapes for simple systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the first-order flocking transition of birds flying in\nlow-visibility conditions by employing three different representative types of\nneural network (NN) based machine learning architectures that are trained via\neither an unsupervised learning approach called \"learning by confusion\" or a\nwidely used supervised learning approach. We find that after the training via\neither the unsupervised learning approach or the supervised learning one, all\nof these three different representative types of NNs, namely, the\nfully-connected NN, the convolutional NN, and the residual NN, are able to\nsuccessfully identify the first-order flocking transition point of this\nnonequilibrium many-body system. This indicates that NN based machine learning\ncan be employed as a promising generic tool to investigate rich physics in\nscenarios associated to first-order phase transitions and nonequilibrium\nmany-body systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we address the problem of multi-person 3D pose estimation from\na single image. A typical regression approach in the top-down setting of this\nproblem would first detect all humans and then reconstruct each one of them\nindependently. However, this type of prediction suffers from incoherent\nresults, e.g., interpenetration and inconsistent depth ordering between the\npeople in the scene. Our goal is to train a single network that learns to avoid\nthese problems and generate a coherent 3D reconstruction of all the humans in\nthe scene. To this end, a key design choice is the incorporation of the SMPL\nparametric body model in our top-down framework, which enables the use of two\nnovel losses. First, a distance field-based collision loss penalizes\ninterpenetration among the reconstructed people. Second, a depth ordering-aware\nloss reasons about occlusions and promotes a depth ordering of people that\nleads to a rendering which is consistent with the annotated instance\nsegmentation. This provides depth supervision signals to the network, even if\nthe image has no explicit 3D annotations. The experiments show that our\napproach outperforms previous methods on standard 3D pose benchmarks, while our\nproposed losses enable more coherent reconstruction in natural images. The\nproject website with videos, results, and code can be found at:\nhttps://jiangwenpl.github.io/multiperson\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We approach the problem of implicit regularization in deep learning from a\ngeometrical viewpoint. We highlight a regularization effect induced by a\ndynamical alignment of the neural tangent features introduced by Jacot et al,\nalong a small number of task-relevant directions. This can be interpreted as a\ncombined mechanism of feature selection and compression. By extrapolating a new\nanalysis of Rademacher complexity bounds for linear models, we motivate and\nstudy a heuristic complexity measure that captures this phenomenon, in terms of\nsequences of tangent kernel classes along optimization paths.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A Hilbert space operator $T\\in B$ is $(m,P)$-expansive, for some positive\ninteger $m$ and operator $P\\in B$, if\n$\\sum_{j=0}^m{(-1)^j\\left(\\begin{array}{clcr}m\\\\j\\end{array}\\right)T^{*j}PT^j}\\leq\n0$. No Drazin invertible operator $T$ can be $(m,I)$-expansive, and if $T$ is\n$(m,P)$-expansive for some positive operator $P$, then necessarily $P$ has a\ndecomposition $P=P_{11}\\oplus 0$. If $T$ is $(m,|T^n|^2)$-expansive for some\npositive integer $n$, then $T^n$ has a decomposition\n$T^n=\\left(\\begin{array}{clcr}U_1P_1 & X\\\\0 & 0\\end{array}\\right)$; if also\n$\\left(\\begin{array}{clcr}I_1 & X\\\\X^* & X^*X\\end{array}\\right)\\geq I$, then\n$\\left(\\begin{array}{clcr}P_1U_1 & P_1X\\\\0 & 0\\end{array}\\right)$ is\n$(m,I)$-expansive and\n$\\left(\\begin{array}{clcr}P^{\\frac{1}{2}}_1U_1P^{\\frac{1}{2}}_1 &\nP_1^{\\frac{1}{2}}X\\\\0 & 0\\end{array}\\right)$ is $(m,I)$-expansive in an\nequivalent norm on $H$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The plasma of the lower solar atmosphere consists of mostly neutral\nparticles, whereas the upper solar atmosphere is mostly ionised particles and\nelectrons. A shock that propagates upwards in the solar atmosphere therefore\nundergoes a transition where the dominant fluid is either neutral or ionised.\nAn upwards propagating shock also passes a point where the sound and Alfv\\'en\nspeed are equal. At this point the energy of the acoustic shock can separated\ninto fast and slow components. How the energy is distributed between the two\nmodes depends on the angle of magnetic field. The separation of neutral and\nionised species in a gravitationally stratified atmosphere is investigated. The\nrole of two-fluid effects on the structure of the shocks post-mode-conversion\nand the frictional heating is quantified for different levels of collisional\ncoupling. Two-fluid numerical simulations are performed using the\n(P\\underline{I}P) code of a wave steepening into a shock in an isothermal,\npartially-ionised atmosphere. The collisional coefficient is varied to\ninvestigate the regimes where the plasma and neutral species are weakly,\nstrongly and finitely coupled. The propagation speeds of the compressional\nwaves hosted by neutral and ionised species vary, therefore velocity drift\nbetween the two species is produced as the plasma attempts to propagate faster\nthan the neutrals. This is most extreme for a fast-mode shock. We find that the\ncollisional coefficient drastically changes the features present in the system,\nspecifically the mode conversion height, type of shocks present, and the finite\nshock widths created by the two-fluid effects. In the finitely-coupled regime\nfast-mode shock widths can exceed the pressure scale height leading to a new\npotential observable of two-fluid effects in the lower solar atmosphere.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce a model-based asynchronous multi-fidelity method for\nhyperparameter and neural architecture search that combines the strengths of\nasynchronous Hyperband and Gaussian process-based Bayesian optimization. At the\nheart of our method is a probabilistic model that can simultaneously reason\nacross hyperparameters and resource levels, and supports decision-making in the\npresence of pending evaluations. We demonstrate the effectiveness of our method\non a wide range of challenging benchmarks, for tabular data, image\nclassification and language modelling, and report substantial speed-ups over\ncurrent state-of-the-art methods. Our new methods, along with asynchronous\nbaselines, are implemented in a distributed framework which will be open\nsourced along with this publication.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ultrasound elastography is an emerging noninvasive imaging technique wherein\npathological alterations can be visualized by revealing the mechanical\nproperties of the tissue. Estimating tissue displacement in all directions is\nrequired to accurately estimate the mechanical properties. Despite capabilities\nof elastography techniques in estimating displacement in both axial and lateral\ndirections, estimation of axial displacement is more accurate than lateral\ndirection due to higher sampling frequency, higher resolution and having a\ncarrier signal propagating in the axial direction. Among different ultrasound\nimaging techniques, Synthetic Aperture (SA) has better lateral resolution than\nothers, but it is not commonly used for ultrasound elastography due to its\nlimitation in imaging depth of field. Virtual source synthetic aperture (VSSA)\nimaging is a technique to implement synthetic aperture beamforming on the\nfocused transmitted data to overcome limitation of SA in depth of field while\nmaintaining the same lateral resolution as SA. Besides lateral resolution, VSSA\nhas the capability of increasing sampling frequency in the lateral direction\nwithout interpolation. In this paper, we utilize VSSA to perform beamforming to\nenable higher resolution and sampling frequency in the lateral direction. The\nbeamformed data is then processed using our recently published elastography\ntechnique, OVERWIND [1]. Simulation and experimental results show substantial\nimprovement in estimation of lateral displacements.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Singular value decomposition is central to many problems in engineering and\nscientific fields. Several quantum algorithms have been proposed to determine\nthe singular values and their associated singular vectors of a given matrix.\nAlthough these algorithms are promising, the required quantum subroutines and\nresources are too costly on near-term quantum devices. In this work, we propose\na variational quantum algorithm for singular value decomposition (VQSVD). By\nexploiting the variational principles for singular values and the Ky Fan\nTheorem, we design a novel loss function such that two quantum neural networks\n(or parameterized quantum circuits) could be trained to learn the singular\nvectors and output the corresponding singular values. Furthermore, we conduct\nnumerical simulations of VQSVD for random matrices as well as its applications\nin image compression of handwritten digits. Finally, we discuss the\napplications of our algorithm in recommendation systems and polar\ndecomposition. Our work explores new avenues for quantum information processing\nbeyond the conventional protocols that only works for Hermitian data, and\nreveals the capability of matrix decomposition on near-term quantum devices.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Though beneficial for encouraging the Visual Question Answering (VQA) models\nto discover the underlying knowledge by exploiting the input-output correlation\nbeyond image and text contexts, the existing knowledge VQA datasets are mostly\nannotated in a crowdsource way, e.g., collecting questions and external reasons\nfrom different users via the internet. In addition to the challenge of\nknowledge reasoning, how to deal with the annotator bias also remains unsolved,\nwhich often leads to superficial over-fitted correlations between questions and\nanswers. To address this issue, we propose a novel dataset named\nKnowledge-Routed Visual Question Reasoning for VQA model evaluation.\nConsidering that a desirable VQA model should correctly perceive the image\ncontext, understand the question, and incorporate its learned knowledge, our\nproposed dataset aims to cutoff the shortcut learning exploited by the current\ndeep embedding models and push the research boundary of the knowledge-based\nvisual question reasoning. Specifically, we generate the question-answer pair\nbased on both the Visual Genome scene graph and an external knowledge base with\ncontrolled programs to disentangle the knowledge from other biases. The\nprograms can select one or two triplets from the scene graph or knowledge base\nto push multi-step reasoning, avoid answer ambiguity, and balanced the answer\ndistribution. In contrast to the existing VQA datasets, we further imply the\nfollowing two major constraints on the programs to incorporate knowledge\nreasoning: i) multiple knowledge triplets can be related to the question, but\nonly one knowledge relates to the image object. This can enforce the VQA model\nto correctly perceive the image instead of guessing the knowledge based on the\ngiven question solely; ii) all questions are based on different knowledge, but\nthe candidate answers are the same for both the training and test sets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Microarchitectural side channel attacks have been very prominent in security\nresearch over the last few years. Caches have been an outstanding covert\nchannel, as they provide high resolution and generic cross-core leakage even\nwith simple user-mode code execution privileges. To prevent these generic\ncross-core attacks, all major cryptographic libraries now provide\ncountermeasures to hinder key extraction via cross-core cache attacks, for\ninstance avoiding secret dependent access patterns and prefetching data. In\nthis paper, we show that implementations protected by 'good-enough'\ncountermeasures aimed at preventing simple cache attacks are still vulnerable.\nWe present a novel attack that uses a special timing technique to determine\nwhen an encryption has started and then evict the data precisely at the desired\ninstant. This new attack does not require special privileges nor explicit\nsynchronization between the attacker and the victim. One key improvement of our\nattack is a method to evict data from the cache with a single memory access and\nin absence of shared memory by leveraging the transient capabilities of TSX and\nrelying on the recently reverse-engineered L3 replacement policy. We\ndemonstrate the efficiency by performing an asynchronous last level cache\nattack to extract an RSA key from the latest wolfSSL library, which has been\nespecially adapted to avoid leaky access patterns, and by extracting an AES key\nfrom the S-Box implementation included in OpenSSL bypassing the per round\nprefetch intended as a protection against cache attacks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A nitridation annealing process is well employed to reduce interface trap\nstates that degrade the channel mobility of 4H-SiC/SiO${}_2$\nmetal-oxide-semiconductor field-effect transistor. In recent experiments, the\nexistence of high N-atom density layers at the annealed interface is reported\nand their concentrations are known to be anisotropic in the crystal planes.\nUntil now, the role of atomic structure and the electronic states surrounding\nthe N atoms incorporated by the nitridation annealing process on the origin of\nanisotropy is not well understood. In this work, we propose a simplified\natomic-scale model structure of 4H-SiC with the a high N-atom density layer\n($\\sim 10^{15}~\\mathrm{atom}/\\mathrm{cm}^2$), which is of the order of the\nexperimental observation. We use bulk 4H-SiC as host crystal and consider\nseveral sets of the atomic configurations of the N-atom incorporated structure\nat the quasi cubic-($k$-) and hexagonal-($h$-)sites on $a$-, $m$-, and\nSi-(C-)planes. Based on the density functional theory calculations, we\ninvestigate the influence of the energy stability on the distribution\ndirections. Although our bulk model is simplified compared to the realistic\ninterface structures, we confirm significant difference among models and\nobserve that the incorporation of N atoms on the $a$-face is stable.\nFurthermore, from the analysis of the electronic states, we suggest that this\nanisotropy of the formation energy originates from the change of the\ncoordinating number due to the difference in geometric configurations of the\nN-atom incorporated structures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Dams impact downstream river dynamics through flow regulation and disruption\nof upstream-downstream linkages. However, current dam operation is far from\nsatisfactory due to the inability to respond the complicated and uncertain\ndynamics of the upstream-downstream system and various usages of the reservoir.\nEven further, the unsatisfactory dam operation can cause floods in downstream\nareas. Therefore, we leverage reinforcement learning (RL) methods to compute\nefficient dam operation guidelines in this work. Specifically, we build offline\nsimulators with real data and different mathematical models for the upstream\ninflow, i.e., generalized least square (GLS) and dynamic linear model (DLM),\nthen use the simulator to train the state-of-the-art RL algorithms, including\nDDPG, TD3 and SAC. Experiments show that the simulator with DLM can efficiently\nmodel the inflow dynamics in the upstream and the dam operation policies\ntrained by RL algorithms significantly outperform the human-generated policy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  At low temperatures (10 K), hydrogen atoms can diffuse quickly on grain ice\nmantles and frequently encounter hydrogen molecules, which cover a notable\nfraction of grain surface. The desorption energy of H atoms on H2 substrates is\nmuch less than that on water ice. The H atom encounter desorption mechanism is\nadopted to study the enhanced desorption of H atoms on H2 substrates. Using a\nsmall reaction network, we show that the steady-state surface H abundances\npredicted by the rate equation model that includes H atom encounter desorption\nagree reasonably well with the results from the more rigorous microscopic Monte\nCarlo method. For a full gas-grain model, H atom encounter desorption can\nreduce surface H abundances. Therefore, if a model adopts the encounter\ndesorption of H atoms, it becomes more difficult for hydrogenation products\nsuch as methanol to form, but it is easier for C, O and N atoms to bond with\neach other on grain surfaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We address the problem of integrability of the sub-Riemannian mean curvature\nof an embedded hypersurface around isolated characteristic points. The main\ncontribution of this note is the introduction of a concept of mildly degenerate\ncharacteristic point for a smooth surface of the Heisenberg group, in a\nneighborhood of which the sub-Riemannian mean curvature is integrable (with\nrespect to the perimeter measure induced by the Euclidean structure). As a\nconsequence we partially answer to a question posed by Danielli-Garofalo-Nhieu\nin [Danielli D., Garofalo N., Nhieu D.M., Proc. Amer. Math. Soc., 2012],\nproving that the mean curvature of a real-analytic surface with discrete\ncharacteristic set is locally integrable.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study theoretically a gas consisting of charged bosons (ions) over the\nflat dielectric surface at low temperatures and its tendency to form a state\nwith a Bose-Einstein condensate. For the stability of a system, an additional\nexternal electric field, which keeps charges at the dielectric surface, is\nintroduced. The formalism is developed in the framework of a\nself-consistent-field approach, which combines the quasiclassical description\nin terms of the Wigner distribution functions and the quantum-mechanical\napproach by employing the Gross-Pitaevskii equation. We predict a formation of\nthe state with a Bose-Einstein condensate and determine the near-critical\nphysical characteristics of the system. It is shown that the thermal and\ncondensate components become spatially separated under these conditions. We\ndiscuss the limitations of the developed semiclassical approach and prospects\nfor the pure quantum-mechanical treatment of the problem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose an efficient scheme for simulating the topological phases of\nmatter based on silicon-vacancy (SiV) center arrays in phononic crystals. This\nphononic band gap structure allows for long-range spin-spin interactions with a\ntunable profile. Under a particular periodic microwave driving, the band-gap\nmediated spin-spin interaction can be further designed with the form of the\nSu-Schrieffer-Heeger (SSH) Hamiltonian. In momentum space, we investigate the\ntopological characters of the SSH model, and show that the topological\nnontrivial phase can be obtained through modulating the periodic driving\nfields. Furthermore, we explore the zero-energy topological edge states at the\nboundary of the color center arrays, and study the robust quantum information\ntransfer via the topological edge states. This setup provides a scalable and\npromising platform for studying topological quantum physics and quantum\ninformation processing with color centers and phononic crystals.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Causal reasoning is essential to science, yet quantum theory challenges it.\nQuantum correlations violating Bell inequalities defy satisfactory causal\nexplanations within the framework of classical causal models. What is more, a\ntheory encompassing quantum systems and gravity is expected to allow causally\nnonseparable processes featuring operations in indefinite causal order, defying\nthat events be causally ordered at all. The first challenge has been addressed\nthrough the recent development of intrinsically quantum causal models, allowing\ncausal explanations of quantum processes -- provided they admit a definite\ncausal order, i.e. have an acyclic causal structure. This work addresses\ncausally nonseparable processes and offers a causal perspective on them through\nextending quantum causal models to cyclic causal structures. Among other\napplications of the approach, it is shown that all unitarily extendible\nbipartite processes are causally separable and that for unitary processes,\ncausal nonseparability and cyclicity of their causal structure are equivalent.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Background: The Efimov effect is a universal phenomenon in physics whereby\nthree-body systems are stabilized via the interaction of an unbound two-body\nsub-systems. A hypothetical state in $^{12}\\mathrm{C}$ at 7.458 MeV excitation\nenergy, comprising of a loose structure of three $\\alpha$-particles in mutual\ntwo-body resonance, has been suggested in the literature to correspond to an\nEfimov state in nuclear physics. The existence of such a state has not been\ndemonstrated experimentally. Method: Using the combined data sets from two\nrecent experiments, one with the TexAT TPC to measure $\\alpha$-decay and the\nother with Gammasphere to measure $\\gamma$-decay of states in $^{12}\\mathrm{C}$\npopulated by $^{12}\\mathrm{N}$ and $^{12}\\mathrm{B}$ $\\beta$-decay\nrespectively, we achieve high sensitivity to states in close-proximity to the\n$\\alpha$-threshold in $^{12}\\mathrm{C}$. Results: No evidence of a state at\n7.458 MeV is seen in either data set. Using a likelihood method, the 95\\% C.L.\n$\\gamma$-decay branching ratio is determined as a function of the $\\beta$-decay\nfeeding strength relative to the Hoyle state. In parallel, calculations of the\ntriple-alpha reaction rate show the inclusion of the Efimov corresponds to a\nlarge increase in the reaction rate around $5 \\times 10^{7}$ K. Conclusion:\nFrom decay spectroscopy - at the 95\\% C.L., the Efimov state cannot exist at\n7.458 MeV with any $\\gamma$-decay branching ratio unless the $\\beta$-strength\nis less than 0.7\\% of the Hoyle state. This limit is evaluated for a range of\ndifferent excitation energies and the results are not favorable for existence\nof the hypothetical Efimov state in $^{12}\\mathrm{C}$. Furthermore, the\ntriple-alpha reaction rate with the inclusion of a state between 7.43 and 7.53\nMeV exceeds the rate required for stars to undergo the red giant phase.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Thompson Sampling (TS) from Gaussian Process (GP) models is a powerful tool\nfor the optimization of black-box functions. Although TS enjoys strong\ntheoretical guarantees and convincing empirical performance, it incurs a large\ncomputational overhead that scales polynomially with the optimization budget.\nRecently, scalable TS methods based on sparse GP models have been proposed to\nincrease the scope of TS, enabling its application to problems that are\nsufficiently multi-modal, noisy or combinatorial to require more than a few\nhundred evaluations to be solved. However, the approximation error introduced\nby sparse GPs invalidates all existing regret bounds. In this work, we perform\na theoretical and empirical analysis of scalable TS. We provide theoretical\nguarantees and show that the drastic reduction in computational complexity of\nscalable TS can be enjoyed without loss in the regret performance over the\nstandard TS. These conceptual claims are validated for practical\nimplementations of scalable TS on synthetic benchmarks and as part of a\nreal-world high-throughput molecular design task.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we show that the Peccei-Quinn (PQ) symmetry with a good\nquality can be realized in a simple $B-L$ extension of the minimal\nsupersymmetric standard model. The PQ symmetry is a remnant of the $B-L$ gauge\nsymmetry at the renormalizable level. Besides, the sufficient quality of the PQ\nsymmetry is preserved by a non anomalous discrete gauged $R$-symmetry and a\nsmall gravitino mass $m_{3/2}\\ll 100$GeV. A viable mass range is\n$m_{3/2}=\\mathcal{O}(1)$eV which allows a high reheating temperature and many\nbaryogenesis scenarios typified by the thermal leptogenesis without any\nastrophysical and cosmological problems. Such a light gravitino may be tested\nin the future 21cm line observations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Developments over the last two decades have opened the path towards quantum\ntechnologies in many quantum systems, such as cold atoms, trapped ions,\ncavity-quantum electrodynamics (QED), and circuit-QED. However the fragility of\nquantum states to the effects of measurement and decoherence still poses one of\nthe greatest challenges in quantum technology. An imperative capability in this\npath is quantum feedback, as it enhances the control possibilities and allows\nfor prolonging coherence times through quantum error correction. While changing\nparameters from shot to shot of an experiment or procedure can be considered\nfeedback, quantum mechanics also allows for the intriguing possibility of\nperforming feedback operations during the measurement process itself. This\nbroader approach to measurements leads to the concepts of weak measurement,\nquantum trajectories and numerous types of feedback with no classical\nanalogues. These types of processes are the primary focus of this review. We\nintroduce the concept of quantum feedback in the context of circuit QED, an\nexperimental platform with significant potential in quantum feedback and\ntechnology. We then discuss several experiments and see how they elucidate the\nconcepts of continuous measurements and feedback. We conclude with an overview\nof coherent feedback, with application to fault-tolerant error correction.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In beyond-Horndeski theories of gravity, the Vainshtein screening mechanism\nmight only be partially effective inside stellar objects. This results in a\nmodification of the pressure balance equation inside stars, often characterized\nby a single parameter ($\\Upsilon$) in isotropic systems. We show how to\nconstrain such theories of modified gravity, using tidal effects. We study such\neffects in cataclysmic variable star binaries and numerically obtain limits on\nthe critical masses of the donor stars, below which they are tidally disrupted,\nby modeling them in beyond-Horndeski theories. This is contrasted with values\nof the donor masses, obtained using existing observational data, by a Monte\nCarlo error progression method. A best fit scenario of the two yields a\nparametric constraint in the theories that we consider, within the\napproximations used. Here, we obtain the allowed range $ 0 \\le \\Upsilon \\le\n0.47 $.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We have systematically investigated the mass spectrum and rearrangement decay\nproperties of the exotic tetraquark states with four different flavors using a\ncolor-magnetic interaction model. Their masses are estimated by assuming that\nthe $X(4140)$ is a $cs\\bar{c}\\bar{s}$ tetraquark state and their decay widths\nare obtained by assuming that the Hamiltonian for decay is a constant.\nAccording to the adopted method, we find that the most stable states are\nprobably the isoscalar $bs\\bar{u}\\bar{d}$ and $cs\\bar{u}\\bar{d}$ with $J^P=0^+$\nand $1^+$. The width for most unstable tetraquarks is about tens of MeVs, but\nthat for unstable $cu\\bar{s}\\bar{d}$ and $cs\\bar{u}\\bar{d}$ can be around 100\nMeV. For the $X(5568)$, our method cannot give consistent mass and width if it\nis a $bu\\bar{s}\\bar{d}$ tetraquark state. For the $I(J^P)=0(0^+),0(1^+)$\ndouble-heavy $T_{bc}=bc\\bar{u}\\bar{d}$ states, their widths can be several\nMeVs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent approaches for modelling dynamics of physical systems with neural\nnetworks enforce Lagrangian or Hamiltonian structure to improve prediction and\ngeneralization. However, when coordinates are embedded in high-dimensional data\nsuch as images, these approaches either lose interpretability or can only be\napplied to one particular example. We introduce a new unsupervised neural\nnetwork model that learns Lagrangian dynamics from images, with\ninterpretability that benefits prediction and control. The model infers\nLagrangian dynamics on generalized coordinates that are simultaneously learned\nwith a coordinate-aware variational autoencoder (VAE). The VAE is designed to\naccount for the geometry of physical systems composed of multiple rigid bodies\nin the plane. By inferring interpretable Lagrangian dynamics, the model learns\nphysical system properties, such as kinetic and potential energy, which enables\nlong-term prediction of dynamics in the image space and synthesis of\nenergy-based controllers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  High energy nuclear collisions manifest a variety of interesting phenomena\nover a broad range of energy scales. Many of these phenomena are related to the\nformation of a hot and dense state of deconfined quarks and gluons known as the\nquark gluon plasma (QGP). Chief among these are the observed near inviscid\nhydrodynamic expansion of the QGP, as measured through azimuthal anisotropy\ncoefficients of low-$p_\\mathrm{T}$ final state hadrons $v_\\mathrm{n}$, and the\nloss of energy of high-$p_\\mathrm{T}$ color charges as they traverse the QGP\nwhich is observed as a quenching of strongly-interacting final state objects\nlike jets. Observations of these phenomena in Au+Au and Pb+Pb collisions at\nRHIC and the LHC provide compelling evidence of QGP formation. Small collision\nsystems, like $p$+Pb, also show evidence for the creation droplets of QGP\nthrough the observation of anisotropic flow; however, these systems show no\nsigns of the energy loss observed in large collision systems. Thus, small\nsystems are an ideal venue to explore the relationship between high- and\nlow-$p_\\mathrm{T}$ QGP phenomena. Furthermore, the low ambient energy of $p$+Pb\ncompared to A+A collisions allow for the precise determination of perturbative\nprocess rates which can be used to understand the nuclear modification of\nnucleon parton densities.\n  This dissertation explores $p$+Pb data collected with the ATLAS detector at\nthe LHC. The cross section and nuclear modification factor for prompt, isolated\nphotons are measured and compared to predictions from perturbative QCD\ncalculations and a model of initial state energy loss. Additionally, the\ncharged hadron azimuthal anisotropy coefficients, are measured via two-particle\ncorrelations as a function of particle $p_\\mathrm{T}$ and event centrality.\nResults are shown from minimum bias events and events selected because of the\npresence of a high-$p_\\mathrm{T}$ jet.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Prior studies have unveiled the vulnerability of the deep neural networks in\nthe context of adversarial machine learning, leading to great recent attention\ninto this area. One interesting question that has yet to be fully explored is\nthe bias-variance relationship of adversarial machine learning, which can\npotentially provide deeper insights into this behaviour. The notion of bias and\nvariance is one of the main approaches to analyze and evaluate the\ngeneralization and reliability of a machine learning model. Although it has\nbeen extensively used in other machine learning models, it is not well explored\nin the field of deep learning and it is even less explored in the area of\nadversarial machine learning.\n  In this study, we investigate the effect of adversarial machine learning on\nthe bias and variance of a trained deep neural network and analyze how\nadversarial perturbations can affect the generalization of a network. We derive\nthe bias-variance trade-off for both classification and regression applications\nbased on two main loss functions: (i) mean squared error (MSE), and (ii)\ncross-entropy. Furthermore, we perform quantitative analysis with both\nsimulated and real data to empirically evaluate consistency with the derived\nbias-variance tradeoffs. Our analysis sheds light on why the deep neural\nnetworks have poor performance under adversarial perturbation from a\nbias-variance point of view and how this type of perturbation would change the\nperformance of a network. Moreover, given these new theoretical findings, we\nintroduce a new adversarial machine learning algorithm with lower computational\ncomplexity than well-known adversarial machine learning strategies (e.g., PGD)\nwhile providing a high success rate in fooling deep neural networks in lower\nperturbation magnitudes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Automatic credit scoring, which assesses the probability of default by loan\napplicants, plays a vital role in peer-to-peer lending platforms to reduce the\nrisk of lenders. Although it has been demonstrated that dynamic selection\ntechniques are effective for classification tasks, the performance of these\ntechniques for credit scoring has not yet been determined. This study attempts\nto benchmark different dynamic selection approaches systematically for ensemble\nlearning models to accurately estimate the credit scoring task on a large and\nhigh-dimensional real-life credit scoring data set. The results of this study\nindicate that dynamic selection techniques are able to boost the performance of\nensemble models, especially in imbalanced training environments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Radio pulses from pulsars are affected by plasma dispersion, which results in\na frequency-dependent propagation delay. Variations in the magnitude of this\neffect lead to an additional source of red noise in pulsar timing experiments,\nincluding pulsar timing arrays that aim to detect nanohertz gravitational\nwaves.\n  We aim to quantify the time-variable dispersion with much improved precision\nand characterise the spectrum of these variations.\n  We use the pulsar timing technique to obtain highly precise dispersion\nmeasure (DM) time series. Our dataset consists of observations of 36\nmillisecond pulsars, which were observed for up to 7.1 years with the LOFAR\ntelescope at a centre frequency of ~150 MHz. Seventeen of these sources were\nobserved with a weekly cadence, while the rest were observed at monthly\ncadence.\n  We achieve a median DM precision of the order of 10^-5 cm^-3 pc for a\nsignificant fraction of our sources. We detect significant variations of the DM\nin all pulsars with a median DM uncertainty of less than 2x10^-4 cm^-3 pc. The\nnoise contribution to pulsar timing experiments at higher frequencies is\ncalculated to be at a level of 0.1-10 us at 1.4 GHz over a timespan of a few\nyears, which is in many cases larger than the typical timing precision of 1 us\nor better that PTAs aim for. We found no evidence for a dependence of DM on\nradio frequency for any of the sources in our sample.\n  The DM time series we obtained using LOFAR could in principle be used to\ncorrect higher-frequency data for the variations of the dispersive delay.\nHowever, there is currently the practical restriction that pulsars tend to\nprovide either highly precise times of arrival (ToAs) at 1.4 GHz or a high DM\nprecision at low frequencies, but not both, due to spectral properties.\nCombining the higher-frequency ToAs with those from LOFAR to measure the\ninfinite-frequency ToA and DM would improve the result.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the spectra of quantum trees of finite cone type. These are quantum\ngraphs whose geometry has a certain homogeneity, and which carry a finite set\nof edge lengths, coupling constants and potentials on the edges. We show the\nspectrum consists of bands of purely absolutely continuous spectrum, along with\na discrete set of eigenvalues. Afterwards, we study random perturbations of\nsuch trees, at the level of edge length and coupling, and prove the stability\nof pure AC spectrum, along with resolvent estimates.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report a comprehensive muon spectroscopy study of the Zn-barlowite series\nof $S={\\frac{1}{2}}$ kagom\\'e antiferromagnets, Zn$_x$Cu$_{4-x}$(OH)$_{6}$FBr,\nfor $x=0.00$ to $0.99(1)$. By combining muon spin relaxation and rotation\nmeasurements with state-of-the-art density-functional theory muon-site\ncalculations, we observe the formation of both $\\mu$--F and $\\mu$--OH complexes\nin Zn-barlowite. From these stopping sites, implanted muon spins reveal the\nsuppression of long-range magnetic order into a possible quantum spin liquid\nstate upon increasing concentration of Zn-substitution. In the parent compound\n($x=0$), static long-range magnetic order below $T_{\\mathsf{N}}=15$ K manifests\nitself in the form of spontaneous oscillations in the time-dependent muon\nasymmetry signal consistent with the dipolar fields expected from the\ncalculated muon stopping sites and the previously determined magnetic structure\nof barlowite. Meanwhile, in the $x=1.0$ end-member of the series---in which\nantiferromagnetic kagom\\'e layers of Cu$^{2+}$ $S={\\frac{1}{2}}$ moments are\ndecoupled by diamagnetic Zn$^{2+}$ ions---we observe that dynamic magnetic\nmoment fluctuations persist down to at least 50 mK, indicative of a quantum\ndisordered ground state. We demonstrate that this crossover from a static to\ndynamic magnetic ground state occurs for compositions of Zn-barlowite with\n$x>0.5$, which bears resemblance to dynamical behaviour of the widely studied\nZn-paratacamite series that contains the quantum spin liquid candidate\nherbertsmithite.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A multivariate score-driven filter is developed to extract signals from noisy\nvector processes. By assuming that the conditional location vector from a\nmultivariate Student's t distribution changes over time, we construct a robust\nfilter which is able to overcome several issues that naturally arise when\nmodeling heavy-tailed phenomena and, more in general, vectors of dependent\nnon-Gaussian time series. We derive conditions for stationarity and\ninvertibility and estimate the unknown parameters by maximum likelihood (ML).\nStrong consistency and asymptotic normality of the estimator are proved and the\nfinite sample properties are illustrated by a Monte-Carlo study. From a\ncomputational point of view, analytical formulae are derived, which consent to\ndevelop estimation procedures based on the Fisher scoring method. The theory is\nsupported by a novel empirical illustration that shows how the model can be\neffectively applied to estimate consumer prices from home scanner data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We solve by Chebyshev spectral collocation some genuinely nonlinear\nLiouville-Bratu-Gelfand type, 1D and a 2D boundary value problems. The problems\nare formulated on the square domain $[-1, 1]\\times[-1, 1]$ and the boundary\ncondition attached is a homogeneous Dirichlet one. We pay a particular\nattention to the bifurcation branch on which a solution is searched and try to\nestimate empirically the attraction basin for each bifurcation variety. The\nfirst eigenvector approximating the corresponding the first eigenfunction of\nthe linear problem is used as an initial guess in solving the non-linear\nalgebraic system of Chebyshev collocation to find the \"small\" solution. For the\nsame value of the bifurcation parameter we use another initial guess, namely\nlowest basis function (1 point approximation), to find the \"big\" solution. The\nNewton-Kantorovich method solves very fast the non-linear algebraic system in\nno more than eight iterations. Beyond being exact, the method is numerically\nstable, robust and easy to implement. Actually, the MATLAB code essentially\ncontains three programming lines. It by far surpasses in simplicity and\naccuracy various methods used to solve some well-known problems. We end up by\nproviding some numerical and graphical outcomes in order to underline the\nvalidity and the effectiveness of our method, i.e., norms of Newton updates in\nsolving the algebraic systems and the decreasing rate of Chebyshev coeffcients\nof solution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The analytical structure of a static transverse component of polarization\ntensor in complex momentum plane is numerically studied, which is\nholographically determined by a Einstein-Maxwell theory in asymptotically\n$D=3+1$ dimensional Anti-de Sitter spacetime. This strongly-coupled transverse\npolarization shows a pair of conjugate simple poles on the imaginary-axis at\nlow temperature, which is different with the longitudinal component of the\ncorresponding polarization and the counterpart in its weakly-coupled version.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We conducted$^{195}$Pt-nuclear magnetic resonance measurements on\nvarious-diameter Pt nanoparticles coated with polyvinylpyrrolidone in order to\ndetect the quantum size effect and the discrete energy levels in the electron\ndensity of states, both of which were predicted by Kubo more than 50 years ago.\nWe succeeded in separating the signals arising from the surface and interior\nregions and found that the nuclear spin-lattice relaxation rates in both\nregions show the metallic behavior at high temperatures. Surprisingly, the\nmagnetic fluctuations in both regions exhibited anomalous behavior below the\nsame temperature $T^*$, which points to a clear size dependence and is well\nscaled with $\\delta_\\mathrm{Kubo}$. These results suggest that a size-tunable\nmetal-insulator transition occurs in the Pt nanoparticles as a result of the\nKubo effect.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Highly configurable systems are highly complex systems, with the Linux kernel\narguably being one of the most well-known ones. Since 2007, it has been a\nfrequent target of the research community, conducting empirical studies and\nbuilding dedicated methods and tools for analyzing, configuring, testing,\noptimizing, and maintaining the kernel in the light of its vast configuration\nspace. However, despite a large body of work, mainly bug fixes that were the\nresult of such research made it back into the kernel's source tree.\nUnfortunately, Linux users still struggle with kernel configuration and\nresolving configuration conflicts, since the kernel largely lacks automated\nsupport. Additionally, there are technical and community requirements for\nsupporting automated conflict resolution in the kernel, such as, for example,\nusing a pure C-based solution that uses only compatible third-party libraries\n(if any). With the aim of contributing back to the Linux community, we present\nCONFIGFIX, a tooling that we integrated with the kernel configurator, that is\npurely implemented in C, and that is finally a working solution able to produce\nfixes for configuration conflicts. In this experience report, we describe our\nexperiences ranging over a decade of building upon the large body of work from\nresearch on the Linux kernel configuration mechanisms as well as how we\ndesigned and realized CONFIGFIX while adhering to the Linux kernel's community\nrequirements and standards. While CONFIGFIX helps Linux kernel users obtaining\ntheir desired configuration, the sound semantic abstraction we implement\nprovides the basis for many of the above techniques supporting kernel\nconfiguration, helping researchers and kernel developers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Frequency metrology outperforms any other branch of metrology in accuracy\n(parts in $10^{-16}$) and small fluctuations ($<10^{-17}$). In turn, among\ncelestial bodies, the rotation speed of millisecond pulsars (MSP) is by far the\nmost stable ($<10^{-18}$). Therefore, the precise measurement of the time of\narrival (TOA) of pulsar signals is expected to disclose information about\ncosmological phenomena, and to enlarge our astrophysical knowledge. Related to\nthis topic, Pulsar Timing Array (PTA) projects have been developed and operated\nfor the last decades. The TOAs from a pulsar can be affected by local emission\nand environmental effects, in the direction of the propagation through the\ninterstellar medium or universally by gravitational waves from super massive\nblack hole binaries. These effects (signals) can manifest as a low-frequency\nfluctuation over time, phenomenologically similar to a red noise. While the\nremaining pulsar intrinsic and instrumental background (noise) are white. This\narticle focuses on the frequency metrology of pulsars. From our standpoint, the\npulsar is an accurate clock, to be measured simultaneously with several\ntelescopes in order to reject the uncorrelated white noise. We apply the modern\nstatistical methods of time-and-frequency metrology to simulated pulsar data,\nand we show the detection limit of the correlated red noise signal between\ntelescopes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We determined the rotational periods of 3122 Florence, 3830 Trelleborg, and\n(131077) 2000 YH105 with the Harvard Clay Telescope and KeplerCam at the Fred\nL. Whipple Observatory. We found the rotational periods to be 2.3580 $\\pm$\n0.0015 h, 17.059 $\\pm$ 0.017 h, and 1.813 $\\pm$ 0.00003 h, respectively. Our\nmeasurement of 3122 Florence's period agrees with Warner (2016), who reported\n2.3580 $\\pm$ 0.0002 h.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This article is an introductory survey of index theory in the context of\nnoncommutative geometry, written for the occasion of the 70th birthday of Alain\nConnes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hate speech, quite common in the age of social media, at times harmless but\ncan also cause mental trauma to someone or even riots in communities. Image of\na religious symbol with derogatory comment or video of a man abusing a\nparticular community, all become hate speech with its every modality (such as\ntext, image, and audio) contributing towards it. Models based on a particular\nmodality of hate speech post on social media are not useful, rather, we need\nmodels like multi-modal fusion models that consider both image and text while\nclassifying hate speech. Text-image fusion models are heavily parameterized,\nhence we propose a quaternion neural network-based model having additional\nfusion components for each pair of modalities. The model is tested on the\nMMHS150K twitter dataset for hate speech classification. The model shows an\nalmost 75% reduction in parameters and also benefits us in terms of storage\nspace and training time while being at par in terms of performance as compared\nto its real counterpart.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Three-field Fluid-Structure Interaction (FSI) formulations for fluid and\nsolid are applied and compared to the standard two field-one field formulation\nfor fluid and solid, respectively. Both formulations are applied in a non\nlinear setting for a Newtonian fluid and a neo-Hookean solid in an updated\nLagrangian form, both approximated using finite elements and stabilized by\nmeans of the Variational Multiscale (VMS) Method to permit the use of arbitrary\ninterpolations. It is shown that this type of coupling leads to a more stable\nsolution. Even though the new formulation poses the necessity of additional\ndegrees of freedom, it is possible to achieve the same degree of accuracy as\nstandard FSI by means of coarser meshes, thus making the method competitive. We\nenhance the stability of the formulation by assuming that the sub-grid scales\nof the model evolve through time. Benchmarking of the formulation is carried\nout. Numerical results are presented for semi-stationary and a fully transient\ncases for well known benchmarks for 2D and 3D scenarios.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is generally held that >100 TeV emission from astrophysical objects\nunambiguously demonstrates the presence of PeV protons or nuclei, due to the\nunavoidable Klein-Nishina suppression of inverse Compton emission from\nelectrons. However, in the presence of inverse Compton dominated cooling, hard\nhigh-energy electron spectra are possible. We show that the environmental\nrequirements for such spectra can naturally be met in spiral arms, and in\nparticular in regions of enhanced star formation activity, the natural\nlocations for the most promising electron accelerators: powerful young pulsars.\nOur scenario suggests a population of hard ultra-high energy sources is likely\nto be revealed in future searches, and may also provide a natural explanation\nfor the 100 TeV sources recently reported by HAWC.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze rapid-cadence, multiwavelength photometry of AR Scorpii from three\nobservatories, covering five observing seasons. We measure the arrival times of\nthe system's beat pulses and use them to compute an updated ephemeris. The\nwhite dwarf spin-down rate is estimated with an uncertainty of only 4%. These\nresults confirm, beyond any doubt, that the white dwarf's spin period is\nincreasing at the rate consistent with by that of Stiller et al. (2018). We\nstudy the evolution of the beat pulse's color index across the orbit. The color\nof the primary pulse maxima varies significantly across the orbit, with the\npeaks being bluer after superior conjunction than in the first half of the\norbit. Specifically, at orbital phase 0.5, the color index of the primary pulse\nshows a very sharp discontinuity towards bluer indices. This supports the\nPotter & Buckley (2018b) synchrotron emission model where the two emitting\npoles differ significantly in color. However, no corresponding jump in the\ncolor of the secondary pulses is seen. Furthermore, our analysis reveals that\nthe arrival times of the pulses can differ by as much as 6s in simultaneous $u$\nand $r$ photometry, depending on the binary orbital phase. If left uncorrected,\nthis wavelength-dependent timing offset could lead to erroneous measurements of\nthe spin-period derivative, particularly with heterogeneous datasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The classical development of neural networks has been primarily for mappings\nbetween a finite-dimensional Euclidean space and a set of classes, or between\ntwo finite-dimensional Euclidean spaces. The purpose of this work is to\ngeneralize neural networks so that they can learn mappings between\ninfinite-dimensional spaces (operators). The key innovation in our work is that\na single set of network parameters, within a carefully designed network\narchitecture, may be used to describe mappings between infinite-dimensional\nspaces and between different finite-dimensional approximations of those spaces.\nWe formulate approximation of the infinite-dimensional mapping by composing\nnonlinear activation functions and a class of integral operators. The kernel\nintegration is computed by message passing on graph networks. This approach has\nsubstantial practical consequences which we will illustrate in the context of\nmappings between input data to partial differential equations (PDEs) and their\nsolutions. In this context, such learned networks can generalize among\ndifferent approximation methods for the PDE (such as finite difference or\nfinite element methods) and among approximations corresponding to different\nunderlying levels of resolution and discretization. Experiments confirm that\nthe proposed graph kernel network does have the desired properties and show\ncompetitive performance compared to the state of the art solvers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We relate the problem of best low-rank approximation in the spectral norm for\na matrix $A$ to Kolmogorov $n$-widths and corresponding optimal spaces. We\ncharacterize all the optimal spaces for the image of the Euclidean unit ball\nunder $A$ and we show that any orthonormal basis in an $n$-dimensional optimal\nspace generates a best rank-$n$ approximation to $A$. We also present a simple\nand explicit construction to obtain a sequence of optimal $n$-dimensional\nspaces once an initial optimal space is known. This results in a variety of\nsolutions to the best low-rank approximation problem and provides alternatives\nto the truncated singular value decomposition. This variety can be exploited to\nobtain best low-rank approximations with problem-oriented properties.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we continue our recent study of a manifold endowed with a\nsingular or regular distribution, determined as the image of the tangent bundle\nunder a smooth endomorphism, and generalize Bochner's technique to the case of\na distribution with a statistical type structure. Following the theory of\nstatistical structures on Riemannian manifolds and construction of an almost\nLie algebroid on a vector bundle, we define the modified statistical connection\nand exterior derivative on tensors. Then we introduce the Weitzenbock type\ncurvature operator on tensors and derive the Bochner-Weitzenbock type formula.\nThese allow us to obtain vanishing theorems about the null space of the Hodge\ntype Laplacian on a distribution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The dynamics of wealth distribution plays a critical role in the economic\nmarket, hence an understanding of its nonequilibrium statistical mechanics is\nof great importance to human society. For this aim, a simple and efficient\none-dimensional (1D) lattice gas automaton (LGA) is presented for wealth\ndistribution of agents with or without saving propensity. The LGA comprises two\nstages, i.e., random propagation and economic transaction. During the former\nphase, an agent either remains motionless or travels to one of its neighboring\nempty sites with a certain probability. In the subsequent procedure, an\neconomic transaction takes place between a pair of neighboring agents randomly.\nIt requires at least 4 neighbors to present correct simulation results. The LGA\nreduces to the simplest model with only random economic transaction if all\nagents are neighbors and no empty sites exist. The 1D-LGA has a higher\ncomputational efficiency than the 2D-LGA and the famous Chakraborti-Chakrabarti\neconomic model. Finally, the LGA is validated with two benchmarks, i.e., the\nwealth distributions of individual agents and dual-earner families. With the\nincreasing saving fraction, both the Gini coefficient and Kolkata index (for\nindividual agents or two-earner families) reduce, while the deviation degree\n(defined to measure the difference between the probability distributions with\nand without saving propensities) increases. It is demonstrated that the wealth\ndistribution is changed significantly by the saving propensity which alleviates\nwealth inequality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Due to the large amount of data that point clouds represent and the\ndifferences in geometry of successive frames, the generation of motion vectors\nfor an entire point cloud dataset may require a significant amount of time and\ncomputational resources. With that in mind, we provide a 3D motion vector\ndatabase for all frames of two popular dynamic point cloud datasets. The motion\nvectors were obtained through translational motion estimation procedure that\npartitions the point clouds into blocks of dimensions M x M x M , and for each\nblock, a motion vector is estimated. Our database contains motion vectors for M\n= 8 and M = 16. The goal of this work is to describe this publicly available 3D\nmotion vector database that can be used for different purposes, such as\ncompression of dynamic point clouds.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Multi-access edge computing (MEC) can enhance the computing capability of\nmobile devices, while non-orthogonal multiple access (NOMA) can provide high\ndata rates. Combining these two strategies can effectively benefit the network\nwith spectrum and energy efficiency. In this paper, we investigate the task\ndelay minimization in multi-user NOMA-MEC networks, where multiple users can\noffload their tasks simultaneously through the same frequency band. We adopt\nthe partial offloading policy, in which each user can partition its computation\ntask into offloading and locally computing parts. We aim to minimize the task\ndelay among users by optimizing their tasks partition ratios and offloading\ntransmit power. The delay minimization problem is first formulated, and it is\nshown that it is a nonconvex one. By carefully investigating its structure, we\ntransform the original problem into an equivalent quasi-convex. In this way, a\nbisection search iterative algorithm is proposed in order to achieve the\nminimum task delay. To reduce the complexity of the proposed algorithm and\nevaluate its optimality, we further derive closed-form expressions for the\noptimal task partition ratio and offloading power for the case of two-user\nNOMA-MEC networks. Simulations demonstrate the convergence and optimality of\nthe proposed algorithm and the effectiveness of the closed-form analysis.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a method for optimal path planning of human walking paths in\nmountainous terrain, using a control theoretic formulation and a\nHamilton-Jacobi-Bellman equation. Previous models for human navigation were\nentirely deterministic, assuming perfect knowledge of the ambient elevation\ndata and human walking velocity as a function of local slope of the terrain.\nOur model includes a stochastic component which can account for uncertainty in\nthe problem, and thus includes a Hamilton-Jacobi-Bellman equation with\nviscosity. We discuss the model in the presence and absence of stochastic\neffects, and suggest numerical methods for simulating the model. We discuss two\ndifferent notions of an optimal path when there is uncertainty in the problem.\nFinally, we compare the optimal paths suggested by the model at different\nlevels of uncertainty, and observe that as the size of the uncertainty tends to\nzero (and thus the viscosity in the equation tends to zero), the optimal path\ntends toward the deterministic optimal path.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We suggest a novel approach for the efficient and reliable approximation of\nthe Pareto front of sufficiently smooth unconstrained bi-criteria optimization\nproblems. Optimality conditions formulated for weighted sum scalarizations of\nthe problem yield a description of (parts of) the Pareto front as a parametric\ncurve, parameterized by the scalarization parameter (i.e., the weight in the\nweighted sum scalarization). Its sensitivity w.r.t. parameter variations can be\ndescribed by an ordinary differential equation (ODE). Starting from an\narbitrary initial Pareto optimal solution, the Pareto front can then be traced\nby numerical integration. We provide an error analysis based on Lipschitz\nproperties and suggest an explicit Runge-Kutta method for the numerical\nsolution of the ODE. The method is validated on bi-criteria convex quadratic\nprogramming problems for which the exact solution is explicitly known, and\nnumerically tested on complex bi-criteria shape optimization problems involving\nfinite element discretizations of the state equation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hyperparallel quantum information processing outperforms its traditional\nparallel one in terms of channel capacity, low loss rate, and processing speed.\nWe present a way for implementing a robust hyper-parallel optical\ncontrolled-phase-flip gate through microcavities. The gate acts on polarization\nand spatial degrees of freedom (DOFs) simultaneously, and the incomplete and\nundesired interactions between photons and quantum dots are prevented.\nInterestingly, the unity fidelity of the gate can be achieved in principle, and\nthe success of the gate is heralded by the single-photon detectors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although the prediction of dialects is an important language processing task,\nwith a wide range of applications, existing work is largely limited to\ncoarse-grained varieties. Inspired by geolocation research, we propose the\nnovel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new\nlanguage model with striking abilities to predict a fine-grained variety (as\nsmall as that of a city) given a single, short message. For modeling, we offer\na range of novel spatially and linguistically-motivated multi-task learning\nmodels. To showcase the utility of our models, we introduce a new, large-scale\ndataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT\npredicts micro-dialects with 9.9% F1, ~76X better than a majority class\nbaseline. Our new language model also establishes new state-of-the-art on\nseveral external tasks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the lepton-jet correlation in deep inelastic scattering. We perform\none-loop calculations for the spin averaged and transverse spin dependent\ndifferential cross sections depending on the total transverse momentum of the\nfinal state lepton and the jet. The transverse momentum dependent (TMD)\nfactorization formalism is applied to describe the relevant observables. To\nshow the physics reach of this process, we perform a phenomenological study for\nHERA kinematics and comment on an ongoing analysis of experimental data. In\naddition, we highlight the potential of this process to constrain small-$x$\ndynamics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The NEID spectrograph is a R $\\sim$ 120,000 resolution fiber-fed and highly\nstabilized spectrograph for extreme radial velocity (RV) precision. It is being\ncommissioned at the 3.5 m WIYN telescope in Kitt Peak National Observatory with\na desired instrumental precision of better than 30 \\cms{}. NEID's bandpass of\n380 -- 930 nm enables the simultaneous wavelength coverage of activity\nindicators from the Ca HK lines in the blue to the Ca IR triplet in the IR. In\nthis paper we will present our efforts to characterize and mitigate optical\nghosts in the NEID spectrograph during assembly, integration and testing, and\nhighlight several of the dominant optical element contributors such as the\ncross dispersion prism and input optics. We shall present simulations of the\n2-D spectrum and discuss the predicted ghost features on the focal plane, and\nhow they may impact the RV performance for NEID. We also present the mitigation\nstrategy adopted for each ghost which may be applied to future instrument\ndesigns. This work will enable other instrument builders to potentially avoid\nsome of these issues, as well as outline mitigation strategies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The search and follow-up observation of electromagnetic (EM) counterparts of\ngravitational waves (GW) is a current hot topic of GW cosmology. Due to the\nlimitation of the accuracy of the GW observation facility at this stage, we can\nonly get a rough sky-localization region for the GW event, and the typical area\nof the region is between 200 and 1500 square degrees. Since GW events occur in\nor near galaxies, limiting the observation target to galaxies can significantly\nspeedup searching for EM counterparts. Therefore, how to efficiently select\nhost galaxy candidates in such a large GW localization region, how to arrange\nthe observation sequence, and how to efficiently identify the GW source from\nobservational data are the problems that need to be solved. International\nVirtual Observatory Alliance has developed a series of technical standards for\ndata retrieval, interoperability and visualization. Based on the application of\nVO technologies, we construct the GW follow-up Observation Planning System\n(GWOPS). It consists of three parts: a pipeline to select host candidates of GW\nand sort their priorities for follow-up observation, an identification module\nto find the transient from follow-up observation data, and a visualization\nmodule to display GW-related data. GWOPS can rapidly respond to GW events. With\nGWOPS, the operations such as follow-up observation planning, data storage,\ndata visualization, and transient identification can be efficiently\ncoordinated, which will promote the success searching rate for GWs EM\ncounterparts.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Unlike gravitational waves from merging black holes and neutron stars that\nchirp significantly over the observational period of ground-based detectors,\ngravitational waves from binary white dwarfs are almost monochromatic. This\nmakes it extremely challenging to measure their individual masses. Here, we\ntake a novel approach of using finite-size effects and applying certain\nuniversal relations to measure individual masses of binary white dwarfs using\nLISA. We found quasi-universal relations among the mass, moment of inertia, and\ntidal deformability of a white dwarf that do not depend sensitively on the\nwhite dwarf composition. These relations allow us to rewrite the moments of\ninertia and tidal deformabilities in the waveform in terms of the masses. We\nthen carried out a Fisher analysis to estimate how accurately one can measure\nthe individual masses from the chirp mass and finite-size measurements. We\nfound that the individual white dwarf masses can be measured with LISA for a\n4-year observation if the initial frequency is high enough ($\\sim 0.02$Hz) and\neither the binary separation is small ($\\sim 1$kpc) or the masses are\nrelatively large $(m \\gtrsim 0.8M_\\odot)$. This opens a new possibility of\nmeasuring individual masses of binary white dwarfs with space-based\ninterferometers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we prove that given a 2-edge-coloured complete graph $K_{4n}$\nthat has the same number of edges of each colour, we can always find a perfect\nmatching with an equal number of edges of each colour. This solves a problem\nposed by Caro, Hansberg, Lauri, and Zarb. The problem is also independently\nsolved by Ehard, Mohr, and Rautenbach.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  M\\\"{o}ssbauer spectroscopy is applied to study ultrasound vibration of a\ngranular material. A pile of powder is placed onto the surface of piezo\ntransducer vibrated with the frequency 12.68 MHz. The size of grains (1.3\n$\\mu$m) is much smaller than the wavelength of ultrasound, but much larger than\nthe vibration amplitude. Due to vibration a single line of the M\\\"{o}ssbauer\ntransmission spectrum is split into a comb structure with a period equal to the\nvibration frequency. This spectrum contains the information about fast and slow\nmodes in granular dynamics. We developed a method which allows to measure decay\nof ultrasound in the granular material and to estimate the results of the\nparticle convection of grains in the pile.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Action recognition is an open and challenging problem in computer vision.\nWhile current state-of-the-art models offer excellent recognition results,\ntheir computational expense limits their impact for many real-world\napplications. In this paper, we propose a novel approach, called AR-Net\n(Adaptive Resolution Network), that selects on-the-fly the optimal resolution\nfor each frame conditioned on the input for efficient action recognition in\nlong untrimmed videos. Specifically, given a video frame, a policy network is\nused to decide what input resolution should be used for processing by the\naction recognition model, with the goal of improving both accuracy and\nefficiency. We efficiently train the policy network jointly with the\nrecognition model using standard back-propagation. Extensive experiments on\nseveral challenging action recognition benchmark datasets well demonstrate the\nefficacy of our proposed approach over state-of-the-art methods. The project\npage can be found at https://mengyuest.github.io/AR-Net\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study a class of Little String Theories (LSTs) of A type, described by $N$\nparallel M5-branes spread out on a circle and which in the low energy regime\nengineer supersymmetric gauge theories with $U(N)$ gauge group. The BPS states\nin this setting correspond to M2-branes stretched between the M5-branes.\nGeneralising an observation made in arXiv:1706.04425, we provide evidence that\nthe BPS counting functions of special subsectors of the latter exhibit a Hecke\nstructure in the Nekrasov-Shatashvili (NS) limit, i.e. the different orders in\nan instanton expansion of the supersymmetric gauge theory are related through\nthe action of Hecke operators. We extract $N$ distinct such reduced BPS\ncounting functions from the full free energy of the LST with the help of\ncontour integrals with respect to the gauge parameters of the $U(N)$ gauge\ngroup. Physically, the states captured by these functions correspond to\nconfigurations where the same number of M2-branes is stretched between some of\nthese neighbouring M5-branes, while the remaining M5-branes are collapsed on\ntop of each other and a particular singular contribution is extracted. The\nHecke structures suggest that these BPS states form the spectra of symmetric\norbifold CFTs. We furthermore show that to leading instanton order (in the\nNS-limit) the reduced BPS counting functions factorise into simpler building\nblocks. These building blocks are the expansion coefficients of the free energy\nfor $N=1$ and the expansion of a particular function, which governs the\ncounting of BPS states of a single M5-brane with single M2-branes ending on it\non either side. To higher orders in the instanton expansion, we observe new\nelements appearing in this decomposition, whose coefficients are related\nthrough a holomorphic anomaly equation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this article, we characterize absolutely norm attaining normal operators\nin terms of the essential spectrum. Later we prove a structure theorem for\nhyponormal absolutely norm attaining (or $\\mathcal{AN}$-operators in short) and\ndeduce conditions for the normality of the operator.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Population protocols are a class of algorithms for modeling distributed\ncomputation in networks of finite-state agents communicating through pairwise\ninteractions. Their suitability for analyzing numerous chemical processes has\nmotivated the adaptation of the original population protocol framework to\nbetter model these chemical systems. In this paper, we further the study of two\nsuch adaptations in the context of solving approximate majority:\npersistent-state agents (or catalysts) and spontaneous state changes (or\nleaks).\n  Based on models considered in recent protocols for populations with\npersistent-state agents, we assume a population with $n$ catalytic input agents\nand $m$ worker agents, and the goal of the worker agents is to compute some\npredicate over the states of the catalytic inputs. We call this model the\nCatalytic Input (CI) model. For $m = \\Theta(n)$, we show that computing the\nparity of the input population with high probability requires at least\n$\\Omega(n^2)$ total interactions, demonstrating a strong separation between the\nCI model and the standard population protocol model. On the other hand, we show\nthat the simple third-state dynamics of Angluin et al. for approximate majority\nin the standard model can be naturally adapted to the CI model: we present such\na constant-state protocol for the CI model that solves approximate majority in\n$O(n \\log n)$ total steps with high probability when the input margin is\n$\\Omega(\\sqrt{n \\log n})$.\n  We then show the robustness of third-state dynamics protocols to the\ntransient leaks events introduced by Alistarh et al. In both the original and\nCI models, these protocols successfully compute approximate majority with high\nprobability in the presence of leaks occurring at each step with probability\n$\\beta \\leq O\\left(\\sqrt{n \\log n}/n\\right)$, exhibiting a resilience to leaks\nsimilar to that of Byzantine agents in previous works.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A {\\em theta} is a graph made of three internally vertex-disjoint chordless\npaths $P_1 = a \\dots b$, $P_2 = a \\dots b$, $P_3 = a \\dots b$ of length at\nleast~2 and such that no edges exist between the paths except the three edges\nincident to $a$ and the three edges incident to $b$. A {\\em pyramid} is a graph\nmade of three chordless paths $P_1 = a \\dots b_1$, $P_2 = a \\dots b_2$, $P_3 =\na \\dots b_3$ of length at least~1, two of which have length at least 2,\nvertex-disjoint except at $a$, and such that $b_1b_2b_3$ is a triangle and no\nedges exist between the paths except those of the triangle and the three edges\nincident to~$a$. An \\emph{even hole} is a chordless cycle of even length. For\nthree non-negative integers $i\\leq j\\leq k$, let $S_{i,j,k}$ be the tree with a\nvertex $v$, from which start three paths with $i$, $j$, and $k$ edges\nrespectively. We denote by $K_t$ the complete graph on $t$ vertices.\n  We prove that for all non-negative integers $i, j, k$, the class of graphs\nthat contain no theta, no $K_3$, and no $S_{i, j, k}$ as induced subgraphs have\nbounded treewidth. We prove that for all non-negative integers $i, j, k, t$,\nthe class of graphs that contain no even hole, no pyramid, no $K_t$, and no\n$S_{i, j, k}$ as induced subgraphs have bounded treewidth. To bound the\ntreewidth, we prove that every graph of large treewidth must contain a large\nclique or a minimal separator of large cardinality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a new family of high-order shock-capturing finite difference\nnumerical methods for systems of conservation laws. These methods, called\nAdaptive Compact Approximation Taylor (ACAT) schemes, use centered $(2p +\n1)$-point stencils, where $p$ may take values in $\\{1, 2, \\dots, P\\}$ according\nto a new family of smoothness indicators in the stencils. The methods are based\non a combination of a robust first order scheme and the Compact Approximate\nTaylor (CAT) methods of order $2p$-order, $p=1,2,\\dots, P$ so that they are\nfirst order accurate near discontinuities and have order $2p$ in smooth\nregions, where $(2p +1)$ is the size of the biggest stencil in which large\ngradients are not detected. CAT methods, introduced in \\cite{CP2019}, are an\nextension to nonlinear problems of the Lax-Wendroff methods in which the\nCauchy-Kovalesky (CK) procedure is circumvented following the strategy\nintroduced in \\cite{ZBM2017} that allows one to compute time derivatives in a\nrecursive way using high-order centered differentiation formulas combined with\nTaylor expansions in time. The expression of ACAT methods for 1D and 2D systems\nof balance laws are given and the performance is tested in a number of test\ncases for several linear and nonlinear systems of conservation laws, including\nEuler equations for gas dynamics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  For real-world deployment of automatic speech recognition (ASR), the system\nis desired to be capable of fast inference while relieving the requirement of\ncomputational resources. The recently proposed end-to-end ASR system based on\nmask-predict with connectionist temporal classification (CTC), Mask-CTC,\nfulfills this demand by generating tokens in a non-autoregressive fashion.\nWhile Mask-CTC achieves remarkably fast inference speed, its recognition\nperformance falls behind that of conventional autoregressive (AR) systems. To\nboost the performance of Mask-CTC, we first propose to enhance the encoder\nnetwork architecture by employing a recently proposed architecture called\nConformer. Next, we propose new training and decoding methods by introducing\nauxiliary objective to predict the length of a partial target sequence, which\nallows the model to delete or insert tokens during inference. Experimental\nresults on different ASR tasks show that the proposed approaches improve\nMask-CTC significantly, outperforming a standard CTC model (15.5% $\\rightarrow$\n9.1% WER on WSJ). Moreover, Mask-CTC now achieves competitive results to AR\nmodels with no degradation of inference speed ($<$ 0.1 RTF using CPU). We also\nshow a potential application of Mask-CTC to end-to-end speech translation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Anomalous transport in a circular comb is considered. The circular motion\ntakes place for a fixed radius, while radii are continuously distributed along\nthe circle. Two scenarios of the anomalous transport, related to the reflecting\nand periodic angular boundary conditions, are studied. The first scenario with\nthe reflection boundary conditions for the circular diffusion corresponds to\nthe conformal mapping of a 2D comb Fokker-Planck equation on the circular comb.\nThis topologically constraint motion is named umbrella comb model. In this\ncase, the reflecting boundary conditions are imposed on the circular (rotator)\nmotion, while the radial motion corresponds to geometric Brownian motion with\nvanishing to zero boundary conditions on infinity. The radial diffusion is\ndescribed by the log-normal distribution, which corresponds to exponentially\nfast motion with the mean squared displacement (MSD) of the order of $e^t$. The\nsecond scenario corresponds to the circular diffusion with periodic boundary\nconditions and the outward radial diffusion with vanishing to zero boundary\nconditions at infinity. In this case the radial motion corresponds to normal\ndiffusion. The circular motion in both scenarios is a superposition of cosine\nfunctions that results in the stationary Bernoulli polynomials for the\nprobability distributions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  One of the main drivers of 5G cellular networks is provision of connectivity\nservice for various Internet of Things (IoT) devices. Considering the potential\nvolume of IoT devices at a global scale, the next leap is to integrate\nNon-Terrestrial Networks (NTN) into 5G terrestrial systems, thereby extend the\ncoverage and complement the terrestrial service. This paper focuses on the use\nof Low-Earth Orbit (LEO) satellite constellations for two specific purposes:\noffloading and backhauling. The former allows offloading IoT traffic from a\ncongested terrestrial network, usually in a very dense area. In the latter\napplication, the constellation provides a multi-hop backhaul that connects a\nremote terrestrial gNB to the 5G core network. After providing an overview of\nthe status of the 3GPP standardization process, we model and analyze the user\ndata performance, specifically in the uplink access and the satellite multi-hop\nconstellation path. The evaluation of the collisions, the delay and the Age of\nInformation, and the comparison of the terrestrial and the satellite access\nnetworks provide useful insights to understand the potential of LEO\nconstellations for offloading and backhauling of IoT traffic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Adversarial perturbations can be added to images to protect their content\nfrom unwanted inferences. These perturbations may, however, be ineffective\nagainst classifiers that were not {seen} during the generation of the\nperturbation, or against defenses {based on re-quantization, median filtering\nor JPEG compression. To address these limitations, we present an adversarial\nattack {that is} specifically designed to protect visual content against {\nunseen} classifiers and known defenses. We craft perturbations using an\niterative process that is based on the Fast Gradient Signed Method and {that}\nrandomly selects a classifier and a defense, at each iteration}. This\nrandomization prevents an undesirable overfitting to a specific classifier or\ndefense. We validate the proposed attack in both targeted and untargeted\nsettings on the private classes of the Places365-Standard dataset. Using\nResNet18, ResNet50, AlexNet and DenseNet161 {as classifiers}, the performance\nof the proposed attack exceeds that of eleven state-of-the-art attacks. The\nimplementation is available at https://github.com/smartcameras/RP-FGSM/.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  To adhere to the stringent time and budget requirements of construction\nprojects, contractors are utilizing prefabricated construction methods to\nexpedite the construction process. Prefabricated construction methods require\nan adequate schedule and understanding by the contractors and constructors to\nbe successful. The specificity of prefabricated construction often leads to\ninefficient scheduling and costly rework time. The designer, contractor, and\nconstructors must have a strong understanding of the assembly process to\nexperience the full benefits of the method. At the root of understanding the\nassembly process is visualizing how the process is intended to be performed.\nCurrently, a virtual construction model is used to explain and better visualize\nthe construction process. However, creating a virtual construction model is\ncurrently time consuming and requires experienced personnel. The proposed\nsimulation of the virtual assembly will increase the automation of virtual\nconstruction modeling by implementing the data available in a building\ninformation modeling (BIM) model. This paper presents various factors (i.e.,\nformalization of construction sequence based on the level of development (LOD))\nthat needs to be addressed for the development of automated virtual assembly.\nTwo case studies are presented to demonstrate these factors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a neural network for zero-shot voice conversion (VC) without any\nparallel or transcribed data. Our approach uses pre-trained models for\nautomatic speech recognition (ASR) and speaker embedding, obtained from a\nspeaker verification task. Our model is fully convolutional and\nnon-autoregressive except for a small pre-trained recurrent neural network for\nspeaker encoding. ConVoice can convert speech of any length without\ncompromising quality due to its convolutional architecture. Our model has\ncomparable quality to similar state-of-the-art models while being extremely\nfast.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The problem of compiling general quantum algorithms for implementation on\nnear-term quantum processors has been introduced to the AI community. Previous\nwork demonstrated that temporal planning is an attractive approach for part of\nthis compilationtask, specifically, the routing of circuits that implement the\nQuantum Alternating Operator Ansatz (QAOA) applied to the MaxCut problem on a\nquantum processor architecture. In this paper, we extend the earlier work to\nroute circuits that implement QAOA for Graph Coloring problems. QAOA for\ncoloring requires execution of more, and more complex, operations on the chip,\nwhich makes routing a more challenging problem. We evaluate the approach on\nstate-of-the-art hardware architectures from leading quantum computing\ncompanies. Additionally, we apply a planning approach to qubit initialization.\nOur empirical evaluation shows that temporal planning compares well to\nreasonable analytic upper bounds, and that solving qubit initialization with a\nclassical planner generally helps temporal planners in finding shorter-makespan\ncompilations for QAOA for Graph Coloring. These advances suggest that temporal\nplanning can be an effective approach for more complex quantum computing\nalgorithms and architectures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The $\\alpha$ Cen stellar system is the closest neighbour to our Sun. Its main\ncomponent is a binary composed of two main-sequence stars, one more massive\nthan the Sun and one less massive. The system's bright magnitude led to a\nwealth of astronomical observations over a long period, making it an appealing\ntestbed for stellar physics. In particular, detection of stellar pulsations in\nboth $\\alpha$ Cen A and B has revealed the potential of asteroseismology for\ndetermining its fundamental stellar parameters. Asteroseismic studies have also\nfocused on the presence of a convective core in the A component, but as yet\nwithout definitive confirmation. Progress in the determination of solar surface\nabundances and stellar opacities have yielded new input for stellar theoretical\nmodels. We investigate their impact on a reference system such as $\\alpha$ Cen\nAB. We seek to confirm the presence of a convective core in $\\alpha$ Cen A by\nanalysing the role of different stellar physics and the potential of\nasteroseismic inverse methods. We present a new series of asteroseismic\ncalibrations carried out using forward approach modelling and including updated\nchemical mixture and opacities in the models. We then complement our analysis\nwith help of recent asteroseismic diagnostic tools based on inverse methods\ndeveloped for solar-like stars. The inclusion of an updated chemical mixture --\nthat is less metal-rich -- appears to reduce the predicted asteroseismic masses\nof each component. Neither classical asteroseismic indicators such as frequency\nratios, nor asteroseismic inversions favour the presence of a convective core\nin $\\alpha$ Cen A. The quality of the observational seismic dataset is the main\nlimiting factor to settle the issue. Implementing new observing strategies to\nimprove the precision on the pulsation frequencies would certainly refine the\noutcome of asteroseismology for this binary system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  I used to believe that my conventions for drawing diagrams for categorical\nstatements could be written down in one page or less, and that the only tricky\npart was the technique for reconstructing objects \"from their names\"... but\nthen I found out that this is not so.\n  This is an attempt to explain, with motivations and examples, all the\nconventions behind a certain diagram, called the \"Basic Example\" in the text.\nOnce the conventions are understood that diagram becomes a \"skeleton\" for a\ncertain lemma related to the Yoneda Lemma, in the sense that both the statement\nand the proof of that lemma can be reconstructed from the diagram. The last\nsections discuss some simple ways to extend the conventions; we see how to\nexpress in diagrams the (\"real\") Yoneda Lemma and a corollary of it, how to\ndefine comma categories, and how to formalize the diagram for \"geometric\nmorphism for children\".\n  People in CT usually only share their ways of visualizing things when their\ndiagrams cross some threshold of of mathematical relevance - and this usually\nhappens when they prove new theorems with their diagrams, or when they can show\nthat their diagrams can translate calculations that used to be huge into things\nthat are much easier to visualize. The diagrammatic language that I present\nhere lies below that threshold - and so it is a \"private\" diagrammatic\nlanguage, that I am making public as an attempt to establish a dialogue with\nother people who have also created their own private diagrammatic languages.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Social media are pervasive in our life, making it necessary to ensure safe\nonline experiences by detecting and removing offensive and hate speech. In this\nwork, we report our submission to the Offensive Language and hate-speech\nDetection shared task organized with the 4th Workshop on Open-Source Arabic\nCorpora and Processing Tools Arabic (OSACT4). We focus on developing purely\ndeep learning systems, without a need for feature engineering. For that\npurpose, we develop an effective method for automatic data augmentation and\nshow the utility of training both offensive and hate speech models off (i.e.,\nby fine-tuning) previously trained affective models (i.e., sentiment and\nemotion). Our best models are significantly better than a vanilla BERT model,\nwith 89.60% acc (82.31% macro F1) for hate speech and 95.20% acc (70.51% macro\nF1) on official TEST data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We explore how to improve the hybrid model description of the particles\noriginating from the wake that a jet produced in a heavy ion collision leaves\nin the droplet of quark-gluon plasma (QGP) through which it propagates, using\nlinearized hydrodynamics on a background Bjorken flow. Jet energy and momentum\nloss described by the hybrid model become currents sourcing linearized\nhydrodynamics. By solving the linearized hydrodynamic equations numerically, we\ninvestigate the development of the wake in the dynamically evolving droplet of\nQGP, study the effect of viscosity, scrutinize energy-momentum conservation,\nand check the validity of the linear approximation. We find that linearized\nhydrodynamics works better in the viscous case because diffusive modes damp the\nenergy-momentum perturbation produced by the jet. We calculate the distribution\nof particles produced from the jet wake by using the Cooper-Frye prescription\nand find that both the transverse momentum spectrum and the distribution of\nparticles in azimuthal angle are similar in shape in linearized hydrodynamics\nand in the hybrid model. Their normalizations are different because the\nmomentum-rapidity distribution in the linearized hydrodynamics analysis is more\nspread out, due to sound modes. Since the Bjorken flow has no transverse\nexpansion, we explore the effect of transverse flow by using local boosts to\nadd it into the Cooper-Frye formula. After including the effects of transverse\nflow in this way, the transverse momentum spectrum becomes harder: more\nparticles with transverse momenta bigger than $2$ GeV are produced than in the\nhybrid model. Although we defer implementing this analysis in a jet Monte\nCarlo, as would be needed to make quantitative comparisons to data, we gain a\nqualitative sense of how the jet wake may modify jet observables by computing\nproxies for two example observables: the lost energy recovered in a cone of\nvarying open angle, and the fragmentation function. We find that linearized\nhydrodynamics with transverse flow effects added improves the description of\nthe jet wake in the hybrid model in just the way that comparison to data\nindicates is needed. Our study illuminates a path to improving the description\nof the wake in the hybrid model, highlighting the need to take into account the\neffects of both transverse flow and the broadening of the energy-momentum\nperturbation in spacetime rapidity on particle production.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report the bivariate HI- and H$_2$-stellar mass distributions of local\ngalaxies in addition of an inventory of galaxy mass functions, MFs, for HI,\nH$_2$, cold gas, and baryonic mass, separately into early- and late-type\ngalaxies. The MFs are determined using the HI and H$_2$ conditional\ndistributions and the galaxy stellar mass function, GSMF. For the conditional\ndistributions we use the compilation presented in Calette et al. 2018. For\ndetermining the GSMF from $M_{\\ast}\\sim3\\times10^{7}$ to $3\\times10^{12}$\n$M_{\\odot}$, we combine two spectroscopic samples from the SDSS at the redshift\nrange $0.0033<z<0.2$. We find that the low-mass end slope of the GSMF, after\ncorrecting from surface brightness incompleteness, is $\\alpha\\approx-1.4$,\nconsistent with previous determinations. The obtained HI MFs agree with radio\nblind surveys. Similarly, the H$_2$ MFs are consistent with CO follow-up\noptically-selected samples. We estimate the impact of systematics due to\nmass-to-light ratios and find that our MFs are robust against systematic\nerrors. We deconvolve our MFs from random errors to obtain the intrinsic MFs.\nUsing the MFs, we calculate cosmic density parameters of all the baryonic\ncomponents. Baryons locked inside galaxies represent 5.4% of the universal\nbaryon content, while $\\sim96$% of the HI and H$_2$ mass inside galaxies reside\nin late-type morphologies. Our results imply cosmic depletion times of H$_2$\nand total neutral H in late-type galaxies of $\\sim 1.3$ and 7.2 Gyr,\nrespectively, which shows that late type galaxies are on average inefficient in\nconverting H$_2$ into stars and in transforming HI gas into H$_2$. Our results\nprovide a fully self-consistent empirical description of galaxy demographics in\nterms of the bivariate gas--stellar mass distribution and their projections,\nthe MFs. This description is ideal to compare and/or to constrain galaxy\nformation models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Composition optimization is widely-applied in nonconvex machine learning.\nVarious advanced stochastic algorithms that adopt momentum and variance\nreduction techniques have been developed for composition optimization. However,\nthese algorithms do not fully exploit both techniques to accelerate the\nconvergence and are lack of convergence guarantee in nonconvex optimization.\nThis paper complements the existing literature by developing various momentum\nschemes with SPIDER-based variance reduction for non-convex composition\noptimization. In particular, our momentum design requires less number of\nproximal mapping evaluations per-iteration than that required by the existing\nKatyusha momentum. Furthermore, our algorithm achieves near-optimal sample\ncomplexity results in both non-convex finite-sum and online composition\noptimization and achieves a linear convergence rate under the gradient dominant\ncondition. Numerical experiments demonstrate that our algorithm converges\nsignificantly faster than existing algorithms in nonconvex composition\noptimization.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, two-fold symmetric in-plane anisotropy of the superconducting\nproperties have been observed in a single crystal of BiCh2-based (Ch: S, Se)\nlayered superconductor LaO0.5F0.5BiSSe having a tetragonal\n(four-fold-symmetric) in-plane structure; the phenomena are very similar to\nthose observed in nematic superconductors. To explore the origin of the\ntwo-fold symmetric anisotropy in the BiCh2-based system, we have investigated\nthe electron-doping dependence on the anisotropy by examining the in-plane\nanisotropy of the magnetoresistance in the superconducting states for a single\ncrystal of LaO0.9F0.1BiSSe under high magnetic fields up to 15 T. We observed a\ntwo-fold symmetry of in-plane anisotropy of magnetoresistance for\nLaO0.9F0.1BiSSe. The results obtained for LaO0.9F0.1BiSSe are quite similar to\nthose observed for LaO0.5F0.5BiSSe, which has a higher electron doping\nconcentration than LaO0.9F0.1BiSSe. Our present finding suggests that the\nemergence of the in-plane symmetry breaking in the superconducting state is\nrobust to the carrier concentration in the series of LaO1-xFxBiSSe.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that generalized eventually narrow sequences on a strongly\ninaccessible cardinal $\\kappa$ are preserved under the Cummings-Shaleh\nnon-linear iterations of the higher Hechler forcing on $\\kappa$. Moreover\nassuming GCH, $\\kappa^{<\\kappa}=\\kappa$, we show that:\n  (1) if $\\kappa$ is strongly unfoldable,\n$\\kappa^+\\leq\\beta=\\hbox{cf}(\\beta)\\leq \\hbox{cf}(\\delta)\\leq\\delta\\leq\\mu$ and\n$\\hbox{cf}(\\mu)>\\kappa$,then there is a cardinal preserving generic extension\nin which\n$$\\mathfrak{s}(\\kappa)=\\kappa^+\\leq\\mathfrak{b}(\\kappa)=\\beta\\leq\\mathfrak{d}(\\kappa)=\\delta\\leq\n2^\\kappa=\\mu.$$ (2) if $\\kappa$ is strongly inaccessible, $\\lambda>\\kappa^+$,\nthen in the generic extension obtained as the $<\\kappa$-support iteration of\n$\\kappa$-Hechler forcing of length $\\lambda$ there are no $\\kappa$-towers of\nlength $\\lambda$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Interactions between biomolecules, electrons and protons are essential to\nmany fundamental processes sustaining life. It is therefore of interest to\nbuild mathematical models of these bioelectrical processes not only to enhance\nunderstanding but also to enable computer models to complement in vitro and in\nvivo experiments.Such models can never be entirely accurate; it is nevertheless\nimportant that the models are compatible with physical principles. Network\nThermodynamics, as implemented with bond graphs, provide one approach to\ncreating physically compatible mathematical models of bioelectrical systems.\nThis is illustrated using simple models of ion channels, redox reactions,\nproton pumps and electrogenic membrane transporters thus demonstrating that the\napproach can be used to build mathematical and computer models of a wide range\nof bioelectrical systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many properties of layered materials change as they are thinned from their\nbulk forms down to single layers, with examples including indirect-to-direct\nband gap transition in 2H semiconducting transition metal dichalcogenides as\nwell as thickness-dependent changes in the valence band structure in\npost-transition metal monochalcogenides and black phosphorus. Here, we use\nangle-resolved photoemission spectroscopy to study the electronic band\nstructure of monolayer ReSe$_{2}$, a semiconductor with a distorted 1T\nstructure and in-plane anisotropy. By changing the polarization of incoming\nphotons, we demonstrate that for ReSe$_{2}$, in contrast to the 2H materials,\nthe out-of-plane transition metal $d_{z^{2}}$ and chalcogen $p_{z}$ orbitals do\nnot contribute significantly to the top of the valence band which explains the\nreported weak changes in the electronic structure of this compound as a\nfunction of layer number. We estimate a band gap of 1.7 eV in pristine\nReSe$_{2}$ using scanning tunneling spectroscopy and explore the implications\non the gap following surface-doping with potassium. A lower bound of 1.4 eV is\nestimated for the gap in the fully doped case, suggesting that doping-dependent\nmany-body effects significantly affect the electronic properties of ReSe$_{2}$.\nOur results, supported by density functional theory calculations, provide\ninsight into the mechanisms behind polarization-dependent optical properties of\nrhenium dichalcogenides and highlight their place amongst two-dimensional\ncrystals.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the photoassociation dynamics of exactly two laser-cooled\n$^{85}$Rb atoms in an optical tweezer and reveal fundamentally different\nbehavior to photoassociation in many-atom ensembles. We observe non-exponential\ndecay in our two-atom experiment that cannot be described by a single rate\ncoefficient and find its origin in our system's pair correlation. This is in\nstark contrast to many-atom photoassociation dynamics, which are governed by\nexponential decay with a single rate coefficient. We also investigate\nphotoassociation in a three-atom system, thereby probing the transition from\ntwo-atom dynamics to many-atom dynamics. Our experiments reveal additional\nreaction dynamics that are only accessible through the control of single atoms\nand suggest photoassociation could measure pair correlations in few-atom\nsystems. It further showcases our complete control over the quantum state of\nindividual atoms and molecules, which provides information unobtainable from\nmany-atom experiments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This thesis focuses on two research areas: non-relativistic field theories\nand complexity. In the first part we review the general classification of the\ntrace anomaly for 2+1 dimensional field theories coupled to a Newton-Cartan\nbackground and we apply the heat kernel method to compute the trace anomaly for\nspecific theories. We find a relation with the conformal anomaly of the 3+1\ndimensional relativistic counterpart which suggests the existence of a\nnon-relativistic version of the a-theorem. We consider a model realizing a\n$\\mathcal{N}=2$ supersymmetric extension of the Bargmann group in 2+1\ndimensions with non-vanishing superpotential, obtained by null reduction of a\nrelativistic Wess-Zumino model. We check that the superpotential is protected\nagainst quantum corrections as in the relativistic parent theory, thus finding\na non-relativistic version of the non-renormalization theorem. We find evidence\nthat the theory is one-loop exact, due to the causal structure of the\nnon-relativistic propagator together with mass conservation. In the second part\nof the thesis we review the holographic conjectures proposed by Susskind to\ndescribe the time-evolution of the Einstein-Rosen bridge in gravity: the\ncomplexity=volume and complexity=action. We investigate both the volume and the\naction for black holes living in warped $\\mathrm{AdS}_3$ spacetime. There exist\nextensions of the proposals when the dual state from the field theory side is\nmixed; we then analytically compute the subregion action complexity for a\ngeneral segment on the boundary in the BTZ black hole background, finding that\nit is equal to the sum of a linearly divergent term proportional to the size of\nthe subregion and of a term proportional to the entanglement entropy. We also\nfind that mutual holographic complexity carries a different content compared to\nmutual information. This means that entropy is not enough!\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Kaluza-Klein (KK) decomposition of higher-dimensional gravity gives rise\nto a tower of KK-gravitons in the effective four-dimensional (4D) theory. Such\nmassive spin-2 fields are known to be connected with unitarity issues and\neasily lead to a breakdown of the effective theory well below the naive scale\nof the interaction. However, the breakdown of the effective 4D theory is\nexpected to be controlled by the parameters of the 5D theory. Working in a\nsimplified Randall-Sundrum model we study the matrix elements for matter\nannihilations into massive gravitons. We find that truncating the KK-tower\nleads to an early breakdown of perturbative unitarity. However, by considering\nthe full tower we obtain a set of sum rules for the couplings between the\ndifferent KK-fields that restore unitarity up to the scale of the 5D theory. We\nprove analytically that these are fulfilled in the model under consideration\nand present numerical tests of their convergence. This work complements earlier\nstudies that focused on graviton self-interactions and yields additional sum\nrules that are required if matter fields are incorporated into warped\nextra-dimensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this report, we study decentralized stochastic optimization to minimize a\nsum of smooth and strongly convex cost functions when the functions are\ndistributed over a directed network of nodes. In contrast to the existing work,\nwe use gradient tracking to improve certain aspects of the resulting algorithm.\nIn particular, we propose the~\\textbf{\\texttt{S-ADDOPT}} algorithm that assumes\na stochastic first-order oracle at each node and show that for a constant\nstep-size~$\\alpha$, each node converges linearly inside an error ball around\nthe optimal solution, the size of which is controlled by~$\\alpha$. For decaying\nstep-sizes~$\\mathcal{O}(1/k)$, we show that~\\textbf{\\texttt{S-ADDOPT}} reaches\nthe exact solution sublinearly at~$\\mathcal{O}(1/k)$ and its convergence is\nasymptotically network-independent. Thus the asymptotic behavior\nof~\\textbf{\\texttt{S-ADDOPT}} is comparable to the centralized stochastic\ngradient descent. Numerical experiments over both strongly convex and\nnon-convex problems illustrate the convergence behavior and the performance\ncomparison of the proposed algorithm.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Machine learning has the potential to fuel further advances in data science,\nbut it is greatly hindered by an ad hoc design process, poor data hygiene, and\na lack of statistical rigor in model evaluation. Recently, these issues have\nbegun to attract more attention as they have caused public and embarrassing\nissues in research and development. Drawing from our experience as machine\nlearning researchers, we follow the machine learning process from algorithm\ndesign to data collection to model evaluation, drawing attention to common\npitfalls and providing practical recommendations for improvements. At each\nstep, case studies are introduced to highlight how these pitfalls occur in\npractice, and where things could be improved.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study correlation functions involving generalized ANEC operators of the\nform $\\int dx^- \\left(x^-\\right)^{n+2} T_{--}(\\vec{x})$ in four dimensions. We\ncompute two, three, and four-point functions involving external scalar states\nin both free and holographic Conformal Field Theories. From this information,\nwe extract the algebra of these light-ray operators. We find a global\nsubalgebra spanned by $n=\\{-2, -1, 0, 1, 2\\}$ which annihilate the conformally\ninvariant vacuum and transform among themselves under the action of the\ncollinear conformal group that preserves the light-ray. Operators outside this\nrange give rise to an infinite central term, in agreement with previous\nsuggestions in the literature. In free theories, even some of the operators\ninside the global subalgebra fail to commute when placed at spacelike\nseparation on the same null-plane. This lack of commutativity is not\nintegrable, presenting an obstruction to the construction of a well defined\nlight-ray algebra at coincident $\\vec{x}$ coordinates. For holographic CFTs the\nbehavior worsens and operators with $n \\neq -2$ fail to commute at spacelike\nseparation. We reproduce this result in the bulk of AdS where we present new\nexact shockwave solutions dual to the insertions of these (exponentiated)\noperators on the boundary.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We use a sample of 27 GRBs at redshift $z=2-6$ to probe the outflows in their\nrespective host galaxies ($\\mathrm{log(M_*/M_{\\odot})}~\\sim~9-11$) and search\nfor possible relations between the outflow properties and those of the host\ngalaxies such as $\\mathrm{M_*}$, SFR, and specific SFR. First, we consider\nthree outflow properties $-$ outflow column density ($\\mathrm{N_{out}}$),\nmaximum outflow velocity ($\\mathrm{V_{max}}$), and normalized maximum velocity\n($\\mathrm{V_{norm}}$ = $\\mathrm{V_{max}/V_{circ, halo}}$, where\n$\\mathrm{V_{circ,halo}}$ is the halo circular velocity). We observe clear\ntrends of $\\mathrm{N_{out}}$ and $\\mathrm{V_{max}}$ with increasing SFR in\nhigh-ion-traced outflows, with a stronger ($>~3\\sigma$) $\\mathrm{V_{max}}-$SFR\ncorrelation. We find that the estimated mass outflow rate and momentum flux of\nthe high-ion outflows scale with SFR and can be supported by the momentum\nimparted by star formation (supernovae and stellar winds). The kinematic\ncorrelations of high-ion-traced outflows with SFR are similar to those observed\nfor star-forming galaxies at low redshifts.\n  The correlations with SFR are weaker in low-ions. This, along with the lower\ndetection fraction in low-ions, indicates that the outflow is primarily\nhigh-ion dominated. We also observe a strong ($>~3\\sigma$) trend of normalized\nvelocity ($\\mathrm{V_{norm}}$) decreasing with halo mass and increasing with\nsSFR, suggesting that outflows from low-mass halos and high-sSFR galaxies are\nmost likely to escape and enrich the outer CGM and IGM with metals. By\ncomparing the CGM-GRB stacks with those of starbursts at $z\\sim2$ and\n$z\\sim0.1$, we find that over a broad redshift range, the outflow strength\nstrongly depends on the main-sequence offset at the respective redshifts rather\nthan simply the SFR.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we introduce the notion of $L^p$-Green-tight measures of\n$L^p$-Kato class in the framework of symmetric Markov processes. The class of\n$L^p$-Green-tight measures of $L^p$-Kato class is defined by the $p$-th power\nof resolvent kernels. We first prove that under the $L^p$-Green tightness of\nthe measure $\\mu$, the embedding of extended Dirichlet space into\n$L^{2p}(E;\\mu)$ is compact under the absolute continuity condition for\ntransient Markov processes, which is an extension of recent seminal work by\nTakeda. Secondly, we prove the coincidence between two classes of\n$L^p$-Green-tightness, one is originally introduced by Zhao, and another one is\ninvented by Chen. Finally, we prove that our class of $L^p$-Green-tight\nmeasures of $L^p$-Kato class coincides with the class of $L^p$-Green tight\nmeasures of Kato class in terms of Green kernel under the global heat kernel\nestimates. We apply our results to $d$-dimensional Brownian motion\nandrotationally symmetric relativistic $\\alpha$-stable processes on\n$\\mathbb{R}^d$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we introduce an algorithm of construction of cyclic\nspace-filling curves. One particular construction provides a family of\nspace-filling curves in all dimensions (H-curves). They are compared here with\nthe Hilbert curve in the sense of clustering properties, and it turns out that\nthe constructed curve is very close and sometimes a bit better than the Hilbert\ncurve. At the same time, its construction is more simple and evaluation is\nsignificantly faster.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this note, we express explicitly the Dunkl kernel and generalized Bessel\nfunctions of type $A_{n-1}$ by the Humbert's function $\\Phi_{2}^{(n)}$, with\none variable specified. The obtained formulas lead to a new proof of Xu's\nintegral expression for the intertwining operator associated to symmetric\ngroups, which was recently reported in [21].\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum systems are promising candidates for sensing of weak signals as they\ncan provide unrivaled performance when estimating parameters of external\nfields. However, when trying to detect weak signals that are hidden by\nbackground noise, the signal-to-noise-ratio is a more relevant metric than raw\nsensitivity. We identify, under modest assumptions about the statistical\nproperties of the signal and noise, the optimal quantum control to detect an\nexternal signal in the presence of background noise using a quantum sensor.\nInterestingly, for white background noise, the optimal solution is the simple\nand well-known spin-locking control scheme. We further generalize, using\nnumerical techniques, these results to the background noise being a correlated\nLorentzian spectrum. We show that for increasing correlation time, pulse based\nsequences such as CPMG are also close to the optimal control for detecting the\nsignal, with the crossover dependent on the signal frequency. These results\nshow that an optimal detection scheme can be easily implemented in near-term\nquantum sensors without the need for complicated pulse shaping.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  MoleMOD is a heterogeneous self-reconfigurable modular robotic system to be\nemployed in architecture and civil engineering. In this paper we present two\ncomponents of the MoleMOD infrastructure - a test environment and a planning\nalgorithm. The test environment for simulation and visualization of active\nparts as well as passive blocks of MoleMOD is based on Gazebo - a powerful\ngeneral-purpose robotic simulator. The key effort has been put into preparation\nof realistic models of passive and active components taking into account their\nphysical characteristics. Moreover, given a starting configuration of the\nMoleMOD system and a final configuration an approach to plan collision-free\ntrajectories for a fleet of active parts is introduced.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Game recommendation is an important application of recommender systems.\nRecommendations are made possible by data sets of historical player and game\ninteractions, and sometimes the data sets include features that describe games\nor players. Collaborative filtering has been found to be the most accurate\npredictor of past interactions. However, it can only be applied to predict new\ninteractions for those games and players where a significant number of past\ninteractions are present. In other words, predictions for completely new games\nand players is not possible. In this paper, we use a survey data set of game\nlikes to present content based interaction models that generalize into new\ngames, new players, and both new games and players simultaneously. We find that\nthe models outperform collaborative filtering in these tasks, which makes them\nuseful for real world game recommendation. The content models also provide\ninterpretations of why certain games are liked by certain players for game\nanalytics purposes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Halogen bond (X-bond) is a noncovalent interaction between a halogen atom and\nan electron donor. It is often rationalized by a region of the positive\nelectrostatic potential on the halogen atom, so-called $\\sigma$-hole. The\nX-bond strength increases with the atomic number of the halogen involved, thus\nfor heavier halogens, relativistic effects become of concern. This poses a\nchallenge for the quantum chemical description of X-bonded complexes. To\nquantify scalar relativistic effects (SREs) on the interaction energies and\n$\\sigma$-hole properties, we have performed highly accurate coupled-cluster\ncalculations at the complete basis set limit of several X-bonded complexes and\ntheir halogenated monomers. The SREs turned to be comparable in magnitude to\nthe effect of basis set. The nonrelativistic calculations typically\nunderestimate the attraction by up to 5% or 23% for brominated and iodinated\ncomplexes, respectively. Counter-intuitively, the electron densities at the\nbond critical points are larger for SRE-free calculations than for the\nrelativistic ones. SREs yield smaller, flatter, and more positive\n$\\sigma$-holes. Finally, we highlight the importance of diffuse functions in\nthe basis sets and provide quantitative arguments for using basis sets with\npseudopotentials as an affordable alternative to a more rigorous\nDouglas-Kroll-Hess relativistic theory.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recurrent Neural Networks (RNNs) are among the most successful machine\nlearning models for sequence modelling, but tend to suffer from an exponential\nincrease in the number of parameters when dealing with large multidimensional\ndata. To this end, we develop a multi-linear graph filter framework for\napproximating the modelling of hidden states in RNNs, which is embedded in a\ntensor network architecture to improve modelling power and reduce parameter\ncomplexity, resulting in a novel Recurrent Graph Tensor Network (RGTN). The\nproposed framework is validated through several multi-way sequence modelling\ntasks and benchmarked against traditional RNNs. By virtue of the domain aware\ninformation processing of graph filters and the expressive power of tensor\nnetworks, we show that the proposed RGTN is capable of not only out-performing\nstandard RNNs, but also mitigating the Curse of Dimensionality associated with\ntraditional RNNs, demonstrating superior properties in terms of performance and\ncomplexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study smooth maps that arise in derived algebraic geometry. Given a map $A\n\\to B$ between non-positive commutative noetherian DG-rings which is of flat\ndimension $0$, we show that it is smooth in the sense of To\\\"{e}n-Vezzosi if\nand only if it is homologically smooth in the sense of Kontsevich. We then show\nthat $B$, being a perfect DG-module over $B\\otimes^{\\mathrm{L}}_A B$ has,\nlocally, an explicit semi-free resolution as a Koszul complex. As an\napplication we show that a strong form of Van den Bergh duality between\n(derived) Hochschild homology and cohomology holds in this setting.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper gives applications of the enclosure method introduced by the\nauthor to typical inverse obstacle and crack scattering problems in two\ndimensions. Explicit extraction formulae of the convex hull of unknown\npolygonal sound-hard obstacles and piecewise linear cracks from the far field\npattern of the scattered field at a fixed wave number and at most two incident\ndirections are given. The main new points of this paper are: a combination of\nthe enclosure method and the Herglotz wave function; explicit construction of\nthe density in the Herglotz wave function by using the idea of the Vekua\ntransform. By virtue of the construction, one can avoid any restriction on the\nwave number in the extraction formulae. An attempt for the case when the far\nfield pattern is given on limited angles is also given.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a modeling framework---mathematical model and\ncomputational framework---to study the response of a plastic material due to\nthe presence and transport of a chemical species in the host material. Such a\nmodeling framework is important to a wide variety of problems ranging from\nLi-ion batteries, moisture diffusion in cementitious materials, hydrogen\ndiffusion in metals, to consolidation of soils under severe loading-unloading\nregimes. The mathematical model incorporates experimental observations reported\nin the literature on how (elastic and plastic) material properties change\nbecause of the presence and transport of a chemical species. Also, the model\naccounts for one-way (transport affects the deformation but not vice versa) and\ntwo-way couplings between deformation and transport subproblems. The resulting\ncoupled equations are not amenable to analytical solutions; so, we present a\nrobust computational framework for obtaining numerical solutions. Given that\npopular numerical formulations do not produce nonnegative solutions, the\ncomputational framework uses an optimized-based nonnegative formulation that\nrespects physical constraints (e.g., nonnegative concentrations). For\ncompleteness, we will also show the effect and propagation of the negative\nconcentrations, often produced by contemporary transport solvers, into the\noverall predictions of deformation and concentration fields. Notably,\nanisotropy of the diffusion process exacerbates these unphysical violations.\nUsing representative numerical examples, we will discuss how the concentration\nfield affects plastic deformations of a degrading solid. Based on these\nnumerical examples, we also discuss how plastic zones spread because of\nmaterial degradation. To illustrate how the proposed computational framework\nperforms, we report various performance metrics such as optimization iterations\nand time-to-solution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper introduces a novel statistical regression framework that allows\nthe incorporation of consistency constraints. A linear and nonlinear\n(kernel-based) formulation are introduced, and both imply closed-form\nanalytical solutions. The models exploit all the information from a set of\ndrivers while being maximally independent of a set of auxiliary, protected\nvariables. We successfully illustrate the performance in the estimation of\nchlorophyll content.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Transport of intracellular cargo is often mediated by teams of molecular\nmotors that function in a chaotic environment and varying conditions. We show\nthat the motors have unique steady state behavior which enables transport\nmodalities that are robust. Under reduced ATP concentrations, multi-motor\nconfigurations are preferred over single motors. Higher load force drives\nmotors to cluster, but very high loads compel them to separate in a manner that\npromotes immediate cargo movement once the load reduces. These inferences,\nbacked by analytical guarantees, provide unique insights into the coordination\nstrategies adopted by motors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze the performance of a reduced-order simulation of geometric\nmeta-materials based on zigzag patterns using a simplified representation. As\ngeometric meta-materials we denote planar cellular structures which can be\nfabricated in 2d and bent elastically such that they approximate doubly-curved\n2-manifold surfaces in 3d space. They obtain their elasticity attributes mainly\nfrom the geometry of their cellular elements and their connections. In this\npaper we focus on cells build from so-called zigzag springs. The physical\nproperties of the base material (i.e., the physical substance) influence the\nbehavior as well, but we essentially factor them out by keeping them constant.\n  The simulation of such complex geometric structures comes with a high\ncomputational cost, thus we propose an approach to reduce it by abstracting the\nzigzag cells by a simpler model and by learning the properties of their elastic\ndeformation behavior. In particular, we analyze the influence of the sampling\nof the full parameter space and the expressiveness of the reduced model\ncompared to the full model. Based on these observations, we draw conclusions on\nhow to simulate such complex meso-structures with simpler models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Distributed machine learning algorithms play a significant role in processing\nmassive data sets over large networks. However, the increasing reliance on\nmachine learning on information and communication technologies (ICTs) makes it\ninherently vulnerable to cyber threats. This work aims to develop secure\ndistributed algorithms to protect the learning from data poisoning and network\nattacks. We establish a game-theoretic framework to capture the conflicting\ngoals of a learner who uses distributed support vector machines (SVMs) and an\nattacker who is capable of modifying training data and labels. We develop a\nfully distributed and iterative algorithm to capture real-time reactions of the\nlearner at each node to adversarial behaviors. The numerical results show that\ndistributed SVM is prone to fail in different types of attacks, and their\nimpact has a strong dependence on the network structure and attack\ncapabilities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A Non-terrestrial Network (NTN) comprising Low Earth Orbit (LEO) satellites\ncan enable connectivity to underserved areas, thus complementing existing\ntelecom networks. The high-speed satellite motion poses several challenges at\nthe physical layer such as large Doppler frequency shifts. In this paper, an\nanalytical framework is developed for statistical characterization of Doppler\nshift in an NTN where LEO satellites provide communication services to\nterrestrial users. Using tools from stochastic geometry, the users within a\ncell are grouped into disjoint clusters to limit the differential Doppler\nacross users. Under some simplifying assumptions, the cumulative distribution\nfunction (CDF) and the probability density function are derived for the Doppler\nshift magnitude at a random user within a cluster. The CDFs are also provided\nfor the minimum and the maximum Doppler shift magnitude within a cluster.\nLeveraging the analytical results, the interplay between key system parameters\nsuch as the cluster size and satellite altitude is examined. Numerical results\nvalidate the insights obtained from the analysis.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The fluctuations of the magnetic order parameter, or longitudinal spin\nexcitations, are investigated theoretically in the ferromagnetic Fe and Ni as\nwell as in the antiferromagnetic phase of the pnictide superconductor FeSe. The\ncharge and spin dynamics of these systems is described by evaluating the\ngeneralized charge and spin density response function calculated from\nfirst-principles linear response time dependent density functional theory\nwithin adiabatic local spin density approximation. We observe that the formally\nnon-interacting Kohn-Sham system features strong coupling between the\nmagnetization and charge dynamics in the longitudinal channel and that the\ncoupling is effectively removed upon the inclusion of the Coulomb interaction\nin the charge channel and the resulting appearance of plasmons. The\nlongitudinal spin fluctuations acquire a collective character without the\nemergence of the Goldstone boson, similar to the case of paramagnon excitations\nin non-magnetic metals like Pd. In ferromagnetic Fe and Ni the longitudinal\nspin dynamics is governed by interactions between low-energy intraband\nelectron-hole pairs while in quasi two dimensional antiferromagnet FeSe it is\ndominated by the interband transitions with energies of the order of exchange\nsplitting. In the later material, the collective longitudinal magnetization\nfluctuations feature well defined energies and long life times for small\nmomenta and appear below the particle-hole continuum. The modes become strongly\nLandau-damped for growing wave-vectors. We relate our theoretical findings to\nexisting experimental spin-polarized electron energy loss spectroscopy results.\nIn bulk bcc Fe, the longitudinal magnetic modes appear above the typical\nenergies of transverse spin-waves, have energies comparable with the Stoner\nspin-flip excitation continuum, and are order of magnitude less energetic than\nthe charge dynamics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A probabilistic performance-oriented controller design approach based on\npolynomial chaos expansion and optimization is proposed for flight dynamic\nsystems. Unlike robust control techniques where uncertainties are\nconservatively handled, the proposed method aims at propagating uncertainties\neffectively and optimizing control parameters to satisfy the probabilistic\nrequirements directly. To achieve this, the sensitivities of violation\nprobabilities are evaluated by the expansion coefficients and the fourth moment\nmethod for reliability analysis, after which an optimization that minimizes\nfailure probability under chance constraints is conducted. Afterward, a\ntime-dependent polynomial chaos expansion is performed to validate the results.\nWith this approach, the failure probability is reduced while guaranteeing the\nclosed-loop performance, thus increasing the safety margin. Simulations are\ncarried out on a longitudinal model subject to uncertain parameters to\ndemonstrate the effectiveness of this approach.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Regularity properties of the pressure are related to phase transitions. In\nthis article we study thermodynamic formalism for systems defined in\nnon-compact phase spaces, our main focus being countable Markov shifts. We\nproduce metric compactifications of the space which allow us to prove that the\npressure is differentiable on a residual set and outside an Aronszajn null set\nin the space of uniformly continuous functions. We establish a criterion, the\nso called sectorially arranged property, which implies that the pressure in the\noriginal system and in the compactification coincide. Examples showing that the\ncompactifications can have rich boundaries, for example a Cantor set, are\nprovided.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hyperspectral image (HSI) has some advantages over natural image for various\napplications due to the extra spectral information. During the acquisition, it\nis often contaminated by severe noises including Gaussian noise, impulse noise,\ndeadlines, and stripes. The image quality degeneration would badly effect some\napplications. In this paper, we present a HSI restoration method named smooth\nand robust low rank tensor recovery. Specifically, we propose a structural\ntensor decomposition in accordance with the linear spectral mixture model of\nHSI. It decomposes a tensor into sums of outer matrix vector products, where\nthe vectors are orthogonal due to the independence of endmember spectrums.\nBased on it, the global low rank tensor structure can be well exposited for HSI\ndenoising. In addition, the 3D anisotropic total variation is used for spatial\nspectral piecewise smoothness of HSI. Meanwhile, the sparse noise including\nimpulse noise, deadlines and stripes, is detected by the l1 norm\nregularization. The Frobenius norm is used for the heavy Gaussian noise in some\nreal world scenarios. The alternating direction method of multipliers is\nadopted to solve the proposed optimization model, which simultaneously exploits\nthe global low rank property and the spatial spectral smoothness of the HSI.\nNumerical experiments on both simulated and real data illustrate the\nsuperiority of the proposed method in comparison with the existing ones.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyse the functional derivative of the cosmic-shear power spectrum\n$C_\\ell^\\gamma$ with respect to the cosmic expansion function. Our interest in\ndoing so is two-fold: (i) In view of attempts to detect minor changes of the\ncosmic expansion function which may be due to a possibly time-dependent\ndark-energy density, we wish to know how sensitive the weak-lensing power\nspectrum is to changes in the expansion function. (ii) In view of recent\nempirical determinations of the cosmic expansion function from distance\nmeasurements, independent of specific cosmological models, we wish to find out\nhow uncertainties in the expansion function translate to uncertainties in the\ncosmic-shear power spectrum. We find the following answers: Relative changes of\nthe expansion function are amplified by the cosmic-shear power spectrum by a\nfactor $\\approx 4-6$, weakly depending on the scale factor where the change is\napplied, and the current uncertainty of one example for an empirically\ndetermined expansion function translates to a relative uncertainty of the\ncosmic-shear power spectrum of $\\approx15\\,\\%$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose two optimization-based heuristics for structure selection and\nidentification of PieceWise Affine (PWA) models with exogenous inputs. The\nfirst method determines the number of affine sub-models assuming known model\norder of the sub-models, while the second approach estimates the model order\nfor a given number of affine sub-models. Both approaches rely on the use of\nregularization-based shrinking strategies, that are exploited within a\ncoordinate-descent algorithm. This allows us to estimate the structure of the\nPWA models along with its model parameters. Starting from an over-parameterized\nmodel, the key idea is to alternate between an identification step and\nstructure refinement, based on the sparse estimates of the model parameters.\nThe performance of the presented strategies is assessed over two benchmark\nexamples.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quasiperiodicity has long been known to be a potential platform to explore\nexotic phenomena, realizing an intricate middle point between ordered solids\nand disordered matter. In particular, quasiperiodic structures are promising\nplaygrounds to engineer critical wavefunctions, a powerful starting point to\nengineer exotic correlated states. Here we show that systems hosting a\nquasiperiodic modulation of antiferromagnetism and spin-singlet\nsuperconductivity, as realized by atomic chains in twisted van der Waals\nmaterials, host a localization-delocalization transition as a function of the\ncoupling strength. Associated with this transition, we demonstrate the\nemergence of a robust quasiperiodic critical point for arbitrary incommensurate\npotentials, that appears for generic relative weights of the spin-singlet\nsuperconductivity and antiferromagnetism. We show that the inclusion of\nresidual electronic interactions leads to an emergent spin-triplet\nsuperconducting state, that gets dramatically enhanced at the vicinity of the\nquasiperiodic critical point. Our results put forward quasiperiodicity as a\npowerful knob to engineer robust superconducting states, providing an\nalternative pathway towards artificially designed unconventional\nsuperconductors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we define an analytical index for continuous families of\nFredholm operators parameterized by a topological space $\\mathbb{X}$ into a\nBanach space $X.$ We also consider the Weyl spectrum for continuous families of\nbounded linear operators and we study its continuity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  One of the goals of machine learning is to eliminate tedious and arduous\nrepetitive work. The manual and semi-automatic classification of millions of\nhours of solar wind data from multiple missions can be replaced by automatic\nalgorithms that can discover, in mountains of multi-dimensional data, the real\ndifferences in the solar wind properties. In this paper we present how\nunsupervised clustering techniques can be used to segregate different types of\nsolar wind. We propose the use of advanced data reduction methods to\npre-process the data, and we introduce the use of Self-Organizing Maps to\nvisualize and interpret 14 years of ACE data. Finally, we show how these\ntechniques can potentially be used to uncover hidden information, and how they\ncompare with previous manual and automatic categorizations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove the nonexistence of two-dimensional solitary gravity water waves\nwith subcritical wave speeds and an arbitrary distribution of vorticity. This\nis a longstanding open problem, and even in the irrotational case there are\nonly partial results relying on sign conditions or smallness assumptions. As a\ncorollary, we obtain a relatively complete classification of solitary waves:\nthey must be supercritical, symmetric, and monotonically decreasing on either\nside of a central crest. The proof introduces a new function which is related\nto the so-called flow force and has several surprising properties. In addition\nto solitary waves, our nonexistence result applies to \"half-solitary\" waves\n(e.g. bores) which decay in only one direction.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Motivated by recent experimental results on GdRu$_2$Si$_2$ [Khanh, N.D.,\nNakajima, T., Yu, X. et al., \\emph{Nat. Nanotechnol.} \\textbf{15}, 444-449\n(2020)], where nanometric square skyrmion lattice was observed, we propose\nsimple analytical mean-field description of the high-temperature part of the\nphase diagram of centrosymmetric tetragonal frustrated antiferromagnets with\ndipolar interaction in the external magnetic field. In the reciprocal space\ndipolar forces provide momentum dependent biaxial anisotropy. It is shown that\nin tetragonal lattice in the large part of the Brillouin zone for mutually\nperpendicular modulation vectors in the $ab$ plane this anisotropy has mutually\nperpendicular easy axes and collinear middle axes, what leads to double-Q\nmodulated spin structure stabilization. The latter turns out to be a square\nskyrmion lattice in the large part of its stability region with the topological\ncharge $\\pm 1$ per magnetic unit cell, which is determined by the frustrated\nexchange coupling, and, thus, nanometer-sized. In the presence of additional\nsingle-ion easy-axis anisotropy, easy and middle axes can be swapped, which\nleads to different phase diagram. It is argued that the latter case is relevant\nto GdRu$_2$Si$_2$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we consider the problem of compressing a trie while supporting\nthe powerful \\emph{locate} queries: to return the pre-order identifiers of all\nnodes reached by a path labeled with a given query pattern. Our result builds\non top of the XBWT tree transform of Ferragina et al. [FOCS 2005] and\ngeneralizes the \\emph{r-index} locate machinery of Gagie et al. [SODA 2018,\nJACM 2020] based on the run-length encoded Burrows-Wheeler transform (BWT). Our\nfirst contribution is to propose a suitable generalization of the run-length\nBWT to tries. We show that this natural generalization enjoys several of the\nuseful properties of its counterpart on strings: in particular, the transform\nnatively supports counting occurrences of a query pattern on the trie's paths\nand its size $r$ captures the trie's repetitiveness and lower-bounds a natural\nnotion of trie entropy. Our main contribution is a much deeper insight into the\ncombinatorial structure of this object. In detail, we show that a data\nstructure of $O(r\\log n) + 2n + o(n)$ bits, where $n$ is the number of nodes,\nallows locating the $occ$ occurrences of a pattern of length $m$ in\nnearly-optimal $O(m\\log\\sigma + occ)$ time, where $\\sigma$ is the alphabet's\nsize. Our solution consists in sampling $O(r)$ nodes that can be used as\n\"anchor points\" during the locate process. Once obtained the pre-order\nidentifier of the first pattern occurrence (in co-lexicographic order), we show\nthat a constant number of constant-time jumps between those anchor points lead\nto the identifier of the next pattern occurrence, thus enabling locating in\noptimal $O(1)$ time per occurrence.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In previous work, using a process we call meshing, the reachable state spaces\nfor various continuous and hybrid systems were approximated as a discrete set\nof states which can then be synthesized into a Markov chain. One of the\napplications for this approach has been to analyze locomotion policies obtained\nby reinforcement learning, in a step towards making empirical guarantees about\nthe stability properties of the resulting system. In a separate line of\nresearch, we introduced a modified reward function for on-policy reinforcement\nlearning algorithms that utilizes a \"fractal dimension\" of rollout\ntrajectories. This reward was shown to encourage policies that induce\nindividual trajectories which can be more compactly represented as a discrete\nmesh. In this work we combine these two threads of research by building meshes\nof the reachable state space of a system subject to disturbances and controlled\nby policies obtained with the modified reward. Our analysis shows that the\nmodified policies do produce much smaller reachable meshes. This shows that\nagents trained with the fractal dimension reward transfer their desirable\nquality of having a more compact state space to a setting with external\ndisturbances. The results also suggest that the previous work using mesh based\ntools to analyze RL policies may be extended to higher dimensional systems or\nto higher resolution meshes than would have otherwise been possible.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the recent past, there has been a growing interest in Neural-Symbolic\nIntegration frameworks, i.e., hybrid systems that integrate connectionist and\nsymbolic approaches to obtain the best of both worlds. In a previous work, we\nproposed KENN (Knowledge Enhanced Neural Networks), a Neural-Symbolic\narchitecture that injects prior logical knowledge into a neural network by\nadding a new final layer which modifies the initial predictions accordingly to\nthe knowledge. Among the advantages of this strategy, there is the inclusion of\nclause weights, learnable parameters that represent the strength of the\nclauses, meaning that the model can learn the impact of each clause on the\nfinal predictions. As a special case, if the training data contradicts a\nconstraint, KENN learns to ignore it, making the system robust to the presence\nof wrong knowledge. In this paper, we propose an extension of KENN for\nrelational data. To evaluate this new extension, we tested it with different\nlearning configurations on Citeseer, a standard dataset for Collective\nClassification. The results show that KENN is capable of increasing the\nperformances of the underlying neural network even in the presence relational\ndata, outperforming other two notable methods that combine learning with logic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object\nfrom the correspondences established between monocular 2D images. Current NRSfM\nmethods lack statistical robustness, which is the ability to cope with\ncorrespondence errors.This prevents one to use automatically established\ncorrespondences, which are prone to errors, thereby strongly limiting the scope\nof NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by\nexploiting isometry. Step 1 computes the optical flow from correspondences,\nstep 2 reconstructs each 3D point's normal vector using multiple reference\nimages and integrates them to form surfaces with the best reference and step 3\nrejects the 3D points that break isometry in their local neighborhood.\nImportantly, each step is designed to discard or flag erroneous\ncorrespondences. Our contributions include the robustification of optical flow\nby warp estimation, new fast analytic solutions to local normal reconstruction\nand their robustification, and a new scale-independent measure of 3D local\nisometric coherence. Experimental results show that our robust NRSfM method\nconsistently outperforms existing methods on both synthetic and real datasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Electron cloud effects have become one of the main performance limitations\nfor circular particle accelerators operating with positively-charged beams.\nAmong other machines worldwide, the CERN Super Proton Synchrotron (SPS), as\nwell as the Large Hadron Collider (LHC) are affected by these phenomena.\nIntense efforts have been devoted in recent years to improve the understanding\nof electron cloud (EC) generation with the aim of finding efficient mitigation\nmeasures. In a different domain of accelerator physics, non-linear resonances\nin the transverse phase space have been proposed as novel means of manipulating\ncharged particle beams. While the original goal was to perform multi-turn\nextraction from the CERN Proton Synchrotron (PS), several other applications\nhave been proposed. In this paper, the study of EC generation in the presence\nof charged particle beams with multimode horizontal distribution is presented.\nSuch a peculiar distribution can be generated by different approaches, one of\nwhich consists in splitting the initial Gaussian beam distribution by crossing\na non-linear resonance. In this paper, the outcome of detailed numerical\nsimulations is presented and discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the structure and stability of traversable wormholes built as\n(spherically symmetric) thin shells in the context of Palatini $f(\\mathcal{R})$\ngravity. Using a suitable junction formalism for these theories we find that\nthe effective number of degrees of freedom on the shell is reduced to a single\none, which fixes the equation of state to be that of massless stress-energy\nfields, contrary to the general relativistic and metric $f(R)$ cases. Another\nmajor difference is that the surface energy density threading the thin-shell,\nneeded in order to sustain the wormhole, can take any sign, and may even\nvanish, depending on the desired features of the corresponding solutions. We\nillustrate our results by constructing thin-shell wormholes by surgically\ngrafting Schwarzschild space-times, and show that these configurations are\nalways linearly unstable. However, surgically joined Reissner-Nordstr\\\"om\nspace-times allow for linearly stable, traversable thin-shell wormholes\nsupported by a positive energy density provided that the (squared)\nmass-to-charge ratio, given by $y=Q^2/M^2$, satisfies the constraint $1<y<9/8$\n(corresponding to overcharged Reissner-Nordstr\\\"om configurations having a\nphoton sphere) and lies in a region bounded by specific curves defined in terms\nof the (dimensionless) radius of the shell $x_0=R/M$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gaseous flows show a diverse set of behaviors on different characteristic\nscales. Given the coarse-grained modeling in theories of fluids, considerable\nuncertainties may exist between the flow-field solutions and the real physics.\nTo study the emergence, propagation and evolution of uncertainties from\nmolecular to hydrodynamic level poses great opportunities and challenges to\ndevelop both sound theories and reliable multi-scale numerical algorithms. In\nthis paper, a new stochastic kinetic scheme will be developed that includes\nuncertainties via a hybridization of stochastic Galerkin and collocation\nmethods. Based on the Boltzmann-BGK model equation, a scale-dependent evolving\nsolution is employed in the scheme to construct governing equations in the\ndiscretized temporal-spatial domain. Therefore typical flow physics can be\nrecovered with respect to different physical characteristic scales and\nnumerical resolutions in a self-adaptive manner. We prove that the scheme is\nformally asymptotic-preserving in different flow regimes with the inclusion of\nrandom variables, so that it can be used for the study of multi-scale\nnon-equilibrium gas dynamics under the effect of uncertainties.\n  Several numerical experiments are shown to validate the scheme. We make new\nphysical observations, such as the wave-propagation patterns of uncertainties\nfrom continuum to rarefied regimes. These phenomena will be presented and\nanalyzed quantitatively. The current method provides a novel tool to quantify\nthe uncertainties within multi-scale flow evolutions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the SIR model and study the first time the number of infected\nindividuals begins to decrease and the first time this population is below a\ngiven threshold. We interpret these times as functions of the initial\nsusceptible and infected populations and characterize them as solutions of a\ncertain partial differential equation. This allows us to obtain integral\nrepresentations of these times and in turn to estimate them precisely for large\npopulations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With the fact that the knowledge in each field in university is keeping\nincreasing, the number of university courses is becoming larger, and the\ncontent and curriculum system is becoming much more complicated than it used to\nbe, which bring many inconveniences to the course arrangement and analysis. In\nthis paper, we aim to construct a method to visualize all courses based on\nGoogle Knowledge Graph. By analysing the properties of the courses and their\npreceding requirements, we want to extract the relationship between the\nprecursors and the successors, so as to build the knowledge graph of the\ncurriculum system. Using the graph database Neo4j [7] as the core aspect for\ndata storage and display for our new curriculum system will be our approach to\nimplement our knowledge graph. Based on this graph, the venation relationship\nbetween courses can be clearly analysed, and some difficult information can be\nobtained, which can help to combine the outline of courses and the need to\nquickly query the venation information of courses.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove an invariance principle for continuous-time random walks in a\ndynamically averaging environment on $\\mathbb Z$. In the beginning, the\nconductances may fluctuate substantially, but we assume that as time proceeds,\nthe fluctuations decrease according to a typical diffusive scaling and\neventually approach constant unit conductances. The proof relies on a coupling\nwith the standard continuous time simple random walk.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  For the polarization of neutrons with an energy level of\n\\textcolor{black}{$>0.1$ eV}, we developed a novel polarized proton spin filter\nbased on dynamic nuclear polarization using photo-excited triplet electron\nspins. The spin filter consists of a single crystal of naphthalene doped with\ndeuterated pentacene and has a size of $\\phi15\\times4$ ${\\rm mm}^3$, allowing\nit to cover a wide beam diameter. It was operated in 0.35 T and at 90 K. We\nsucceeded in polarizing neutrons in the energy range $0.1-10$ eV using a RIKEN\naccelerator-driven compact neutron source. The averaged values of the proton\nand neutron polarization were $0.250\\pm0.050$ and $0.076\\pm0.015$,\nrespectively.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper is concerned with the Rayleigh-Taylor instability for the\nnonhomogeneous incompressible Navier-Stokes equations with Navier-slip boundary\nconditions around a steady-state in an infinite slab, where the Navier-slip\ncoefficients do not have defined sign and the slab is horizontally periodic.\nMotivated by [18], we extend the result from Dirichlet boundary condition to\nNavier-slip boundary conditions. Our results indicate the factor that \"heavier\ndensity with increasing height\" still plays a key role in the instability under\nNavier-slip boundary conditions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Behavioral gender differences are known to exist for a wide range of human\nactivities including the way people communicate, move, provision themselves, or\norganize leisure activities. Using mobile phone data from 1.2 million devices\nin Austria (15% of the population) across the first phase of the COVID-19\ncrisis, we quantify gender-specific patterns of communication intensity,\nmobility, and circadian rhythms. We show the resilience of behavioral patterns\nwith respect to the shock imposed by a strict nation-wide lock-down that\nAustria experienced in the beginning of the crisis with severe implications on\npublic and private life. We find drastic differences in gender-specific\nresponses during the different phases of the pandemic. After the lock-down\ngender differences in mobility and communication patterns increased massively,\nwhile sleeping patterns and circadian rhythms tend to synchronize. In\nparticular, women had fewer but longer phone calls than men during the\nlock-down. Mobility declined massively for both genders, however, women tend to\nrestrict their movement stronger than men. Women showed a stronger tendency to\navoid shopping centers and more men frequented recreational areas. After the\nlock-down, males returned back to normal quicker than women; young age-cohorts\nreturn much quicker. Differences are driven by the young and adolescent\npopulation. An age stratification highlights the role of retirement on\nbehavioral differences. We find that the length of a day of men and women is\nreduced by one hour. We discuss the findings in the light of gender-specific\ncoping strategies in response to stress and crisis.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a phenomenon which sometimes occurs in tetravalent\nbipartite locally dart-transitive graphs, called a Base Graph -- Connection\nGraph dissection. In this dissection, each white vertex is split into two\nvertices of valence 2 so that the connected components of the result are\nisomorphic. Given the Base Graph whose subdivision is isomorphic to each\ncomponent, and the Connection Graph, which describes how the components\noverlap, we can, in some cases, provide a construction which can make a graph\nhaving such a decomposition. This paper investigates the general phenomenon as\nwell as the special cases in which the connection graph has no more than one\nedge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This book chapter reviews progress in crystallographic image processing (CIP)\nfor scanning probe microscopy (SPM) that has occurred since our description of\nthe technique was first put into open access in this book series in the year\n2010. The signal to noise ratio in all kinds of experimental images of more or\nless regular 2D periodic arrays is significantly enhanced by CIP and the\ntechnique is independent of the type of recording device. In the SPM imaging\ncontext, CIP can be understood as an a posteriori sharpening of the effective\nexperimental scanning probe tip by computational means. It is now possible to\nremove multiple scanning probe mini-tip effects in images from 2D periodic\narrays of physical objects that either self-assembled or were created\nartificially. Accepted within the scientific community is by now also the fact\nthat SPM tips can change their shape and fine structure during the operation of\na microscope and, thereby, obfuscate the recorded images in systematic ways.\nCIP restores much of the smeared out information in such images. The adaptation\nof a geometric Akaike Information Criterion from the robotics and computer\nvision community to the unambiguous detection of 2D translation symmetries\nenabled much of our recent progress. In the main body of this book chapter, we\ndiscuss this adaptation and briefly illustrate its utility on an example.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is widely known that very small datasets produce overfitting in Deep\nNeural Networks (DNNs), i.e., the network becomes highly biased to the data it\nhas been trained on. This issue is often alleviated using transfer learning,\nregularization techniques and/or data augmentation. This work presents a new\napproach, independent but complementary to the previous mentioned techniques,\nfor improving the generalization of DNNs on very small datasets in which the\ninvolved classes share many visual features. The proposed methodology, called\nFuCiTNet (Fusion Class inherent Transformations Network), inspired by GANs,\ncreates as many generators as classes in the problem. Each generator, $k$,\nlearns the transformations that bring the input image into the k-class domain.\nWe introduce a classification loss in the generators to drive the leaning of\nspecific k-class transformations. Our experiments demonstrate that the proposed\ntransformations improve the generalization of the classification model in three\ndiverse datasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we propose an eco-friendly optimization of banana or plantain\nyield by the control of the pest burrowing nematode \\textit{Radopholus\nsimilis}. This control relies on fallow deployment, with greater respect for\nthe environment than chemical methods. The optimization is based on a\nmulti-seasonal model in which fallow periods follow cropping seasons. The aim\nis to find the best way, in terms of profit, to allocate the durations of\nfallow periods between the cropping seasons, over a fixed time horizon spanning\nseveral seasons. The existence of an optimal allocation is proven and an\nadaptive random search algorithm is proposed to solve the optimization problem.\nFor a relatively long time horizon, deploying one season less than the maximum\npossible number of cropping seasons allows to increase the fallow period\ndurations and results in a better multi-seasonal profit. For regular fallow\ndurations, the profit is lower than the optimal solution, but the final soil\ninfestation is also lower.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper concerns the universal approximation property with neural networks\nin variable Lebesgue spaces. We show that, whenever the exponent function of\nthe space is bounded, every function can be approximated with shallow neural\nnetworks with any desired accuracy. This result subsequently leads to determine\nthe universality of the approximation depending on the boundedness of the\nexponent function. Furthermore, whenever the exponent is unbounded, we obtain\nsome characterization results for the subspace of functions that can be\napproximated.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Alice and Bob each have half of a pair of entangled qubits. Bob measures his\nhalf and then passes his qubit to a second Bob who measures again and so on.\nThe goal is to maximize the number of Bobs that can have an expected violation\nof the Clauser-Horne-Shimony-Holt (CHSH) Bell inequality with the single Alice.\nThis scenario was introduced in [Phys. Rev. Lett. 114, 250401 (2015)] where the\nauthors mentioned evidence that when the Bobs act independently and with\nunbiased inputs then at most two of them can expect to violate the CHSH\ninequality with Alice. Here we show that, contrary to this evidence,\narbitrarily many independent Bobs can have an expected CHSH violation with the\nsingle Alice. Our proof is constructive and our measurement strategies can be\ngeneralized to work with a larger class of two-qubit states that includes all\npure entangled two-qubit states. Since violation of a Bell inequality is\nnecessary for device-independent tasks, our work represents a step towards an\neventual understanding of the limitations on how much device-independent\nrandomness can be robustly generated from a single pair of qubits.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Purpose: This study proposes an adaptive bandwidth management system which\ncan be explicitly used by educational institutions. The primary goal of the\nsystem is to increase the bandwidth of the users who access more on educational\nwebsites. Through this proposed bandwidth management, the users of the campus\nnetworks is encouraged to utilize the internet for educational purposes.\n  Method: The weblog from a university's pfSense proxy server was utilized and\nundergo Web Usage Mining (WUM) to determine the number of educational and\nnon-educational websites accessed by the users. Certain formulas were used in\nthe computation of the bandwidth which was dynamically assigned to the users. A\nprototyping technique was applied in developing adaptive bandwidth management\nsystem. The prototype was simulated and evaluated by experts in compliance with\nISO/IEC 14598-6 and ISO/IEC 9126-1 standards.\n  Results: This study found that the prototype is capable of adjusting the\nbandwidth of the network users dynamically. The users who browsed more on\neducational websites or contents were assigned with higher bandwidth compared\nto those who are not. Further, the evaluated prototype met the software\nstandards of ISO.\n  Conclusion: The proposed adaptive bandwidth management can contribute to the\ncontinuous development in the area of computer networking, especially in\ndesigning and managing campus networks. It also helps the network\nadministrators or IT managers in allocating bandwidth with minimal effort.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many tasks in computer vision and graphics fall within the framework of\nconditional image synthesis. In recent years, generative adversarial nets\n(GANs) have delivered impressive advances in quality of synthesized images.\nHowever, it remains a challenge to generate both diverse and plausible images\nfor the same input, due to the problem of mode collapse. In this paper, we\ndevelop a new generic multimodal conditional image synthesis method based on\nImplicit Maximum Likelihood Estimation (IMLE) and demonstrate improved\nmultimodal image synthesis performance on two tasks, single image\nsuper-resolution and image synthesis from scene layouts. We make our\nimplementation publicly available.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the context of linear theories of generalized elasticity including those\nfor homogeneous micropolar media, quasicrystals, piezoelectric and\npiezomagnetic media, we explore the concept of null Lagrangians. For obtaining\nthe family of null Lagrangians we employ the sufficient conditions of H. Rund.\nIn some cases a non-zero null Lagrangian is found and the stored energy admits\na split into a null Lagrangian and a remainder. However, the null Lagrangian\nvanishes whenever the relevant elasticity tensor obeys certain symmetry\nconditions which can be construed as an analogue of the Cauchy relations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Schwarzschild black holes in a de Sitter background were studied in terms of\ntheir thermodynamics based on the R\\'enyi statistics. This led to\nthermodynamically stable black hole configurations for some certain range of\nblack hole radii; namely within this range the corresponding black holes have\npositive heat capacity. Moreover, for a certain background temperature there\ncan exist at most three configurations of black hole; one among which is\nthermodynamically stable. These configurations were investigated in terms of\ntheir free energies, resulting in the moderate-sized stable black hole\nconfiguration being the most preferred configuration. Furthermore, a specific\ncondition on the R\\'enyi non-extensive parameter is required if a given hot\nspacetime were to evolve thermally into the moderate-sized stable black hole.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We construct Lagrangian translating solitons by desingularizing the\nintersection points between Lagrangian Grim Reaper cylinders with the same\nphase using special Lagrangian Lawlor necks. The resulting Lagrangian\ntranslating solitons could have arbitrarily many ends and non-contractible\nloops.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In some industrial applications such as fraud detection, the performance of\ncommon supervision techniques may be affected by the poor quality of the\navailable labels : in actual operational use-cases, these labels may be weak in\nquantity, quality or trustworthiness. We propose a benchmark to evaluate the\nnatural robustness of different algorithms taken from various paradigms on\nartificially corrupted datasets, with a focus on noisy labels. This paper\nstudies the intrinsic robustness of some leading classifiers. The algorithms\nunder scrutiny include SVM, logistic regression, random forests, XGBoost,\nKhiops. Furthermore, building on results from recent literature, the study is\nsupplemented with an investigation into the opportunity to enhance some\nalgorithms with symmetric loss functions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A non-equilibrium open-dissipative neural network, such as a coherent Ising\nmachine based on mutually coupled optical parametric oscillators, has been\nproposed and demonstrated as a novel computing machine for hard combinatorial\noptimization problems. However, there are two challenges in the previously\nproposed approach: (1) The machine can be trapped by local minima which\nincreases exponentially with problem size and (2) the machine fails to map a\ntarget Hamiltonian correctly on the loss landscape of a neural network due to\noscillator amplitude heterogeneity. Both of them lead to erroneous solutions\nrather than correct answers. In this paper, we show that it is possible to\novercome these two problems partially but simultaneously by introducing error\ndetection and correction feedback mechanism. The proposed machine achieves\nefficient sampling of degenerate ground states and low-energy excited states\nvia its inherent migration property during a solution search process.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The support vector machines (SVM) is one of the most widely used and\npractical optimization based classification models in machine learning because\nof its interpretability and flexibility to produce high quality results.\nHowever, the big data imposes a certain difficulty to the most sophisticated\nbut relatively slow versions of SVM, namely, the nonlinear SVM. The complexity\nof nonlinear SVM solvers and the number of elements in the kernel matrix\nquadratically increases with the number of samples in training data. Therefore,\nboth runtime and memory requirements are negatively affected. Moreover, the\nparameter fitting has extra kernel parameters to tune, which exacerbate the\nruntime even further. This paper proposes an adaptive multilevel learning\nframework for the nonlinear SVM, which addresses these challenges, improves the\nclassification quality across the refinement process, and leverages\nmulti-threaded parallel processing for better performance. The integration of\nparameter fitting in the hierarchical learning framework and adaptive process\nto stop unnecessary computation significantly reduce the running time while\nincrease the overall performance. The experimental results demonstrate reduced\nvariance on prediction over validation and test data across levels in the\nhierarchy, and significant speedup compared to state-of-the-art nonlinear SVM\nlibraries without a decrease in the classification quality. The code is\naccessible at https://github.com/esadr/amlsvm.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A CMY colour camera differs from its RGB counterpart in that it employs a\nsubtractive colour space of cyan, magenta and yellow. CMY cameras tend to\nperforms better than RGB cameras in low light conditions due to their much\nhigher transmittance. However, conventional CMY colour filter technology made\nof pigments and dyes are limited in performance for the next generation image\nsensors with submicron pixel sizes. These conventional filters are difficult to\nfabricate at nanoscale dimensions as they use their absorption properties to\nsubtract colours. This paper presents a CMOS compatible nanoscale thick CMY\ncolour mosaic made of Al-TiO2-Al nanorods forming an array 0.82 million colour\npixels of 4.4 micron each, arranged in a CMYM pattern. The colour mosaic was\nthen integrated onto a MT9P031 monochrome image sensor to make a CMY camera and\nthe colour imaging demonstrated using a 12 colour Macbeth chart. The developed\ntechnology will have applications in astronomy, low exposure time imaging in\nbiology and photography.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ambient neutrons may cause significant background for underground\nexperiments. Therefore, it is necessary to investigate their flux and energy\nspectrum in order to devise a proper shielding. Here, two sets of altogether\nten moderated $^3$He neutron counters are used for a detailed study of the\nambient neutron background in tunnel IV of the Felsenkeller facility,\nunderground below 45 meters of rock in Dresden/Germany. One of the moderators\nis lined with lead and thus sensitive to neutrons of energies higher than 10\nMeV. For each $^3$He counter-moderator assembly, the energy dependent neutron\nsensitivity was calculated with the FLUKA code. The count rates of the ten\ndetectors were then fitted with the MAXED and GRAVEL packages. As a result,\nboth the neutron energy spectrum from 10$^{-9}$ MeV to 300 MeV and the flux\nintegrated over the same energy range were determined experimentally.\n  The data show that at a given depth, both the flux and the spectrum vary\nsignificantly depending on local conditions. Energy integrated fluxes of $(0.61\n\\pm 0.05)$, $(1.96 \\pm 0.15)$, and $(4.6 \\pm 0.4) \\times 10^{-4}$ cm$^{-2}$\ns$^{-1}$, respectively, are measured for three sites within Felsenkeller tunnel\nIV which have similar muon flux but different shielding wall configurations.\n  The integrated neutron flux data and the obtained spectra for the three sites\nare matched reasonably well by FLUKA Monte Carlo calculations that are based on\nthe known muon flux and composition of the measurement room walls.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum computers have the potential of solving problems more efficiently\nthan classical computers. While first commercial prototypes have become\navailable, the performance of such machines in practical application is still\nsubject to exploration. Quantum computers will not entirely replace classical\nmachines, but serve as accelerators for specific problems. This necessitates\nintegrating quantum computational primitives into existing applications.\n  In this paper, we perform a case study on how to augment existing software\nwith quantum computational primitives for the Boolean satisfiability problem\n(SAT) implemented using a quantum annealer (QA). We discuss relevant quality\nmeasures for quantum components, and show that mathematically equivalent, but\nstructurally different ways of transforming SAT to a QA can lead to substantial\ndifferences regarding these qualities. We argue that engineers need to be aware\nthat (and which) details, although they may be less relevant in traditional\nsoftware engineering, require considerable attention in quantum computing.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The goal of this paper is speaker diarisation of videos collected 'in the\nwild'. We make three key contributions. First, we propose an automatic\naudio-visual diarisation method for YouTube videos. Our method consists of\nactive speaker detection using audio-visual methods and speaker verification\nusing self-enrolled speaker models. Second, we integrate our method into a\nsemi-automatic dataset creation pipeline which significantly reduces the number\nof hours required to annotate videos with diarisation labels. Finally, we use\nthis pipeline to create a large-scale diarisation dataset called VoxConverse,\ncollected from 'in the wild' videos, which we will release publicly to the\nresearch community. Our dataset consists of overlapping speech, a large and\ndiverse speaker pool, and challenging background conditions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Wireless content caching has recently been considered as an efficient way in\nfog radio access networks (FRANs) to alleviate the heavy burden on\ncapacity-limited fronthaul links and reduce delivery latency. In this paper, an\nadvanced minimal delay association policy is proposed to minimize latency while\nguaranteeing spectral efficiency in F-RANs. By utilizing stochastic geometry\nand queueing theory, closed-form expressions of successful delivery\nprobability, average ergodic rate, and average delivery latency are derived,\nwhere both the traditional association policy based on accessing the base\nstation with maximal received power and the proposed minimal delay association\npolicy are concerned. Impacts of key operating parameters on the aforementioned\nperformance metrics are exploited. It is shown that the proposed association\npolicy has a better delivery latency than the traditional association policy.\nIncreasing the cache size of fog-computing based access points (F-APs) can more\nsignificantly reduce average delivery latency, compared with increasing the\ndensity of F-APs. Meanwhile, the latter comes at the expense of decreasing\naverage ergodic rate. This implies the deployment of large cache size at F-APs\nrather than high density of F-APs can promote performance effectively in\nF-RANs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Reaching gigagauss magnetic fields opens new horizons both in atomic and\nplasma physics. At these magnetic field strengths, the electron cyclotron\nenergy $\\hbar\\omega_{c}$ becomes comparable to the atomic binding energy (the\nRydberg), and the cyclotron frequency $\\omega_{c}$ approaches the plasma\nfrequency at solid state densities that significantly modifies optical\nproperties of the target. The generation of such strong quasistatic magnetic\nfields in laboratory remains a challenge. Using supercomputer simulations, we\ndemonstrate how it can be achieved all-optically by irradiating a micro-channel\ntarget by a circularly polarized relativistic femtosecond laser. The laser\npulse drives a strong electron vortex along the channel wall, inducing a\nmegagauss longitudinal magnetic field in the channel by the inverse Faraday\neffect. This seed field is then amplified up to a gigagauss level and\nmaintained on a picosecond time scale via dynamos driven by plasma thermal\nexpansion off the channel walls. Our scheme sets a possible platform for\nproducing long living extreme magnetic fields in laboratories using readily\navailable lasers. The concept might also be relevant for applications such as\nmagneto-inertial fusion.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The problem of mass diffusion in layered systems has relevance to\napplications in different scientific disciplines, e.g., chemistry, material\nscience, soil science, and biomedical engineering. The mathematical challenge\nin these type of model systems is to match the solutions of the time-dependent\ndiffusion equation in each layer, such that the boundary conditions at the\ninterfaces between them are satisfied. As the number of layers increases, the\nsolutions may become increasingly complicated. Here, we describe an alternative\ncomputational approach to multi-layer diffusion problems, which is based on the\ndescription of the overdamped Brownian motion of particles via the underdamped\nLangevin equation. In this approach, the probability distribution function is\ncomputed from the statistics of an ensemble of independent single particle\ntrajectories. To allow for simulations of Langevin dynamics in layered systems,\nthe numerical integrator must be supplemented with algorithms for the\ntransitions across the discontinuous interfaces. Algorithms for three common\ntypes of discontinuities are presented: (i) A discontinuity in the friction\ncoefficient, (ii) a semi-permeable membrane, and (iii) a step-function chemical\npotential. The general case of an interface where all three discontinuities are\npresent (Kedem-Katchalsky boundary) is also discussed. We demonstrate the\nvalidity and accuracy of the derived algorithms by considering a simple\ntwo-layer model system and comparing the Langevin dynamics statistics with\nanalytical solutions and alternative computational results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Storing data on an external server with information-theoretic security, while\nusing a key shorter than the data itself, is impossible. As an alternative, we\npropose a scheme that achieves information-theoretically secure tamper\nevidence: The server is able to obtain information about the stored data, but\nnot while staying undetected. Moreover, the client only needs to remember a key\nwhose length is much shorter than the data.\n  We provide a security proof for our scheme, based on an entropic uncertainty\nrelation, similar to QKD proofs. Our scheme works if Alice is able to\n(reversibly) randomise the message to almost-uniformity with only a short key.\nBy constructing an explicit attack we show that short-key unconditional tamper\nevidence cannot be achieved without this randomisability.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Establishing the role of active galactic nuclei (AGN) during the formation of\ngalaxies remains one of the greatest challenges of galaxy formation theory.\nTowards addressing this, we summarise our recent work investigating: (1) the\nphysical drivers of ionised outflows and (2) observational signatures of the\nimpact by jets/outflows on star formation and molecular gas content in AGN host\ngalaxies. We confirm a connection between radio emission and extreme ionised\ngas kinematics in AGN hosts. Emission-line selected AGN are significantly more\nlikely to exhibit ionised outflows (as traced by the [O III] emission line) if\nthe projected linear extent of the radio emission is confined within the\nspectroscopic aperture. Follow-up high resolution radio observations and\nintegral field spectroscopy of 10 luminous Type 2 AGN reveal moderate power,\nyoung (or frustrated) jets interacting with the interstellar medium. We find\nthat these sources live in highly star forming and gas rich galaxies.\nAdditionally, by combining ALMA-derived dust maps with integral field\nspectroscopy for eight host galaxies of z~2 X-ray AGN, we show that H-alpha\nemission is an unreliable tracer of star formation. For the five targets with\nionised outflows we find no dramatic in-situ shut down of the star formation.\nAcross both of these studies we find that if these AGN do have a negative\nimpact upon their host galaxies, it must be happening on small (unresolved)\nspatial scales and/or an observable galaxy-wide impact has yet to occur.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study learning of indexed families from positive data where a learner can\nfreely choose a hypothesis space (with uniformly decidable membership)\ncomprising at least the languages to be learned. This abstracts a very\nuniversal learning task which can be found in many areas, for example learning\nof (subsets of) regular languages or learning of natural languages. We are\ninterested in various restrictions on learning, such as consistency,\nconservativeness or set-drivenness, exemplifying various natural learning\nrestrictions.\n  Building on previous results from the literature, we provide several maps\n(depictions of all pairwise relations) of various groups of learning criteria,\nincluding a map for monotonicity restrictions and similar criteria and a map\nfor restrictions on data presentation. Furthermore, we consider, for various\nlearning criteria, whether learners can be assumed consistent.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The main goal of this paper is to formulate a constructive analogue of\nAckermann's observation about finite set theory and arithmetic. We will see\nthat Heyting arithmetic is bi-interpretable with $\\mathsf{CZF^{fin}}$, the\nfinitary version of $\\mathsf{CZF}$. We also examine bi-interpretability between\nsubtheories of finitary $\\mathsf{CZF}$ and Heyting arithmetic based on the\nmodification of Fleischmann's hierarchy of formulas, and the set of\nhereditarily finite sets over $\\mathsf{CZF}$, which turns out to be a model of\n$\\mathsf{CZF^{fin}}$ but not a model of finitary $\\mathsf{IZF}$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Using language makes human beings surpass animals in wisdom. To let machines\nunderstand, learn, and use language flexibly, we propose a human-like general\nlanguage processing (HGLP) architecture, which contains sensorimotor,\nassociation, and cognitive systems. The HGLP network learns from easy to hard\nlike a child, understands word meaning by coactivating multimodal neurons,\ncomprehends and generates sentences by real-time constructing a virtual world\nmodel, and can express the whole thinking process verbally. HGLP rapidly\nlearned 10+ different tasks including object recognition, sentence\ncomprehension, imagination, attention control, query, inference, motion\njudgement, mixed arithmetic operation, digit tracing and writing, and\nhuman-like iterative thinking process guided by language. Language in the HGLP\nframework is not matching nor correlation statistics, but a script that can\ndescribe and control the imagination.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Dielectric laser accelerators (DLAs) use the nearfields created when a laser\npulse impinges on a dielectric structure to accelerate charged particles. We\nprovide an overview of the theory of operation of photon driven accelerators,\nfrom photons interacting with charged particles in a vacuum, to the advantages\ngained by introducing dielectric structures, with a discussion of their\nadvantages and limitations. Furthermore we show the state of the art of the\ncurrent development of dielectric laser accelerators, including acceleration,\nfocusing, deflection, beam position monitoring, and advanced topics from the\ngeneration of microbunches to the adaptation of alternating phase focusing,\nallowing for the next to lossless transport of the charged particles over long\ndistances.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this letter, we derive a fully-stabilized narrow-linewidth optical\nfrequency comb (OFC) reference to a kilometer-long fiber delay line for the\nfirst time, to the best of our knowledge. The 1537-nm comb modes and 1566-nm\ncomb modes in the OFC are phase-locked to the fiber delay line with 40-kHz\nlocking bandwidth. From out-of-loop measurement, the 1542-nm comb mode has\nresidual phase noise of 925 mrad (integrated from 10 MHz to 1 kHz), fractional\nfrequency stability of 9.13*10(-13) at 12.8 ms average time and 580 Hz\nlinewidth. The linewidth has been compressed by a factor of ~ 170 compared to\nthe free-running condition. Short-term stability of presented OFC exceeds most\ncommercial microwave oscillators. The entire phase-locking system is compact\nand highly-integrated benefiting from absence of optical amplifiers, f-2f\ninterferometers and optical/radio references. The presented OFC shows\nsignificant potential of being reliable laser source in low-noise-OFC-based\nprecise metrology, microwave generation and dual-comb spectroscopic\napplications outside the laboratory.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Fast radio burst (FRB) discoveries are occurring rapidly, with thousands\nexpected from upcoming surveys. The dispersion measures (DM) observed for FRB\ninclude important information on cosmological distances and the ionization\nstate of the universe from the redshift of emission until today. Rather than\nconsidering the DM--redshift relation, we investigate the statistical ensemble\nof the distribution of dispersion measures. We explore the use of this\nabundance information, with and without redshift information, to probe helium\nreionization. Carrying out Monte Carlo simulations of FRB survey samples, we\nexamine the effect of different source redshift distributions, host galaxy\nmodels, sudden vs gradual reionization, and covariance with cosmological\nparameters on determination of helium reionization properties. We find that a\nfluence limited survey with 10$^4$ FRBs can discriminate different helium\nreionization histories at $\\sim6\\sigma$ using the DM-distribution of bursts,\nwithout redshift information (and $\\sim10\\sigma$ with redshifts).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this white paper we provide a vision for 6G Edge Intelligence. Moving\ntowards 5G and beyond the future 6G networks, intelligent solutions utilizing\ndata-driven machine learning and artificial intelligence become crucial for\nseveral real-world applications including but not limited to, more efficient\nmanufacturing, novel personal smart device environments and experiences, urban\ncomputing and autonomous traffic settings. We present edge computing along with\nother 6G enablers as a key component to establish the future 2030 intelligent\nInternet technologies as shown in this series of 6G White Papers.\n  In this white paper, we focus in the domains of edge computing infrastructure\nand platforms, data and edge network management, software development for edge,\nand real-time and distributed training of ML/AI algorithms, along with\nsecurity, privacy, pricing, and end-user aspects. We discuss the key enablers\nand challenges and identify the key research questions for the development of\nthe Intelligent Edge services. As a main outcome of this white paper, we\nenvision a transition from Internet of Things to Intelligent Internet of\nIntelligent Things and provide a roadmap for development of 6G Intelligent\nEdge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, leveraging pre-trained Transformer based language models in down\nstream, task specific models has advanced state of the art results in natural\nlanguage understanding tasks. However, only a little research has explored the\nsuitability of this approach in low resource settings with less than 1,000\ntraining data points. In this work, we explore fine-tuning methods of BERT -- a\npre-trained Transformer based language model -- by utilizing pool-based active\nlearning to speed up training while keeping the cost of labeling new data\nconstant. Our experimental results on the GLUE data set show an advantage in\nmodel performance by maximizing the approximate knowledge gain of the model\nwhen querying from the pool of unlabeled data. Finally, we demonstrate and\nanalyze the benefits of freezing layers of the language model during\nfine-tuning to reduce the number of trainable parameters, making it more\nsuitable for low-resource settings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we investigate automorphisms of compact K\\\"ahler manifolds\nwith different levels of topological triviality. In particular, we provide\nseveral examples of smooth complex projective surfaces X whose groups of\n$C^\\infty$-isotopically trivial automorphisms, resp. cohomologically trivial\nautomorphisms, have a number of connected components which can be arbitrarily\nlarge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We address the macroscopic quantumness of the state of mechanical systems\nsubjected to conditional protocols devised for state engineering in cavity\noptomechanics. We use a measure of macroscopicity based on phase-space methods.\nWe cover the domain from weak to strong optomechanical coupling, illustrating\nhow measurements performed over the cavity field that drives the dynamics of a\nmechanical system are able to steer the latter towards large quantum coherent\nstates. The effect of losses is evaluated for the case of an open cavity, and\nanalyzed in terms of the features of the Wigner functions of the state of the\nmechanical system. We also address the case of engineered phonon-subtracted\nmechanical systems, in full open-system configuration, demonstrating the\nexistence of optimal working points for the sake of mesoscopic quantumness. Our\nstudy is relevant for and applicable to a broad range of settings, from clamped\nto levitated mechanical systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  I propose novel partial identification bounds on infection prevalence from\ninformation on test rate and test yield. The approach utilizes user-specified\nbounds on (i) test accuracy and (ii) the extent to which tests are targeted,\nformalized as restriction on the effect of true infection status on the odds\nratio of getting tested and thereby embeddable in logit specifications. The\nmotivating application is to the COVID-19 pandemic but the strategy may also be\nuseful elsewhere.\n  Evaluated on data from the pandemic's early stage, even the weakest of the\nnovel bounds are reasonably informative. Notably, and in contrast to\nspeculations that were widely reported at the time, they place the infection\nfatality rate for Italy well above the one of influenza by mid-April.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We discuss aspects of non-perturbative unitarity in quantum field theory. The\nadditional ghost degrees of freedom arising in \"truncations\" of an effective\naction at a finite order in derivatives could be fictitious degrees of freedom.\nTheir contributions to the fully-dressed propagator -- the residues of the\ncorresponding ghost-like poles -- vanish once all operators compatible with the\nsymmetry of the theory are included in the effective action. These \"fake\nghosts\" do not indicate a violation of unitarity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Driven mesoscopic system is a topic of great recent interest. The temporal\nevolution of the fluxes(particle and energy) are studied in a system of a\ndriven single level quantum dot. At a very low reservoir temperature\n$T\\rightarrow 0$ and for common chemical potentials of the two reservoirs, we\nhave presented analytical expressions for time dependent particle and energy\nfluxes in a very simple form. Apart from these fluxes, the behavior of the dot\noccupation and the power developed in the system due to the presence of the\ntime dependent drive are also being studied. Importantly, for a very low\nfrequency of the drive, one finds a directed energy flow towards the leads.\nIncreasing the frequency from low to medium, one finds change in the direction\nof the energy flow depending on the time. These results can also be verified\nexperimentally.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We define cohomology of associative H-pseudoalgebras, and we show that it\ndescribes module extensions, abelian pseudoalgebra extensions, and\npseudoalgebra first order deformations. We describe in details the same results\nfor the special case of associative conformal algebras.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Late times dark energy transitions at redshifts $z \\ll 0.1$ can raise the\npredicted value of the Hubble constant to the SH0ES value, $74.03\\pm 1.42$ (km\ns$^{-1}$ Mpc$^{-1})$ or more, while providing an equally good fit as\n$\\Lambda$CDM at $67.73 \\pm 0.41$ to higher redshift data, in particular from\nthe cosmic microwave background and baryon acoustic oscillations. These models\nhowever do not fully resolve the true source of tension between the distance\nladder and high redshift observations: the local calibration of supernovae\nluminosities well out into the Hubble flow. When tested in this manner by\ntransferring the SH0ES calibration to the Pantheon supernovae dataset, the\nability of such transitions to raise the Hubble constant is reduced to $69.17\n\\pm 1.09$. Such an analysis should also be used when testing any dynamical dark\nenergy model which can produce similarly fine features in redshift or local\nvoid models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The recent detection of gravitational waves from black hole coalescences and\nthe first image of the black hole shadow enhance the possibilities of testing\ngravitational theories in the strong-field regime. In this paper, we study the\nphysical properties and the shadow image of a class of Kerr-like rotating black\nholes, whose $\\mathbb{Z}_2$ symmetry is generically broken. Such black hole\nsolutions could arise in effective low-energy theories of a fundamental quantum\ntheory of gravity, such as string theory. Within a theory-agnostic framework,\nwe require that the Kerr-like solutions are asymptotically flat, and assume\nthat a Carter-like constant is preserved, enabling the geodesic equations to be\nfully separable. Subject to these two requirements, we find that the\n$\\mathbb{Z}_2$ asymmetry of the spacetime is characterized by two arbitrary\nfunctions of polar angle. The shadow image turns out to be $\\mathbb{Z}_2$\nsymmetric on the celestial coordinates. Furthermore, the shadow is completely\nblind to one of the arbitrary functions. The other function, although would\naffect the apparent size of the shadow, it hardly distorts the shadow contour\nand has merely no degeneracy with the spin parameter. Therefore, the parameters\nin this function can be constrained with black hole shadows, only when the mass\nand the distance of the black hole from the earth are measured with great\nprecision.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  High-performance and safety-critical system architects must accurately\nevaluate the application-level silent data corruption (SDC) rates of processors\nto soft errors. Such an evaluation requires error propagation all the way from\nparticle strikes on low-level state up to the program output. Existing\napproaches that rely on low-level simulations with fault injection cannot\nevaluate full applications because of their slow speeds, while\napplication-level accelerated fault testing in accelerated particle beams is\noften impractical. We present a new two-level methodology for application\nresilience evaluation that overcomes these challenges. The proposed approach\ndecomposes application failure rate estimation into (1) identifying how\nparticle strikes in low-level unprotected state manifest at the\narchitecture-level, and (2) measuring how such architecture-level\nmanifestations propagate to the program output. We demonstrate the\neffectiveness of this approach on GPU architectures. We also show that using\njust one of the two steps can overestimate SDC rates and produce different\ntrends---the composition of the two is needed for accurate reliability\nmodeling.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We deal with a weakly coupled system of ODEs of the type $$ x_j'' + n_j^2\n\\,x_j + h_j(x_1,\\ldots,x_d) = p_j(t), \\qquad j=1,\\ldots,d, $$ with $h_j$\nlocally Lipschitz continuous and bounded, $p_j$ continuous and $2\\pi$-periodic,\n$n_j \\in \\mathbb{N}$ (so that the system is at resonance). By means of a\nLyapunov function approach for discrete dynamical systems, we prove the\nexistence of unbounded solutions, when either global or asymptotic conditions\non the coupling terms $h_1,\\ldots,h_d$ are assumed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This article focuses on a biobjective extension of the maximum flow network\ninterdiction problem, where each arc in the network is associated with two\ncapacity values. Two maximum flows from a source to a sink are to be computed\nindependently of each other with respect to the first and second capacity\nfunction, respectively, while an interdictor aims to minimize the value of both\nmaximum flows by interdicting arcs. We show that this problem is intractable\nand that the decision problem, which asks whether or not a feasible\ninterdiction strategy is efficient, is NP-complete. We propose a\npseudopolynomial time algorithm in the case of two-terminal series-parallel\ngraphs and positive integer-valued interdiction costs. We extend this algorithm\nto a fully polynomial-time approximation scheme for the case of unit\ninterdiction costs by appropriately partitioning the objective space.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Unsupervised domain adaptation studies the problem of utilizing a relevant\nsource domain with abundant labels to build predictive modeling for an\nunannotated target domain. Recent work observe that the popular adversarial\napproach of learning domain-invariant features is insufficient to achieve\ndesirable target domain performance and thus introduce additional training\nconstraints, e.g. cluster assumption. However, these approaches impose the\nconstraints on source and target domains individually, ignoring the important\ninterplay between them. In this work, we propose to enforce training\nconstraints across domains using mixup formulation to directly address the\ngeneralization performance for target data. In order to tackle potentially huge\ndomain discrepancy, we further propose a feature-level consistency regularizer\nto facilitate the inter-domain constraint. When adding intra-domain mixup and\ndomain adversarial learning, our general framework significantly improves\nstate-of-the-art performance on several important tasks from both image\nclassification and human activity recognition.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We develop a practical discrete model of hysteresis based on nonlinear play\nand generalized play, for use in first-order conservation laws with\napplications to adsorption-desorption hysteresis models. The model is easy to\ncalibrate from sparse data, and offers rich secondary curves. We compare it\nwith discrete regularized Preisach models. We also prove well-posedness and\nnumerical stability of the class of hysteresis operators involving all those\ntypes, describe implementation and present numerical examples using\nexperimental data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The behaviour of the nuclear symmetry energy near saturation density is\nimportant for our understanding of dense nuclear matter. This density\ndependence can be parameterised by the nuclear symmetry energy and its\nderivatives evaluated at nuclear saturation density. In this work we show that\nthe core-crust interface mode of a neutron star is sensitive to these\nparameters, through the (density-weighted) shear-speed within the crust, which\nis in turn dependent on the symmetry energy profile of dense matter. We\ncalculate the frequency at which the neutron star quadrupole ($\\ell = 2$)\ncrust-core interface mode must be driven by the tidal field of its binary\npartner to trigger a Resonant Shattering Flare (RSF). We demonstrate that\ncoincident multimessenger timing of an RSF and gravitational wave chirp from a\nneutron star merger would enable us to place constraints on the symmetry energy\nparameters that are competitive with those from current nuclear experiments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The critical role that Network Time Protocol (NTP) plays in the Internet led\nto multiple efforts to secure it against time-shifting attacks. A recent\nproposal for enhancing the security of NTP with Chronos against on-path\nattackers seems the most promising one and is on a standardisation track of the\nIETF. In this work we demonstrate off-path attacks against Chronos enhanced NTP\nclients. The weak link is a central security feature of Chronos: The server\npool generation mechanism using DNS. We show that the insecurity of DNS allows\nto subvert the security of Chronos making the time-shifting attacks against\nChronos-NTP even easier than attacks against plain NTP.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  To broaden the application scenario and reduce energy consumption, we propose\nan energy-efficient fixed-gain (FG) amplify-and-forward (AF) relay assisted\northogonal frequency-division multiplexing with index modulation (OFDM-IM)\nscheme in this letter. The proposed system needs neither instantaneous channel\nstate information (CSI) nor complicated processing at the relay node. It\noperates based on the power allocation scheme that minimizes the sum of\ntransmit power at both source and relay node, given an outage probability\nconstraint. Through a series of problem transformation and simplification, we\nconvert the original power allocation problem to its relaxed version and solve\nit using convex programming techniques. To reveal the computing efficiency of\nthe proposed power allocation scheme, we analyze its computational complexity.\nNumerical simulations substantiate that the proposed optimization scheme has a\nneglectable loss compared with the brute force search, but the computational\ncomplexity can be considerably reduced.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Affine ind-varieties are infinite dimensional generalizations of algebraic\nvarieties which appear naturally in many different contexts, in particular in\nthe study of automorphism groups of affine spaces. In this article we introduce\nand develop the basic algebraic theory of topologically integrable derivations\nof complete topological rings. We establish a bijective algebro-geometric\ncorrespondence between additive group actions on affine ind-varieties and\ntopologically integrable derivations of their coordinate pro-rings which\nextends the classical fruitful correspondence between additive group actions on\naffine varieties and locally nilpotent derivations of their coordinate rings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we consider the linearized version of a system of partial\ndifferential equations arising from a fluid-structure interaction model. We\nprove the existence and the uniqueness of the solution under natural regularity\nassumptions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although significant effort has been applied to fact-checking, the prevalence\nof fake news over social media, which has profound impact on justice, public\ntrust and our society, remains a serious problem. In this work, we focus on\npropagation-based fake news detection, as recent studies have demonstrated that\nfake news and real news spread differently online. Specifically, considering\nthe capability of graph neural networks (GNNs) in dealing with non-Euclidean\ndata, we use GNNs to differentiate between the propagation patterns of fake and\nreal news on social media. In particular, we concentrate on two questions: (1)\nWithout relying on any text information, e.g., tweet content, replies and user\ndescriptions, how accurately can GNNs identify fake news? Machine learning\nmodels are known to be vulnerable to adversarial attacks, and avoiding the\ndependence on text-based features can make the model less susceptible to the\nmanipulation of advanced fake news fabricators. (2) How to deal with new,\nunseen data? In other words, how does a GNN trained on a given dataset perform\non a new and potentially vastly different dataset? If it achieves\nunsatisfactory performance, how do we solve the problem without re-training the\nmodel on the entire data from scratch? We study the above questions on two\ndatasets with thousands of labelled news items, and our results show that: (1)\nGNNs can achieve comparable or superior performance without any text\ninformation to state-of-the-art methods. (2) GNNs trained on a given dataset\nmay perform poorly on new, unseen data, and direct incremental training cannot\nsolve the problem---this issue has not been addressed in the previous work that\napplies GNNs for fake news detection. In order to solve the problem, we propose\na method that achieves balanced performance on both existing and new datasets,\nby using techniques from continual learning to train GNNs incrementally.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce the notions of shifted bisymplectic and shifted double Poisson\nstructures on differential graded associative algebras, and more generally on\nnon-commutative derived moduli functors with well-behaved cotangent complexes.\nFor smooth algebras concentrated in degree $0$, these structures recover the\nclassical notions of bisymplectic and double Poisson structures, but in general\nthey involve an infinite hierarchy of higher homotopical data, ensuring that\nthey are invariant under quasi-isomorphism. The structures induce shifted\nsymplectic and shifted Poisson structures on the underlying commutative derived\nmoduli functors, and also on underlying representation functors.\n  We show that there are canonical equivalences between the spaces of shifted\nbisymplectic structures and of non-degenerate $n$-shifted double Poisson\nstructures. We also give canonical shifted bisymplectic and bi-Lagrangian\nstructures on various derived non-commutative moduli functors of modules over\nCalabi--Yau dg categories.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the problem of learning a real-valued function that satisfies the\nDemographic Parity constraint. It demands the distribution of the predicted\noutput to be independent of the sensitive attribute. We consider the case that\nthe sensitive attribute is available for prediction. We establish a connection\nbetween fair regression and optimal transport theory, based on which we derive\na close form expression for the optimal fair predictor. Specifically, we show\nthat the distribution of this optimum is the Wasserstein barycenter of the\ndistributions induced by the standard regression function on the sensitive\ngroups. This result offers an intuitive interpretation of the optimal fair\nprediction and suggests a simple post-processing algorithm to achieve fairness.\nWe establish risk and distribution-free fairness guarantees for this procedure.\nNumerical experiments indicate that our method is very effective in learning\nfair models, with a relative increase in error rate that is inferior to the\nrelative gain in fairness.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The synchrosqueezing transform (SST) has been developed as a powerful\nEMD-like tool for instantaneous frequency (IF) estimation and component\nseparation of non-stationary multicomponent signals. Recently, a direct method\nof the time-frequency approach, called signal separation operation (SSO), was\nintroduced to solving the problem of multicomponent signal separation. While\nboth SST and SSO are mathematically rigorous on IF estimation, SSO avoids the\nsecond step of the two-step SST method in component recovery (mode retrieval).\nIn addition, SSO is simple: the IF of a component is estimated by a\ntime-frequency ridge of the SSO plane; and this component is recovered by\nsimply plugging the time-frequency ridge to the SSO operation. In recent paper\n\"Direct signal separation via extraction of local frequencies with adaptive\ntime-varying parameters\", after showing that the SSO operation is related to\nthe adaptive short-time Fourier transform (STFT), the authors obtained a more\naccurate component recovery formula derived from the linear chirp (also called\nlinear frequency modulation signal) approximation at any local time and they\nalso proposed a recovery scheme to extract the signal components one by one\nwith the time-varying window updated for each component. However the\ntheoretical analysis of the recovery formula derived from linear chirp local\napproximation has not been studied there. In this paper, we carry out such\nanalysis and obtain error bounds for IF estimation and component recovery.\nThese results provide a mathematical guarantee to the proposed adaptive\nSTFT-based non-stationary multicomponent signal separation method.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Logs have been widely adopted in software system development and maintenance\nbecause of the rich system runtime information they contain. In recent years,\nthe increase of software size and complexity leads to the rapid growth of the\nvolume of logs. To handle these large volumes of logs efficiently and\neffectively, a line of research focuses on intelligent log analytics powered by\nAI (artificial intelligence) techniques. However, only a small fraction of\nthese techniques have reached successful deployment in industry because of the\nlack of public log datasets and necessary benchmarking upon them. To fill this\nsignificant gap between academia and industry and also facilitate more research\non AI-powered log analytics, we have collected and organized loghub, a large\ncollection of log datasets. In particular, loghub provides 17 real-world log\ndatasets collected from a wide range of systems, including distributed systems,\nsupercomputers, operating systems, mobile systems, server applications, and\nstandalone software. In this paper, we summarize the statistics of these\ndatasets, introduce some practical log usage scenarios, and present a case\nstudy on anomaly detection to demonstrate how loghub facilitates the research\nand practice in this field. Up to the time of this paper writing, loghub\ndatasets have been downloaded over 15,000 times by more than 380 organizations\nfrom both industry and academia.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the problem of localizing a manned, semi-autonomous, or\nautonomous vehicle in the environment using information coming from the\nvehicle's sensors, a problem known as navigation or simultaneous localization\nand mapping (SLAM) depending on the context. To infer knowledge from sensors'\nmeasurements, while drawing on a priori knowledge about the vehicle's dynamics,\nmodern approaches solve an optimization problem to compute the most likely\ntrajectory given all past observations, an approach known as smoothing.\nImproving smoothing solvers is an active field of research in the SLAM\ncommunity. Most work is focused on reducing computation load by inverting the\ninvolved linear system while preserving its sparsity. The present paper raises\nan issue which, to the knowledge of the authors, has not been addressed yet:\nstandard smoothing solvers require explicitly using the inverse of sensor noise\ncovariance matrices. This means the parameters that reflect the noise magnitude\nmust be sufficiently large for the smoother to properly function. When matrices\nare close to singular, which is the case when using high precision modern\ninertial measurement units (IMU), numerical issues necessarily arise,\nespecially with 32-bits implementation demanded by most industrial aerospace\napplications. We discuss these issues and propose a solution that builds upon\nthe Kalman filter to improve smoothing algorithms. We then leverage the results\nto devise a localization algorithm based on fusion of IMU and vision sensors.\nSuccessful real experiments using an actual car equipped with a tactical grade\nhigh performance IMU and a LiDAR illustrate the relevance of the approach to\nthe field of autonomous vehicles.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The current critical review aims to be more than a simple summary and\nreproduction of previously published work. Many comprehensive reviews and\ncollections can be found in the literature. The main intention is to provide an\naccount of the progress made in selected aspects of photoinduced phenomena in\nnon-crystalline chalcogenides, presenting the current understanding of the\nmechanisms underlying such effects. An essential motive for the present review\narticle has been to assess critically published experimental work in the field.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep learning applications require global optimization of non-convex\nobjective functions, which have multiple local minima. The same problem is\noften found in physical simulations and may be resolved by the methods of\nLangevin dynamics with Simulated Annealing, which is a well-established\napproach for minimization of many-particle potentials. This analogy provides\nuseful insights for non-convex stochastic optimization in machine learning.\nHere we find that integration of the discretized Langevin equation gives a\ncoordinate updating rule equivalent to the famous Momentum optimization\nalgorithm. As a main result, we show that a gradual decrease of the momentum\ncoefficient from the initial value close to unity until zero is equivalent to\napplication of Simulated Annealing or slow cooling, in physical terms. Making\nuse of this novel approach, we propose CoolMomentum -- a new stochastic\noptimization method. Applying Coolmomentum to optimization of Resnet-20 on\nCifar-10 dataset and Efficientnet-B0 on Imagenet, we demonstrate that it is\nable to achieve high accuracies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The linear wave and geostrophic (vortex) solutions are shown to be a complete\nbasis for physical variables $(u,v,w,\\rho)$ in a rotating non-hydrostatic\nBoussinesq model with arbitrary stratification. As a consequence, the fluid can\nbe unambiguously separated into linear wave and geostrophic components at each\ninstant in time, without the need for temporal filtering. The fluid can then be\ndiagnosed for temporal changes in wave and geostrophic coefficients at each\nunique wavenumber and mode, including those that inevitably occur due to\nnonlinear interactions.\n  We demonstrate that this methodology can be used to determine which physical\ninteractions cause the transfer of energy between modes by projecting the\nnonlinear equations of motion onto the wave-vortex basis. In the particular\nexample given, we show that an eddy in geostrophic balance superimposed with\ninertial oscillations at the surface transfers energy from the inertial\noscillations to internal gravity wave modes. This approach can be applied more\ngenerally to determine which mechanisms are involved in energy transfers\nbetween wave and vortices, including their respective scales.\n  Finally, we show that the nonlinear equations of motion expressed in a\nwave-vortex basis are computationally efficient for certain problems. In cases\nwhere stratification profiles vary strongly with depth, this approach may be an\nattractive alternative to traditional spectral models for rotating Boussinesq\nflow.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Data cleaning is a pervasive problem for organizations as they try to reap\nvalue from their data. Recent advances in networking and cloud computing\ntechnology have fueled a new computing paradigm called Database-as-a-Service,\nwhere data management tasks are outsourced to large service providers. In this\npaper, we consider a Data Cleaning-as-a-Service model that allows a client to\ninteract with a data cleaning provider who hosts curated, and sensitive data.\nWe present PACAS: a Privacy-Aware data Cleaning-As-a-Service model that\nfacilitates interaction between the parties with client query requests for\ndata, and a service provider using a data pricing scheme that computes prices\naccording to data sensitivity. We propose new extensions to the model to define\ngeneralized data repairs that obfuscate sensitive data to allow data sharing\nbetween the client and service provider. We present a new semantic distance\nmeasure to quantify the utility of such repairs, and we re-define the notion of\nconsistency in the presence of generalized values. The PACAS model uses\n(X,Y,L)-anonymity that extends existing data publishing techniques to consider\nthe semantics in the data while protecting sensitive values. Our evaluation\nover real data show that PACAS safeguards semantically related sensitive\nvalues, and provides lower repair errors compared to existing privacy-aware\ncleaning techniques.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  How to steer a given joint state probability density function to another over\nfinite horizon subject to a controlled stochastic dynamics with hard state\n(sample path) constraints? In applications, state constraints may encode safety\nrequirements such as obstacle avoidance. In this paper, we perform the feedback\nsynthesis for minimum control effort density steering (a.k.a. Schr\\\"{o}dinger\nbridge) problem subject to state constraints. We extend the theory of\nSchr\\\"{o}dinger bridges to account the reflecting boundary conditions for the\nsample paths, and provide a computational framework building on our previous\nwork on proximal recursions, to solve the same.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study a phenomenological model that mimics the characteristics of QCD\ntheory at finite temperature. The model involves fermions coupled with a\nmodified Abelian gauge field in a tachyon matter. It reproduces some important\nQCD features such as, confinement, deconfinement, chiral symmetry and\nquark-gluon-plasma (QGP) phase transitions. The study may shed light on both\nlight and heavy quark potentials and their string tensions. Flux-tube and\nCornell potentials are developed depending on the regime under consideration.\nOther confining properties such as scalar glueball mass, gluon mass,\nglueball-meson mixing states, gluon and chiral condensates are exploited as\nwell. The study is focused on two possible regimes, the ultraviolet (UV) and\nthe infrared (IR) regimes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The relationship of comments to code, and in particular, the task of\ngenerating useful comments given the code, has long been of interest. The\nearliest approaches have been based on strong syntactic theories of\ncomment-structures, and relied on textual templates. More recently, researchers\nhave applied deep learning methods to this task, and specifically, trainable\ngenerative translation models which are known to work very well for Natural\nLanguage translation (e.g., from German to English). We carefully examine the\nunderlying assumption here: that the task of generating comments sufficiently\nresembles the task of translating between natural languages, and so similar\nmodels and evaluation metrics could be used. We analyze several recent\ncode-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We\ncompare them with WMT19, a standard dataset frequently used to train state of\nthe art natural language translators. We found some interesting differences\nbetween the code-comment data and the WMT19 natural language data. Next, we\ndescribe and conduct some studies to calibrate BLEU (which is commonly used as\na measure of comment quality). using \"affinity pairs\" of methods, from\ndifferent projects, in the same project, in the same class, etc; Our study\nsuggests that the current performance on some datasets might need to be\nimproved substantially. We also argue that fairly naive information retrieval\n(IR) methods do well enough at this task to be considered a reasonable\nbaseline. Finally, we make some suggestions on how our findings might be used\nin future research in this area.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Learning from Observation (LfO), also known as Behavioral Cloning, is an\napproach for building software agents by recording the behavior of an expert\n(human or artificial) and using the recorded data to generate the required\nbehavior. jLOAF is a platform that uses Case-Based Reasoning to achieve LfO. In\nthis paper we interface jLOAF with the popular OpenAI Gym environment. Our\nexperimental results show how our approach can be used to provide a baseline\nfor comparison in this domain, as well as identify the strengths and weaknesses\nwhen dealing with environmental complexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The structure and $B(E1)$ transition strength of $^{19}$B are investigated in\na $^{17}\\text{B}+n+n$ model, triggered by a recent experiment showing that\n$^{19}$B exhibits a well pronounced two-neutron halo structure. Preliminary\nanalysis of the experimental data was performed by employing contact $n$-$n$\ninteractions, which are known to underestimate the $s$-wave content in other\nhalo nuclei such as $^{11}$Li. In the present work, the three-body\nhyperspherical formalism with finite-range two-body interactions is used to\ndescribe $^{19}$B. In particular, two different finite-range $n$-$n$\ninteractions will be used, as well as a simple central Gaussian potential whose\nrange is progressively reduced. The purpose is to determine the main properties\nof the nucleus and investigate how they change when using contact-like $n$-$n$\npotentials. Special attention is also paid to the dependence on the\nprescription used to account for three-body effects, i.e., a three-body force\nor a density-dependent $n$-$n$ potential. We have found that the three-body\nmodel plus finite-range potentials provide a description of $^{19}$B consistent\nwith the experimental data. The results are essentially independent of the\nshort-distance details of the two-body potentials, giving rise to an\n$(s_{1/2})^2$ content of about 55%, clearly larger than the initial estimates.\nVery little dependence has been found as well on the prescription used for the\nthree-body effects. The total computed $B(E1)$ strength is compatible with the\nexperimental result, although we slightly overestimate the data around the\nlow-energy peak of the $dB(E1)/d\\varepsilon$ distribution. Finally, we show\nthat a reduction of the $n$-$n$ interaction range produces a significant\nreduction of the $s$-wave contribution, which then should be expected in\ncalculations using contact interactions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Tunable Diode Laser Absorption Spectroscopy (TDLAS) tomography has been\nwidely used for in situ combustion diagnostics, yielding images of both species\nconcentration and temperature. The temperature image is generally obtained from\nthe reconstructed absorbance distributions for two spectral transitions, i.e.\ntwo-line thermometry. However, the inherently ill-posed nature of tomographic\ndata inversion leads to noise in each of the reconstructed absorbance\ndistributions. These noise effects propagate into the absorbance ratio and\ngenerate artefacts in the retrieved temperature image. To address this problem,\nwe have developed a novel algorithm, which we call Relative Entropy Tomographic\nRecOnstruction (RETRO), for TDLAS tomography. A relative entropy regularisation\nis introduced for high-fidelity temperature image retrieval from jointly\nreconstructed two-line absorbance distributions. We have carried out numerical\nsimulations and proof-of-concept experiments to validate the proposed\nalgorithm. Compared with the well-established Simultaneous Algebraic\nReconstruction Technique (SART), the RETRO algorithm significantly improves the\nquality of the tomographic temperature images, exhibiting excellent robustness\nagainst TDLAS tomographic measurement noise. RETRO offers great potential for\nindustrial field applications of TDLAS tomography, where it is common for\nmeasurements to be performed in very harsh environments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Radiative and non-radiative electron spin flip probabilities are analysed in\nboth plane wave and focussed laser backgrounds. We provide a simple and\nphysically transparent description of spin dynamics in plane waves, and\ndemonstrate that there exists a kinematic regime in which the usual leading\norder perturbative hierarchy of QED is reversed, and non-radiative loop effects\ndominate over radiative tree-level spin-flips. We show that while this\nloop-dominance becomes suppressed in focussed laser pulses due to a high\nsensitivity to field geometry, there is nevertheless a regime in which, in\nprinciple, loop effects on spin transitions can be discerned.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An artificial neural network can be trained by uniformly broadcasting a\nreward signal to units that implement a REINFORCE learning rule. Though this\npresents a biologically plausible alternative to backpropagation in training a\nnetwork, the high variance associated with it renders it impractical to train\ndeep networks. The high variance arises from the inefficient structural credit\nassignment since a single reward signal is used to evaluate the collective\naction of all units. To facilitate structural credit assignment, we propose\nreplacing the reward signal to hidden units with the change in the $L^2$ norm\nof the unit's outgoing weight. As such, each hidden unit in the network is\ntrying to maximize the norm of its outgoing weight instead of the global\nreward, and thus we call this learning method Weight Maximization. We prove\nthat Weight Maximization is approximately following the gradient of rewards in\nexpectation. In contrast to backpropagation, Weight Maximization can be used to\ntrain both continuous-valued and discrete-valued units. Moreover, Weight\nMaximization solves several major issues of backpropagation relating to\nbiological plausibility. Our experiments show that a network trained with\nWeight Maximization can learn significantly faster than REINFORCE and slightly\nslower than backpropagation. Weight Maximization illustrates an example of\ncooperative behavior automatically arising from a population of self-interested\nagents in a competitive game without any central coordination.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Time-periodic (Floquet) drive is a powerful method to engineer quantum phases\nof matter, including fundamentally non-equilibrium states that are impossible\nin static Hamiltonian systems. One characteristic example is the anomalous\nFloquet insulator, which exhibits topologically quantized chiral edge states\nsimilar to a Chern insulator, yet is amenable to bulk localization. We study\nthe response of this topological system to time-dependent noise, which breaks\nthe topologically protecting Floquet symmetry. Surprisingly, we find that the\nquantized response, given by partially filling the fermionic system and\nmeasuring charge pumped per cycle, remains quantized up to finite noise\namplitude. We trace this robust topology to an interplay between diffusion and\nPauli blocking of edge state decay, which we expect should be robust against\ninteractions. We determine the boundaries of the topological phase for a system\nwith spatial disorder numerically through level statistics, and corroborate our\nresults in the limit of vanishing disorder through an analytical Floquet\nsuperoperator approach. This approach suggests an interpretation of the state\nof the system as a non-Hermitian Floquet topological phase. We comment on\nquantization of other topological responses in the absence of Floquet symmetry\nand potential experimental realizations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider a class of density-dependent branching processes which\ngeneralises exponential, logistic and Gompertz growth. A population begins with\na single individual, grows exponentially initially, and then growth may slow\ndown as the population size moves towards a carrying capacity. At a time while\nthe population is still growing superlinearly, a fixed number of individuals\nare sampled and their coalescent tree is drawn. Taking the sampling time and\ncarrying capacity simultaneously to infinity, we prove convergence of the\ncoalescent tree to a limiting tree which is in a sense universal over our class\nof models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the higher order clockwork theory of gravitational interactions,\nwhereby a number of gravitons are coupled together with TeV strength, but\nnevertheless generate a Planck scale coupling to matter without the need for a\ndilaton. It is shown that the framework naturally lends itself to a\nfive-dimensional geometry, and we find the 5D continuum version of such\ndeconstructed 4D gravitational clockwork models. Moreover, the clockwork\npicture has matter coupled to particular gravitons, which in the 5D framework\nlooks like a braneworld model, with the Randall-Sundrum model being a special\ncase. More generally, the gravitational clockwork leads to a family of\nscalar-tensor braneworld models, where the scalar is not a dilaton.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The document describes a numerical algorithm to simulate plasmas and fluids\nin the 3 dimensional space by the Euler method, in which the spatial meshes are\nfixed to the space. The plasmas and fluids move through the spacial Euler mesh\nboundary. The Euler method can represent a large deformation of the plasmas and\nfluids. On the other hand, when the plasmas or fluids are compressed to a high\ndensity, the spatial resolution should be ensured to describe the density\nchange precisely. The present 3D Euler code is developed to simulate a nuclear\nfusion fuel ignition and burning. Therefore, the 3D Euler code includes the DT\nfuel reactions, the alpha particle diffusion, the alpha particle deposition to\nheat the DT fuel and the DT fuel depletion by the DT reactions, as well as the\nthermal energy diffusion based on the three-temperature compressible fluid\nmodel.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We classify $\\mathcal{R}$- and $\\mathcal{L}$-cross-sections of wreath\nproducts of finite inverse symmetric semigroups $\\mathcal{IS}_m \\wr_p\n\\mathcal{IS}_n$ up to isomorphism. We show that every isomorphism of\n$\\mathcal{R}$ ($\\mathcal{L}$-) cross-sections of $\\mathcal{IS}_m \\wr_p\n\\mathcal{IS}_n$ is a conjugacy. As an auxiliary result, we get that every\nisomorphism of $\\mathcal{R}$- ($\\mathcal{L}$-) cross-sections of\n$\\mathcal{IS}_n$ is also a conjugacy. We also compute the number of\nnon-isomorphic $\\mathcal{R}$- ($\\mathcal{L}$-) cross-sections of\n$\\mathcal{IS}_m \\wr_p \\mathcal{IS}_n$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove the existence of a semiflow selection with range the space of\nc\\`agl\\`ad, i.e. left--continuous and having right--hand limits functions\ndefined on $[0,\\infty)$ and taking values in a Hilbert space. Afterwards, we\napply this abstract result to the system arising from a compressible viscous\nfluid with a barotropic pressure of the type $a\\varrho^{\\gamma}$, $\\gamma \\geq\n1$, with a viscous stress tensor being a nonlinear function of the symmetric\nvelocity gradient.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The increasing availability of cloud computing services for science has\nchanged the way scientific code can be developed, deployed, and run. Many\nmodern scientific workflows are capable of running on cloud computing\nresources. Consequently, there is an increasing interest in the scientific\ncomputing community in methods, tools, and implementations that enable moving\nan application to the cloud and simplifying the process, and decreasing the\ntime to meaningful scientific results. In this paper, we have applied the\nconcepts of containerization for portability and multi-cloud automated\ndeployment with industry-standard tools to three scientific workflows. We show\nhow our implementations provide reduced complexity to portability of both the\napplications themselves, and their deployment across private and public clouds.\nEach application has been packaged in a Docker container with its dependencies\nand necessary environment setup for production runs. Terraform and Ansible have\nbeen used to automate the provisioning of compute resources and the deployment\nof each scientific application in a Multi-VM cluster. Each application has been\ndeployed on the AWS and Aristotle Cloud Federation platforms. Variation in data\nmanagement constraints, Multi-VM MPI communication, and embarrassingly parallel\ninstance deployments were all explored and reported on. We thus present a\nsample of scientific workflows that can be simplified using the tools and our\nproposed implementation to deploy and run in a variety of cloud environments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Accurate prediction of spray atomization process using an Euler-Lagrange (EL)\napproach is challenging because of high volume fraction of the liquid phase in\ndense regimes. This would in reality displace a remarkable portion of the\ngaseous phase which is commonly ignored in the standard EL approaches. In\naddition, deformation of droplet due to the interaction of aerodynamic force,\nsurface tension and viscous forces is typically neglected in modeling dense\nsprays. In this work, to capture the volumetric displacement effects using an\nEL approach, the spatio-temporal changes in the volume fraction of the gaseous\nphase are taken into account. This leads to zero-Mach number, variable density\nequations that give rise to a source term in both momentum and continuity\nequations. It is shown that the continuity source term increases the velocity\nand dynamics of the carrier phase close to the nozzle. However, owing to the\njet spread and dispersion of droplets, these effects decrease further\ndownstream. In order to quantify the droplet deformation effects, different\nmodels are compared together with an experimental data. Different breakup\nregimes are studied in order to identify the best model for each regime. The\nshape deformation effect is isolated by performing a single droplet injected\ninto the cross flow with flow conditions similar to the bag-type breakup. A\nsignificant deviation in the motion of droplet is observed compared to a case\nwhere deformation is neglected.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, the initial-boundary value problem to the three-dimensional\ninhomogeneous, incompressible and heat-conducting Navier-Stokes equations with\ntemperature-depending viscosity coefficient is considered in a bounded domain.\nThe viscosity coefficient is degenerate and may vanish in the region of\nabsolutely zero temperature. Global existence of weak solutions to such a\nsystem is established for the large initial data. The proof is based on a\nthree-level approximate scheme, the De Giorgi's method and compactness\narguments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the factory of the future traditional and formerly isolated Operational\nTechnology (OT) hardware will become connected with all kinds of networks. This\nleads to more complex security challenges during design, deployment and use of\nindustrial control systems. As it is infeasible to perform security tests on\nproduction hardware and it is expensive to build hardware setups dedicated to\nsecurity testing, virtualised testbeds are gaining interest. We create a\ntestbed based on a virtualised factory which can be controlled by real and\nvirtualised hardware. This allows for a flexible evaluation of security\nstrategies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hysteresis-controlled devices are widely used in industrial applications. For\nexample, cooling devices usually contain a two-point controller, resulting in a\nnonlinear hybrid system with two discrete states. Dynamic models of systems are\nessential for optimizing such industrial supply technology. However,\nconventional system identification approaches can hardly handle\nhysteresis-controlled devices. Thus, the new identification method Sparse\nIdentification of Nonlinear Dynamics (SINDy) is extended to consider hybrid\nsystems. SINDy composes models from basis functions out of a customized library\nin a data-driven manner. For modeling systems that behave dependent on their\nown past as in the case of natural hysteresis, Ferenc Preisach introduced the\nrelay hysteron as an elementary mathematical description. In this new method\n(SINDyHybrid), tailored basis functions in form of relay hysterons are added to\nthe library which is used by SINDy. Experiments with a hysteresis controlled\nwater basin show that this approach correctly identifies state transitions of\nhybrid systems and also succeeds in modeling the dynamics of the discrete\nsystem states. A novel proximity hysteron achieves the robustness of this\nmethod. The impacts of the sampling rate and the signal noise ratio of the\nmeasurement data are examined accordingly.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A challenge when dealing with survival analysis data is accounting for a cure\nfraction, meaning that some subjects will never experience the event of\ninterest. Mixture cure models have been frequently used to estimate both the\nprobability of being cured and the time to event for the susceptible subjects,\nby usually assuming a parametric (logistic) form of the incidence. We propose a\nnew estimation procedure for a parametric cure rate that relies on a\npreliminary smooth estimator and is independent of the model assumed for the\nlatency. We investigate the theoretical properties of the estimators and show\nthrough simulations that, in the logistic/Cox model, presmoothing leads to more\naccurate results compared to the maximum likelihood estimator. To illustrate\nthe practical use, we apply the new estimation procedure to two studies of\nmelanoma survival data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The daily amount of Android malicious applications (apps) targeting the app\nrepositories is increasing, and their number is overwhelming the process of\nfingerprinting. To address this issue, we propose an enhanced Cypider\nframework, a set of techniques and tools aiming to perform a systematic\ndetection of mobile malware by building a scalable and obfuscation resilient\nsimilarity network infrastructure of malicious apps. Our approach is based on\nour proposed concept, namely malicious community, in which we consider\nmalicious instances that share common features are the most likely part of the\nsame malware family. Using this concept, we presumably assume that multiple\nsimilar Android apps with different authors are most likely to be malicious.\nSpecifically, Cypider leverages this assumption for the detection of variants\nof known malware families and zero-day malicious apps. Cypider applies\ncommunity detection algorithms on the similarity network, which extracts\nsub-graphs considered as suspicious and possibly malicious communities.\nFurthermore, we propose a novel fingerprinting technique, namely community\nfingerprint, based on a one-class machine learning model for each malicious\ncommunity. Besides, we proposed an enhanced Cypider framework, which requires\nless memory, x650, and less time to build the similarity network, x700,\ncompared to the original version, without affecting the fingerprinting\nperformance of the framework. We introduce a systematic approach to locate the\nbest threshold on different feature content vectors, which simplifies the\noverall detection process.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we show the dependence of bulk viscosity on Polyakov loop in\n3+1 dimensional topologically massive model (TMM). This model contains equally\nmassive non-Abelian gauge fields without spontaneous symmetry breaking. In\nearlier works, the bulk viscosity was found from the trace anomaly in massless\n$\\phi^4$ model and Yang-Mills (YM) theory and its dependence on the quantum\ncorrections was established. In TMM, the trace anomaly is absent due to the\npresence of kinetic term of a two-form field $B$ in the action. This model also\nprovides the dependence of bulk viscosity on the mass of the gauge bosons. The\nmass of the gauge bosons in TMM acts as magnetic mass in the perturbative\nthermal field theory. This magnetic mass is gauge independent unlike what is\nfound in massless YM theory. The ratio of bulk viscosity and entropy density vs\ntemperature is plotted which shows the required behaviour near critical point.\nWe also observe that the strong coupling constant has the same behaviour at\nhigh energy limit (i.e. asymptotic freedom) as that of massless YM theory at\nzero temperature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A general attenuator $\\Phi_{\\lambda, \\sigma}$ is a bosonic quantum channel\nthat acts by combining the input with a fixed environment state $\\sigma$ in a\nbeam splitter of transmissivity $\\lambda$. If $\\sigma$ is a thermal state the\nresulting channel is a thermal attenuator, whose quantum capacity vanishes for\n$\\lambda\\leq 1/2$. We study the quantum capacity of these objects for generic\n$\\sigma$, proving a number of unexpected results. Most notably, we show that\nfor any arbitrary value of $\\lambda>0$ there exists a suitable single-mode\nstate $\\sigma(\\lambda)$ such that the quantum capacity of\n$\\Phi_{\\lambda,\\sigma(\\lambda)}$ is larger than a universal constant $c>0$. Our\nresult holds even when we fix an energy constraint at the input of the channel,\nand implies that quantum communication at a constant rate is possible even in\nthe limit of arbitrarily low transmissivity, provided that the environment\nstate is appropriately controlled. We also find examples of states $\\sigma$\nsuch that the quantum capacity of $\\Phi_{\\lambda,\\sigma}$ is not monotonic in\n$\\lambda$. These findings may have implications for the study of communication\nlines running across integrated optical circuits, of which general attenuators\nprovide natural models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the present paper we describe the computational implementation of some\nintegral terms that arise from mixed virtual element methods (mixed-VEM) in\ntwo-dimensional pseudostress-velocity formulations. The implementation\npresented here consider any polynomial degree $k \\geq 0$ in a natural way by\nbuilding several local matrices of small size through the matrix multiplication\nand the Kronecker product. In particular, we apply the foregoing mentioned\nmatrices to the Navier-Stokes equations with Dirichlet boundary conditions,\nwhose mixed-VEM formulation was originally proposed and analyzed in a recent\nwork using virtual element subspaces for $H(\\text{div})$ and $H^1$,\nsimultaneously. In addition, an algorithm is proposed for the assembly of the\nassociated global linear system for the Newton's iteration. Finally, we present\na numerical example in order to illustrate the performance of the mixed-VEM\nscheme and confirming the expected theoretical convergence rates.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work we extend the recently proposed synchronous broadcast algorithm\namnesiac flooding to the case of intermittent communication channels. In\namnesiac flooding a node forwards a received message in the subsequent round.\nThere are several reasons that render an immediate forward of a message\nimpossible: Higher priority traffic, overloaded channels, etc. We show that\npostponing the forwarding for one or more rounds prevents termination. Our\nextension overcomes this shortcoming while retaining the advantages of the\nalgorithm: Nodes don't need to memorize the reception of a message to guarantee\ntermination and messages are sent at most twice per edge. This extension allows\nto solve more general broadcast tasks such as multi-source broadcasts and\nconcurrent broadcasts for systems with bounded channel capacities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a framework to address a class of sequential decision making\nproblems. Our framework features learning the optimal control policy with\nrobustness to noisy data, determining the unknown state and action parameters,\nand performing sensitivity analysis with respect to problem parameters. We\nconsider two broad categories of sequential decision making problems modelled\nas infinite horizon Markov Decision Processes (MDPs) with (and without) an\nabsorbing state. The central idea underlying our framework is to quantify\nexploration in terms of the Shannon Entropy of the trajectories under the MDP\nand determine the stochastic policy that maximizes it while guaranteeing a low\nvalue of the expected cost along a trajectory. This resulting policy enhances\nthe quality of exploration early on in the learning process, and consequently\nallows faster convergence rates and robust solutions even in the presence of\nnoisy data as demonstrated in our comparisons to popular algorithms such as\nQ-learning, Double Q-learning and entropy regularized Soft Q-learning. The\nframework extends to the class of parameterized MDP and RL problems, where\nstates and actions are parameter dependent, and the objective is to determine\nthe optimal parameters along with the corresponding optimal policy. Here, the\nassociated cost function can possibly be non-convex with multiple poor local\nminima. Simulation results applied to a 5G small cell network problem\ndemonstrate successful determination of communication routes and the small cell\nlocations. We also obtain sensitivity measures to problem parameters and\nrobustness to noisy environment data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents 6T SRAM cell-based bit-parallel in-memory computing (IMC)\narchitecture to support various computations with reconfigurable bit-precision.\nIn the proposed technique, bit-line computation is performed with a short WL\nfollowed by BL boosting circuits, which can reduce BL computing delays. By\nperforming carry-propagation between each near-memory circuit, bit-parallel\ncomplex computations are also enabled by iterating operations with low latency.\nIn addition, reconfigurable bit-precision is also supported based on\ncarry-propagation size. Our 128KB in/near memory computing architecture has\nbeen implemented using a 28nm CMOS process, and it can achieve 2.25GHz clock\nfrequency at 1.0V with 5.2% of area overhead. The proposed architecture also\nachieves 0.68, 8.09 TOPS/W for the parallel addition and multiplication,\nrespectively. In addition, the proposed work also supports a wide range of\nsupply voltage, from 0.6V to 1.1V.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Band crossings observed in a wide range of condensed matter systems are\nrecognized as a key to understand low-energy fermionic excitations that behave\nas massless Dirac particles. Despite rapid progress in this field, the\nexploration of non-equilibrium topological states remains scarce and it has\npotential ability of providing a new platform to create unexpected massless\nDirac states. Here we show that in a semiconductor quantum-well driven by a\ncw-laser with linear polarization, the optical Stark effect conducts bulk-band\ncrossing, and the resulting Floquet-Dirac semimetallic phase supports an\nunconventional edge state in the projected one-dimensional Brillouin zone under\na boundary condition that an electron is confined in the direction\nperpendicular to that of the laser polarization. Further, we reveal that this\nedge state mediates a transition between topological and non-topological edge\nstates that is caused by tuning the laser intensity. We also show that the\nproperties of the edge states are strikingly changed under a different boundary\ncondition. It is found that such difference originates from that nearly\nfourfold-degenerate points exist in a certain intermediate region of the bulk\nBrillouin zone between high-symmetry points.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $S,T$ be two distinct finite Abelian groups with $|S|=|T|$. A fundamental\ntheorem of Tutte shows that a graph admits a nowhere-zero $S$-flow if and only\nif it admits a nowhere-zero $T$-flow. Jaeger, Linial, Payan and Tarsi in 1992\nintroduced group connectivity as an extension of flow theory, and they asked\nwhether such a relation holds for group connectivity analogy. It was negatively\nanswered by Hu\\v{s}ek, Moheln\\'{i}kov\\'{a} and \\v{S}\\'{a}mal in 2017 for graphs\nwith edge-connectivity 2 for the groups $S=\\mathbb{Z}_4$ and\n$T=\\mathbb{Z}_2^2$. In this paper, we extend their results to\n$3$-edge-connected graphs (including both cubic and general graphs), which\nanswers open problems proposed by Hu\\v{s}ek, Moheln\\'{i}kov\\'{a} and\n\\v{S}\\'{a}mal(2017) and Lai, Li, Shao and Zhan(2011). Combining some previous\nresults, this characterizes all the equivalence of group connectivity under\n$3$-edge-connectivity, showing that every $3$-edge-connected $S$-connected\ngraph is $T$-connected if and only if $\\{S,T\\}\\neq\n\\{\\mathbb{Z}_4,\\mathbb{Z}_2^2\\}$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Systems engineering approaches use high-level models to capture the\narchitecture and behavior of the system. However, when safety engineers conduct\nsafety and reliability analysis, they have to create formal models, such as\nfault-trees, according to the behavior described by the high-level engineering\nmodels and environmental/fault assumptions. Instead of creating low-level\nanalysis models, our approach builds on engineering models in safety analysis\nby exploiting the simulation capabilities of recent probabilistic programming\nand simulation advancements. Thus, it could be applied in accordance with\nstandards and best practices for the analysis of a critical automotive system\nas part of an industrial collaboration, while leveraging high-level block\ndiagrams and statechart models created by engineers. We demonstrate the\napplicability of our approach in a case study adapted from the automotive\nsystem from the collaboration.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This work is devoted to study the analytical and regular solutions of a\nparticular self-gravitating object (i.e., gravastar) in a particular theory of\ngravity. We derive the corresponding field equations in the presence of\neffective energy momentum tensor associated with the perfect fluid\nconfiguration of a spherical system. We then describe the mathematical\nformulations of the three respective regions i.e., inner, shell and exterior of\na gravastar separately. Additionally, the significance and physical\ncharacteristics along with the graphical representation of gravastars are\ndiscussed in detail. It is seen that under some specific constraints,\n$f(R,T,R_{\\mu\\nu}T^{\\mu\\nu})$ gravity is likely to host gravastars.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We develop a multimode model that describes the dynamics on a rotating\nBose-Einstein condensate confined by a ring-shaped optical lattice with large\nfilling numbers. The parameters of the model are obtained as a function of the\nrotation frequency using full 3D Gross-Pitaevskii simulations. From such\nnumerical calculations, we extract the velocity field induced at each site and\nanalyze the relation and the differences between the phase of the hopping\nparameter of our model and the Peierls phase. To this end, a detailed\ndiscussion of such phases is presented in geometrical terms which takes into\naccount the position of the junctions for different configurations. For\ncircularly symmetric onsite densities a simple analytical relation between the\nhopping phase and the angular momentum is found for arbitrary number of sites.\nFinally, we confront the results of the rotating multimode model dynamics with\nGross-Pitaevskii simulations finding a perfect agreement.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We construct the 4-dimensional ${\\cal N}=\\frac12$ and ${\\cal N}=1$\ninhomogeneously mass-deformed super Yang-Mills theories from the ${\\cal N}\n=1^*$ and ${\\cal N} =2^*$ theories, respectively, and analyse their\nsupersymmetric vacua. The inhomogeneity is attributed to the dependence of\nbackground fluxes in the type IIB supergravity on a single spatial coordinate.\nThis gives rise to inhomogeneous mass functions in the ${\\cal N} =4$ super\nYang-Mills theory which describes the dynamics of D3-branes. The Killing spinor\nequations for those inhomogeneous theories lead to the supersymmetric vacuum\nequation and a boundary condition. We investigate two types of solutions in the\n$ {\\cal N}=\\frac12$ theory, corresponding to the cases of asymptotically\nconstant mass functions and periodic mass functions. For the former case, the\nboundary condition gives a relation between the parameters of two possibly\ndistinct vacua at the asymptotic boundaries. Brane interpretations for\ncorresponding vacuum solutions in type IIB supergravity are also discussed. For\nthe latter case, we obtain explicit forms of the periodic vacuum solutions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Higher-order exchange interactions and quantum effects are widely known to\nplay an important role in describing the properties of low-dimensional magnetic\ncompounds. Here we identify the recently discovered two-dimensional (2D) van\nder Waals (vdW) CrI3 as a quantum non-Heisenberg material with properties far\nbeyond an Ising magnet as initially assumed. We find that biquadratic exchange\ninteractions are essential to quantitatively describe the magnetism of CrI3 but\nrequiring quantum rescaling corrections to reproduce its thermal properties.\nThe quantum nature of the heat bath represented by discrete electron-spin and\nphonon-spin scattering processes induced the formation of spin fluctuations in\nthe low temperature regime. These fluctuations induce the formation of\nmetastable magnetic domains evolving into a single macroscopic magnetization or\neven a monodomain over surface areas of a few micrometers. Such domains display\nhybrid characteristics of Neel and Bloch types with a narrow domain wall width\nin the range of 3-5 nm. Similar behaviour is expected for the majority of 2D\nvdW magnets where higher-order exchange interactions are appreciable.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze the neutrino mass spectrum and discuss the extra-dimensional\ninterpretation of a three-site Pati-Salam model which i) unifies all families\nof quark and leptons, ii) provides a natural description of the Standard Model\nYukawa couplings, iii) could account for the recent $B$-physics anomalies. The\nkey feature of the model is a breaking of the Pati-Salam and electroweak gauge\nsymmetries localized on opposite sites, communicated to the other sites in an\nattenuated manner via nearest-neighbor interactions. We show that in this\ncontext gauge-singlet fermions localized on each site, receiving hierarchical\nMajorana masses, can allow the implementation of an inverse seesaw mechanism\nleading to light anarchic neutrino masses consistent with data. The continuum\nlimit of this three-site setup has a natural interpretation in terms of a\nwarped extra dimension with three defects, where the required exponential\nhierarchies can be achieved from $\\mathcal{O}(1)$ differences in the bulk field\nmasses.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The present work is an example of the application of the dynamical system\nanalysis in the context of cosmology. Here cosmic evolution is considered in\nthe background of homogeneous and isotropic flat\nFriedmann-Lema\\^{i}tre-Robertson-Walker space-time with interacting dark energy\nand varying mass dark matter as the matter content. The Dark Energy (DE) is\nchosen as phantom scalar field with self-interacting potential while the Dark\nMatter (DM) is in the form of dust. The potential of the scalar field and the\nmass function of dark matter are chosen as exponential or power-law form or in\ntheir product form. Using suitable dimensionless variables the Einstein field\nequations and the conservation equations constitute an autonomous system. The\nstability of the non-hyperbolic critical points are analyzed by using center\nmanifold theory. Finally, cosmological phase transitions have been detected\nthrough bifurcation analysis which has been done by Poincar\\'{e} index theory.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In countries with a severe outbreak of COVID-19, most governments are\nconsidering whether anti-transmission measures are worth social and economic\ncosts. The seriousness of economic costs such as the closure of some\nworkplaces, unemployment, reduction in production, and social costs such as\nschool closures, disruptions in education could be observable. However, the\neffect of the measures taken on the spread of the epidemic, such as the number\nof delayed or prevented cases, could not be observed. For this reason, the\ndirect effects of the measures taken on health, that is, the effects on the\ncourse of the epidemic, are important research subjects. For this purpose, in\nthis study, the breakpoint linear regression analysis is performed to analyze\nthe trends of daily active cases, recovered, and deaths in Turkey. The analysis\nreveals that there has been a remarkable impact on lockdown and other\nprecautions. Using the breakpoint regression model, we also analyze the active\ncases' trajectory for eight affected countries and compare the patterns in\nthese countries with Turkey.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A widely used approach to mathematically describe the atmosphere is to\nconsider it as a geophysical fluid in a shallow domain -- and thus to model it\nusing classical fluid dynamical equations combined with the explicit inclusion\nof an {\\epsilon} parameter representing the small aspect ratio of the physical\ndomain. In our previous paper [15] we proved a weak convergence theorem for the\npolluted atmosphere described by the Navier-Stokes equations extended by an\nadvection-diffusion equation. We obtained a justification of the generalised\nhydrostatic limit model including the pollution effect described for the case\nof classical, east-north-upwards oriented local Cartesian coordinates. Here we\ngive a two-fold improvement of this statement. Firstly, we consider a\nmeteorologically more meaningful coordinate system, incorporate the analytical\nconsequences of this coordinate change into the governing equations, and verify\nthat the weak convergence still holds for this altered system. Secondly, still\nconsidering this new, so-called downwind-matching coordinate system, we prove\nan analogous strong convergence result, which we make complete by providing a\nclosely related existence theorem as well.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We discuss the feasibility of measurement-based braiding in\nsemiconductor-superconductor (SM-SC) heterostructures in the so-called\nquasi-Majorana regime $-$ the topologically-trivial regime due to\npartially-separated Andreev bound states (ps-ABSs). These low energy ABSs\nconsist of component Majorana bound states (quasi-Majorana modes) that are\nspatially separated by a length scale smaller than the length of the system, in\ncontrast with the Majorana zero modes (MZMs), which are separated by the length\nof the wire. In the quasi-Majorana regime, the ZBCPs appear to be robust to\nvarious perturbations as long as the energy splitting of the ps-ABS is less\nthan the typical width $\\e_w$ of the low-energy conductance peaks $\\e_w$.\nHowever, the feasibility of measurement-based braiding depends on a different\nenergy scale $\\e_m$. In this paper we show that it is possible to prepare the\nSM-SC system in the quasi-Majorana regime with energy splittings below the\n$\\e_m$ threshold, so that measurement-based braiding is possible in principle.\nStarting with ps-ABSs with energy below $\\e_m$, we identify the maximum\namplitudes of different types of perturbations that are consistent with\nperturbation-induced energy splittings not exceeding the $\\e_m$ limit. We argue\nthat measurements generating perturbations larger than the threshold amplitudes\nappropriate for $\\e_m$ cannot realize measurement-based braiding in SM-SC\nheterostructures in the quasi-Majorana regime. We find that, if possible at\nall, quantum computation using measurement-based braiding in the quasi-Majorana\nregime would be plagued with errors introduced by the measurement processes\nthemselves, while such errors are significantly less likely in a scheme\ninvolving topological MZMs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Diwali is among the most important Indian festivals, and elaborate firework\ndisplays mark the evening's festivities. This study assesses the impact of\nDiwali on the concentration, composition, and sources of ambient PM2.5. We\nobserved the total PM2.5 concentrations to rise to 16 times the pre-firework\nlevels, while each of the elemental, organic, and black carbon fractions of\nambient PM2.5 increased by a factor of 46.1, 3.7, and 5.6, respectively. The\nconcentration of species like K, Al, Sr, Ba, S, and Bi displayed distinct peaks\nduring the firework event and were identified as tracers. The average\nconcentrations of potential carcinogens, like As, exceeded US EPA screening\nlevels for industrial air by a factor of ~9.6, while peak levels reached up to\n16.1 times the screening levels. The source apportionment study, undertaken\nusing positive matrix factorization, revealed the fireworks to account for 95%\nof the total elemental PM2.5 during Diwali. The resolved primary organic\nemissions, too, were enhanced by a factor of 8 during Diwali. Delhi has\nencountered serious haze events following Diwali in recent years; this study\nhighlights that biomass burning emissions rather than the fireworks drive the\npoor air quality in the days following Diwali.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Fuzz testing proved its great effectiveness in finding software bugs in the\nlatest years, however, there are still open challenges. Coverage-guided fuzzers\nsuffer from the fact that covering a program point does not ensure the trigger\nof a fault. Other more sensitive techniques that in theory should cope with\nthis problem, such as the coverage of the memory values, easily lead to path\nexplosion. In this thesis, we propose a new feedback for Feedback-driven Fuzz\ntesting that combines code coverage with the \"shape\" of the data. We learn\nlikely invariants for each basic block in order to divide into regions the\nspace described by the variables used in the block. The goal is to distinguish\nin the feedback when a block is executed with values that fall in different\nregions of the space. This better approximates the program state coverage and,\non some targets, improves the ability of the fuzzer in finding faults. We\ndeveloped a prototype using LLVM and AFL++ called InvsCov.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  High-flux entangled photon source is the key resource for quantum optical\nstudy and application. Here it is realized in a lithium niobate on isolator\n(LNOI) chip, with 2.79*10^11 Hz/mW photon pair rate and 1.53*10^9 Hz/nm/mW\nspectral brightness. These data are boosted by over two orders of magnitude\ncompared to existing technologies. A 130-nm broad bandwidth is engineered for\n8-channel multiplexed energy-time entanglement. Harnessed by high-extinction\nfrequency correlation and Franson interferences up to 99.17% visibility, such\nenergy-time entanglement multiplexing further enhances high-flux data rate, and\nwarrants broad applications in quantum information processing on a chip.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study Lagrangian cobordisms with the tools provided by Lagrangian quantum\nhomology. In particular, we develop the theory for the setting of Lagrangian\ncobordisms or Lagrangians with cylindrical ends in a Lefschetz fibration, and\nput the different versions of the quantum homology groups into relation by a\nlong exact sequence. We prove various practical relations of maps in this long\nexact sequence and we extract invariants that generalize the notion of\ndiscriminants to Lagrangian cobordisms in Lefschetz fibrations. We prove\nresults on the relation of the discriminants of the ends of a cobordism and the\ncobordism itself. We also give examples arising from Lagrangian spheres and\nrelate the discriminant to open Gromov Witten invariants. We show that for some\nconfigurations of Lagrangian spheres the discriminant always vanishes. We study\na set of examples that arise from Lefschetz pencils of complex quadric\nhypersurfaces of the complex projective space. These quadrics are symplectic\nmanifolds endowed with real structures and their real part are the Lagrangians\nof interest. Using the results established in this thesis, we compute the\ndiscriminants of all these Lagrangians by reducing the calculation to the\npreviously established case of a real Lagrangian sphere in the quadric.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this article, we describe the system that we used for the memotion\nanalysis challenge, which is Task 8 of SemEval-2020. This challenge had three\nsubtasks where affect based sentiment classification of the memes was required\nalong with intensities. The system we proposed combines the three tasks into a\nsingle one by representing it as multi-label hierarchical classification\nproblem.Here,Multi-Task learning or Joint learning Procedure is used to train\nour model.We have used dual channels to extract text and image based features\nfrom separate Deep Neural Network Backbone and aggregate them to create task\nspecific features. These task specific aggregated feature vectors ware then\npassed on to smaller networks with dense layers, each one assigned for\npredicting one type of fine grain sentiment label. Our Proposed method show the\nsuperiority of this system in few tasks to other best models from the\nchallenge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work we present a strategic network formation model predicting the\nemergence of multigroup structures. Individuals decide to form or remove links\nbased on the benefits and costs those connections carry; we focus on bilateral\nconsent for link formation. An exogenous system specifies the frequency of\ncoordination issues arising among the groups. We are interested in structures\nthat arise to resolve coordination issues and, specifically, structures in\nwhich groups are linked through bridging, redundant, and co-membership\ninterconnections. We characterize the conditions under which certain structures\nare stable and study their efficiency as well as the convergence of formation\ndynamics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper computes the Dirac index of all the weakly fair\n$A_{\\mathfrak{q}}(\\lambda)$ modules of $U(p, q)$. Although counter-examples\nhave been found to a conjecture of Vogan on the unitary dual of $U(p, q)$\nphrased by Trapa in 2001, we believe that any irreducible unitary\nrepresentation of $U(p, q)$ with non-zero Dirac cohomology must be a weakly\nfair $A_{\\mathfrak{q}}(\\lambda)$ module.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Active learning emerged as an alternative to alleviate the effort to label\nhuge amount of data for data hungry applications (such as image/video indexing\nand retrieval, autonomous driving, etc.). The goal of active learning is to\nautomatically select a number of unlabeled samples for annotation (according to\na budget), based on an acquisition function, which indicates how valuable a\nsample is for training the model. The learning loss method is a task-agnostic\napproach which attaches a module to learn to predict the target loss of\nunlabeled data, and select data with the highest loss for labeling. In this\nwork, we follow this strategy but we define the acquisition function as a\nlearning to rank problem and rethink the structure of the loss prediction\nmodule, using a simple but effective listwise approach. Experimental results on\nfour datasets demonstrate that our method outperforms recent state-of-the-art\nactive learning approaches for both image classification and regression tasks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper is concerned with a family of Reaction-Diffusion systems that we\nintroduced in [15], and that generalizes the SIR type models from epidemiology.\nSuch systems are now also used to describe collective behaviors.In this paper,\nwe propose a modeling approach for these apparently diverse phenomena through\nthe example of the dynamics of social unrest. The model involves two\nquantities: the level of social unrest, or more generally activity, u, and a\nfield of social tension v, which play asymmetric roles. We think of u as the\nactually observed or explicit quantity while v is an ambiant, sometimes\nimplicit, field of susceptibility that modulates the dynamics of u. In this\narticle, we explore this class of model and prove several theoretical results\nbased on the framework developed in [15], of which the present work is a\ncompanion paper. We particularly emphasize here two subclasses of systems:\ntension inhibiting and tension enhancing. These are characterized by\nrespectively a negative or a positivefeedback of the unrest on social tension.\nWe establish several properties for these classes and also study some\nextensions. In particular, we describe the behavior of the system following an\ninitial surge of activity. We show that the model can give rise to many diverse\nqualitative dynamics. We also provide a variety of numerical simulations to\nillustrate our results and to reveal further properties and open questions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We define an equivalence relation among coherent sheaves on a projective\nvariety called biliaison. We prove the existence of sheaves that are minimal in\na biliaison class in a suitable sense, and show that all sheaves in the same\nclass can be obtained from a minimal one using certain deformations and other\nbasic moves. Our results generalize the main theorems of liaison theory of\nsubvarieties to sheaves, and provide a framework to study sheaves and\nsubvarieties simultaneously.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Thermodynamics is accepted as a universal truth, encompassing all macroscopic\nobjects. Therefore, it is surprising to find that, within our current\nunderstanding, the photovoltaic effect has so far eluded the first and second\nlaws of thermodynamics. The inconsistency emerges from the fact that\nphotovoltaics obey a distinct law of detailed balance1. Since radiative\nprocesses depend on only two independent variables that are the chemical\npotential and the temperature, the detailed balance, and the two laws of\nthermodynamics cannot be mutually solved. In this work, we resolve this\nincompatibility by proposing that the system is controlled by yet a third\nindependent variable, which is related to the emissivity. This unification not\nonly advances our fundamental understanding of light-matter interactions but,\nperhaps more importantly, allows us to assess the limiting factors of advanced\nphotovoltaic concepts designed for elevated temperatures. These include\nthermophotovoltaics2, thermoradiative and thermophotonic solar power\nconversion, and radiative cooling, which are instrumental in our ability to\ndevelop advanced renewable energy technologies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We calculate the low-energy constant $L_{10}$ in a two-representation SU(4)\nlattice gauge theory that is close to a composite-Higgs model. From this we\nobtain the contribution of the new strong sector to the $S$ parameter. This\nleads to an upper bound on the vacuum misalignment parameter $\\xi$ which is\nsimilar to current estimates of this bound. Our result agrees with large-$N_c$\nscaling expectations, within large systematic uncertainties.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We discuss a joint microscopic theory for the laser-induced magnetization\ndynamics and spin transport in magnetic heterostructures based on the $s$-$d$\ninteraction. Angular momentum transfer is mediated by scattering of itinerant\n$s$ electrons with the localized ($d$ electron) spins. We use the corresponding\nrate equations and focus on a spin one-half $d$ electron system, leading to a\nsimplified analytical expression for the dynamics of the local magnetization\nthat is coupled to an equation for the non-equilibrium spin accumulation of the\n$s$ electrons. We show that this description converges to the microscopic\nthree-temperature model in the limit of a strong $s$-$d$ coupling. The equation\nfor the spin accumulation is used to introduce diffusive spin transport. The\npresented numerical solutions show that during the laser-induced\ndemagnetization in a ferromagnetic metal a short-lived spin accumulation is\ncreated that counteracts the demagnetization process. Moreover, the spin\naccumulation leads to the generation of a spin current at the interface of a\nferromagnetic and non-magnetic metal. Depending on the specific magnetic\nsystem, both local spin dissipation and interfacial spin transport are able to\nenhance the demagnetization rate by providing relaxation channels for the spin\naccumulation that is build up during demagnetization in the ferromagnetic\nmaterial.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In two recent papers, we used the hadro-quarkonium model to study the\nproperties of hidden-charm and bottom tetraquarks and pentaquarks. Here, we\nextend the previous results and calculate the masses of\nheavy-quarkonium-kaon/hyperon systems. We also compute the spectrum of\nhidden-charm and bottom tetraquarks with strangeness in the compact tetraquark\n(diquark-antidiquark) model. If heavy-light exotic systems with non-null\nstrangeness content were to be observed experimentally, it might be possible to\ndistinguish among the large variety of available theoretical pictures for\ntetra- and pentaquark states and, possibly, rule out those which are not\ncompatible with the data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In a power system, unlike some critical and standalone assets that are\nequipped with condition monitoring devices, the conditions of most regular\nin-group assets are acquired through periodic inspection work. Due to their\nlarge quantities, significant amount of manual inspection effort and sometimes\ndata management issues, it is not uncommon to see the asset condition data in a\ntarget study area is unavailable or incomplete. Lack of asset condition data\nundermines the reliability assessment work. To solve this data problem and\nenhance data availability, this paper explores an unconventional\nmethod-generating numerical and non-numerical asset condition data based on\ncondition degradation, condition correlation and categorical distribution\nmodels. Empirical knowledge from human experts can also be incorporated in the\nmodeling process. Also, a probabilistic diversification step can be taken to\nmake the generated numerical condition data probabilistic. This method can\ngenerate close-to-real asset condition data and has been validated\nsystematically based on two public datasets. An area reliability assessment\nexample based on cables is given to demonstrate the usefulness of this method\nand its generated data. This method can also be used to conveniently generate\nhypothetical asset condition data for research purposes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report broad-band (1.2 - 1.9 GHz) radio continuum observations at\narcminute resolutions of two nearby disk galaxies, NGC 1808 and NGC 1097, and\nfour AGN powered radio sources; PKS B1934-638, PKS B0407-658, J0240-231, and\nJ0538-440. We use Rotation Measure Synthesis to analyze their Faraday\ncomplexity. Observations were made with the KAT\\,7 radio telescope array, in\nSouth Africa. The AGN powered sources fall into two \"Faraday\" categories --\nsimple and complex. The most polarized sources, J0538-440 and J0240-231, are\nfound to have complex Faraday spectra that can be time variable (J0538-440\ncase) and also indicative of complex Faraday emitting and rotating components\nalong the line of sight. PKS B0407-658 shows a simple Faraday spectrum while\nPKS B1934-638 is undetected in polarization. The disk galaxies are classified\nas complex, albeit at low signal-to-noise. This may indicate depolarization due\nto turbulence of the magnetised plasma in the bar and circumnuclear regions\nand/or frequency-dependent depolarization at L-band\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A quantum receiver is an essential element of quantum illumination (QI) which\noutperforms its classical counterpart, called classical-illumination (CI).\nHowever, there are only few proposals for realizable quantum receiver, which\nexploits nonlinear effects leading to increasing the complexity of receiver\nsetups. To compensate this, in this article, we design a quantum receiver with\nlinear optical elements for Gaussian QI. Rather than exploiting nonlinear\neffect, our receiver consists of a 50:50 beam splitter and homodyne detection.\nUsing double homodyne detection after the 50:50 beam splitter, we analyze the\nperformance of the QI in different regimes of target reflectivity, source\npower, and noise level. We show that our receiver has better signal-to-noise\nratio and more robust against noise than the existing simple-structured\nreceivers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent work showed that stabilizing affine control systems to desired (sets\nof) states while optimizing quadratic costs and observing state and control\nconstraints can be reduced to quadratic programs (QP) by using control barrier\nfunctions (CBF) and control Lyapunov functions. In our own recent work, we\ndefined high order CBFs (HOCBFs) to accommodating systems and constraints with\narbitrary relative degrees, and a penalty method to increase the feasibility of\nthe corresponding QPs. In this paper, we introduce adaptive CBF (AdaCBFs) that\ncan accommodate time-varying control bounds and dynamics noise, and also\naddress the feasibility problem. Central to our approach is the introduction of\npenalty functions in the definition of an AdaCBF and the definition of\nauxiliary dynamics for these penalty functions that are HOCBFs and are\nstabilized by CLFs. We demonstrate the advantages of the proposed method by\napplying it to a cruise control problem with different road surfaces, tires\nslipping, and dynamics noise.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep neural networks have attained remarkable performance when applied to\ndata that comes from the same distribution as that of the training set, but can\nsignificantly degrade otherwise. Therefore, detecting whether an example is\nout-of-distribution (OoD) is crucial to enable a system that can reject such\nsamples or alert users. Recent works have made significant progress on OoD\nbenchmarks consisting of small image datasets. However, many recent methods\nbased on neural networks rely on training or tuning with both in-distribution\nand out-of-distribution data. The latter is generally hard to define a-priori,\nand its selection can easily bias the learning. We base our work on a popular\nmethod ODIN, proposing two strategies for freeing it from the needs of tuning\nwith OoD data, while improving its OoD detection performance. We specifically\npropose to decompose confidence scoring as well as a modified input\npre-processing method. We show that both of these significantly help in\ndetection performance. Our further analysis on a larger scale image dataset\nshows that the two types of distribution shifts, specifically semantic shift\nand non-semantic shift, present a significant difference in the difficulty of\nthe problem, providing an analysis of when ODIN-like strategies do or do not\nwork.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The modern age of digital music access has increased the availability of data\nabout music consumption and creation, facilitating the large-scale analysis of\nthe complex networks that connect music together. Data about user streaming\nbehaviour, and the musical collaboration networks are particularly important\nwith new data-driven recommendation systems. Without thorough analysis, such\ncollaboration graphs can lead to false or misleading conclusions. Here we\npresent a new collaboration network of artists from the online music streaming\nservice Spotify, and demonstrate a critical change in the eigenvector\ncentrality of artists, as low popularity artists are removed. The critical\nchange in centrality, from classical artists to rap artists, demonstrates\ndeeper structural properties of the network. A Social Group Centrality model is\npresented to simulate this critical transition behaviour, and switching between\ndominant eigenvectors is observed. This model presents a novel investigation of\nthe effect of popularity bias on how centrality and importance are measured,\nand provides a new tool for examining such flaws in networks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The use of hybrid scheme combining the message passing programming models for\ninter-node parallelism and the shared memory programming models for node-level\nparallelism is widely spread. Existing extensive practices on hybrid Message\nPassing Interface (MPI) plus Open Multi-Processing (OpenMP) programming account\nfor its popularity. Nevertheless, strong programming efforts are required to\ngain performance benefits from the MPI+OpenMP code. An emerging hybrid method\nthat combines MPI and the MPI shared memory model (MPI+MPI) is promising.\nHowever, writing an efficient hybrid MPI+MPI program -- especially when the\ncollective communication operations are involved -- is not to be taken for\ngranted.\n  In this paper, we propose a new design method to implement hybrid MPI+MPI\ncontext-based collective communication operations. Our method avoids on-node\nmemory replications (on-node communication overheads) that are required by\nsemantics in pure MPI. We also offer wrapper primitives hiding all the design\ndetails from users, which comes with practices on how to structure hybrid\nMPI+MPI code with these primitives. The micro-benchmarks show that our\ncollectives are comparable or superior to those in pure MPI context. We have\nfurther validated the effectiveness of the hybrid MPI+MPI model (which uses our\nwrapper primitives) in three computational kernels, by comparison to the pure\nMPI and hybrid MPI+OpenMP models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We provide a unified approach to the three main non-compact models of random\ngeometry, namely the Brownian plane, the infinite-volume Brownian disk, and the\nBrownian half-plane. This approach allows us to investigate relations between\nthese models, and in particular to prove that complements of hulls in the\nBrownian plane are infinite-volume Brownian disks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This article illustrates the development of a software named GoldEnvSim for\nsimulation of the dispersion of radionuclides in the atmosphere. The software\nis written in JavaFX programming language to couple the Weather Research and\nForecasting (WRF) model and the FLEXPART-WRF model. The highlight function of\nthis software is to provide convenience for users to run a simulation workflow\nwith a user-friendly interface. Many toolkits for post-processing and\nvisualizing output are also incorporated to make this software more\ncomprehensive. At this first version, GoldEnvSim is specifically designed to\nanalyze and predict the dispersion of radioactive materials in the atmosphere,\nbut it has potential for further development and applicable to other fields of\nenvironmental science. For demonstration, a simulation of the dispersion of the\nCs-137 that is assumed to be released from the Fangchenggang nuclear power\nplant to whole Vietnam territory was performed. The simulation result on\nmeteorological in comparison to the monitoring data taken from a first-class\nmeteorological observatory was used to evaluate the accuracy of dispersion\nsimulation result.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this article we consider generalisation of Michelson contrast for positive\noperators of countably decomposable $W^*$-algebras and find a few several of\nits properties.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A deterministic multi-stage malaria model with a non-therapeutic control\nmeasure, the use of mosquito bednet is formulated and analyzed. The model basic\nreproduction number is derived, and analytical results show that the models\nequilibria are locally and globally asymptotically stable when certain\nthreshold conditions are satisfied. Pontryagin's Maximum Principle with respect\nto a time dependent constant is used to derive the necessary conditions for the\noptimal usage of the Long-Lasting Insecticide-treated bednets(LLINs) to\nmitigate the malaria transmission dynamics. This is accomplished by introducing\nbiologically admissible control and e-approximate sub-optimal control. The\nresults from this study could help public health planners and policy\ndecision-makers to design reachable and more practical malaria prevention\nprograms \"close\" to the optimal strategy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We experimentally study a fiber-based optical ring cavity integrated with a\nmechanical resonator mirror and an optical amplifier. The device exhibits a\nvariety of intriguing nonlinear effects including synchronization and\nself-excited oscillation. Passively generated optical pulses are observed when\nthe frequency of the optical ring cavity is tuned very close to the mechanical\nfrequency of the suspended mirror. The optical power at the threshold of this\nprocess of mechanical mode locking is found to be related to quantum noise of\nthe optical amplifier.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The vulnerability of deep networks to adversarial attacks is a central\nproblem for deep learning from the perspective of both cognition and security.\nThe current most successful defense method is to train a classifier using\nadversarial images created during learning. Another defense approach involves\ntransformation or purification of the original input to remove adversarial\nsignals before the image is classified. We focus on defending naturally-trained\nclassifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based\nModel (EBM) for adversarial purification. In contrast to adversarial training,\nour approach is intended to secure pre-existing and highly vulnerable\nclassifiers.\n  The memoryless behavior of long-run MCMC sampling will eventually remove\nadversarial signals, while metastable behavior preserves consistent appearance\nof MCMC samples after many steps to allow accurate long-run prediction.\nBalancing these factors can lead to effective purification and robust\nclassification. We evaluate adversarial defense with an EBM using the strongest\nknown attacks against purification. Our contributions are 1) an improved method\nfor training EBM's with realistic long-run MCMC samples, 2) an\nExpectation-Over-Transformation (EOT) defense that resolves theoretical\nambiguities for stochastic defenses and from which the EOT attack naturally\nfollows, and 3) state-of-the-art adversarial defense for naturally-trained\nclassifiers and competitive defense compared to adversarially-trained\nclassifiers on Cifar-10, SVHN, and Cifar-100. Code and pre-trained models are\navailable at https://github.com/point0bar1/ebm-defense.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent deep neural networks based techniques, especially those equipped with\nthe ability of self-adaptation in the system level such as deep reinforcement\nlearning (DRL), are shown to possess many advantages of optimizing robot\nlearning systems (e.g., autonomous navigation and continuous robot arm\ncontrol.) However, the learning-based systems and the associated models may be\nthreatened by the risks of intentionally adaptive (e.g., noisy sensor\nconfusion) and adversarial perturbations from real-world scenarios. In this\npaper, we introduce timing-based adversarial strategies against a DRL-based\nnavigation system by jamming in physical noise patterns on the selected time\nframes. To study the vulnerability of learning-based navigation systems, we\npropose two adversarial agent models: one refers to online learning; another\none is based on evolutionary learning. Besides, three open-source robot\nlearning and navigation control environments are employed to study the\nvulnerability under adversarial timing attacks. Our experimental results show\nthat the adversarial timing attacks can lead to a significant performance drop,\nand also suggest the necessity of enhancing the robustness of robot learning\nsystems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this note, we study non-transitive graphs and prove a number of results\nwhen they satisfy a coarse version of transitivity. Also, for each finitely\ngenerated group $G$, we produce continuum many pairwise non-quasi-isometric\nregular graphs that have the same growth rate, number of ends, and asymptotic\ndimension as $G$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We give necessary conditions satisfied by the set of odd prime divisors of\nbinary perfect polynomials. This allows us to get a new characterization of all\nthe known perfect binary polynomials.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Biracks and biquandles, which are useful for studying the knot theory, are\nspecial families of solutions of the set-theoretic Yang-Baxter equation. A\nhomology theory for the set-theoretic Yang-Baxter equation was developed by\nCarter, Elhamdadi, and Saito in order to construct knot invariants. In this\npaper, we construct a normalized (co)homology theory of a set-theoretic\nsolution of the Yang-Baxter equation. We obtain some concrete examples of\nnon-trivial $n$-cocycles for Alexander biquandles. For a biquandle $X,$ its\ngeometric realization $BX$ is discussed, which has the potential to build\ninvariants of links and knotted surfaces. In particular, we demonstrate that\nthe second homotopy group of $BX$ is finitely generated if the biquandle $X$ is\nfinite.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We intend to study a new class of cosmological models in $f(R, T)$ modified\ntheories of gravity, hence define the cosmological constant $\\Lambda$ as a\nfunction of the trace of the stress energy-momentum-tensor $T$ and the Ricci\nscalar $R$, and name such a model \"$\\Lambda(R, T)$ gravity\" where we have\nspecified a certain form of $\\Lambda(R, T)$. $\\Lambda(R, T)$ is also defined in\nthe perfect fluid and dust case. Some physical and geometric properties of the\nmodel are also discussed. The pressure, density and energy conditions are\nstudied both when $\\Lambda$ is a positive constant and when\n$\\Lambda=\\Lambda(t)$, i.e a function of cosmological time, t. We study the\nbehaviour of some cosmological quantities such as Hubble and deceleration\nparameters. The model is innovative in the sense that it has been described in\nterms of both $R$ and $T$ and display a better understanding of the\ncosmological observations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The present work is devoted to the modelling which is based on the modified\nCahn-Hilliard equation, the interplay of equilibrium and non-equilibrium phase\ntransitions. The non-equilibrium phase transitions are modelled by the\nSchl\\\"ogl reactions systems. We consider the advancing fronts which combine\nthese both transitions. The traveling wave solutions are obtained; the\nconditions of their existence and dependence on the parameters of the models\nare studied in detail. The possibility of the existance of non-equilibrated\nphase is discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Augmentation of disease diagnosis and decision-making in healthcare with\nmachine learning algorithms is gaining much impetus in recent years. In\nparticular, in the current epidemiological situation caused by COVID-19\npandemic, swift and accurate prediction of disease diagnosis with machine\nlearning algorithms could facilitate identification and care of vulnerable\nclusters of population, such as those having multi-morbidity conditions. In\norder to build a useful disease diagnosis prediction system, advancement in\nboth data representation and development of machine learning architectures are\nimperative. First, with respect to data collection and representation, we face\nsevere problems due to multitude of formats and lack of coherency prevalent in\nElectronic Health Records (EHRs). This causes hindrance in extraction of\nvaluable information contained in EHRs. Currently, no universal global data\nstandard has been established. As a useful solution, we develop and publish a\nPython package to transform public health dataset into an easy to access\nuniversal format. This data transformation to an international health data\nformat facilitates researchers to easily combine EHR datasets with clinical\ndatasets of diverse formats. Second, machine learning algorithms that predict\nmultiple disease diagnosis categories simultaneously remain underdeveloped. We\npropose two novel model architectures in this regard. First, DeepObserver,\nwhich uses structured numerical data to predict the diagnosis categories and\nsecond, ClinicalBERT_Multi, that incorporates rich information available in\nclinical notes via natural language processing methods and also provides\ninterpretable visualizations to medical practitioners. We show that both models\ncan predict multiple diagnoses simultaneously with high accuracy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Bipartite graph embedding has recently attracted much attention due to the\nfact that bipartite graphs are widely used in various application domains. Most\nprevious methods, which adopt random walk-based or reconstruction-based\nobjectives, are typically effective to learn local graph structures. However,\nthe global properties of bipartite graph, including community structures of\nhomogeneous nodes and long-range dependencies of heterogeneous nodes, are not\nwell preserved. In this paper, we propose a bipartite graph embedding called\nBiGI to capture such global properties by introducing a novel local-global\ninfomax objective. Specifically, BiGI first generates a global representation\nwhich is composed of two prototype representations. BiGI then encodes sampled\nedges as local representations via the proposed subgraph-level attention\nmechanism. Through maximizing the mutual information between local and global\nrepresentations, BiGI enables nodes in bipartite graph to be globally relevant.\nOur model is evaluated on various benchmark datasets for the tasks of top-K\nrecommendation and link prediction. Extensive experiments demonstrate that BiGI\nachieves consistent and significant improvements over state-of-the-art\nbaselines. Detailed analyses verify the high effectiveness of modeling the\nglobal properties of bipartite graph.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions or raindrops, from a short sequence of\nimages captured by a moving camera. Our method leverages the motion differences\nbetween the background and the obstructing elements to recover both layers.\nSpecifically, we alternate between estimating dense optical flow fields of the\ntwo layers and reconstructing each layer from the flow-warped images via a deep\nconvolutional neural network. The learning-based layer reconstruction allows us\nto accommodate potential errors in the flow estimation and brittle assumptions\nsuch as brightness consistency. We show that training on synthetically\ngenerated data transfers well to real images. Our results on numerous\nchallenging scenarios of reflection and fence removal demonstrate the\neffectiveness of the proposed method.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Within the coronae of stars, abundances of those elements with low first\nionization potential (FIP) often differ from their photospheric values. The\ncoronae of the Sun and solar-type stars mostly show enhancements of low- FIP\nelements (the FIP effect) while more active stars such as M-dwarfs have coronae\ngenerally characterized by the inverse-FIP effect (I-FIP). Here we observe\npatches of I-FIP effect solar plasma in AR 12673, a highly complex\nbeta/gamma/delta active region. We argue that the umbrae of coalescing sunspots\nand more specifically strong light bridges within the umbrae, are preferential\nlocations for observing I-FIP effect plasma. Furthermore, the magnetic\ncomplexity of the active region and major episodes of fast flux emergence also\nlead to repetitive and intense flares. The induced evaporation of the\nchromospheric plasma in flare ribbons crossing umbrae enables the observation\nof four localized patches of I-FIP effect plasma in the corona of AR 12673.\nThese observations can be interpreted in the context of the ponderomotive force\nfractionation model which predicts that plasma with I-FIP effect composition is\ncreated by the refraction of waves coming from below the chromosphere. We\npropose that the waves generating the I-FIP effect plasma in solar active\nregions are generated by sub-photospheric reconnection of coalescing flux\nsystems. Although we only glimpse signatures of I-FIP effect fractionation\nproduced by this interaction in patches on the Sun, on highly active M-stars it\nmay be the dominant process.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Much work on social media opinion polarization focuses on a flat\ncategorization of stances (or orthogonal beliefs) of different communities from\nmedia traces. We extend in this work in two important respects. First, we\ndetect not only points of disagreement between communities, but also points of\nagreement. In other words, we estimate community beliefs in the presence of\noverlap. Second, in lieu of flat categorization, we consider hierarchical\nbelief estimation, where communities might be hierarchically divided. For\nexample, two opposing parties might disagree on core issues, but within a\nparty, despite agreement on fundamentals, disagreement might occur on further\ndetails. We call the resulting combined problem a hierarchical overlapping\nbelief estimation problem. To solve it, this paper develops a new class of\nunsupervised Non-negative Matrix Factorization (NMF) algorithms, we call Belief\nStructured Matrix Factorization (BSMF). Our proposed unsupervised algorithm\ncaptures both the latent belief intersections and dissimilarities, as well as a\nhierarchical structure. We discuss the properties of the algorithm and evaluate\nit on both synthetic and real-world datasets. In the synthetic dataset, our\nmodel reduces error by 40%. In real Twitter traces, it improves accuracy by\naround 10%. The model also achieves 96.08% self-consistency in a sanity check.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Saturn's moon Titan is the only extraterrestrial body known to host stable\nlakes and a hydrological cycle. Titan's lakes predominantly contain liquid\nmethane, ethane, and nitrogen, with methane evaporation driving its\nhydrological cycle. Molecular interactions between these three species lead to\nnon-ideal behavior that causes Titan's lakes to behave differently than Earth's\nlakes. Here, we numerically investigate how methane evaporation and non-ideal\ninteractions affect the physical properties, structure, dynamics, and evolution\nof shallow lakes on Titan. We find that, under certain temperature regimes,\nmethane-rich mixtures are denser than relatively ethane-rich mixtures. This\nallows methane evaporation to stratify Titan's lakes into ethane-rich upper\nlayers and methane-rich lower layers, separated by a strong compositional\ngradient. At temperatures above 86K, lakes remain well-mixed and unstratified.\nBetween 84 and 86K, lakes can stratify episodically. Below 84K, lakes\npermanently stratify, and develop very methane-depleted epilimnia. Despite\nsmall seasonal and diurnal deviations (<5K) from typical surface temperatures,\nTitan's rain-filled ephemeral lakes and \"phantom lakes\" may nevertheless\nexperience significantly larger temperature fluctuations, resulting in\npolymictic or even meromictic stratification, which may trigger ethane ice\nprecipitation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Citation count prediction is the task of predicting the number of citations a\npaper has gained after a period of time. Prior work viewed this as a static\nprediction task. As papers and their citations evolve over time, considering\nthe dynamics of the number of citations a paper will receive would seem\nlogical. Here, we introduce the task of sequence citation prediction. The goal\nis to accurately predict the trajectory of the number of citations a scholarly\nwork receives over time. We propose to view papers as a structured network of\ncitations, allowing us to use topological information as a learning signal.\nAdditionally, we learn how this dynamic citation network changes over time and\nthe impact of paper meta-data such as authors, venues and abstracts. To\napproach the new task, we derive a dynamic citation network from Semantic\nScholar spanning over 42 years. We present a model which exploits topological\nand temporal information using graph convolution networks paired with sequence\nprediction, and compare it against multiple baselines, testing the importance\nof topological and temporal information and analyzing model performance. Our\nexperiments show that leveraging both the temporal and topological information\ngreatly increases the performance of predicting citation counts over time.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent surveys of protoplanetary disks show that substructure in dust thermal\ncontinuum emission maps is common in protoplanetary disks. These substructures,\nmost prominently rings and gaps, shape and change the chemical and physical\nconditions of the disk, along with the dust size distributions. In this work,\nwe use a thermochemical code to focus on the chemical evolution that is\noccurring within the gas-depleted gap and the dust-rich ring often observed\nbehind it. The composition of these spatial locations are of great import, as\nthe gas and ice-coated grains will end up being part of the atmospheres of gas\ngiants and/or the seeds of rocky planets. Our models show that the dust\ntemperature at the midplane of the gap increases, enough to produce local\nsublimation of key volatiles and pushing the molecular layer closer to the\nmidplane, while it decreases in the dust-rich ring, causing a higher volatile\ndeposition onto the dust grain surfaces. Further, the ring itself presents a\nfreeze-out trap for volatiles in local flows powered by forming planets,\nbecoming a site of localized volatile enhancement. Within the gas depleted gap,\nthe line emission depends on several different parameters, such as: the depth\nof the gap in surface density, the location of the dust substructure, and the\nabundance of common gas tracers, such as CO. In order to break this uncertainty\nbetween abundance and surface density, other methods such as disk kinematics,\nbecome necessary to constrain the disk structure and its chemical evolution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this article, we completely classify torus bundles over the circle that\nbound 4-manifolds with the rational homology of the circle. Along the way, we\nclassify certain integral surgeries along chain links that bound rational\nhomology balls and explore a connection to 3-braid closures whose double\nbranched covers bound rational homology 4-balls.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We calculate the automorphism group of certain Enriques surfaces. The\nEnriques surfaces that we investigate include very general $n$-nodal Enriques\nsurfaces and very general cuspidal Enriques surfaces. We also describe the\naction of the automorphism group on the set of smooth rational curves and on\nthe set of elliptic fibrations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ad hoc network is a collection of different types of nodes, which are\nconnected in heterogeneous or homogeneous manner. It is also known as\nself-organizing-wireless network. The dynamic nature of ad hoc networks make\nthem more attractive, which is used in many different applications. Every coin\nhas two sides: one is the advantage part and other is disadvantages, in the\nsame manner nature of ad hoc network make it more attractive from one side in\nother hand there are some issues too. Energy efficiency is a core factor which\neffects on ad hoc network in terms of battery life, throughput, overhead of\nmessages, transmission error. For solving issues of energy constraints,\ndifferent mechanisms are proposed by various researchers. In this paper, we\nsurvey various existing schemes which attempt to improve energy efficiency of\ndifferent types of ad hoc routing protocol to increase network lifetime.\nFurthermore we outline future scope of these existing schemes which may help\nresearches to carry out further research in this direction.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In recent years, much research effort on recommendation has been devoted to\nmining user behaviors, i.e., collaborative filtering, along with the general\ninformation which describes users or items, e.g., textual attributes,\ncategorical demographics, product images, and so on. Price, an important factor\nin marketing --- which determines whether a user will make the final purchase\ndecision on an item --- surprisingly, has received relatively little scrutiny.\n  In this work, we aim at developing an effective method to predict user\npurchase intention with the focus on the price factor in recommender systems.\nThe main difficulties are two-fold: 1) the preference and sensitivity of a user\non item price are unknown, which are only implicitly reflected in the items\nthat the user has purchased, and 2) how the item price affects a user's\nintention depends largely on the product category, that is, the perception and\naffordability of a user on item price could vary significantly across\ncategories. Towards the first difficulty, we propose to model the transitive\nrelationship between user-to-item and item-to-price, taking the inspiration\nfrom the recently developed Graph Convolution Networks (GCN). The key idea is\nto propagate the influence of price on users with items as the bridge, so as to\nmake the learned user representations be price-aware. For the second\ndifficulty, we further integrate item categories into the propagation progress\nand model the possible pairwise interactions for predicting user-item\ninteractions. We conduct extensive experiments on two real-world datasets,\ndemonstrating the effectiveness of our GCN-based method in learning the\nprice-aware preference of users. Further analysis reveals that modeling the\nprice awareness is particularly useful for predicting user preference on items\nof unexplored categories.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Measuring risk is at the center of modern financial risk management. As the\nworld economy is becoming more complex and standard modeling assumptions are\nviolated, the advanced artificial intelligence solutions may provide the right\ntools to analyze the global market. In this paper, we provide a novel approach\nfor measuring market risk called Encoded Value-at-Risk (Encoded VaR), which is\nbased on a type of artificial neural network, called Variational Auto-encoders\n(VAEs). Encoded VaR is a generative model which can be used to reproduce market\nscenarios from a range of historical cross-sectional stock returns, while\nincreasing the signal-to-noise ratio present in the financial data, and\nlearning the dependency structure of the market without any assumptions about\nthe joint distribution of stock returns. We compare Encoded VaR out-of-sample\nresults with eleven other methods and show that it is competitive to many other\nwell-known VaR algorithms presented in the literature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The retail industry is already facing an inevitable transformation worldwide,\nand with the current pandemic situation, it is even accelerating. Indeed,\nconsumer habits are shifting from brick-and-mortar stores to online shopping.\nThe bottleneck in the end-to-end online shopping experience remains the\nefficient and quick delivery of goods to consumers. In this context, unmanned\naerial vehicle (UAV) technology is seen as a potential solution to address\ncargo delivery issues. Hence, the number of cargo-UAVs is expected to skyrocket\nin the next few decades and the airspace to become densely crowded. To\nsuccessfully deploy UAVs for mass cargo delivery, seamless and reliable\ncellular connectivity for highly mobile UAVs is required. There is an urgent\nneed for organized and connected routes in the sky. Like highways for cargo\ntrucks, 3D routes in the airspace should be designed for cargo-UAVs to fulfill\ntheir operations safely and efficiently. We refer to these routes as 3D aerial\nhighway. In this paper, we thoroughly investigate the feasibility of the aerial\nhighways paradigm. First, we discuss the motivations and concerns of the aerial\nhighway paradigm. Then, we present our vision of the 3D aerial highway\nframework. Finally, we present related connectivity issues and their potential\nsolutions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We recast Byerly's formula for integrals of products of Legendre polynomials.\nThen we adopt the idea to the case of Jacobi polynomials. After that, we use\nthe formula to derive an asymptotic formula for integrals of products of Jacobi\npolynomials. The asymptotic formula is similar to an analogous one recently\nobtained by the first author and Jeff Geronimo for a different case. Thus, it\nsuggests that such an asymptotic behavior is rather generic for integrals of\nproducts of orthogonal polynomials.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we study population protocols governed by the {\\em random\nscheduler}, which uniformly at random selects pairwise interactions between $n$\nagents. The main result of this paper is the first time and space optimal {\\em\nexact majority population protocol} which also works with high probability. The\nnew protocol operates in the optimal {\\em parallel time} $O(\\log n),$ which is\nequivalent to $O(n\\log n)$ sequential {\\em pairwise interactions}, where each\nagent utilises the optimal number of $O(\\log n)$ states.\n  The time optimality of the new majority protocol is possible thanks to the\nnovel concept of fixed-resolution phase clocks introduced and analysed in this\npaper. The new phase clock allows to count approximately constant parallel time\nin population protocols.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We calculate the motion of binary mass systems in gravity up to the sixth\npost--Newtonian order to the $G_N^3$ terms ab initio using momentum expansions\nwithin an effective field theory approach based on Feynman amplitudes in\nharmonic coordinates. For these contributions we construct a canonical\ntransformation to isotropic and to EOB coordinates at 5PN and agree with the\nresults in the literature \\cite{Bern:2019nnu,Damour:2019lcq}. At 6PN we compare\nto the Hamiltonians in isotropic coordinates either given in\n\\cite{Bern:2019nnu} or resulting from the scattering angle. We find a canonical\ntransformation from our Hamiltonian in harmonic coordinates to\n\\cite{Bern:2019nnu}, but not to \\cite{Damour:2019lcq}. This implies that we\nalso agree on all observables with \\cite{Bern:2019nnu} to the sixth\npost--Newtonian order to $G_N^3$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Beamforming is evidently a core technology in recent generations of mobile\ncommunication networks. Nevertheless, an iterative process is typically\nrequired to optimize the parameters, making it ill-placed for real-time\nimplementation due to high complexity and computational delay. Heuristic\nsolutions such as zero-forcing (ZF) are simpler but at the expense of\nperformance loss. Alternatively, deep learning (DL) is well understood to be a\ngeneralizing technique that can deliver promising results for a wide range of\napplications at much lower complexity if it is sufficiently trained. As a\nconsequence, DL may present itself as an attractive solution to beamforming. To\nexploit DL, this article introduces general data- and model-driven beamforming\nneural networks (BNNs), presents various possible learning strategies, and also\ndiscusses complexity reduction for the DL-based BNNs. We also offer enhancement\nmethods such as training-set augmentation and transfer learning in order to\nimprove the generality of BNNs, accompanied by computer simulation results and\ntestbed results showing the performance of such BNN solutions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A luminous body embedded in an accretion disk can generate asymmetric density\nperturbations that lead to a net torque and thus orbital migration of the body.\nLinear theory has shown that this heating torque gives rise to a migration term\nlinear in the body's mass that can oppose or even reverse that arising from the\nsum of gravitational Lindblad and co-orbital torques. We use high-resolution\nlocal simulations in an unstratified disk to assess the accuracy and domain of\napplicability of the linear theory. We find agreement between analytic and\nsimulation results to better than 10\\% in the appropriate regime (low\nluminosity, low thermal conductivity), but measure deviations in the non-linear\n(high luminosity) regime and in the high thermal conductivity regime. In the\nnon-linear regime, linear theory overpredicts the acceleration due to the\nheating torque, which we find to be due to the neglect of non-linear terms in\nthe heat flux. In the high thermal conductivity regime linear theory\nunderpredicts the acceleration, although here both non-linear and computational\nconstraints play a role. We discuss the impact of the heating torque for the\nevolution of low-mass planets in protoplanetary disks, and for massive stars or\naccreting compact objects embedded in AGN disks. For the latter case, we show\nthat the thermal torque is likely to be the dominant physical effect at disk\nradii where the optical depth drops below a critical value.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The distribution of Coxeter descents and block number over the set of fully\ncommutative elements in the hyperoctahedral group $B_n$, $\\FC(B_n)$, is studied\nin this paper. We prove that the associated Chow quasi-symmetric generating\nfunction is equal to a non-negative sum of products of two Schur functions. The\nproof involves a decomposition of $\\FC(B_n)$ into a disjoint union of two-sided\nBarbash-Vogan combinatorial cells, a type $B$ extension of Rubey's descent\npreserving involution on $321$-avoiding permutations and a detailed study of\nthe intersection of $\\FC(B_n)$ with $S_n$-cosets which yields a new\ndecomposition of $\\FC(B_n)$ into disjoint subsets called fibers. We also\ncompare two different type $B$ Schur-positivity notions, arising from works of\nChow and Poirier\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Crystals of the chemical elements manganese, tellurium, and selenium can show\nthe effects of handedness. In order to sense the possible effects of a changing\nsense of chirality on the properties of samples from these elements, the\npotential presence of two, enantiomorphic, physically different, variants of\nthese elemental crystal structures needs to be resolved in crystallographic\nanalyses. Due to fundamental limitations of kinematical X-ray scattering in\ncrystals, however, the effects of chirality in single-element crystals are very\ndifficult to sense using standard X-ray diffraction techniques. In the present\npaper, we show that dynamical Kikuchi diffraction in the scanning electron\nmicroscope is sensitive to the local sense of chirality in crystals of single\nchemical elements. We demonstrate chirality assignment in $\\beta$-manganese,\nand we determine the sense of crystal chirality from Kikuchi diffraction\npatterns of the trigonal structures of tellurium and selenium.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The inverse function $x \\mapsto x^{-1}$ on $\\mathbb{F}_{2^n}$ is one of the\nmost studied functions in cryptography due to its widespread use as an S-box in\nblock ciphers like AES. In this paper, we show that, if $n\\geq 5$, every\nfunction that is CCZ-equivalent to the inverse function is already\nEA-equivalent to it. This confirms a conjecture by Budaghyan, Calderini and\nVilla. We also prove that every permutation that is CCZ-equivalent to the\ninverse function is already affine equivalent to it. The majority of the paper\nis devoted to proving that there are no permutation polynomials of the form\n$L_1(x^{-1})+L_2(x)$ over $\\mathbb{F}_{2^n}$ if $n\\geq 5$, where $L_1,L_2$ are\nnonzero linear functions. In the proof, we combine Kloosterman sums, quadratic\nforms and tools from additive combinatorics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We develop a universal approach enabling the study of any multimode quantum\noptical system evolving under a quadratic Hamiltonian. Our strategy generalizes\nthe standard symplectic analysis and permits the treatment of multimode systems\neven in situations where traditional theoretical methods cannot be applied.\nThis enables the description and investigation of a broad variety of\nkey-resources for experimental quantum optics, ranging from optical parametric\noscillators, to silicon-based micro-ring resonator, as well as opto-mechanical\nsystems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although a plethora of techniques are now available for controlling the group\nvelocity of an optical wave packet, there are very few options for creating\naccelerating or decelerating wave packets whose group velocity varies\ncontrollably along the propagation axis. Here we show that `space-time' wave\npackets in which each wavelength is associated with a prescribed spatial\nbandwidth enable the realization of optical acceleration and deceleration in\nfree space. Endowing the field with precise spatio-temporal structure leads to\ngroup-velocity changes as high as $\\sim c$ observed over a distance of $\\sim20$\nmm in free space, which represents a boost of at least $\\sim4$ orders of\nmagnitude over X-waves and Airy pulses. The acceleration implemented is in\nprinciple independent of the initial group velocity, and we have verified this\neffect in both the subluminal and superluminal regimes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  BACKGROUND: Augmented reality (AR) is gaining popularity in varying field\nsuch as computer gaming and medical education fields. However, still few of\napplications in real surgeries. Orthopedic surgical applications are currently\nlimited and underdeveloped. - METHODS: The clinic validation was prepared with\nthe currently available AR equipment and software. A total of 1 Vertebroplasty,\n2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal Femoral Fracture, 1 CRIF\nfor distal radius fracture and 2 ORIF for Tibia Fracture cases were performed\nwith fluoroscopy combined with AR smart surgical glasses system. - RESULTS: A\ntotal of 1 Vertebroplasty, 2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal\nFemoral Fracture, 1 CRIF for distal radius fracture and 2 ORIF for Tibia\nFracture cases are performed to evaluate the benefits of AR surgery. Among the\nAR surgeries, surgeons wear the smart surgical are lot reduce of eyes of turns\nto focus on the monitors. This paper shows the potential ability of augmented\nreality technology for trauma surgery.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The adversarial model presented by trusted execution environments (TEEs) has\nprompted researchers to investigate unusual attack vectors. One particularly\npowerful class of controlled-channel attacks abuses page-table modifications to\nreliably track enclave memory accesses at a page-level granularity. In contrast\nto noisy microarchitectural timing leakage, this line of deterministic\ncontrolled-channel attacks abuses indispensable architectural interfaces and\nhence cannot be mitigated by tweaking microarchitectural resources.\n  We propose an innovative controlled-channel attack, named CopyCat, that\ndeterministically counts the number of instructions executed within a single\nenclave code page. We show that combining the instruction counts harvested by\nCopyCat with traditional, coarse-grained page-level leakage allows the accurate\nreconstruction of enclave control flow at a maximal instruction-level\ngranularity. CopyCat can identify intra-page and intra-cache line branch\ndecisions that ultimately may only differ in a single instruction, underscoring\nthat even extremely subtle control flow deviations can be deterministically\nleaked from secure enclaves. We demonstrate the improved resolution and\npracticality of CopyCat on Intel SGX in an extensive study of single-trace and\ndeterministic attacks against cryptographic implementations, and give novel\nalgorithmic attacks to perform single-trace key extraction that exploit subtle\nvulnerabilities in the latest versions of widely-used cryptographic libraries.\nOur findings highlight the importance of stricter verification of cryptographic\nimplementations, especially in the context of TEEs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Learning semantic segmentation models requires a huge amount of pixel-wise\nlabeling. However, labeled data may only be available abundantly in a domain\ndifferent from the desired target domain, which only has minimal or no\nannotations. In this work, we propose a novel framework for domain adaptation\nin semantic segmentation with image-level weak labels in the target domain. The\nweak labels may be obtained based on a model prediction for unsupervised domain\nadaptation (UDA), or from a human annotator in a new weakly-supervised domain\nadaptation (WDA) paradigm for semantic segmentation. Using weak labels is both\npractical and useful, since (i) collecting image-level target annotations is\ncomparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the\nopportunity for category-wise domain alignment. Our framework uses weak labels\nto enable the interplay between feature alignment and pseudo-labeling,\nimproving both in the process of domain adaptation. Specifically, we develop a\nweak-label classification module to enforce the network to attend to certain\ncategories, and then use such training signals to guide the proposed\ncategory-wise alignment method. In experiments, we show considerable\nimprovements with respect to the existing state-of-the-arts in UDA and present\na new benchmark in the WDA setting. Project page is at\nhttp://www.nec-labs.com/~mas/WeakSegDA.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spatial modulation (SM) is a particularly important form of\nmultiple-input-multiple-output (MIMO). Unlike traditional MIMO, it uses both\nmodulation symbols and antenna indices to carry information. In this paper, to\navoid the high cost and circuit complexity of fully-digital SM, we mainly\nconsider the hybrid SM system with a hybrid precoding transmitter architecture,\ncombining a digital precoder and an analog precoder. Here, the\npartially-connected structure is adopted with each radio frequency chain (RF)\nbeing connected to a transmit antenna subarray (TAS). In such a system, we made\nan investigation of secure hybrid precoding and transmit antenna subarray\nselection (TASS) methods. Two hybrid precoding methods, called maximizing the\napproximate secrecy rate (SR) via gradient ascent (Max-ASR-GA) and maximizing\nthe approximate SR via alternating direction method of multipliers\n(Max-ASR-ADMM), are proposed to improve the SR performance. As for TASS, a\nhigh-performance method of maximizing the approximate SR (Max-ASR) TASS method\nis first presented. To reduce its high complexity, two low-complexity TASS\nmethods, namely maximizing the eigenvalue (Max-EV) and maximizing the product\nof signal-to-interference-plus-noise ratio and artificial\nnoise-to-signal-plus-noise ratio (Max-P-SINR-ANSNR), are proposed. Simulation\nresults will demonstrate that the proposed Max-ASR-GA and Max-ASR-ADMM hybrid\nprecoders harvest substantial SR performance gains over existing method. For\nTASS, the proposed three methods Max-ASR, Max-EV, and Max-P-SINR-ANSNR perform\nbetter than existing leakage method. Particularly, the proposed Max-EV and\nMax-P-SINR-ANSNR is low-complexity at the expense of a little performance loss\ncompared with Max-ASR.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We provide several crucial technical extensions of the theory of stable\nindependence notions in accessible categories. In particular, we describe\ncircumstances under which a stable independence notion can be transferred from\na subcategory to a category as a whole, and examine a number of applications to\ncategories of groups and modules, extending results of [MAa]. We prove, too,\nthat under the hypotheses of [LRV], a stable independence notion immediately\nyields higher-dimensional independence as in [SV].\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We infer the crater chronologies of Ceres and Vesta from a self-consistent\ndynamical model of asteroid impactors. The model accounts for planetary\nmigration/instability early in the solar system history and tracks asteroid\norbits over 4.56 Gy. It is calibrated on the current population of the asteroid\nbelt. The model provides the number of asteroid impacts on different worlds at\nany time throughout the solar system history. We combine the results with an\nimpactor-crater scaling relationship to determine the crater distribution of\nCeres and Vesta and compare these theoretical predictions with observations. We\nfind that: (i) The Ceres and Vesta chronologies are similar, whereas they\nsignificantly differ from the lunar chronology. Therefore, using the lunar\nchronology for main belt asteroids, as often done in previous publications, is\nincorrect. (ii) The model results match the number and size distribution of\nlarge (diameter $>90$ km) craters observed on Vesta, but overestimate the\nnumber of large craters on Ceres. This implies that large crater erasure is\nrequired for Ceres. (iii) In a model where planetary migration/instability\nhappens early, the probability to form the Rheasilvia basin on Vesta during the\nlast 1 Gy is 10\\%, a factor of $\\sim1.5$ higher than for the late instability\ncase and $\\sim2.5$ times higher than found in previous studies. Thus, while the\nformation of the Rheasilvia at $\\sim1$ Gy ago (Ga) would be somewhat unusual,\nit cannot be ruled out at more than $\\simeq1.5\\sigma$. In broader context, our\nwork provides a self-consistent framework for modeling asteroid crater records.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Neural style transfer is a well-known branch of deep learning research, with\nmany interesting works and two major drawbacks. Most of the works in the field\nare hard to use by non-expert users and substantial hardware resources are\nrequired. In this work, we present a solution to both of these problems. We\nhave applied neural style transfer to real-time video (over 25 frames per\nsecond), which is capable of running on mobile devices. We also investigate the\nworks on achieving temporal coherence and present the idea of fine-tuning,\nalready trained models, to achieve stable video. What is more, we also analyze\nthe impact of the common deep neural network architecture on the performance of\nmobile devices with regard to number of layers and filters present. In the\nexperiment section we present the results of our work with respect to the iOS\ndevices and discuss the problems present in current Android devices as well as\nfuture possibilities. At the end we present the qualitative results of\nstylization and quantitative results of performance tested on the iPhone 11 Pro\nand iPhone 6s. The presented work is incorporated in Kunster - AR Art Video\nMaker application available in the Apple's App Store.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A key appeal of the recently proposed Neural Ordinary Differential\nEquation(ODE) framework is that it seems to provide a continuous-time extension\nof discrete residual neural networks. As we show herein, though, trained Neural\nODE models actually depend on the specific numerical method used during\ntraining. If the trained model is supposed to be a flow generated from an ODE,\nit should be possible to choose another numerical solver with equal or smaller\nnumerical error without loss of performance. We observe that if training relies\non a solver with overly coarse discretization, then testing with another solver\nof equal or smaller numerical error results in a sharp drop in accuracy. In\nsuch cases, the combination of vector field and numerical method cannot be\ninterpreted as a flow generated from an ODE, which arguably poses a fatal\nbreakdown of the Neural ODE concept. We observe, however, that there exists a\ncritical step size beyond which the training yields a valid ODE vector field.\nWe propose a method that monitors the behavior of the ODE solver during\ntraining to adapt its step size, aiming to ensure a valid ODE without\nunnecessarily increasing computational cost. We verify this adaption algorithm\non two common bench mark datasets as well as a synthetic dataset. Furthermore,\nwe introduce a novel synthetic dataset in which the underlying ODE directly\ngenerates a classification task.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a novel language-independent approach to improve the efficiency\nfor Grammatical Error Correction (GEC) by dividing the task into two subtasks:\nErroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD\nidentifies grammatically incorrect text spans with an efficient sequence\ntagging model. Then, ESC leverages a seq2seq model to take the sentence with\nannotated erroneous spans as input and only outputs the corrected text for\nthese spans. Experiments show our approach performs comparably to conventional\nseq2seq approaches in both English and Chinese GEC benchmarks with less than\n50% time cost for inference.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this second paper of a two-part series, we prove that whenever a contact\n3-manifold admits a uniform spinal open book decomposition with planar pages,\nits (weak, strong and/or exact) symplectic and Stein fillings can be classified\nup to deformation equivalence in terms of diffeomorphism classes of Lefschetz\nfibrations. This extends previous results of the third author to a much wider\nclass of contact manifolds, which we illustrate here by classifying the strong\nand Stein fillings of all oriented circle bundles with non-tangential\n$S^1$-invariant contact structures. Further results include new vanishing\ncriteria for the ECH contact invariant and algebraic torsion in SFT,\nclassification of fillings for certain non-orientable circle bundles, and a\ngeneral \"symplectic quasiflexibility\" result about deformation classes of Stein\nstructures in real dimension four.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a generic theoretical model for the structuring of a relativistic\njet propagating through the ejecta of a binary neutron star merger event,\nintroducing the effects of the neutron conversion-diffusion, which provides a\nbaryon flux propagating transversely from the ejecta towards the jet axis. This\nresults naturally in an increased baryon load structure of the outer jet with\nthe approximate isotropic energy distribution $E_{iso}(\\theta) \\propto\n\\theta^{-4}$, which is compatible with the first gravitational wave and short\ngamma-ray burst event GW170817/GRB 170817A observed at an off-axis angle of the\njet.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We formulate a set of general rules for computing $d$-dimensional four-point\nglobal conformal blocks of operators in arbitrary Lorentz representations in\nthe context of the embedding space operator product expansion formalism\narXiv:1905.00434. With these rules, the procedure for determining any conformal\nblock of interest is reduced to (1) identifying the relevant projection\noperators and tensor structures and (2) applying the conformal rules to obtain\nthe blocks. To facilitate the bookkeeping of contributing terms, we introduce a\nconvenient diagrammatic notation. We present several concrete examples to\nillustrate the general procedure as well as to demonstrate and test the\nexplicit application of the rules. In particular, we consider four-point\nfunctions involving scalars $S$ and some specific irreducible representations\n$R$, namely $\\langle SSSS\\rangle$, $\\langle SSSR\\rangle$, $\\langle SRSR\\rangle$\nand $\\langle SSRR\\rangle$ (where, when allowed, $R$ is a vector or a fermion),\nand determine the corresponding blocks for all possible exchanged\nrepresentations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Neural probes are in vivo invasive devices that combine electrophysiology and\noptogenetics to gain insight into how the brain operates, down to the single\nneuron and its network activity. Their integration of stimulation sites and\nsensors allows for recording and manipulating neurons` activity with a high\nspatiotemporal resolution. State of the art probes are limited by tradeoffs\nbetween their lateral dimension, the number of sensors, and the ability to\nselectively access independent stimulation sites. Here, we realize a highly\nscalable probe that features a three-dimensional integration of small footprint\narrays of sensors and nanophotonic circuits and scales the density of sensors\nper cross-section by one order of magnitude with respect to state of the art\ndevices. For the first time, we overcome the spatial limit of the nanophotonic\ncircuit by coupling only one waveguide to numerous optical ring resonators as\npassive nanophotonic switches. With our strategy, we achieve accurate on-demand\nlight localization while avoiding spatial demanding bundles of waveguides and\ndemonstrate the feasibility of a proof of concept device and its additional\nscalability, towards high resolution and low damaging neural optoelectrodes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the periodic initial-value problem for the Korteweg-de Vries\nequation that we discretize in space by a spectral Fourier-Galerkin method and\nin time by an implicit, high order, Runge-Kutta scheme of composition type\nbased on the implicit midpoint rule. We prove $L^{2}$ error estimates for the\nresulting semidiscrete and the fully discrete approximations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The network embedding problem that maps nodes in a graph to vectors in\nEuclidean space can be very useful for addressing several important tasks on a\ngraph. Recently, graph neural networks (GNNs) have been proposed for solving\nsuch a problem. However, most embedding algorithms and GNNs are difficult to\ninterpret and do not scale well to handle millions of nodes. In this paper, we\ntackle the problem from a new perspective based on the equivalence of three\nconstrained optimization problems: the network embedding problem, the trace\nmaximization problem of the modularity matrix in a sampled graph, and the\nmatrix factorization problem of the modularity matrix in a sampled graph. The\noptimal solutions to these three problems are the dominant eigenvectors of the\nmodularity matrix. We proposed two algorithms that belong to a special class of\ngraph convolutional networks (GCNs) for solving these problems: (i) Clustering\nAs Feature Embedding GCN (CAFE-GCN) and (ii) sphere-GCN. Both algorithms are\nstable trace maximization algorithms, and they yield good approximations of\ndominant eigenvectors. Moreover, there are linear-time implementations for\nsparse graphs. In addition to solving the network embedding problem, both\nproposed GCNs are capable of performing dimensionality reduction. Various\nexperiments are conducted to evaluate our proposed GCNs and show that our\nproposed GCNs outperform almost all the baseline methods. Moreover, CAFE-GCN\ncould be benefited from the labeled data and have tremendous improvements in\nvarious performance metrics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  To thrive in dynamic environments, animals must be capable of rapidly and\nflexibly adapting behavioral responses to a changing context and internal\nstate. Examples of behavioral flexibility include faster stimulus responses\nwhen attentive and slower responses when distracted. Contextual or\nstate-dependent modulations may occur early in the cortical hierarchy and may\nbe implemented via top-down projections from cortico-cortical or\nneuromodulatory pathways. However, the computational mechanisms mediating the\neffects of such projections are not known. Here, we introduce a theoretical\nframework to classify the effects of cell-type specific top-down perturbations\non the information processing speed of cortical circuits. Our theory\ndemonstrates that perturbation effects on stimulus processing can be predicted\nby intrinsic gain modulation, which controls the timescale of the circuit\ndynamics. Our theory leads to counter-intuitive effects such as improved\nperformance with increased input variance. We tested the model predictions\nusing large-scale electrophysiological recordings from the visual hierarchy in\nfreely running mice, where we found that a decrease in single-cell intrinsic\ngain during locomotion led to an acceleration of visual processing. Our results\nestablish a novel theory of cell-type specific perturbations, applicable to\ntop-down modulation as well as optogenetic and pharmacological manipulations.\nOur theory links connectivity, dynamics, and information processing via gain\nmodulation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Bloch-Torrey equation governs the evolution of the transverse\nmagnetization in diffusion magnetic resonance imaging, where two mechanisms are\nat play: diffusion of spins (Laplacian term) and their precession in a magnetic\nfield gradient (imaginary potential term). In this paper, we study this\nequation in a periodic medium: a unit cell repeated over the nodes of a\nlattice. Although the gradient term of the equation is not invariant by lattice\ntranslations, the equation can be analyzed within a single unit cell by\nreplacing a continuous-time gradient profile by narrow pulses. In this\napproximation, the effects of precession and diffusion are separated and the\nproblem is reduced to the study of a sequence of diffusion equations with\npseudo-periodic boundary conditions. This representation allows for efficient\nnumerical computations as well as new theoretical insights into the formation\nof the signal in periodic media. In particular, we study the eigenmodes and\neigenvalues of the Bloch-Torrey operator. We show how the localization of\neigenmodes is related to branching points in the spectrum and we discuss low-\nand high-gradient asymptotic behaviors. The range of validity of the\napproximation is discussed; interestingly the method turns out to be more\naccurate and efficient at high gradient, being thus an important complementary\ntool to conventional numerical methods that are most accurate at low gradients.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Sign Language helps people with Speaking and Hearing Disabilities communicate\nwith others efficiently. Sign Language identification is a challenging area in\nthe field of computer vision and recent developments have been able to achieve\nnear perfect results for the task, though some challenges are yet to be solved.\nIn this paper we propose a novel machine learning based pipeline for American\nSign Language identification using hand track points. We convert a hand gesture\ninto a series of hand track point coordinates that serve as an input to our\nsystem. In order to make the solution more efficient, we experimented with 28\ndifferent combinations of pre-processing techniques, each run on three\ndifferent machine learning algorithms namely k-Nearest Neighbours, Random\nForests and a Neural Network. Their performance was contrasted to determine the\nbest pre-processing scheme and algorithm pair. Our system achieved an Accuracy\nof 95.66% to identify American sign language gestures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This note states and proves a representation theorem for regular quantity\nfunctions, based on the theory of quantity spaces, thereby giving a new\nperspective on dimensional analysis and the classical $\\pi$ theorem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we explore the decoherence dynamics of a probing spin coupled\nto a spin bath, where the spin bath is given by a controllable 1D\ntransverse-field Ising chain. The 1D transverse-field Ising chain with\nfree-ends boundary condition is equivalent to a modified Kitaev model with\nnon-local Majorana bound states in its topological phase. We find that the\nprobing spin non-Markovian decoherence dynamics can manifest the topological\nstructure of the spin chain. By controlling the external magnetic field on the\nIsing chain, we find the close relationships between the quantum phase\ntransitions, the topological edge states, and the non-Markovian dynamics in\nreal-time domain. We also investigate the corresponding quantum entanglement\ndynamics in this topological system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  There are several methods for timing occultations. Many astronomers may not\nhave access to standard video timing tools, but many of them have access to\ndigital single-lens reflex (DSLR) cameras. In order to increase the accuracy of\ntiming, creative methods were investigated for the DSLR camera technique. These\ncan be a good substitute for the less accurate visual timing method. Two\nmethods of continuous shooting and afocal filming were examined in the\nexperimental phase, which was calculated using maximum speed sequential\nphotography 5 shots per second, 0.1 seconds precision and 60 frames per second\nshooting speed resulting in 0.0083 seconds precision timing. Two different\nsources of time were used for video timing: Internet clock and GPS, where GPS\nbase results were more accurate than the Internet clock.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Learning ensembles by bagging can substantially improve the generalization\nperformance of low-bias, high-variance estimators, including those evolved by\nGenetic Programming (GP). To be efficient, modern GP algorithms for evolving\n(bagging) ensembles typically rely on several (often inter-connected)\nmechanisms and respective hyper-parameters, ultimately compromising ease of\nuse. In this paper, we provide experimental evidence that such complexity might\nnot be warranted. We show that minor changes to fitness evaluation and\nselection are sufficient to make a simple and otherwise-traditional GP\nalgorithm evolve ensembles efficiently. The key to our proposal is to exploit\nthe way bagging works to compute, for each individual in the population,\nmultiple fitness values (instead of one) at a cost that is only marginally\nhigher than the one of a normal fitness evaluation. Experimental comparisons on\nclassification and regression tasks taken and reproduced from prior studies\nshow that our algorithm fares very well against state-of-the-art ensemble and\nnon-ensemble GP algorithms. We further provide insights into the proposed\napproach by (i) scaling the ensemble size, (ii) ablating the changes to\nselection, (iii) observing the evolvability induced by traditional subtree\nvariation. Code: https://github.com/marcovirgolin/2SEGP.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the strongly anisotropic spin-1/2 $XXZ$ model on the\nsawtooth-chain lattice with ferromagnetic longitudinal interaction\n$J^{zz}=\\Delta J$ and aniferromagnetic transversal interaction\n$J^{xx}=J^{yy}=J>0$. At $\\Delta=-1/2$ the lowest one-magnon excitation band is\ndispersionless (flat) leading to a massively degenerate set of ground states.\nInterestingly, this model admits a three-coloring representation of the\nground-state manifold [H.~J.~Changlani et at., Phys. Rev. Lett. {\\bf 120},\n117202 (2018)]. We characterize this ground-state manifold and elaborate the\nlow-temperature thermodynamics of the system. We illustrate the manifestation\nof the flat-band physics of the anisotropic model by comparison with two\nisotropic flat-band Heisenberg sawtooth chains. Our analytical consideration is\ncomplemented by exact diagonalization and finite-temperature Lanczos method\ncalculations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Modern mobile devices, although resourceful, cannot train state-of-the-art\nmachine learning models without the assistance of servers, which require access\nto, potentially, privacy-sensitive user data. Split learning has recently\nemerged as a promising technique for training complex deep learning (DL) models\non low-powered mobile devices. The core idea behind this technique is to train\nthe sensitive layers of a DL model on mobile devices while offloading the\ncomputationally intensive layers to a server. Although a lot of works have\nalready explored the effectiveness of split learning in simulated settings, a\nusable toolkit for this purpose does not exist. In this work, we highlight the\ntheoretical and technical challenges that need to be resolved to develop a\nfunctional framework that trains ML models in mobile devices without\ntransferring raw data to a server. Focusing on these challenges, we propose\nSplitEasy, a framework for training ML models on mobile devices using split\nlearning. Using the abstraction provided by SplitEasy, developers can run\nvarious DL models under split learning setting by making minimal modifications.\nWe provide a detailed explanation of SplitEasy and perform experiments with six\nstate-of-the-art neural networks. We demonstrate how SplitEasy can train models\nthat cannot be trained solely by a mobile device while incurring nearly\nconstant time per data sample.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An important but little-studied property of spin glasses is the stability of\ntheir ground states to changes in one or a finite number of couplings. It was\nshown in earlier work that, if multiple ground states are assumed to exist,\nthen fluctuations in their energy differences --- and therefore the possibility\nof multiple ground states --- are closely related to the stability of their\nground states. Here we examine the stability of ground states in two models,\none of which is presumed to have a ground state structure that is qualitatively\nsimilar to other realistic short-range spin glasses in finite dimensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Model-Agnostic Meta-Learning (MAML), a popular gradient-based meta-learning\nframework, assumes that the contribution of each task or instance to the\nmeta-learner is equal. Hence, it fails to address the domain shift between base\nand novel classes in few-shot learning. In this work, we propose a novel robust\nmeta-learning algorithm, NestedMAML, which learns to assign weights to training\ntasks or instances. We consider weights as hyper-parameters and iteratively\noptimize them using a small set of validation tasks set in a nested bi-level\noptimization approach (in contrast to the standard bi-level optimization in\nMAML). We then apply NestedMAML in the meta-training stage, which involves (1)\nseveral tasks sampled from a distribution different from the meta-test task\ndistribution, or (2) some data samples with noisy labels. Extensive experiments\non synthetic and real-world datasets demonstrate that NestedMAML efficiently\nmitigates the effects of \"unwanted\" tasks or instances, leading to significant\nimprovement over the state-of-the-art robust meta-learning methods.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Fixing molecules in space is a crucial step for the imaging of molecular\nstructure and dynamics. Here, we demonstrate three-dimensional (3D) field-free\nalignment of the prototypical asymmetric top molecule indole using elliptically\npolarized, shaped, off-resonant laser pulses. A truncated laser pulse is\nproduced using a combination of extreme linear chirping and controlled phase\nand amplitude shaping using a spatial-light-modulator (SLM) based pulse shaper\nof a broadband laser pulse. The angular confinement is detected through\nvelocity-map imaging of H$^+$ and C$^{2+}$ fragments resulting from\nstrong-field ionization and Coulomb explosion of the aligned molecules by\nintense femtosecond laser pulses. The achieved three-dimensional alignment is\ncharacterized by comparing the result of ion-velocity-map measurements for\ndifferent alignment directions and for different times during and after the\nalignment laser pulse to accurate computational results. The achieved strong\nthree-dimensional field-free alignment of $\\langle \\cos^{2}\\delta \\rangle=0.89$\ndemonstrates the feasibility of both, strong three-dimensional alignment of\ngeneric complex molecules and its quantitative characterization.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The attention mechanism has demonstrated superior performance for inference\nover nodes in graph neural networks (GNNs), however, they result in a high\ncomputational burden during both training and inference. We propose FastGAT, a\nmethod to make attention based GNNs lightweight by using spectral\nsparsification to generate an optimal pruning of the input graph. This results\nin a per-epoch time that is almost linear in the number of graph nodes as\nopposed to quadratic. We theoretically prove that spectral sparsification\npreserves the features computed by the GAT model, thereby justifying our\nalgorithm. We experimentally evaluate FastGAT on several large real world graph\ndatasets for node classification tasks under both inductive and transductive\nsettings. FastGAT can dramatically reduce (up to \\textbf{10x}) the\ncomputational time and memory requirements, allowing the usage of attention\nbased GNNs on large graphs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  As the COVID-19 pandemic expands, the shortening of medical equipment is\nswelling. A key piece of equipment getting far-out attention has been\nventilators. The difference between supply and demand is substantial to be\nhandled with normal production techniques, especially under social distancing\nmeasures in place. The study explores the rationale of human-robot teams to\nramp up production using advantages of both the ease of integration and\nmaintaining social distancing. The paper presents a model for faster\nintegration of collaborative robots and design guidelines for workstation. The\nscenarios are evaluated for an open source ventilator through continuous\nhuman-robot simulation and amplification of results in a discrete event\nsimulation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider a version of the continuum armed bandit where an action induces a\nfiltered realisation of a non-homogeneous Poisson process. Point data in the\nfiltered sample are then revealed to the decision-maker, whose reward is the\ntotal number of revealed points. Using knowledge of the function governing the\nfiltering, but without knowledge of the Poisson intensity function, the\ndecision-maker seeks to maximise the expected number of revealed points over T\nrounds. We propose an upper confidence bound algorithm for this problem\nutilising data-adaptive discretisation of the action space. This approach\nenjoys O(T^(2/3)) regret under a Lipschitz assumption on the reward function.\nWe provide lower bounds on the regret of any algorithm for the problem, via new\nlower bounds for related finite-armed bandits, and show that the orders of the\nupper and lower bounds match up to a logarithmic factor.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep learning-based methods have achieved promising results on surgical\ninstrument segmentation. However, the high computation cost may limit the\napplication of deep models to time-sensitive tasks such as online surgical\nvideo analysis for robotic-assisted surgery. Moreover, current methods may\nstill suffer from challenging conditions in surgical images such as various\nlighting conditions and the presence of blood. We propose a novel Multi-frame\nFeature Aggregation (MFFA) module to aggregate video frame features temporally\nand spatially in a recurrent mode. By distributing the computation load of deep\nfeature extraction over sequential frames, we can use a lightweight encoder to\nreduce the computation costs at each time step. Moreover, public surgical\nvideos usually are not labeled frame by frame, so we develop a method that can\nrandomly synthesize a surgical frame sequence from a single labeled frame to\nassist network training. We demonstrate that our approach achieves superior\nperformance to corresponding deeper segmentation models on two public surgery\ndatasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study ordered configuration spaces $C(n;p,q)$ of $n$ hard squares in a $p\n\\times q$ rectangle, a generalization of the well-known \"15 Puzzle\". Our main\ninterest is in the topology of these spaces. Our first result is to describe a\ncubical cell complex and prove that is homotopy equivalent to the configuration\nspace. We then focus on determining for which $n$, $j$, $p$, and $q$ the\nhomology group $H_j [ C(n;p,q) ]$ is nontrivial. We prove three\nhomology-vanishing theorems, based on discrete Morse theory on the cell\ncomplex. Then we describe several explicit families of nontrivial cycles, and a\nmethod for interpolating between parameters to fill in most of the picture for\n\"large-scale\" nontrivial homology.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gravitational-wave detectors on earth have detected gravitational waves from\nmerging compact objects in the local Universe. In future we will detect\ngravitational waves from higher-redshift sources, which trace the high-redshift\nstructure formation history. That is, by observing high-redshift\ngravitational-wave events we will be able to probe structure formation history.\nThis will provide additional insight into the early Universe when primordial\nfluctuations are generated and also into the nature of dark matter.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Potential buyers of a product or service, before making their decisions, tend\nto read reviews written by previous consumers. We consider Bayesian consumers\nwith heterogeneous preferences, who sequentially decide whether to buy an item\nof unknown quality, based on previous buyers' reviews. The quality is\nmulti-dimensional and may occasionally vary over time; the reviews are also\nmulti-dimensional. In the simple uni-dimensional and static setting, beliefs\nabout the quality are known to converge to its true value. Our paper extends\nthis result in several ways. First, a multi-dimensional quality is considered,\nsecond, rates of convergence are provided, third, a dynamical Markovian model\nwith varying quality is studied. In this dynamical setting the cost of learning\nis shown to be small.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Mapping algorithms that rely on registering point clouds inevitably suffer\nfrom local drift, both in localization and in the built map. Applications that\nrequire accurate maps, such as environmental monitoring, benefit from\nadditional sensor modalities that reduce such drift. In our work, we target the\nfamily of mappers based on the Iterative Closest Point (ICP) algorithm which\nuse additional orientation sources such as the Inertial Measurement Unit (IMU).\nWe introduce a new angular penalty term derived from Lie algebra. Our\nformulation avoids the need for tuning arbitrary parameters. Orientation\ncovariance is used instead, and the resulting error term fits into the ICP cost\nfunction minimization problem. Experiments performed on our own real-world data\nand on the KITTI dataset show consistent behavior while suppressing the effect\nof outlying IMU measurements. We further discuss promising experiments, which\nshould lead to optimal combination of all error terms in the ICP cost function\nminimization problem, allowing us to smoothly combine the geometric and\ninertial information provided by robot sensors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A concise review of precision measurements in the Higgs sector of the\nStandard Model (SM) of particle physics is given using ATLAS and CMS data. The\nresults are based on LHC Run-2 data, taken between 2015 and 2018. Impressive\nprogress has been made since the discovery of the Higgs boson in 2012 for\nmeasuring all major production and decay modes. Good agreement with the SM\npredictions was observed in all measurements.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An average gate fidelity is a standard performance metric to quantify\ndeviation between an ideal unitary gate transformation and its realistic\nexperimental implementation. The average is taken with respect to states\nuniformly distributed over the full Hilbert space. We analytically\n(single-qubit) and numerically (two-qubit) show how this average changes if the\nuniform distribution condition is relaxed, replaced by parametrized\ndistributions - polar cap and von Mises-Fisher distributions - and how the\nresulting fidelities can differentiate certain noise models. In particular, we\ndemonstrate that Pauli channels with different noise rates along the three axes\ncan be faithfully distinguished using these augmented fidelities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we use a Banach fixed point theorem to obtain suficient\nconditions satisfying the convergence and exponential convergence of solutions\nfor the linear system of advanced differential equations. The considered system\nwith multiple variable advanced arguments is discussed as well. The obtained\ntheorems generalize previous results of Dung [8], from the one dimension to the\nn dimension.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep learning-based systems have been shown to be vulnerable to adversarial\nattacks in both digital and physical domains. While feasible, digital attacks\nhave limited applicability in attacking deployed systems, including face\nrecognition systems, where an adversary typically has access to the input and\nnot the transmission channel. In such setting, physical attacks that directly\nprovide a malicious input through the input channel pose a bigger threat. We\ninvestigate the feasibility of conducting real-time physical attacks on face\nrecognition systems using adversarial light projections. A setup comprising a\ncommercially available web camera and a projector is used to conduct the\nattack. The adversary uses a transformation-invariant adversarial pattern\ngeneration method to generate a digital adversarial pattern using one or more\nimages of the target available to the adversary. The digital adversarial\npattern is then projected onto the adversary's face in the physical domain to\neither impersonate a target (impersonation) or evade recognition (obfuscation).\nWe conduct preliminary experiments using two open-source and one commercial\nface recognition system on a pool of 50 subjects. Our experimental results\ndemonstrate the vulnerability of face recognition systems to light projection\nattacks in both white-box and black-box attack settings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  PyRoss is an open-source Python library that offers an integrated platform\nfor inference, prediction and optimisation of NPIs in age- and\ncontact-structured epidemiological compartment models. This report outlines the\nrationale and functionality of the PyRoss library, with various illustrations\nand examples focusing on well-mixed, age-structured populations. The PyRoss\nlibrary supports arbitrary structured models formulated stochastically (as\nmaster equations) or deterministically (as ODEs) and allows mid-run\ntransitioning from one to the other. By supporting additional compartmental\nsubdivision ad libitum, PyRoss can emulate time-since-infection models and\nallows medical stages such as hospitalization or quarantine to be modelled and\nforecast. The PyRoss library enables fitting to epidemiological data, as\navailable, using Bayesian parameter inference, so that competing models can be\nweighed by their evidence. PyRoss allows fully Bayesian forecasts of the impact\nof idealized NPIs by convolving uncertainties arising from epidemiological\ndata, model choice, parameters, and intrinsic stochasticity. Algorithms to\noptimize time-dependent NPI scenarios against user-defined cost functions are\nincluded. PyRoss's current age-structured compartment framework for well-mixed\npopulations will in future reports be extended to include compartments\nstructured by location, occupation, use of travel networks and other attributes\nrelevant to assessing disease spread and the impact of NPIs. We argue that such\ncompartment models, by allowing social data of arbitrary granularity to be\ncombined with Bayesian parameter estimation for poorly-known disease variables,\ncould enable more powerful and robust prediction than other approaches to\ndetailed epidemic modelling. We invite others to use the PyRoss library for\nresearch to address today's COVID-19 crisis, and to plan for future pandemics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we describe a construction method for numerical Godeaux\nsurfaces based on homological algebra. We show the existence of an\n8-dimensional locally complete family of simply connected numerical Godeaux\nsurfaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  High intensity XUV radiation from a free-electron (FEL) was used to create a\nnanoplasma inside ammonia clusters with the intent of studying the resulting\nelectron-ion interactions and their interplay with plasma evolution. In a\nplasma-like state, electrons with kinetic energy lower than the local\ncollective Coulomb potential of the positive ionic core are trapped in the\ncluster and take part in secondary processes (e.g. electron-impact\nexcitation/ionization and electron-ion recombination) which lead to subsequent\nexcited and neutral molecular fragmentation. Using a time-delayed UV laser, the\ndynamics of the excited atomic and molecular states are probed from -0.1 ps to\n18 ps. We identify three different phases of molecular fragmentation that are\nclearly distinguished by the effect of the probe laser on the ionic and\nelectronic yield. We propose a simple model to rationalize our data and further\nidentify two separate channels leading to the formation of excited hydrogen.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Graph database management systems (GDBMSs) are highly optimized to perform\nfast traversals, i.e., joins of vertices with their neighbours, by indexing the\nneighbourhoods of vertices in adjacency lists. However, existing GDBMSs have\nsystem-specific and fixed adjacency list structures, which makes each system\nefficient on only a fixed set of workloads. We describe a new tunable indexing\nsubsystem for GDBMSs, we call A+ indexes, with materialized view support. The\nsubsystem consists of two types of indexes: (i) vertex-partitioned indexes that\npartition 1-hop materialized views into adjacency lists on either the source or\ndestination vertex IDs; and (ii) edge-partitioned indexes that partition 2-hop\nviews into adjacency lists on one of the edge IDs. As in existing GDBMSs, a\nsystem by default requires one forward and one backward vertex-partitioned\nindex, which we call the primary A+ index. Users can tune the primary index or\nsecondary indexes by adding nested partitioning and sorting criteria. Our\nsecondary indexes are space-efficient and use a technique we call offset lists.\nOur indexing subsystem allows a wider range of applications to benefit from\nGDBMSs' fast join capabilities. We demonstrate the tunability and space\nefficiency of A+ indexes through extensive experiments on three workloads.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper gives a concept of an integral operator defined on a manifold $M$\nconsisting of triple of points in $\\mathbb{R}^{d}$ making up a regular\n$3$-simplex with the origin. The boundedness of such operator is investigated.\nThe boundedness region contains more than the Banach range - a fact that\nmirrors the spherical $L^{p}$-improving estimate. The purpose of this paper is\ntwo-fold: one is to investigate into an integral operator over a manifold\ncreated from high-dimensional regular simplices, two is to start a maximal\noperator theory for such integral operator.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Keyword Spotting (KWS) plays a vital role in human-computer interaction for\nsmart on-device terminals and service robots. It remains challenging to achieve\nthe trade-off between small footprint and high accuracy for KWS task. In this\npaper, we explore the application of multi-scale temporal modeling to the\nsmall-footprint keyword spotting task. We propose a multi-branch temporal\nconvolution module (MTConv), a CNN block consisting of multiple temporal\nconvolution filters with different kernel sizes, which enriches temporal\nfeature space. Besides, taking advantage of temporal and depthwise convolution,\na temporal efficient neural network (TENet) is designed for KWS system. Based\non the purposed model, we replace standard temporal convolution layers with\nMTConvs that can be trained for better performance. While at the inference\nstage, the MTConv can be equivalently converted to the base convolution\narchitecture, so that no extra parameters and computational costs are added\ncompared to the base model. The results on Google Speech Command Dataset show\nthat one of our models trained with MTConv performs the accuracy of 96.8% with\nonly 100K parameters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Digital Transforms have important applications on subjects such as channel\ncoding, cryptography and digital signal processing. In this paper, two Fourier\nTransforms are considered, the discrete time Fourier transform (DTFT) and the\nfinite field Fourier transform (FFFT). A finite field version of the DTFT is\nintroduced and the FFFT is redefined with a complex kernel, which makes it a\nmore appropriate finite field version of the Discrete Fourier Transform. These\ntransforms can handle FIR and IIR filters defined over finite algebraic\nstructures.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The 5G band allocated in the 26 GHz spectrum referred to as 3GPP band n258,\nhas generated a lot of anxiety and concern in the meteorological data\nforecasting community including the National Oceanic and Atmospheric\nAdministration (NOAA). Unlike traditional spectrum coexistence problems, the\nissue here stems from the leakage of n258 band transmissions impacting the\nobservations of passive sensors (e.g. AMSU-A) operating at 23.8 GHz on weather\nsatellites used to detect the amount of water vapor in the atmosphere, which in\nturn affects weather forecasting and predictions. In this paper, we study the\nimpact of 5G leakage on the accuracy of data assimilation based weather\nprediction algorithms by using a first order propagation model to characterize\nthe effect of the leakage signal on the brightness temperature (atmospheric\nradiance) and the induced noise temperature at the receiving antenna of the\npassive sensor (radiometer) on the weather observation satellite. We then\ncharacterize the resulting inaccuracies when using the Weather Research and\nForecasting Data Assimilation model (WRFDA) to predict temperature and\nrainfall. For example, the impact of 5G leakage of -20dBW to -15dBW on the\nwell-known Super Tuesday Tornado Outbreak data set, affects the meteorological\nforecasting up to 0.9 mm in precipitation and 1.3 {\\deg}C in 2m-temperature. We\noutline future directions for both improved modeling of 5G leakage effects as\nwell as mitigation using cross-layer antenna techniques coupled with resource\nallocation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  At present, in the face of the huge and complex data in cloud computing, the\nparallel computing ability of quantum computing is particularly important.\nQuantum principal component analysis algorithm is used as a method of quantum\nstate tomography. We perform feature extraction on the eigenvalue matrix of the\ndensity matrix after feature decomposition to achieve dimensionality reduction,\nproposed quantum principal component extraction algorithm (QPCE). Compared with\nthe classic algorithm, this algorithm achieves an exponential speedup under\ncertain conditions. The specific realization of the quantum circuit is given.\nAnd considering the limited computing power of the client, we propose a quantum\nhomomorphic ciphertext dimension reduction scheme (QHEDR), the client can\nencrypt the quantum data and upload it to the cloud for computing. And through\nthe quantum homomorphic encryption scheme to ensure security. After the\ncalculation is completed, the client updates the key locally and decrypts the\nciphertext result. We have implemented a quantum ciphertext dimensionality\nreduction scheme implemented in the quantum cloud, which does not require\ninteraction and ensures safety. In addition, we have carried out experimental\nverification on the QPCE algorithm on IBM's real computing platform, and given\na simple example of executing hybrid quantum circuits in the cloud to verify\nthe correctness of our scheme. Experimental results show that the algorithm can\nperform ciphertext dimension reduction safely and effectively.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Context: COVID-19 pandemic has impacted different business sectors around the\nworld. Objective. This study investigates the impact of COVID-19 on software\nprojects and software development professionals. Method: We conducted a mining\nsoftware repository study based on 100 GitHub projects developed in Java using\nten different metrics. Next, we surveyed 279 software development professionals\nfor better understanding the impact of COVID-19 on daily activities and\nwellbeing. Results: We identified 12 observations related to productivity, code\nquality, and wellbeing. Conclusions: Our findings highlight that the impact of\nCOVID-19 is not binary (reduce productivity vs. increase productivity) but\nrather a spectrum. For many of our observations, substantial proportions of\nrespondents have differing opinions from each other. We believe that more\nresearch is needed to uncover specific conditions that cause certain outcomes\nto be more prevalent.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper describes a working prototype that adapts Lucene, the world's most\npopular and most widely deployed open-source search library, to operate within\na serverless environment in the cloud. Although the serverless search concept\nis not new, this work represents a substantial improvement over a previous\nimplementation in eliminating most custom code and in enabling interactive\nsearch. While there remain limitations to the design, it nevertheless\nchallenges conventional thinking about search architectures for particular\noperating points.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a cognitive behavioral-based driver mood repairment\nplatform in intelligent transportation cyber-physical systems (IT-CPS) for road\nsafety. In particular, we propose a driving safety platform for distracted\ndrivers, namely \\emph{drive safe}, in IT-CPS. The proposed platform recognizes\nthe distracting activities of the drivers as well as their emotions for mood\nrepair. Further, we develop a prototype of the proposed drive safe platform to\nestablish proof-of-concept (PoC) for the road safety in IT-CPS. In the\ndeveloped driving safety platform, we employ five AI and statistical-based\nmodels to infer a vehicle driver's cognitive-behavioral mining to ensure safe\ndriving during the drive. Especially, capsule network (CN), maximum likelihood\n(ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian\nnetwork (BN) are deployed for driver activity recognition, environmental\nfeature extraction, mood recognition, sequential pattern mining, and content\nrecommendation for affective mood repairment of the driver, respectively.\nBesides, we develop a communication module to interact with the systems in\nIT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle\ndrivers when they are distracted from driving due to the cognitive-behavioral\nfactors. Finally, we have performed a qualitative evaluation to measure the\nusability and effectiveness of the developed drive safe platform. We observe\nthat the P-value is 0.0041 (i.e., < 0.05) in the ANOVA test. Moreover, the\nconfidence interval analysis also shows significant gains in prevalence value\nwhich is around 0.93 for a 95% confidence level. The aforementioned statistical\nresults indicate high reliability in terms of driver's safety and mental state.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Generalized additive models (GAMs) have become a leading modelclass for\ninterpretable machine learning. However, there are many algorithms for training\nGAMs, and these can learn different or even contradictory models, while being\nequally accurate. Which GAM should we trust? In this paper, we quantitatively\nand qualitatively investigate a variety of GAM algorithms on real and simulated\ndatasets. We find that GAMs with high feature sparsity (only using afew\nvariables to make predictions) can miss patterns in the data and be unfair to\nrare subpopulations. Our results suggest that inductive bias plays a crucial\nrole in what interpretable models learn and that tree-based GAMs represent the\nbest balance of sparsity, fidelity and accuracy and thus appear to be the most\ntrustworthy GAM.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that a finitely generated subgroup of the genus two handlebody group\nis stable if and only if the orbit map to the disk graph is a quasi-isometric\nembedding. To this end, we prove that the genus two handlebody group is a\nhierarchically hyperbolic group, and that the maximal hyperbolic space in the\nhierarchy is quasi-isometric to the disk graph of a genus two handlebody by\nappealing to a construction of Hamenst\\\"adt-Hensel. We then utilize the\ncharacterization of stable subgroups of hierarchically hyperbolic groups\nprovided by Abbott-Behrstock-Berlyne-Durham-Russell. We also present several\napplications of the main theorems, and show that the higher genus analogues of\nthe genus two results do not hold.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  On a lattice with 2+1-flavor dynamical domain-wall fermions at the physical\npion mass, we calculate the decay constants of $D_{s}^{(*)}$, $D^{(*)}$ and\n$\\phi$. The lattice size is $48^3\\times96$, which corresponds to a spatial\nextension of $\\sim5.5$ fm with the lattice spacing $a\\approx 0.114$ fm. For the\nvalence light, strange and charm quarks, we use overlap fermions at several\nmass points close to their physical values. Our results at the physical point\nare $f_D=213(5)$ MeV, $f_{D_s}=249(7)$ MeV, $f_{D^*}=234(6)$ MeV,\n$f_{D_s^*}=274(7)$ MeV, and $f_\\phi=241(9)$ MeV. The couplings of $D^*$ and\n$D_s^*$ to the tensor current ($f_V^T$) can be derived, respectively, from the\nratios $f_{D^*}^T/f_{D^*}=0.91(4)$ and $f_{D_s^*}^T/f_{D_s^*}=0.92(4)$, which\nare the first lattice QCD results. We also obtain the ratios\n$f_{D^*}/f_D=1.10(3)$ and $f_{D_s^*}/f_{D_s}=1.10(4)$, which reflect the size\nof heavy quark symmetry breaking in charmed mesons. The ratios\n$f_{D_s}/f_{D}=1.16(3)$ and $f_{D_s^*}/f_{D^*}=1.17(3)$ can be taken as a\nmeasure of SU(3) flavor symmetry breaking.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Established techniques for deterministically creating dark solitons in\nrepulsively interacting atomic Bose-Einstein condensates (BECs) can only access\na narrow range of soliton velocities. Because velocity affects the stability of\nindividual solitons and the properties of soliton-soliton interactions, this\ntechnical limitation has hindered experimental progress. Here we create dark\nsolitons in highly anisotropic cigar-shaped BECs with arbitrary position and\nvelocity by simultaneously engineering the amplitude and phase of the\ncondensate wavefunction, improving upon previous techniques which only\nexplicitly manipulated the condensate phase. The single dark soliton solution\npresent in true 1D systems corresponds to the kink soliton in anisotropic 3D\nsystems and is joined by a host of additional dark solitons including vortex\nring and solitonic vortex solutions. We readily create dark solitons with\nspeeds from zero to half the sound speed. The observed soliton oscillation\nfrequency suggests that we imprinted solitonic vortices, which for our\ncigar-shaped system are the only stable solitons expected for these velocities.\nOur numerical simulations of 1D BECs show this technique to be equally\neffective for creating kink solitons when they are stable. We demonstrate the\nutility of this technique by deterministically colliding dark solitons with\ndomain walls in two-component spinor BECs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we prove that both topological Hochschild homology and\ntopological cyclic homology are sheaves for the fpqc topology on connective\ncommutative ring spectra, by exploiting the May filtration on topological\nHochschild homology.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present several rigidity results for initial data sets motivated by the\npositive mass theorem. An important step in our proofs here is to establish\nconditions that ensure that a marginally outer trapped surface is \"weakly\noutermost\". A rigidity result for Riemannian manifolds with a lower bound on\ntheir scalar curvature is included as a special case.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The quality of fetal ultrasound images is significantly affected by motion\nblur while the imaging system requires low motion quality in order to capture\naccurate data. This can be achieved with a mathematical model of motion blur in\ntime or frequency domain. We propose a new model of linear motion blur in both\nfrequency and moment domain to analyse the invariant features of blur\nconvolution for ultrasound images. Moreover, the model also helps to provide an\nestimation of motion parameters for blur length and angle. These outcomes might\nimply great potential of this invariant method in ultrasound imaging\napplication.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The nature of information sharing in common distributed consensus algorithms\npermits network eavesdroppers to expose sensitive system information. An\nimportant parameter within distributed systems, often neglected under the scope\nof privacy preservation, is the influence structure - the weighting each agent\nplaces on the sources of their opinion pool. This paper proposes a local (i.e.\ncomputed individually by each agent), time varying mask to prevent the\ndiscovery of the influence structure by an external observer with access to the\nentire information flow, network knowledge and mask formulation. This result is\nproduced through the auxiliary demonstration of the preserved stability of a\nFriedkin-Johnsen system under a set of generalised conditions. The mask is\ndeveloped under these constraints and involves perturbing the influence\nstructure by decaying pseudonoise. This paper provides the information matrix\nof the best influence structure estimate by an eavesdropper lacking a priori\nknowledge and uses stochastic simulations to analyse the performance of the\nmask against ranging system hyperparameters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we study the continuous dynamical sampling problem at infinite\ntime in a complex Hilbert space $\\mathcal{H}$. We find necessary and sufficient\nconditions on a bounded linear operator $A\\in\\mathcal{B}(\\mathcal{H})$ and a\nset of vectors $\\mathcal{G}\\subset \\mathcal{H}$, in order to obtain that\n$\\{e^{tA}g\\}_{g\\in\\mathcal{G}, t\\in[0,\\infty)}$ is a semi-continuous frame for\n$\\mathcal{H}$. We study if it is possible to discretize the time variable $t$\nand still have a frame for $\\mathcal{H}$. We also relate the continuous\niteration $e^{tA}$ on a set $\\mathcal{G}$ to the discrete iteration\n$(A^\\prime)^n$ on $\\mathcal{G}^\\prime$ for an adequate operator $A^\\prime$ and\nset $\\mathcal{G}^\\prime\\subset \\mathcal{H}$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Understanding how energy is released in flares is one of the central problems\nof solar and stellar astrophysics. Observations of high temperature flare\nplasma hold many potential clues as to the nature of this energy release. It is\nclear, however, that flares are not composed of a few impulsively heated loops,\nbut are the result of heating on many small-scale threads that are energized\nover time, making it difficult to compare observations and numerical\nsimulations in detail. Several previous studies have shown that it is possible\nto reproduce some aspects of the observed emission by considering the flare as\na sequence of independently heated loops, but these studies generally focus on\nsmall-scale features while ignoring the global features of the flare. In this\npaper, we develop a multithreaded model that encompasses the time-varying\ngeometry and heating rate for a series of successively-heated loops comprising\nan arcade. To validate, we compare with spectral observations of five flares\nmade with the MinXSS CubeSat as well as light curves measured with GOES/XRS and\nSDO/AIA. We show that this model can successfully reproduce the light curves\nand quasi-periodic pulsations in GOES/XRS, the soft X-ray spectra seen with\nMinXSS, and the light curves in various AIA passbands. The AIA light curves are\nmost consistent with long duration heating, but elemental abundances cannot be\nconstrained with the model. Finally, we show how this model can be used to\nextrapolate to spectra of extreme events that can predict irradiance across a\nwide wavelength range including unobserved wavelengths.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper proposes a cluster-based method to analyze the evolution of\nmultivariate time series and applies this to the COVID-19 pandemic. On each\nday, we partition countries into clusters according to both their case and\ndeath counts. The total number of clusters and individual countries' cluster\nmemberships are algorithmically determined. We study the change in both\nquantities over time, demonstrating a close similarity in the evolution of\ncases and deaths. The changing number of clusters of the case counts precedes\nthat of the death counts by 32 days. On the other hand, there is an optimal\noffset of 16 days with respect to the greatest consistency between cluster\ngroupings, determined by a new method of comparing affinity matrices. With this\noffset in mind, we identify anomalous countries in the progression from\nCOVID-19 cases to deaths. This analysis can aid in highlighting the most and\nleast significant public policies in minimizing a country's COVID-19 mortality\nrate.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recently, natural van der Waals heterostructures of (MnBi2Te4)m(Bi2Te3)n have\nbeen theoretically predicted and experimentally shown to host tunable magnetic\nproperties and topologically nontrivial surface states. In this work, we\nsystematically investigate both the structural and electronic responses of\nMnBi2Te4 and MnBi4Te7 to external pressure. In addition to the suppression of\nantiferromagnetic order, MnBi2Te4 is found to undergo a\nmetal-semiconductor-metal transition upon compression. The resistivity of\nMnBi4Te7 changes dramatically under high pressure and a non-monotonic evolution\nof \\r{ho}(T) is observed. The nontrivial topology is proved to persists before\nthe structural phase transition observed in the high-pressure regime. We find\nthat the bulk and surface states respond differently to pressure, which is\nconsistent with the non-monotonic change of the resistivity. Interestingly, a\npressure-induced amorphous state is observed in MnBi2Te4, while two high\npressure phase transitions are revealed in MnBi4Te7. Our combined theoretical\nand experimental research establishes MnBi2Te4 and MnBi4Te7 as highly tunable\nmagnetic topological insulators, in which phase transitions and new ground\nstates emerge upon compression.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present the NNLO calculation for single-inclusive jet production in\npolarized DIS $\\vec{e}\\vec{p} \\rightarrow {\\rm jet} +X$. We perform the\ncomputation based on the Projection-to-Born method by combining our recent NLO\nresult for di-jet production in polarized DIS along with the NNLO coefficients\nfor the inclusive cross section. In this way, we achieve NNLO accuracy in a\nfully exclusive way for single-jet observables, the first time for a polarized\ncross section. We study the perturbative stability and phenomenological\nconsequences of the QCD corrections for Electron Ion Collider kinematics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Pancreas stereotactic body radiotherapy treatment planning requires planners\nto make sequential, time consuming interactions with the treatment planning\nsystem (TPS) to reach the optimal dose distribution. We seek to develop a\nreinforcement learning (RL)-based planning bot to systematically address\ncomplex tradeoffs and achieve high plan quality consistently and efficiently.\nThe focus of pancreas SBRT planning is finding a balance between organs-at-risk\nsparing and planning target volume (PTV) coverage. Planners evaluate dose\ndistributions and make planning adjustments to optimize PTV coverage while\nadhering to OAR dose constraints. We have formulated such interactions between\nthe planner and the TPS into a finite-horizon RL model. First, planning status\nfeatures are evaluated based on human planner experience and defined as\nplanning states. Second, planning actions are defined to represent steps that\nplanners would commonly implement to address different planning needs. Finally,\nwe have derived a reward system based on an objective function guided by\nphysician-assigned constraints. The planning bot trained itself with 48 plans\naugmented from 16 previously treated patients and generated plans for 24 cases\nin a separate validation set. All 24 bot-generated plans achieve similar PTV\ncoverages compared to clinical plans while satisfying all clinical planning\nconstraints. Moreover, the knowledge learned by the bot can be visualized and\ninterpreted as consistent with human planning knowledge, and the knowledge maps\nlearned in separate training sessions are consistent, indicating\nreproducibility of the learning process.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the nonlinear ion-acoustic wave induced by the orbiting charged\nspace debris in the plasma environment generated at Low Earth Orbital (LEO)\nregion. The generated nonlinear ion-acoustic wave is shown to be governed by\nthe forced Korteweg-de Vries equation with the forcing function dependent on\nthe charged space debris function. For a specific relationship between the\nforcing debris function and the nonlinear ion-acoustic wave, the forced KdV\nequation turns to be a completely integrable system where the debris function\nobeys a definite non-holonomic constraint. A special exact accelerated soliton\nsolution (velocity of the soliton changes over time whereas its amplitude\nremains constant) has been derived for the ion-acoustic wave for the first\ntime. On the other hand, the amplitude of the solitonic debris function varies\nwith time, and its shape changes during propagation. Approximate ion-acoustic\nsolitary wave solutions with time-varying amplitude and velocity, have been\nderived for different weak localized charged debris functions. Possible\napplications of the obtained results in space plasma physics are stated along\nwith future the direction of research.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work we define the surfaces spherical type via support function (in\nshort, SS-surfaces). We present a Weierstrass type representation for\nSS-surfaces with prescribed Gauss map which depends on two holomorphic\nfunctions. Also, we use this representation to classify the surfaces of\nrotation. Moreover, we show that every compact and connected SS-surface is the\nsphere and we give explicit examples of SS-surfaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  3D reconstruction is a challenging current topic in medical research. We\nperform 3D reconstructions from serial sections stained by immunohistological\nmethods. This paper presents an immersive visualisation solution to quality\ncontrol (QC), inspect, and analyse such reconstructions. QC is essential to\nestablish correct digital processing methodologies. Visual analytics, such as\nannotation placement, mesh painting, and classification utility, facilitates\nmedical research insights. We propose a visualisation in virtual reality (VR)\nfor these purposes. In this manner, we advance the microanatomical research of\nhuman bone marrow and spleen. Both 3D reconstructions and original data are\navailable in VR. Data inspection is streamlined by subtle implementation\ndetails and general immersion in VR.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Knee osteoarthritis (OA) is very common progressive and degenerative\nmusculoskeletal disease worldwide creates a heavy burden on patients with\nreduced quality of life and also on society due to financial impact. Therefore,\nany attempt to reduce the burden of the disease could help both patients and\nsociety. In this study, we propose a fully automated novel method, based on\ncombination of joint shape and convolutional neural network (CNN) based bone\ntexture features, to distinguish between the knee radiographs with and without\nradiographic osteoarthritis. Moreover, we report the first attempt at\ndescribing the bone texture using CNN. Knee radiographs from Osteoarthritis\nInitiative (OAI) and Multicenter Osteoarthritis (MOST) studies were used in the\nexperiments. Our models were trained on 8953 knee radiographs from OAI and\nevaluated on 3445 knee radiographs from MOST. Our results demonstrate that\nfusing the proposed shape and texture parameters achieves the state-of-the art\nperformance in radiographic OA detection yielding area under the ROC curve\n(AUC) of 95.21%\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Entanglement related properties work as nice fingerprint of the quantum\nmany-body wave function. However, those of fermionic models are hard to\nevaluate in standard numerical methods because they suffer from finite size\neffects. We show that a so-called density embedding theory (DET) can evaluate\nthem without size scaling analysis in comparably high quality with those\nobtained by the large-size density matrix renormalization group analysis. This\nmethod projects the large scale original many-body Hamiltonian to the small\nnumber of basis sets defined on a local cluster, and optimizes the choice of\nthese bases by tuning the local density matrix. The DET entanglement spectrum\nof one-dimensional interacting fermions perfectly reproduces the exact ones and\nworks as a marker of the phase transition point. It is further shown that the\nphase transitions in two-dimension could be determined by the entanglement\nentropy and the fidelity that reflects the change of the structure of the wave\nfunction.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We analyze gravitationally localized states of multiple fermions with high\nangular momenta, in the formalism introduced by Finster, Smoller, and Yau [Phys\nRev. D 59, 104020 (1999)]. We show that the resulting soliton-like wave\nfunctions can be naturally interpreted in terms of a form of self-trapping,\nwhere the fermions become localized on shells the locations of which correspond\nto those of `bulges' in the optical geometry created by their own energy\ndensity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Federations of RDF data sources provide great potential when queried for\nanswers and insights that cannot be obtained from one data source alone. A\nchallenge for planning the execution of queries over such a federation is that\nthe federation may be heterogeneous in terms of the types of data access\ninterfaces provided by the federation members. This challenge has not received\nmuch attention in the literature. This paper provides a solid formal foundation\nfor future approaches that aim to address this challenge. Our main conceptual\ncontribution is a formal language for representing query execution plans;\nadditionally, we identify a fragment of this language that can be used to\ncapture the result of selecting relevant data sources for different parts of a\ngiven query. As technical contributions, we show that this fragment is more\nexpressive than what is supported by existing source selection approaches,\nwhich effectively highlights an inherent limitation of these approaches.\nMoreover, we show that the source selection problem is NP-hard and in\n$\\Sigma_2^\\mathrm{P}$, and we provide a comprehensive set of rewriting rules\nthat can be used as a basis for query optimization.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Since the discovery of CP violation more than 5 decades ago, this phenomenon\nis still attracting a lot of interest. Among the many fascinating aspects of\nthis subject, this review is dedicated to direct CP violation in non-leptonic\ndecays. The advances within the last decade have been enormous, driven by the\nincreasingly large samples of b- and c-hadron decays, and have led to very\ninteresting results such as large CP asymmetries in charmless B decays and the\nobservation of direct CP violation in the charm sector. We address the quest\nfor understanding the origin of strong phases, the importance of final state\ninteractions and the relation with CPT symmetry, and different approaches to\nmeasure direct CP violation in these decays. The main experimental results and\ntheir implications are then discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Nuclear stellar clusters are common in the center of galaxies. We consider\nthe possibility that their progenitors assumed to be globular clusters may have\nformed elsewhere, migrated to and assembled near their present location. The\nmain challenge for this scenario is whether globular clusters can withstand the\ntidal field of their host galaxies. Our analysis suggests that provided the\nmass-density distribution of background potential is relatively shallow, as in\nsome galaxies with relatively flat surface brightness profiles, the tidal field\nnear the center of galaxies may be shown to be able to compress rather than\ndisrupt a globular cluster at a distance from the center much smaller than the\nconventionally defined `tidal disruption radius', $r_t$. To do so, we adopt a\npreviously constructed formalism and consider the secular evolution of star\nclusters with a homogeneous mass density distribution. We analytically solve\nthe secular equations in the limit that the mass density of stars in the\ngalactic center approaches a uniform distribution. Our model indicates that a\nstar cluster could travel to distances much smaller than $r_t$ without\ndisruption, thus potentially contributing to the formation of the nuclear\ncluster. However, appropriate numerical N-body simulations are needed to\nconfirm our analytic findings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We provide an explicit description of the Poincar\\'e dual of each generator\nof the rational cohomology ring of the $SU(2)$ character variety for a genus\n$g$ surface with central extension -- equivalently, that of the moduli space of\nstable holomorphic bundles of rank 2 and odd degree.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a growth model for special Cosserat rods that allows for induced\nrotation of cross-sections. The growth law considers two controls, one for\nlengthwise growth and other for rotations. This is explored in greater detail\nfor straight rods with helical and hemitropic material symmetries by\nintroduction of a symmetry preserving growth to account for the microstructure.\nThe example of a guided-guided rod possessing a chiral microstructure is\nconsidered to study its deformation due to growth. We show the occurrence of\ngrowth induced out-of-plane buckling in such rods.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In noisy conditions, knowing speech contents facilitates listeners to more\neffectively suppress background noise components and to retrieve pure speech\nsignals. Previous studies have also confirmed the benefits of incorporating\nphonetic information in a speech enhancement (SE) system to achieve better\ndenoising performance. To obtain the phonetic information, we usually prepare a\nphoneme-based acoustic model, which is trained using speech waveforms and\nphoneme labels. Despite performing well in normal noisy conditions, when\noperating in very noisy conditions, however, the recognized phonemes may be\nerroneous and thus misguide the SE process. To overcome the limitation, this\nstudy proposes to incorporate the broad phonetic class (BPC) information into\nthe SE process. We have investigated three criteria to build the BPC, including\ntwo knowledge-based criteria: place and manner of articulatory and one\ndata-driven criterion. Moreover, the recognition accuracies of BPCs are much\nhigher than that of phonemes, thus providing more accurate phonetic information\nto guide the SE process under very noisy conditions. Experimental results\ndemonstrate that the proposed SE with the BPC information framework can achieve\nnotable performance improvements over the baseline system and an SE system\nusing monophonic information in terms of both speech quality intelligibility on\nthe TIMIT dataset.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Higgs sector of the Standard model (SM) is replaced by quantum flavor\ndynamics (QFD), the gauged flavor $SU(3)_f$ symmetry with scale $\\Lambda$.\nAnomaly freedom requires addition of three $\\nu_R$. The approximate QFD\nSchwinger-Dyson equation for the Euclidean infrared fermion self-energies\n$\\Sigma_f(p^2)$ has the spontaneous-chiral-symmetry-breaking solutions ideal\nfor seesaw: (1) $\\Sigma_f(p^2)=M_{fR}^2/p$ where three Majorana masses $M_{fR}$\nof $\\nu_{fR}$ are of order $\\Lambda$. (2) $\\Sigma_f(p^2)=m_f^2/p$ where three\nDirac masses $m_f=m_{(0)}1+m_{(3)}\\lambda_3+m_{(8)}\\lambda_8$ of SM fermions\nare {\\it exponentially suppressed w.r.t. $\\Lambda$}, and {\\it degenerate for\nall SM fermions in $f$}. (1) $M_{fR}$ break $SU(3)_f$ symmetry completely;\n$m_{(3)},m_{(8)}$ superimpose the tiny breaking to $U(1) \\times U(1)$. All\nflavor gluons thus acquire self-consistently the masses $\\sim \\Lambda$. (2) All\n$m_f$ break the electroweak $SU(2)_L \\times U(1)_Y$ to $U(1)_{em}$. Symmetry\npartners of the composite Nambu-Goldstone bosons are the genuine Higgs\nparticles: (1) Three $\\nu_{R}$-composed Higgses $\\chi_i$ with masses $\\sim\n\\Lambda$. (2) Two new SM-fermion-composed Higgses $h_3, h_8$ with masses $\\sim\nm_{(3)}, m_{(8)}$, respectively. (3) The SM-like SM-fermion-composed Higgs $h$\nwith mass $\\sim m_{(0)}$, the effective Fermi scale. $\\Sigma_f(p^2)$-dependent\nvertices in the electroweak Ward-Takahashi identities imply: The axial-vector\nones give rise to the $W$ and $Z$ masses at Fermi scale. The polar-vector ones\ngive rise to the fermion mass splitting in $f$. At the present exploratory\nstage the splitting comes out unrealistic.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The correlation between fresh gas accretion onto haloes and galaxy star\nformation is critical to understanding galaxy formation. Different theoretical\nmodels have predicted different correlation strengths between halo accretion\nrates and galaxy star formation rates, ranging from strong positive\ncorrelations to little or no correlation. Here, we present a technique to\nobservationally measure this correlation strength for isolated Milky Way-mass\ngalaxies with $z < 0.123$. This technique is based on correlations between dark\nmatter accretion rates and the projected density profile of neighbouring\ngalaxies; these correlations also underlie past work with splashback radii. We\napply our technique to both observed galaxies in the Sloan Digital Sky Survey\nas well as simulated galaxies in the UniverseMachine where we can test any\ndesired correlation strength. We find that positive correlations between dark\nmatter accretion and recent star formation activity are ruled out with $\\gtrsim\n85\\%$ confidence. Our results suggest that star formation activity may not be\ncorrelated with fresh accretion for isolated Milky Way-mass galaxies at $z=0$\nand that other processes, such as gas recycling, dominate further galaxy\ngrowth.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove #P-completeness results for counting edge colorings on simple\ngraphs. These strengthen the corresponding results on multigraphs from [4]. We\nprove that for any $\\kappa \\ge r \\ge 3$ counting $\\kappa$-edge colorings on\n$r$-regular simple graphs is #P-complete. Furthermore, we show that for planar\n$r$-regular simple graphs where $r \\in \\{3, 4, 5\\}$ counting edge colorings\nwith \\k{appa} colors for any $\\kappa \\ge r$ is also #P-complete. As there are\nno planar $r$-regular simple graphs for any $r > 5$, these statements cover all\ninteresting cases in terms of the parameters $(\\kappa, r)$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We explore the new physics reach for the off-shell Higgs boson measurement in\nthe ${pp \\to H^* \\rightarrow Z(\\ell^{+}\\ell^{-})Z(\\nu\\bar{\\nu})}$ channel at\nthe high-luminosity LHC. The new physics sensitivity is parametrized in terms\nof the Higgs boson width, effective field theory framework, and a non-local\nHiggs-top coupling form factor. Adopting Machine-learning techniques, we\ndemonstrate that the combination of a large signal rate and a precise\nphenomenological probe for the process energy scale, due to the transverse $ZZ$\nmass, leads to significant sensitivities beyond the existing results in the\nliterature for the new physics scenarios considered.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a strategic formulation for the joint source-channel coding\nproblem in which the encoder and the decoder are endowed with distinct\ndistortion functions. We provide the solutions in four different scenarios.\nFirst, we assume that the encoder and the decoder cooperate in order to achieve\na certain pair of distortion values. Second, we suppose that the encoder\ncommits to a strategy whereas the decoder implements a best response, as in the\npersuasion game where the encoder is the Stackelberg leader. Third, we consider\nthat the decoder commits to a strategy, as in the mismatched rate-distortion\nproblem or as in the mechanism design framework. Fourth, we investigate the\ncheap talk game in which the encoding and the decoding strategies form a Nash\nequilibrium.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present optical and infrared photometry of the classical nova V906 Car,\nalso known as Nova Car 2018 and ASASSN-18fv, discovered by ASASS-SN survey on\n16.32 March 2018 UT (MJD 58193.0). The nova reached its maximum on MJD 58222.56\nat $V_{\\rm{max}} = 5.84 \\pm 0.09$ mag and had decline times of $t_{2,V} = 26.2\n$ d and $t_{3,V} = 33.0 $ d. The data from Evryscope shows that the nova had\nalready brightened to $g'\\simeq 13$\\,mag five days before discovery, as\ncompared to its quiescent magnitude of $g=$20.13$\\pm$0.03. The extinction\ntowards the nova, as derived from high resolution spectroscopy, shows an\nestimate consistent with foreground extinction to the Carina Nebula of $A_V =\n1.11_{-0.39}^{+0.54}$. The light curve resembles a rare C (cusp) class nova\nwith a steep decline slope of $\\alpha=-3.94$ post cusp flare. From the\nlightcurve decline rate, we estimate the mass of white dwarf to be $M_{WD}$ = $\n< 0.8$M\\textsubscript{\\(\\odot\\)}, consistent with $M_{WD}=0.71^{+0.23}_{-0.19}$\nderived from modelling the accretion disk of the system in quiescence. The\ndonor star is likely a K-M dwarf of 0.23-0.43\\,\\Msun, which is being heated by\nits companion.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we propose a computationally tractable and theoretically\nsupported non-linear low-dimensional generative model to represent real-world\ndata in the presence of noise and sparse outliers. The non-linear\nlow-dimensional manifold discovery of data is done through describing a joint\ndistribution over observations, and their low-dimensional representations (i.e.\nmanifold coordinates). Our model, called generative low-dimensional background\nmodel (G-LBM) admits variational operations on the distribution of the manifold\ncoordinates and simultaneously generates a low-rank structure of the latent\nmanifold given the data. Therefore, our probabilistic model contains the\nintuition of the non-probabilistic low-dimensional manifold learning. G-LBM\nselects the intrinsic dimensionality of the underling manifold of the\nobservations, and its probabilistic nature models the noise in the observation\ndata. G-LBM has direct application in the background scenes model estimation\nfrom video sequences and we have evaluated its performance on SBMnet-2016 and\nBMC2012 datasets, where it achieved a performance higher or comparable to other\nstate-of-the-art methods while being agnostic to the background scenes in\nvideos. Besides, in challenges such as camera jitter and background motion,\nG-LBM is able to robustly estimate the background by effectively modeling the\nuncertainties in video observations in these scenarios.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Data forms a key component of any enterprise. The need for high quality and\neasy access to data is further amplified by organizations wishing to leverage\nmachine learning or artificial intelligence for their operations. To this end,\nmany organizations are building resources for managing heterogenous data,\nproviding end-users with an organization wide view of available data, and\nacting as a centralized repository for data owned/collected by an organization.\nVery broadly, we refer to these class of techniques as a \"data hub.\" While\nthere is no clear definition of what constitutes a data hub, some of the key\ncharacteristics include: data catalog; links to data sets or owners of data\nsets or centralized data repository; basic ability to serve / visualize data\nsets; access control policies that ensure secure data access and respects\npolicies of data owners; and computing capabilities tied with data hub\ninfrastructure. Of course, developing such a data hub entails numerous\nchallenges. This document provides background in databases, data management and\noutlines best practices and recommendations for developing and deploying a\nworking data hub.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that a combined analysis of CMB anisotropy power spectra obtained by\nthe Planck satellite and luminosity distance data simultaneously excludes a\nflat universe and a cosmological constant at $99 \\%$ CL. These results hold\nseparately when combining Planck with three different datasets: the two\ndeterminations of the Hubble constant from Riess et al. 2019 and Freedman et\nal. 2020, and the Pantheon catalog of high redshift supernovae type-Ia. We\nconclude that either LCDM needs to be replaced by a different model, or else\nthere are significant but still undetected systematics. Our result calls for\nnew observations and stimulates the investigation of alternative theoretical\nmodels and solutions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Mathematical models of epidemiological systems enable investigation of and\npredictions about potential disease outbreaks. However, commonly used models\nare often highly simplified representations of incredibly complex systems.\nBecause of these simplifications, the model output, of say new cases of a\ndisease over time, or when an epidemic will occur, may be inconsistent with\navailable data. In this case, we must improve the model, especially if we plan\nto make decisions based on it that could affect human health and safety, but\ndirect improvements are often beyond our reach. In this work, we explore this\nproblem through a case study of the Zika outbreak in Brazil in 2016. We propose\nan embedded discrepancy operator---a modification to the model equations that\nrequires modest information about the system and is calibrated by all relevant\ndata. We show that the new enriched model demonstrates greatly increased\nconsistency with real data. Moreover, the method is general enough to easily\napply to many other mathematical models in epidemiology.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We are concerned with the global existence theory for spherically symmetric\nsolutions of the multidimensional compressible Euler equations with large\ninitial data of positive far-field density. The central feature of the\nsolutions is the strengthening of waves as they move radially inward toward the\norigin. Various examples have shown that the spherically symmetric solutions of\nthe Euler equations blow up near the origin at certain time. A fundamental\nunsolved problem is whether the density of the global solution would form\nconcentration to become a measure near the origin for the case when the total\ninitial-energy is unbounded and the wave propagation is not at a finite speed\nstarting initially. In this paper, we establish a global existence theory for\nspherically symmetric solutions of the compressible Euler equations with large\ninitial data of positive far-field density and relative finite-energy. This is\nachieved by developing a new approach via adapting a class of degenerate\ndensity-dependent viscosity terms, so that a rigorous proof of the vanishing\nviscosity limit of global weak solutions of the Navier-Stokes equations with\nthe density-dependent viscosity terms to the corresponding global solution of\nthe Euler equations with large initial data of spherical symmetry and positive\nfar-field density can be obtained. One of our main observations is that the\nadapted class of degenerate density-dependent viscosity terms not only includes\nthe viscosity terms for the Navier-Stokes equations for shallow water (Saint\nVenant) flows but also, more importantly, is suitable to achieve our key\nobjective of this paper. These results indicate that concentration is not\nformed in the vanishing viscosity limit for the Navier-Stokes approximations\nconstructed in this paper even when the total initial-energy is unbounded.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The emergence of the world-wide COVID-19 pandemic has forced academic\nconferences to be held entirely in a virtual manner. While prior studies have\nadvocated the merits of virtual conferences in terms of energy and cost\nsavings, organizers are increasingly facing the prospect of planning and\nexecuting them systematically, in order to deliver a rich\nconference-attending-experience for all participants. Starting from March 2020,\ntens of conferences have been held virtually. Past conferences have revealed\nnumerous challenges, from budget planning, to selecting the supporting virtual\nplatforms. Among these, two special challenges were identified: 1) how to\ndeliver talks to geo-distributed attendees and 2) how to stimulate social\ninteractions among attendees. These are the two important goals of an academic\nconference. In this paper, we advocate a mirror program approach for academic\nconferences. More specifically, the conference program is executed in multiple\nparallel (mirrored) programs, so that each mirror program can fit a different\ntime zone. This can effectively address the first challenge.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We use gradient sparsification to reduce the adverse effect of differential\nprivacy noise on performance of private machine learning models. To this aim,\nwe employ compressed sensing and additive Laplace noise to evaluate\ndifferentially-private gradients. Noisy privacy-preserving gradients are used\nto perform stochastic gradient descent for training machine learning models.\nSparsification, achieved by setting the smallest gradient entries to zero, can\nreduce the convergence speed of the training algorithm. However, by\nsparsification and compressed sensing, the dimension of communicated gradient\nand the magnitude of additive noise can be reduced. The interplay between these\neffects determines whether gradient sparsification improves the performance of\ndifferentially-private machine learning models. We investigate this\nanalytically in the paper. We prove that, for small privacy budgets,\ncompression can improve performance of privacy-preserving machine learning\nmodels. However, for large privacy budgets, compression does not necessarily\nimprove the performance. Intuitively, this is because the effect of\nprivacy-preserving noise is minimal in large privacy budget regime and thus\nimprovements from gradient sparsification cannot compensate for its slower\nconvergence.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present a generic deep convolutional neural network (DCNN)\nfor multi-class image segmentation. It is based on a well-established\nsupervised end-to-end DCNN model, known as U-net. U-net is firstly modified by\nadding widely used batch normalization and residual block (named as BRU-net) to\nimprove the efficiency of model training. Based on BRU-net, we further\nintroduce a dynamically weighted cross-entropy loss function. The weighting\nscheme is calculated based on the pixel-wise prediction accuracy during the\ntraining process. Assigning higher weights to pixels with lower segmentation\naccuracies enables the network to learn more from poorly predicted image\nregions. Our method is named as feedback weighted U-net (FU-net). We have\nevaluated our method based on T1- weighted brain MRI for the segmentation of\nmidbrain and substantia nigra, where the number of pixels in each class is\nextremely unbalanced to each other. Based on the dice coefficient measurement,\nour proposed FU-net has outperformed BRU-net and U-net with statistical\nsignificance, especially when only a small number of training examples are\navailable. The code is publicly available in GitHub (GitHub link:\nhttps://github.com/MinaJf/FU-net).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present SUMO, a neural attention-based approach that learns to establish\nthe correctness of textual claims based on evidence in the form of text\ndocuments (e.g., news articles or Web documents). SUMO further generates an\nextractive summary by presenting a diversified set of sentences from the\ndocuments that explain its decision on the correctness of the textual claim.\nPrior approaches to address the problem of fact checking and evidence\nextraction have relied on simple concatenation of claim and document word\nembeddings as an input to claim driven attention weight computation. This is\ndone so as to extract salient words and sentences from the documents that help\nestablish the correctness of the claim. However, this design of claim-driven\nattention does not capture the contextual information in documents properly. We\nimprove on the prior art by using improved claim and title guided hierarchical\nattention to model effective contextual cues. We show the efficacy of our\napproach on datasets concerning political, healthcare, and environmental\nissues.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A long-standing area of materials science research has been the study of\nelectrostatic, magnetic, and elastic fields in composite with densely packed\ninclusions whose material properties differ from that of the background. For a\ngeneral elliptic system, when the coefficients are piecewise H\\\"{o}lder\ncontinuous and uniformly bounded, an $\\varepsilon$-independent bound of the\ngradient was obtained by Li and Nirenberg \\cite{ln}, where $\\varepsilon$\nrepresents the distance between the interfacial surfaces. However, in\nhigh-contrast composites, when $\\varepsilon$ tends to zero, the stress always\nconcentrates in the narrow regions. As a contrast to the uniform boundedness\nresult of Li and Nirenberg, in order to investigate the role of $\\varepsilon$\nplayed in such kind of concentration phenomenon, in this paper we establish the\nblow-up asymptotic expressions of the gradients of solutions to the Lam\\'{e}\nsystem with partially infinite coefficients in dimensions two and three. We\ndiscover the relationship between the blow-up rate of the stress and the\nrelative convexity of adjacent surfaces, and find a family of blow-up factor\nmatrices with respect to the boundary data. Therefore, this work completely\nsolves the Babu\\u{s}ka problem on blow-up analysis of stress concentration in\nhigh-contrast composite media. Moreover, as a byproduct of these local\nanalysis, we establish an extended Flaherty-Keller formula on the global\neffective elastic property of a periodic composite with densely packed fibers,\nwhich is related to the \"Vigdergauz microstructure\" in the shape optimization\nof fibers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The spatially homogeneous BGK equation is obtained as the limit if a model of\na many particle system, similar to Mark Kac's charicature of the spatially\nhomogeneous Boltzmann equation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Context: Software code review aims to early find code anomalies and to\nperform code improvements when they are less expensive. However, issues and\nchallenges faced by developers who do not apply code review practices regularly\nare unclear. Goal: Investigate difficulties developers face to apply code\nreview practices without limiting the target audience to developers who already\nuse this practice regularly. Method: We conducted a web-based survey with 350\nBrazilian practitioners engaged on the software development industry. Results:\nCode review practices are widespread among Brazilian practitioners who\nrecognize its importance. However, there is no routine for applying these\npractices. In addition, they report difficulties to fit static analysis tools\nin the software development process. One possible reason recognized by\npractitioners is that most of these tools use a single metric threshold, which\nmight be not adequate to evaluate all system classes. Conclusion: Improving\nguidelines to fit code review practices into the software development process\ncould help to make them widely used. Additionally, future studies should\ninvestigate whether multiple metric thresholds that take source code context\ninto account reduce static analysis tool false alarms. Finally, these tools\nshould allow their use in distinct phases of the software development process.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report on the time evolution of a charged strongly coupled $N=4$ SYM\nplasma with an axial anomaly subjected to strong electromagnetic fields. The\nevolution of this plasma corresponds to a fully backreacted asymptotically\nAdS$_5$ solution to the Einstein-Maxwell-Chern-Simons theory. We explore the\nevolution of the axial current and production of axial charges. As an\napplication we show that after a sufficiently long time both the entropy and\nthe holographic entanglement entropy of a strip-like topology ( both parallel\nto and transverse to the flow of axial current) grow linearly in time.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Since 2006 the theory of slice hyperholomorphic functions and the related\nspectral theory on the S-spectrum have had a very fast development. This new\nspectral theory based on the S-spectrum has applications, for example, in the\nformulation of quaternionic quantum mechanics, in Schur analysis and in\nfractional diffusion problems. In this paper we introduce and study the theory\nof poly slice monogenic functions, also proving some Cauchy type integral\nformulas. Then we introduce the associated functional calculus, called\nPS-functional calculus, which is the polyanalytic version of the S-functional\ncalculus and which is based on the notion of S-spectrum. We study some\ndifferent formulations of the calculus and we prove some of its properties,\namong which the product rules.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hidden Markov Model (HMM) combined with Gaussian Process (GP) emission can be\neffectively used to estimate the hidden state with a sequence of complex\ninput-output relational observations. Especially when the spectral mixture (SM)\nkernel is used for GP emission, we call this model as a hybrid HMM-GPSM. This\nmodel can effectively model the sequence of time-series data. However, because\nof a large number of parameters for the SM kernel, this model can not\neffectively be trained with a large volume of data having (1) long sequence for\nstate transition and 2) a large number of time-series dataset in each sequence.\nThis paper proposes a scalable learning method for HMM-GPSM. To effectively\ntrain the model with a long sequence, the proposed method employs a Stochastic\nVariational Inference (SVI) approach. Also, to effectively process a large\nnumber of data point each time-series data, we approximate the SM kernel using\nReparametrized Random Fourier Feature (R-RFF). The combination of these two\ntechniques significantly reduces the training time. We validate the proposed\nlearning method in terms of its hidden-sate estimation accuracy and computation\ntime using large-scale synthetic and real data sets with missing values.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the problem of generating informative questions in\ninformation-asymmetric conversations. Unlike previous work on question\ngeneration which largely assumes knowledge of what the answer might be, we are\ninterested in the scenario where the questioner is not given the context from\nwhich answers are drawn, but must reason pragmatically about how to acquire new\ninformation, given the shared conversation history. We identify two core\nchallenges: (1) formally defining the informativeness of potential questions,\nand (2) exploring the prohibitively large space of potential questions to find\nthe good candidates. To generate pragmatic questions, we use reinforcement\nlearning to optimize an informativeness metric we propose, combined with a\nreward function designed to promote more specific questions. We demonstrate\nthat the resulting pragmatic questioner substantially improves the\ninformativeness and specificity of questions generated over a baseline model,\nas evaluated by our metrics as well as humans.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  When the COVID-19 disease pandemic infiltrated the world, there was an\nimmediate need for accurate information. As with any outbreak, the outbreak\nfollows a clear trajectory, and subsequently, the supporting information for\nthat outbreak needs to address the needs associated with that stage of the\noutbreak. At first, there was a need to inform the public of the information\nrelated to the initial situation related to the \"who\" of the COVID-19 disease.\nHowever, as time continued, the \"where\", \"when\" and \"how to\" related questions\nstarted to emerge in relation to the public healthcare system themselves.\nQuestions surrounding the health facilities including COVID-19 hospital bed\ncapacity, locations of designated COVID-19 facilities, and general information\nrelated to these facilities were not easily accessible to the general public.\nFurthermore, the available information was found to be outdated, fragmented\nacross several platforms, and still had gaps in the data related to these\nfacilities. To rectify this problem, a group of volunteers working on the\ncovid19za project stepped in to assist. Each member leading a part of the\nproject chose to focus on one of four problems related to the challenges\nassociated with the Hospital information including: data quality, data\ncompleteness, data source validation and data visualisation capacity. As the\nproject developed, so did the sophistication of the data, visualisation and\ncore function of the project. The future prospects of this project relate to a\nProgressive Web Application that will avail this information for the public as\nwell as healthcare workers through comprehensive mapping and data quality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Recent work has identified a number of formally incompatible operational\nmeasures for the unfairness of a machine learning (ML) system. As these\nmeasures all capture intuitively desirable aspects of a fair system, choosing\n\"the one true\" measure is not possible, and instead a reasonable approach is to\nminimize a weighted combination of measures. However, this simply raises the\nquestion of how to choose the weights. Here, we formulate Legally Grounded\nFairness Objectives (LGFO), which uses signals from the legal system to\nnon-arbitrarily measure the social cost of a specific degree of unfairness. The\nLGFO is the expected damages under a putative lawsuit that might be awarded to\nthose who were wrongly classified, in the sense that the ML system made a\ndecision different to that which would have be made under the court's preferred\nmeasure. Notably, the two quantities necessary to compute the LGFO, the court's\npreferences about fairness measures, and the expected damages, are unknown but\nwell-defined, and can be estimated by legal advice. Further, as the damages\nawarded by the legal system are designed to measure and compensate for the harm\ncaused to an individual by an unfair classification, the LGFO aligns closely\nwith society's estimate of the social cost.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Cell-cell adhesion is an inherently nonlocal phenomenon. Numerous partial\ndifferential equation models with nonlocal term have been recently presented to\ndescribe this phenomenon, yet the mathematical properties of nonlocal adhesion\nmodel are not well understood. Here we consider a model with two kinds of\nnonlocal cell-cell adhesion, satisfying no-flux conditions in a\nmultidimensional bounded domain. We show global-in-time well-posedness of the\nsolution to this model and obtain the uniform boundedness of solution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the evolution of axions interacting with primordial magnetic fields\n(PMFs) starting just from the QCD phase transition in the expanding universe.\nThis interaction is owing to the Primakoff effect. Adopting the zero mode\napproximation for axions, we derive the system of equations for axions and\nmagnetic fields, where the expansion of the universe and the spectra of\nmagnetic fields are accounted for exactly. We find that the contribution of the\nPrimakoff effect to the dynamics of axions and magnetic fields is rather weak.\nIt confirms some previous estimates leading to analogous conclusions, when\naccounting here for the Hubble expansion both for an uniform axion field and\nnon-uniform PMFs using Fourier spectra for their energy and helicity densities.\nWe solve the corresponding system of the evolution equations and find that the\naxion zero mode, when evolving during radiation era, has its amplitude at the\nlevel sufficient for that axion to be a good candidate for the cold dark\nmatter.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Traditional learning-based approaches for run-time Hardware Trojan detection\nrequire complex and expensive on-chip data acquisition frameworks and thus\nincur high area and power overhead. To address these challenges, we propose to\nleverage the power correlation between the executing instructions of a\nmicroprocessor to establish a machine learning-based run-time Hardware Trojan\n(HT) detection framework, called MacLeR. To reduce the overhead of data\nacquisition, we propose a single power-port current acquisition block using\ncurrent sensors in time-division multiplexing, which increases accuracy while\nincurring reduced area overhead. We have implemented a practical solution by\nanalyzing multiple HT benchmarks inserted in the RTL of a system-on-chip (SoC)\nconsisting of four LEON3 processors integrated with other IPs like vga_lcd,\nRSA, AES, Ethernet, and memory controllers. Our experimental results show that\ncompared to state-of-the-art HT detection techniques, MacLeR achieves 10\\%\nbetter HT detection accuracy (i.e., 96.256%) while incurring a 7x reduction in\narea and power overhead (i.e., 0.025% of the area of the SoC and <0.07% of the\npower of the SoC). In addition, we also analyze the impact of process variation\nand aging on the extracted power profiles and the HT detection accuracy of\nMacLeR. Our analysis shows that variations in fine-grained power profiles due\nto the HTs are significantly higher compared to the variations in fine-grained\npower profiles caused by the process variations (PV) and aging effects.\nMoreover, our analysis demonstrates that, on average, the HT detection accuracy\ndrop in MacLeR is less than 1% and 9% when considering only PV and PV with\nworst-case aging, respectively, which is ~10x less than in the case of the\nstate-of-the-art ML-based HT detection technique.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Anomaly detection when observing a large number of data streams is essential\nin a variety of applications, ranging from epidemiological studies to\nmonitoring of complex systems. High-dimensional scenarios are usually tackled\nwith scan-statistics and related methods, requiring stringent modeling\nassumptions for proper calibration. In this work we take a non-parametric\nstance, and propose a permutation-based variant of the higher criticism\nstatistic not requiring knowledge of the null distribution. This results in an\nexact test in finite samples which is asymptotically optimal in the wide class\nof exponential models. We demonstrate the power loss in finite samples is\nminimal with respect to the oracle test. Furthermore, since the proposed\nstatistic does not rely on asymptotic approximations it typically performs\nbetter than popular variants of higher criticism that rely on such\napproximations. We include recommendations such that the test can be readily\napplied in practice, and demonstrate its applicability in monitoring the\ncontent uniformity of an active ingredient for a batch-produced drug product.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Given a smooth, projective variety $X$ and an effective divisor\n$D\\,\\subseteq\\, X$, it is well-known that the (topological) obstruction to the\ndeformation of the fundamental class of $D$ as a Hodge class, lies in\n$H^2(\\mathcal{O}_X)$. In this article, we replace $H^2(\\mathcal{O}_X)$ by\n$H^2_D(\\mathcal{O}_X)$ and give an analogous topological obstruction theory. We\ncompare the resulting local topological obstruction theory with the geometric\nobstruction theory (i.e., the obstruction to the deformation of $D$ as an\neffective Cartier divisor of a first order infinitesimal deformations of $X$).\nWe apply this to study the jumping locus of families of linear systems and the\nNoether-Lefschetz locus. Finally, we give examples of first order deformations\n$X_t$ of $X$ for which the cohomology class $[D]$ deforms as a Hodge class but\n$D$ does not lift as an effective Cartier divisor of $X_t$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the study of micro-swimmers, both artificial and biological ones,\nmany-query problems arise naturally. Even with the use of advanced high\nperformance computing (HPC), it is not possible to solve this kind of problems\nin an acceptable amount of time. Various approximations of the Stokes equation\nhave been considered in the past to ease such computational efforts but they\nintroduce non-negligible errors that can easily make the solution of the\nproblem inaccurate and unreliable. Reduced order modeling solves this issue by\ntaking advantage of a proper subdivision between a computationally expensive\noffline phase and a fast and efficient online stage.\n  This work presents the coupling of Boundary Element Method (BEM) and Reduced\nBasis (RB) Reduced\n  Order Modeling (ROM) in two models of practical interest, obtaining accurate\nand reliable solutions to different many-query problems. Comparisons of\nstandard reduced order modeling approaches in different simulation settings and\na comparison to typical approximations to Stokes equations are also shown.\nDifferent couplings between a solver based on a HPC boundary element method for\nmicro-motility problems and reduced order models are presented in detail. The\nmethodology is tested on two different models: a robotic-bacterium-like and an\nEukaryotic-like swimmer, and in each case two resolution strategies for the\nswimming problem, the split and monolithic one, are used as starting points for\nthe ROM. An efficient and accurate reconstruction of the performance of\ninterest is achieved in both cases proving the effectiveness of our strategy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Power systems are undergoing a transformation toward a low-carbon\nnon-synchronous generation portfolio. A major concern for system planners and\noperators is the system dynamics in the high renewable penetration future.\nBecause of the scale of the system and numerous components involved, it is\nextremely difficult to develop high PV dynamic models based upon actual power\nsystem models. The main contribution of this paper is providing an example of\ndeveloping high PV penetration models based on the validated dynamic model of\nan actual large-scale power grid - the U.S. Eastern Interconnection system. The\ndisplacement of conventional generators by PV is realized by optimization.\nCombining the PV distribution optimization and the validated dynamic model\ninformation, this approach avoids the uncertainties brought about by\ntransmission planning. As the existing dynamic models can be validated by\nmeasurements, this approach improves the credibility of the high PV models in\nrepresenting future power grids. This generic approach can be applied to\ndevelop high PV dynamic models for other actual large-scale systems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we provide a sublinear function $p$ on ordered Banach spaces,\nwhich depends on the order structure of the space. With respect to this $p$, we\nstudy the relation between $p$-contractivity of positive semigroups and the\n$p$-dissipativity of its generators. The positive off-diagonal property of\ngenerators is also studied in ordered vector spaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Most polymers are long-lasting and produced from monomers derived from fossil\nfuel sources. Bio-based and/or biodegradable plastics have been proposed as a\nsustainable alternative. Amongst those available, polyhydroxyalkanoate (PHA)\nshows great potential across a large variety of applications but is currently\nlimited to packaging, cosmetics and tissue engineering due to its relatively\npoor physical properties. An expansion of its uses can be accomplished by\ndeveloping nanocomposites where PHAs are used as the polymer matrix. Herein, a\nPHA biopolyester was melt blended with graphene nanoplatelets (GNPs) or with a\n1:1 hybrid mixture of GNPs and carbon nanofibers (CNFs). The resulting\nnanocomposites exhibited enhanced thermal stability while their Young's modulus\nroughly doubled compared to pure PHA. The hybrid nanocomposites percolated\nelectrically at lower nanofiller loadings compared to the GNP-PHA system. The\nelectrical conductivity at 15 wt.% loading was ~ 6 times higher than the\nGNP-based sample. As a result, the electromagnetic interference shielding\nperformance of the hybrid material was around 50% better than the pure GNPs\nnanocomposites, exhibiting shielding effectiveness above 20 dB, which is the\nthreshold for common commercial applications. The thermal conductivity\nincreased significantly for both types of bio-nanocomposites and reached values\naround 5 W K-1 m-1 with the hybrid-based material displaying the best\nperformance. Considering the solvent-free and industrially compatible\nproduction method, the proposed multifunctional materials are promising to\nexpand the range of application of PHAs and increase the environmental\nsustainability of the plastic and plastic electronics industry.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  While much work on deep latent variable models of text uses continuous latent\nvariables, discrete latent variables are interesting because they are more\ninterpretable and typically more space efficient. We consider several\napproaches to learning discrete latent variable models for text in the case\nwhere exact marginalization over these variables is intractable. We compare the\nperformance of the learned representations as features for low-resource\ndocument and sentence classification. Our best models outperform the previous\nbest reported results with continuous representations in these low-resource\nsettings, while learning significantly more compressed representations.\nInterestingly, we find that an amortized variant of Hard EM performs\nparticularly well in the lowest-resource regimes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Obtaining sharp estimates for quantities involved in a given model is an\nintegral part of the modeling process. For dynamical systems whose orbits\ndisplay a complicated, perhaps chaotic, behaviour, the aim is usually to\nestimate time or ensemble averages of given quantities. This is the case, for\ninstance, in turbulent flows. In this work, the aim is to present a minimax\noptimization formula that yields optimal bounds for time and/or ensemble\naverages for the two- and three-dimensional Navier-Stokes equations. The\nresults presented here are extensions to the infinite-dimensional setting of a\nrecent result on the finite-dimensional case given by Tobasco, Goluskin, and\nDoering in 2017. The optimal result occurs in the form of a minimax\noptimization problem and does not require knowledge of the solutions, only the\nlaw of the system. The minimax optimization problem appears in the form of a\nmaximization over a portion of the phase space of the system and a minimization\nover a family of auxiliary functions made of cylindrical test functionals\ndefined on the phase space. The function to be optimized is the desired\nquantity plus the duality product between the law of the system and the\nderivative of the auxiliary function.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Bitcoin P2P network is at the core of all communications between clients.\nThe reachable part of this network has been explored and analyzed by numerous\nstudies. Unreachable nodes, however, are, in most part, overlooked.\nNonetheless, they are a relevant part of the network and play an essential role\nin the propagation of messages. In this paper, we focus on transaction\npropagation and show that increasing the participation of unreachable nodes can\npotentially improve the robustness and efficiency of the network. In order to\ndo that, we propose a few changes to the network protocol. Additionally, we\ndesign a novel transaction propagation protocol that explicitly involves\nunreachable nodes to provide better protection against deanonymization attacks.\nOur solutions are simple to implement and can effectively bring immediate\nbenefits to the Bitcoin network.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The following paper is devoted to the study of type I locally compact quantum\ngroups. We show how various operators related to the modular theory of the Haar\nintegrals on $\\mathbb{G}$ and $\\widehat{\\mathbb{G}}$ act on the level of direct\nintegrals. Using these results we derive a web of implications between\nproperties such as unimodularity or traciality of the Haar integrals. We also\nstudy in detail two examples: discrete quantum group\n$\\widehat{\\mathrm{SU}_q(2)}$ and the quantum $az+b$ group.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the problem of constructing bulk and surface embedded modes (EMs)\ninside the quasi-continuum band of a square lattice, using a potential\nengineering approach \\`a la Wigner and von Neumann. Building on previous\nresults for the one-dimensional (1D) lattice, and making use of separability,\nwe produce examples of two-dimensional envelope functions and the\ntwo-dimensional (2D) potentials that produce them. The 2D embedded mode decays\nlike a stretched exponential, with a supporting potential that decays as a\npower law. The separability process can cause that a 1D impurity state (outside\nthe 1D band) can give rise to a 2D embedded mode (inside the band). The\nembedded mode survives the addition of random perturbations of the potential;\nhowever, this process introduces other localized modes inside the band, and\ncauses a general tendency towards localization of the perturbed modes.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Robust and accurate trajectory estimation of mobile agents such as people and\nrobots is a key requirement for providing spatial awareness for emerging\ncapabilities such as augmented reality or autonomous interaction. Although\ncurrently dominated by optical techniques e.g., visual-inertial odometry, these\nsuffer from challenges with scene illumination or featureless surfaces. As an\nalternative, we propose milliEgo, a novel deep-learning approach to robust\negomotion estimation which exploits the capabilities of low-cost mmWave radar.\nAlthough mmWave radar has a fundamental advantage over monocular cameras of\nbeing metric i.e., providing absolute scale or depth, current single chip\nsolutions have limited and sparse imaging resolution, making existing\npoint-cloud registration techniques brittle. We propose a new architecture that\nis optimized for solving this challenging pose transformation problem.\nSecondly, to robustly fuse mmWave pose estimates with additional sensors, e.g.\ninertial or visual sensors we introduce a mixed attention approach to deep\nfusion. Through extensive experiments, we demonstrate our proposed system is\nable to achieve 1.3% 3D error drift and generalizes well to unseen\nenvironments. We also show that the neural architecture can be made highly\nefficient and suitable for real-time embedded applications.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We demonstrate the construction of Poisson structures via Lie algebroids on\nmoduli spaces of stable Higgs bundles over stacky curves. Considering specific\ncases of such Deligne-Mumford stacks, as well as of twistings, provides various\nimportant examples of moduli spaces that can be equipped with a Poisson\nstructure. Special attention is given to moduli spaces of parabolic Higgs\nbundles and of Higgs bundles over root stacks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper analyzes two classes of second order level set PDE in periodic\nmedia in the parabolic scaling. First, we study fully nonlinear geometric\noperators under general assumptions in dimension $d = 2$ and prove that the\nassociated equations homogenize in this case. Next, we treat a class of\nquasi-linear geometric operators in arbitrary dimensions $d \\geq 2$. In this\nsetting, by adapting arguments form the study of oscillating boundary value\nproblems, we prove that the effective coefficients are generically\ndiscontinuous in all dimensions $d \\geq 3$. This necessitates a study of level\nset PDE driven by operators that are discontinuous at every rational direction\non the sphere. We prove that, in fact, the effective operators so obtained do\nhave a comparison principle and, thus, homogenization occurs. Finally, we\ninvestigate the connection between the effective mobility obtained in the\nquasi-linear case and linear response, drawing a connection between our results\nand those obtained in the hyperbolic scaling.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Gradient boosted trees are competition-winning, general-purpose,\nnon-parametric regressors, which exploit sequential model fitting and gradient\ndescent to minimize a specific loss function. The most popular implementations\nare tailored to univariate regression and classification tasks, precluding the\npossibility of capturing multivariate target cross-correlations and applying\nstructured penalties to the predictions. In this paper, we present a\ncomputationally efficient algorithm for fitting multivariate boosted trees. We\nshow that multivariate trees can outperform their univariate counterpart when\nthe predictions are correlated. Furthermore, the algorithm allows to\narbitrarily regularize the predictions, so that properties like smoothness,\nconsistency and functional relations can be enforced. We present applications\nand numerical results related to forecasting and control.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  While classification of arbitrary structures in high dimensions may require\ncomplete quantitative information, for simple geometrical structures,\nlow-dimensional qualitative information about the boundaries defining the\nstructures can suffice. Rather than using dense, multi-dimensional data, we\npropose a deep neural network (DNN) classification framework that utilizes a\nminimal collection of one-dimensional representations, called \\emph{rays}, to\nconstruct the \"fingerprint\" of the structure(s) based on substantially reduced\ninformation. We empirically study this framework using a synthetic dataset of\ndouble and triple quantum dot devices and apply it to the classification\nproblem of identifying the device state. We show that the performance of the\nray-based classifier is already on par with traditional 2D images for low\ndimensional systems, while significantly cutting down the data acquisition\ncost.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Motivated by inferring cellular signaling networks using noisy flow cytometry\ndata, we develop procedures to draw inference for Bayesian networks based on\nerror-prone data. Two methods for inferring causal relationships between nodes\nin a network are proposed based on penalized estimation methods that account\nfor measurement error and encourage sparsity. We discuss consistency of the\nproposed network estimators and develop an approach for selecting the tuning\nparameter in the penalized estimation methods. Empirical studies are carried\nout to compare the proposed methods and a naive method that ignores measurement\nerror with applications to synthetic data and to single cell flow cytometry\ndata.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Emerging transportation technologies offer unprecedented opportunities to\nimprove the efficiency of the transportation system from the perspectives of\nenergy consumption, congestion, and emissions. One of these technologies is\nconnected and autonomous vehicles (CAVs). With the prospective duality of\noperations of CAVs and human driven vehicles in the same roadway space (also\nreferred to as a mixed stream), CAVs are expected to address a variety of\ntraffic problems particularly those that are either caused or exacerbated by\nthe heterogeneous nature of human driving. In efforts to realize such specific\nbenefits of CAVs in mixed-stream traffic, it is essential to understand and\nsimulate the behavior of human drivers in such environments, and microscopic\ntraffic flow (MTF) models can be used to carry out this task. By helping to\ncomprehend the fundamental dynamics of traffic flow, MTF models serve as a\npowerful approach to assess the impacts of such flow in terms of safety,\nstability, and efficiency. In this paper, we seek to calibrate MTF models based\non empirical trajectory data as basis of not only understanding traffic\ndynamics such as traffic instabilities, but ultimately using CAVs to mitigate\nstop-and-go wave propagation. The paper therefore duly considers the\nheterogeneity and uncertainty associated with human driving behavior in order\nto calibrate the dynamics of each HDV. Also, the paper designs the CAV\ncontrollers based on the microscopic HDV models that are calibrated in real\ntime. The data for the calibration is from the Next Generation SIMulation\n(NGSIM) trajectory datasets. The results are encouraging, as they indicate the\nefficacy of the designed controller to significantly improve not only the\nstability of the mixed traffic stream but also the safety of both CAVs and HDVs\nin the traffic stream.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Context: GR290 (M33 V0532=Romano's star) is a luminous M33 object undergoing\nphotometric variability typical for luminous blue variable (LBV) stars. It lies\ninside Wolf-Rayet region in the Hertzsprung-Russell diagram and possesses a WN8\ntype spectrum at the light minima. Analysis of Gran Telescopio Canarias (GTC)\nspectra obtained in 2016 led to the conclusion that it is surrounded by an\nunresolved HII region formed mostly of ejected material from the central star,\nand disclosed the presence of a second, more extended asymmetrical emission\nregion. Aims: The aim of this paper is to further explore the structure of the\nnearby environment of GR290. Methods: Long-slit spectra of GR290 were obtained\nwith three slit orientations in the visual and red spectral regions. The\nemission-line distribution for each slit was analyzed. Results: We confirm the\npresence of an asymmetric HII region that extends ~50 pc to the south; ~30pc to\nthe north and southeast; ~20 pc to the east and northwest and ~10pc to the\nwest. We also present the first spectrum to be acquired of a star belonging to\nthe neighboring OB88 association, J013501.87+304157.3, which we classify as a\nB-type supergiant with a possible binary companion.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Slow and fast light is an important and fascinating phenomenon in quantum\noptics. Here, we theoretically study how to achieve the ultraslow and ultrafast\nlight in a passive-active optomechanical system, based on the ideal\noptomechanically induced transparency (OMIT). Under the conditions of the ideal\nOMIT, an abnormal (inverted) transparency window will emerge accompanied with a\nvery steep dispersion, resulting that the ultraslow light can be easily\nachieved at the transparency window by adjusting the dissipation rates of the\ntwo cavities, even with usual mechanical linewidth (such as Hz linewidth).\nParticularly, as the decay rate of the passive cavity tends to the gain rate of\nthe active cavity, the ideal stopped light can be achieved. Similarly, the\nultrafast light can be achieved at transparency window by tuning the coupling\nstrength and the decay rates in the system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Proper traffic simulation of electric vehicles, which draw energy from\noverhead wires, requires adequate modeling of traction infrastructure. Such\nvehicles include trains, trams or trolleybuses. Since the requested power\ndemands depend on a traffic situation, the overhead wire DC electrical circuit\nis associated with a non-linear power flow problem. Although the Newton-Raphson\nmethod is well-known and widely accepted for seeking its solution, the\nexistence of such a solution is not guaranteed. Particularly in situations\nwhere the vehicle power demands are too high (during acceleration), the\nsolution of the studied problem may not exist. To deal with such cases, we\nintroduce a numerical method which seeks maximal suppliable power demands for\nwhich the solution exists. This corresponds to introducing a scaling parameter\nto reduce the demanded power. The interpretation of the scaling parameter is\nthe amount of energy which is absent in the system, and which needs to be\nprovided by external sources such as on-board batteries. We propose an\nefficient two-stage algorithm to find the optimal scaling parameter and the\nresulting potentials in the overhead wire network. We perform a comparison with\na naive approach and present a real-world simulation of part of the Pilsen city\nin the Czech Republic. These simulations are performed in the traffic\nmicro-simulator SUMO, a popular open-source traffic simulation platform.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove a local in time well-posedness result for quasi-linear Hamiltonian\nSchr\\\"odinger equations on $\\mathbb{T}^d$ for any $d\\geq 1$. For any initial\ncondition in the Sobolev space $H^s$, with $s$ large, we prove the existence\nand unicity of classical solutions of the Cauchy problem associated to the\nequation. The lifespan of such a solution depends only on the size of the\ninitial datum. Moreover we prove the continuity of the solution map.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Low-dimensional representations, or embeddings, of a graph's nodes facilitate\ndata mining tasks. Known embedding methods explicitly or implicitly rely on a\nsimilarity measure among nodes. As the similarity matrix is quadratic, a\ntradeoff between space complexity and embedding quality arises; past research\ninitially opted for heuristics and linear-transform factorizations, which allow\nfor linear space but compromise on quality; recent research has proposed a\nquadratic-space solution as a viable option too.\n  In this paper we observe that embedding methods effectively aim to preserve\nthe covariance among the rows of a similarity matrix, and raise the question:\nis there a method that combines (i) linear space complexity, (ii) a nonlinear\ntransform as its basis, and (iii) nontrivial quality guarantees? We answer this\nquestion in the affirmative, with FREDE(FREquent Directions Embedding), a\nsketching-based method that iteratively improves on quality while processing\nrows of the similarity matrix individually; thereby, it provides, at any\niteration, column-covariance approximation guarantees that are, in due course,\nalmost indistinguishable from those of the optimal row-covariance approximation\nby SVD. Our experimental evaluation on variably sized networks shows that FREDE\nperforms as well as SVD and competitively against current state-of-the-art\nmethods in diverse data mining tasks, even when it derives an embedding based\non only 10% of node similarities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the paper, we investigate two problems on strings. The first one is the\nString matching problem, and the second one is the String comparing problem. We\nprovide a quantum algorithm for the String matching problem that uses\nexponentially less quantum memory than existing ones. The algorithm uses the\nhashing technique for string matching, quantum parallelism, and ideas of\nGrover's search algorithm. Using the same ideas, we provide two algorithms for\nthe String comparing problem. These algorithms also use exponentially less\nquantum memory than existing ones. Additionally, the second algorithm works\nexponentially faster than the existing one.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The field of machine translation has progressed tremendously in recent years.\nEven though the translation quality has improved significantly, current systems\nare still unable to produce uniformly acceptable machine translations for the\nvariety of possible use cases. In this work, we put machine translation in a\ncross-lingual pipeline and introduce downstream tasks to define task-specific\nacceptability of machine translations. This allows us to leverage parallel data\nto automatically generate acceptability annotations on a large scale, which in\nturn help to learn acceptability detectors for the downstream tasks. We conduct\nexperiments to demonstrate the effectiveness of our framework for a range of\ndownstream tasks and translation models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Coherent control of the spatial properties of light is central to a wide\nvariety of applications from high bandwidth quantum and classical communication\nto high power fiber lasers. Low-loss conversion amongst a complete and\northogonal set of modes is particularly important for robust mode-multiplexed\ncommunication. Here, we introduce tunable impedance mismatch between coupled\nFabry-P\\'erot resonators as a powerful tool for manipulation of the spatial and\ntemporal properties of optical fields. In the single-mode regime, frequency\ndependent impedance matching enables tunable finesse optical resonators, with\npotential applications in quantum science and sensing. Introducing the spatial\ndependence of the impedance mismatch as an additional ingredient enables\ncoherent spatial mode conversion of optical photons at near-unity efficiency.\nWe implement these ideas, experimentally demonstrating a NIR resonator whose\nfinesse is tunable over a decade, and an optical mode converter with efficiency\n$>\\!\\!75\\%$ for the first six Hermite-Gauss modes. We anticipate that this new\nperspective on coupled multimode resonators will have exciting applications in\nmicro- and nano- photonics and computer-aided inverse design. In particular,\ncombination with in-cavity electro-optics will open new horizons for real-time\ncontrol of the spatio-spectral properties of lasers, resonators, and optical\nfilters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present ESP4ML, an open-source system-level design flow to build and\nprogram SoC architectures for embedded applications that require the hardware\nacceleration of machine learning and signal processing algorithms. We realized\nESP4ML by combining two established open-source projects (ESP and HLS4ML) into\na new, fully-automated design flow. For the SoC integration of accelerators\ngenerated by HLS4ML, we designed a set of new parameterized interface circuits\nsynthesizable with high-level synthesis. For accelerator configuration and\nmanagement, we developed an embedded software runtime system on top of Linux.\nWith this HW/SW layer, we addressed the challenge of dynamically shaping the\ndata traffic on a network-on-chip to activate and support the reconfigurable\npipelines of accelerators that are needed by the application workloads\ncurrently running on the SoC. We demonstrate our vertically-integrated\ncontributions with the FPGA-based implementations of complete SoC instances\nbooting Linux and executing computer-vision applications that process images\ntaken from the Google Street View database.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Mutually unbiased bases (MUBs) are a crucial ingredient for many protocols in\nquantum information processing. Measurements performed in these bases are\nunbiased to the maximally possible extent, which is used to prove randomness or\nsecrecy of measurement results. In this work we show that certain properties of\nsets of MUBs crucially depend on their specific choice, including, somewhat\nsurprisingly, measurement outcome labelling. If measurements are chosen in a\ncoherent way, the secrecy of the result can be completely lost for specific\nsets of MUB measurements, while partially retained for others. This can\npotentially impact a broad spectrum of applications, where MUBs are utilized.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Young neutron stars (NSs) born in core-collapse explosions are promising\ncandidates for the central engines of fast radio bursts (FRBs), since the first\nlocalized repeating burst FRB 121102 happens in a star forming dwarf galaxy,\nwhich is similar to the host galaxies of superluminous supernovae (SLSNe) and\nlong gamma-ray bursts (LGRBs). However, FRB 180924 and FRB 190523 are localized\nto massive galaxies with low rates of star formation, compared with the host of\nFRB 121102. Meanwhile, the offsets between the bursts and host centers are\nabout 4 kpc and 29 kpc for FRB 180924 and FRB 190523, respectively. These\nproperties of hosts are similar to short gamma-ray bursts \\textbf{(SGRBs)},\nwhich are produced by mergers of binary neutron star (BNS) or neutron\nstar-black hole (NS-BH). Therefore, the NSs powering FRBs may be formed in BNS\nmergers. In this paper, we study the BNS merger rates, merger times, and\npredict their most likely merger locations for different types of host galaxies\nusing population synthesis method. We find that the BNS merger channel is\nconsistent with the recently reported offsets of FRB 180924 and FRB 190523. The\noffset distribution of short GRBs is well reproduced by population synthesis\nusing galaxy model which is similar to GRB hosts. The event rate of FRBs\n(including non-repeating and repeating), is larger than those of BNS merger and\nshort GRBs, which requires a large fraction of observed FRBs emitting several\nbursts. Using curvature radiation by bunches in NS magnetospheres, we also\npredict the observational properties of FRBs from BNS mergers, including the\ndispersion measure, and rotation measure. At late times ($t\\geq1$yr), the\ncontribution to dispersion measure and rotation measure from BNS merger ejecta\ncould be neglected.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spintronics, since its inception, has mainly focused on ferromagnetic\nmaterials for manipulating the spin degree of freedom in addition to the charge\ndegree of freedom, whereas much less attention has been paid to\nantiferromagnetic materials. Thanks to the advances of micro-nano-fabrication\ntechniques and the electrical control of the N\\'eel order parameter,\nantiferromagnetic spintronics is booming as a result of abundant room\ntemperature materials, robustness against external fields and dipolar coupling,\nand rapid dynamics in the terahertz regime. For the purpose of applications of\nantiferromagnets, it is essential to have a comprehensive understanding of the\nantiferromagnetic dynamics at the microscopic level. Here, we first review the\ngeneral form of equations that govern both antiferromagnetic and ferrimagnetic\ndynamics. This general form unifies the previous theories in the literature. We\nalso provide a survey for the recent progress related to antiferromagnetic\ndynamics, including the motion of antiferromagnetic domain walls and skyrmions,\nthe spin pumping and quantum antiferromagnetic spintronics. In particular, open\nproblems in several topics are outlined. Furthermore, we discuss the\ndevelopment of antiferromagnetic quantum magnonics and its potential\nintegration with modern information science and technology.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Solar system materials are variably depleted in moderately volatile elements\n(MVEs) relative to the proto-solar composition. To address the origin of this\nMVE depletion, we conducted a systematic study of high-precision K isotopic\ncomposition on 16 carbonaceous chondrites (CCs) of types CM1-2, CO3, CV3, CR2,\nCK4-5 and CH3 and 28 ordinary chondrites (OCs) covering petrological types 3 to\n6 and chemical groups H, L, and LL. We observed significant overall K isotope\n(delta41K) variations (-1.54 to 0.70 permil). The K isotope compositions of CCs\nare largely higher than the Bulk Silicate Earth (BSE) value, whereas OCs show\ntypically lower values than BSE. Neither CCs nor OCs show resolvable\ncorrelations between K isotopes and chemical groups, petrological types, shock\nlevels, exposure ages, fall or find occurrence, or terrestrial weathering. The\nlack of a clear trend between K isotopes and K content indicates that the K\nisotope fractionations were decoupled from the relative elemental K depletions.\nThe range of K isotope variations in the CCs is consistent with a\nfour-component (chondrule, refractory inclusion, matrix and water) mixing model\nthat is able to explain the bulk elemental and isotopic compositions of the\nmain CC groups, but requires a fractionation in K isotopic compositions in\nchondrules. We propose that the major control of the isotopic compositions of\ngroup averages is condensation or vaporization in nebular environments that is\npreserved in the compositional variation of chondrules. Parent-body processes\n(aqueous alteration, thermal metamorphism, and metasomatism) can mobilize K and\naffect the K isotopes in individual samples. In the case of the OCs, the full\nrange of K isotopic variations can only be explained by the combined effects of\nthe size and relative abundances of chondrules, parent-body aqueous and thermal\nalteration.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove that a PL manifold admits a handle decomposition into handles of\nindex $\\le k$ if and only if $M$ is $k$-stacked, i.e., it admits a PL\ntriangulation in which all $(d-k-1)$-faces are on $\\partial M$.\n  We use this to solve a problem posed in 2008 by Kalai: In any dimension\nhigher than four, there are infinitely many homology-spheres with $g_3 =0$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The International Workshop on Locational Analysis and Related Problems will\ntake place during November 25-27, 2015 in Barcelona (Spain). It is organized by\nthe Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a\nworking group on location belonging to the Statistics and Operations Research\nSpanish Society. The Spanish Location Network is a group of more than 140\nresearchers distributed into 16 nodes corresponding to several Spanish\nuniversities. The Network has been funded by the Spanish Government. Every\nyear, the Network organizes a meeting to promote the communication among its\nmembers and between them and other researchers, and to contribute to the\ndevelopment of the location field and related problems. Previous meetings took\nplace in Sevilla (October 1-3, 2014), Torremolinos (M\\'alaga, June 19-21,\n2013), Granada (May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5,\n2011) and Sevilla (February 1-3, 2010). The topics of interest are location\nanalysis and related problems. This includes location, routing, networks,\ntransportation and logistics models; exact and heuristic solution methods, and\ncomputational geometry, among others.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In swarm intelligence, Particle Swarm Optimization (PSO) and Differential\nEvolution (DE) have been successfully applied in many optimization tasks, and a\nlarge number of variants, where novel algorithm operators or components are\nimplemented, has been introduced to boost the empirical performance. In this\npaper, we first propose to combine the variants of PSO or DE by modularizing\neach algorithm and incorporating the variants thereof as different options of\nthe corresponding modules. Then, considering the similarity between the inner\nworkings of PSO and DE, we hybridize the algorithms by creating two populations\nwith variation operators of PSO and DE respectively, and selecting individuals\nfrom those two populations. The resulting novel hybridization, called PSODE,\nencompasses most up-to-date variants from both sides, and more importantly\ngives rise to an enormous number of unseen swarm algorithms via different\ninstantiations of the modules therein.\n  In detail, we consider 16 different variation operators originating from\nexisting PSO- and DE algorithms, which, combined with 4 different selection\noperators, allow the hybridization framework to generate 800 novel algorithms.\nThe resulting set of hybrid algorithms, along with the combined 30 PSO- and DE\nalgorithms that can be generated with the considered operators, is tested on\nthe 24 problems from the well-known COCO/BBOB benchmark suite, across multiple\nfunction groups and dimensionalities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider a gas of interacting bosons trapped in a box of side length one\nin the Gross-Pitaevskii limit. We review the proof of the validity of\nBogoliubov's prediction for the ground state energy and the low-energy\nexcitation spectrum. This note is based on joint work with C. Brennecke, S.\nCenatiempo and B. Schlein.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper demonstrates a system capable of combining a sparse, indirect,\nmonocular visual SLAM, with both offline and real-time Multi-View Stereo (MVS)\nreconstruction algorithms. This combination overcomes many obstacles\nencountered by autonomous vehicles or robots employed in agricultural\nenvironments, such as overly repetitive patterns, need for very detailed\nreconstructions, and abrupt movements caused by uneven roads. Furthermore, the\nuse of a monocular SLAM makes our system much easier to integrate with an\nexisting device, as we do not rely on a LiDAR (which is expensive and power\nconsuming), or stereo camera (whose calibration is sensitive to external\nperturbation e.g. camera being displaced). To the best of our knowledge, this\npaper presents the first evaluation results for monocular SLAM, and our work\nfurther explores unsupervised depth estimation on this specific application\nscenario by simulating RGB-D SLAM to tackle the scale ambiguity, and shows our\napproach produces reconstructions that are helpful to various agricultural\ntasks. Moreover, we highlight that our experiments provide meaningful insight\nto improve monocular SLAM systems under agricultural settings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A $\\{0,1\\}$-valued function on a two-dimensional rectangular grid is called\nthreshold if its sets of zeros and ones are separable by a straight line. In\nthis paper we study 2-threshold functions, i.e. functions representable as the\nconjunction of two threshold functions. We provide a characterization of\n2-threshold functions by pairs of oriented prime segments, where each such\nsegment is defined by an ordered pair of adjacent integer points.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present a machine learning approach to move a group of\nrobots in a formation. We model the problem as a multi-agent reinforcement\nlearning problem. Our aim is to design a control policy for maintaining a\ndesired formation among a number of agents (robots) while moving towards a\ndesired goal. This is achieved by training our agents to track two agents of\nthe group and maintain the formation with respect to those agents. We consider\nall agents to be homogeneous and model them as unicycle [1]. In contrast to the\nleader-follower approach, where each agent has an independent goal, our\napproach aims to train the agents to be cooperative and work towards the common\ngoal. Our motivation to use this method is to make a fully decentralized\nmulti-agent formation system and scalable for a number of agents.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we prove that the large scale properties of a number of\ntwo-dimensional lattice models are rotationally invariant. More precisely, we\nprove that the random-cluster model on the square lattice with cluster-weight\n$1\\le q\\le 4$ exhibits rotational invariance at large scales. This covers the\ncase of Bernoulli percolation on the square lattice as an important example. We\ndeduce from this result that the correlations of the Potts models with\n$q\\in\\{2,3,4\\}$ colors and of the six-vertex height function with\n$\\Delta\\in[-1,-1/2]$ are rotationally invariant at large scales.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Establishing common ground and maintaining shared awareness amongst\nparticipants is a key challenge in collaborative visualization. For real-time\ncollaboration, existing work has primarily focused on synchronizing constituent\nvisualizations - an approach that makes it difficult for users to work\nindependently, or selectively attend to their collaborators' activity. To\naddress this gap, we introduce a design space for representing synchronous\nmulti-user collaboration in visualizations defined by two orthogonal axes:\nsituatedness, or whether collaborators' interactions are overlaid on or shown\noutside of a user's view, and specificity, or whether collaborators are\ndepicted through abstract, generic representations or through specific means\ncustomized for the given visualization. We populate this design space with a\nvariety of examples including generic and custom synchronized cursors, and user\nlegends that collect these cursors together or reproduce collaborators' views\nas thumbnails. To build common ground, users can interact with these\nrepresentations by peeking to take a quick look at a collaborator's view,\ntracking to follow along with a collaborator in real-time, and forking to\nindependently explore the visualization based on a collaborator's work. We\npresent a reference implementation of a wrapper library that converts\ninteractive Vega-Lite charts into collaborative visualizations. We find that\nour approach affords synchronous collaboration across an expressive range of\nvisual designs and interaction techniques.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The affine Kauffmann category is a strict monoidal category and can be\nconsidered as a $q$-analogue of the affine Brauer category in (Rui et al. in\nMath. Zeit. 293, 503-550, 2019). In this paper, we prove a basis theorem for\nthe morphism spaces in the affine Kauffmann category. The cyclotomic Kauffmann\ncategory is a quotient category of the affine Kauffmann category. We also prove\nthat any morphism space in this category is free over an integral domain\n$\\mathbb K$ with maximal rank if and only if the $\\mathbf u$-admissible\ncondition holds in the sense of Definition 1.13.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The Lanthanum Halide scintillator detectors have been widely used for nuclear\nspectroscopy experiments because of their excellent energy and time\nresolutions. Despite having these advantages, the intrinsic alpha and beta\ncontaminations in these scintillators pose a severe limitation in their usage\nin rare-event detections. In the present work, pulse shape discrimination (PSD)\nwith a fast digitizer has been shown to be an efficient method to separate the\neffect of alpha contamination from the spectrum. The shape of the beta spectrum\nhas been generated with the help of Monte Carlo based simulation code, and its\ncontribution has been eliminated from the spectrum. The reduction in the\nbackground events generated by both intrinsic beta and alpha activities has\nbeen demonstrated. The present study will encourage the application of these\ndetectors in low cross-section measurement experiments relevant to nuclear\nastrophysics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We developed functional surfaces of laser-microstructured silicon coated with\nblends of polystyrene (PS) and poly(N-isopropylacrylamide) (PNIPAM) and we\nstudy their switching wetting behavior between hydrophilicity and\nhydrophobicity. Large areas of silicon are processed with reproducible surface\nmicromorphology and spin-coated with PS/PNIPAM blends of two blend ratios. The\nwetting behavior of the surfaces is modified by the combination of surface\ntopography and surface chemistry effects. PS/PNIPAM films are casted on flat\nand microstructured silicon substrates with or without a native SiO2 layer. All\nfilms respond to the stimulus of temperature and films casted on\nmicrostructured silicon substrates with a native SiO2 layer show the highest\nthermoresponsiveness presumably because they adopt a more favorable structure.\nMicrostructuring provides a large specific area that extends the contact of\nPNIPAM chains with water molecules according to the Wenzel model, and thus\nincreasing the film thermoresponsiveness, resulting in a reversible transition\nfrom hydrophilicity to hydrophobicity upon heating. The absence of the native\nSiO2 layer from the silicon substrates affects the PS and PNIPAM arrangement in\nthe films, increasing the water contact angle both below and above the lower\ncritical solution temperature of PNIPAM and decreasing their\nthermoresponsiveness.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Automatic speaker recognition algorithms typically use pre-defined\nfilterbanks, such as Mel-Frequency and Gammatone filterbanks, for\ncharacterizing speech audio. However, it has been observed that the features\nextracted using these filterbanks are not resilient to diverse audio\ndegradations. In this work, we propose a deep learning-based technique to\ndeduce the filterbank design from vast amounts of speech audio. The purpose of\nsuch a filterbank is to extract features robust to non-ideal audio conditions,\nsuch as degraded, short duration, and multi-lingual speech. To this effect, a\n1D convolutional neural network is designed to learn a time-domain filterbank\ncalled DeepVOX directly from raw speech audio. Secondly, an adaptive triplet\nmining technique is developed to efficiently mine the data samples best suited\nto train the filterbank. Thirdly, a detailed ablation study of the DeepVOX\nfilterbanks reveals the presence of both vocal source and vocal tract\ncharacteristics in the extracted features. Experimental results on VOXCeleb2,\nNIST SRE 2008, 2010 and 2018, and Fisher speech datasets demonstrate the\nefficacy of the DeepVOX features across a variety of degraded, short duration,\nand multi-lingual speech. The DeepVOX features also shown to improve the\nperformance of existing speaker recognition algorithms, such as the\nxVector-PLDA and the iVector-PLDA.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A vertex colouring $f:V(G)\\to C$ of a graph $G$ is complete if for any two\ndistinct colours $c_1,c_2\\in C$ there is an edge $\\{v_1,v_2\\}\\in E(G)$ such\nthat $f(v_i)=c_i$, $i=1,2$. The achromatic number of $G$ is the maximum number\n$\\mathrm{achr}(G)$ of colours in a proper complete vertex colouring of $G$. In\nthe paper it is proved that $\\mathrm{achr}(K_6\\square K_7)=18$. This result\nfinalises the determination of $\\mathrm{achr}(K_6\\square K_q)$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the asymptotic behavior of the surface quasi-geostrophic\nequation, subject to a small external force. Under suitable assumptions on the\nforcing, we first construct the steady states and we provide a number of useful\na posteriori estimates for them. Importantly, to do so, we only impose minimal\ncancellation conditions on the forcing function.\n  Our main result is that all $L^1\\cap L^\\infty$ localized initial data\nproduces global solutions of the forced SQG, which converge to the steady\nstates in $L^p(\\mathbf R^2), 1<p\\leq 2$ as time goes to infinity. This\nestablishes that the steady states serve as one point attracting set. Moreover,\nby employing the method of scaling variables, we compute the sharp relaxation\nrates, by requiring slightly more localized initial data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Aided by technology, people are increasingly able to communicate across\ngeographical, cultural, and language barriers. This ability also results in new\nchallenges, as interlocutors need to adapt their communication approaches to\nincreasingly diverse circumstances. In this work, we take the first steps\ntowards automatically assisting people in adjusting their language to a\nspecific communication circumstance.\n  As a case study, we focus on facilitating the accurate transmission of\npragmatic intentions and introduce a methodology for suggesting paraphrases\nthat achieve the intended level of politeness under a given communication\ncircumstance. We demonstrate the feasibility of this approach by evaluating our\nmethod in two realistic communication scenarios and show that it can reduce the\npotential for misalignment between the speaker's intentions and the listener's\nperceptions in both cases.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce a notion of normalised oplax $3$-functor suitable for the\nelementary homotopy theory of strict $3$-categories, following the\ncombinatorics of orientals. We show that any such morphism induces a morphism\nof simplicial sets between the Street nerves and we characterise those\nmorphisms of simplicial sets coming from normalised oplax $3$-functors. This\nallows us to prove that normalised oplax $3$-functors compose. Finally we\nconstruct a strictification for normalised oplax $3$-functors whose source is a\n$1$-category without split-monos or split-epis.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We classify all $q$-ary $\\Delta$-divisible linear codes which are spanned by\ncodewords of weight $\\Delta$. The basic building blocks are the simplex codes,\nand for $q=2$ additionally the first order Reed-Muller codes and the parity\ncheck codes. This generalizes a result of Pless and Sloane, where the binary\nself-orthogonal codes spanned by codewords of weight $4$ have been classified,\nwhich is the case $q=2$ and $\\Delta=4$ of our classification. As an\napplication, we give an alternative proof of a theorem of Liu on binary\n$\\Delta$-divisible codes of length $4\\Delta$ in the projective case.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The emerging global infectious COVID-19 coronavirus disease by novel Severe\nAcute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) presents critical threats\nto global public health and the economy since it was identified in late\nDecember 2019 in China. The virus has gone through various pathways of\nevolution. For understanding the evolution and transmission of SARS-CoV-2,\ngenotyping of virus isolates is of great importance. We present an accurate\nmethod for effectively genotyping SARS-CoV-2 viruses using complete genomes.\nThe method employs the multiple sequence alignments of the genome isolates with\nthe SARS-CoV-2 reference genome. The SNP genotypes are then measured by Jaccard\ndistances to track the relationship of virus isolates. The genotyping analysis\nof SARS-CoV-2 isolates from the globe reveals that specific multiple mutations\nare the predominated mutation type during the current epidemic. Our method\nserves a promising tool for monitoring and tracking the epidemic of pathogenic\nviruses in their gradual and local genetic variations. The genotyping analysis\nshows that the genes encoding the S proteins and RNA polymerase, RNA primase,\nand nucleoprotein, undergo frequent mutations. These mutations are critical for\nvaccine development in disease control.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This is a short review in the theory of chaos in Bohmian Quantum Mechanics\nbased on our series of works in this field. Our first result is the development\nof a generic theoretical mechanism responsible for the generation of chaos in\nan arbitrary Bohmian system (in 2 and 3 dimensions). This mechanism allows us\nto explore the effect of chaos on Bohmian trajectories and study in detail\n(both analytically and numerically) the different kinds of Bohmian trajectories\nwhere, in general, chaos and order coexist. Finally we explore the effect of\nquantum entanglement on the evolution of the Bohmian trajectories and study\nchaos and ergodicity in qubit systems which are of great theoretical and\npractical interest. We find that the chaotic trajectories are also ergodic,\ni.e. they give the same final distribution of their points after a long time\nregardless of their initial conditions. In the case of strong entanglement most\ntrajectories are chaotic and ergodic and an arbitrary initial distribution of\nparticles will tends to Born's rule over the course of time. On the other hand,\nin the case of weak entanglement the distribution of Born's rule is dominated\nby ordered trajectories and consequently an arbitrary initial configuration of\nparticles will not tend, in general, to Born's rule, unless it is initially\nsatisfied. Our results shed light on a fundamental problem in Bohmian\nMechanics, namely whether there is a dynamical approximation of Born's rule by\nan arbitrary initial distribution of Bohmian particles.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper re-examines the content of a standard advanced course in\nCybersecurity from the perspective of Cloud Computing. More precisely, we\nreview the core concepts of Cybersecurity, as presented in a senior\nundergraduate or graduate class, in light of the Amazon Web Services (AWS)\ncloud.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Measuring the prevalence of active SARS-CoV-2 infections in the general\npopulation is difficult because tests are conducted on a small and non-random\nsegment of the population. However, people admitted to the hospital for\nnon-COVID reasons are tested at very high rates, even though they do not appear\nto be at elevated risk of infection. This sub-population may provide valuable\nevidence on prevalence in the general population. We estimate upper and lower\nbounds on the prevalence of the virus in the general population and the\npopulation of non-COVID hospital patients under weak assumptions on who gets\ntested, using Indiana data on hospital inpatient records linked to SARS-CoV-2\nvirological tests. The non-COVID hospital population is tested fifty times as\noften as the general population, yielding much tighter bounds on prevalence. We\nprovide and test conditions under which this non-COVID hospitalization bound is\nvalid for the general population. The combination of clinical testing data and\nhospital records may contain much more information about the state of the\nepidemic than has been previously appreciated. The bounds we calculate for\nIndiana could be constructed at relatively low cost in many other states.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, a deep neural network with interpretable motion compensation\ncalled CS-MCNet is proposed to realize high-quality and real-time decoding of\nvideo compressive sensing. Firstly, explicit multi-hypothesis motion\ncompensation is applied in our network to extract correlation information of\nadjacent frames(as shown in Fig. 1), which improves the recover performance.\nAnd then, a residual module further narrows down the gap between reconstruction\nresult and original signal. The overall architecture is interpretable by using\nalgorithm unrolling, which brings the benefits of being able to transfer prior\nknowledge about the conventional algorithms. As a result, a PSNR of 22dB can be\nachieved at 64x compression ratio, which is about 4% to 9% better than\nstate-of-the-art methods. In addition, due to the feed-forward architecture,\nthe reconstruction can be processed by our network in real time and up to three\norders of magnitude faster than traditional iterative methods.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  There are two famous function decomposition methods in math: Taylor Series\nand Fourier Series. Fourier series developed into Fourier spectrum, which was\napplied to signal decomposition\\analysis. However, because the Taylor series\nwhose function without a definite functional expression cannot be solved,\nTaylor Series has rarely been used in engineering. Here, we developed Taylor\nseries by our Dendrite Net, constructed a relation spectrum, and applied it to\nmodel or system decomposition\\analysis. Specific engineering: the knowledge of\nthe intuitive link between muscle activity and the finger movement is vital for\nthe design of commercial prosthetic hands that do not need user pre-training.\nHowever, this link has yet to be understood due to the complexity of human\nhand. In this study, the relation spectrum was applied to analyze the\nmuscle-finger system. One single muscle actuates multiple fingers, or multiple\nmuscles actuate one single finger simultaneously. Thus, the research was in\nmuscle synergy and muscle coupling for hand. This paper has two main\ncontributions. (1) The findings of hand contribute to designing prosthetic\nhands. (2) The relation spectrum makes the online model human-readable, which\nunifies online performance and offline results. Code (novel tool for most\nfields) is available at https://github.com/liugang1234567/Gang-neuron.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We give improved separations for the query complexity analogue of the\nlog-approximate-rank conjecture i.e. we show that there are a plethora of total\nBoolean functions on $n$ input bits, each of which has approximate Fourier\nsparsity at most $O(n^3)$ and randomized parity decision tree complexity\n$\\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and\nSherif (JACM '20) both qualitatively (in terms of designing a large number of\nexamples) and quantitatively (improving the gap from quartic to cubic). We\nleave open the problem of proving a randomized communication complexity lower\nbound for XOR compositions of our examples. A linear lower bound would lead to\nnew and improved refutations of the log-approximate-rank conjecture. Moreover,\nif any of these compositions had even a sub-linear cost randomized\ncommunication protocol, it would demonstrate that randomized parity decision\ntree complexity does not lift to randomized communication complexity in general\n(with the XOR gadget).\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Felsner, Li and Trotter showed that the dimension of the adjacency poset of\nan outerplanar graph is at most 5, and gave an example of an outerplanar graph\nwhose adjacency poset has dimension 4. We improve their upper bound to 4, which\nis then best possible.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many procedures for SAT and SAT-related problems -- in particular for those\nrequiring the complete enumeration of satisfying truth assignments -- rely\ntheir efficiency on the detection of partial assignments satisfying an input\nformula. In this paper we analyze the notion of partial-assignment\nsatisfiability -- in particular when dealing with non-CNF and\nexistentially-quantified formulas -- raising a flag about the ambiguities and\nsubtleties of this concept, and investigating their practical consequences.\nThis may drive the development of more effective assignment-enumeration\nalgorithms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper revisits the temporal difference (TD) learning algorithm for the\npolicy evaluation tasks in reinforcement learning. Typically, the performance\nof TD(0) and TD($\\lambda$) is very sensitive to the choice of stepsizes.\nOftentimes, TD(0) suffers from slow convergence. Motivated by the tight link\nbetween the TD(0) learning algorithm and the stochastic gradient methods, we\ndevelop a provably convergent adaptive projected variant of the TD(0) learning\nalgorithm with linear function approximation that we term AdaTD(0). In contrast\nto the TD(0), AdaTD(0) is robust or less sensitive to the choice of stepsizes.\nAnalytically, we establish that to reach an $\\epsilon$ accuracy, the number of\niterations needed is\n$\\tilde{O}(\\epsilon^{-2}\\ln^4\\frac{1}{\\epsilon}/\\ln^4\\frac{1}{\\rho})$ in the\ngeneral case, where $\\rho$ represents the speed of the underlying Markov chain\nconverges to the stationary distribution. This implies that the iteration\ncomplexity of AdaTD(0) is no worse than that of TD(0) in the worst case. When\nthe stochastic semi-gradients are sparse, we provide theoretical acceleration\nof AdaTD(0). Going beyond TD(0), we develop an adaptive variant of\nTD($\\lambda$), which is referred to as AdaTD($\\lambda$). Empirically, we\nevaluate the performance of AdaTD(0) and AdaTD($\\lambda$) on several standard\nreinforcement learning tasks, which demonstrate the effectiveness of our new\napproaches.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  DRAM is the prevalent main memory technology, but its long access latency can\nlimit the performance of many workloads. Although prior works provide DRAM\ndesigns that reduce DRAM access latency, their reduced storage capacities\nhinder the performance of workloads that need large memory capacity. Because\nthe capacity-latency trade-off is fixed at design time, previous works cannot\nachieve maximum performance under very different and dynamic workload demands.\n  This paper proposes Capacity-Latency-Reconfigurable DRAM (CLR-DRAM), a new\nDRAM architecture that enables dynamic capacity-latency trade-off at low cost.\nCLR-DRAM allows dynamic reconfiguration of any DRAM row to switch between two\noperating modes: 1) max-capacity mode, where every DRAM cell operates\nindividually to achieve approximately the same storage density as a\ndensity-optimized commodity DRAM chip and 2) high-performance mode, where two\nadjacent DRAM cells in a DRAM row and their sense amplifiers are coupled to\noperate as a single low-latency logical cell driven by a single logical sense\namplifier.\n  We implement CLR-DRAM by adding isolation transistors in each DRAM subarray.\nOur evaluations show that CLR-DRAM can improve system performance and DRAM\nenergy consumption by 18.6% and 29.7% on average with four-core multiprogrammed\nworkloads. We believe that CLR-DRAM opens new research directions for a system\nto adapt to the diverse and dynamically changing memory capacity and access\nlatency demands of workloads.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Social media can be used for disaster risk reduction as a complement to\ntraditional information sources, and the literature has suggested numerous ways\nto achieve this. In the case of floods, for instance, data collection from\nsocial media can be triggered by a severe weather forecast and/or a flood\nprediction. By way of contrast, in this paper we explore the possibility of\nhaving an entirely independent flood monitoring system which is based\ncompletely on social media, and which is completely self-activated. This\nindependence and self-activation would bring increased robustness, as the\nsystem would not depend on other mechanisms for forecasting. We observe that\nsocial media can indeed help in the early detection of some flood events that\nwould otherwise not be detected until later, albeit at the cost of many false\npositives. Overall, our experiments suggest that social media signals should\nonly be used to complement existing monitoring systems, and we provide various\nexplanations to support this argument.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a minimally-invasive endoscope based on a multimode fiber that\ncombines photoacoustic and fluorescence sensing. From the measurement of a\ntransmission matrix during a prior calibration step, a focused spot is produced\nand raster-scanned over a sample at the distal tip of the fiber by use of a\nfast spatial light modulator. An ultra-sensitive fiber-optic ultrasound sensor\nfor photoacoustic detection placed next to the fiber is combined with a\nphotodetector to obtain both fluorescence and photoacoustic images with a\ndistal imaging tip no larger than 250um. The high signal-to-noise ratio\nprovided by wavefront shaping based focusing and the ultra-sensitive ultrasound\nsensor enables imaging with a single laser shot per pixel, demonstrating fast\ntwo-dimensional hybrid imaging of red blood cells and fluorescent beads.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Large pools of synthetic DNA molecules have been recently used to reliably\nstore significant volumes of digital data. While DNA as a storage medium has\nenormous potential because of its high storage density, its practical use is\ncurrently severely limited because of the high cost and low throughput of\navailable DNA synthesis technologies. We study the role of batch optimization\nin reducing the cost of large scale DNA synthesis, which translates to the\nfollowing algorithmic task. Given a large pool $\\mathcal{S}$ of random\nquaternary strings of fixed length, partition $\\mathcal{S}$ into batches in a\nway that minimizes the sum of the lengths of the shortest common supersequences\nacross batches. We introduce two ideas for batch optimization that both improve\n(in different ways) upon a naive baseline: (1) using both $(ACGT)^{*}$ and its\nreverse $(TGCA)^{*}$ as reference strands, and batching appropriately, and (2)\nbatching via the quantiles of an appropriate ordering of the strands. We also\nprove asymptotically matching lower bounds on the cost of DNA synthesis,\nshowing that one cannot improve upon these two ideas. Our results uncover a\nsurprising separation between two cases that naturally arise in the context of\nDNA data storage: the asymptotic cost savings of batch optimization are\nsignificantly greater in the case where strings in $\\mathcal{S}$ do not contain\nrepeats of the same character (homopolymers), as compared to the case where\nstrings in $\\mathcal{S}$ are unconstrained.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Using the failure forecast method we describe a first assessment of failure\ntime on present-day unrest signals at Campi Flegrei caldera (Italy) based on\nthe horizontal deformation data collected in [2011, 2020] at eleven GPS\nstations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we consider the problem of certifying the robustness of neural\nnetworks to perturbed and adversarial input data. Such certification is\nimperative for the application of neural networks in safety-critical\ndecision-making and control systems. Certification techniques using convex\noptimization have been proposed, but they often suffer from relaxation errors\nthat void the certificate. Our work exploits the structure of ReLU networks to\nimprove relaxation errors through a novel partition-based certification\nprocedure. The proposed method is proven to tighten existing linear programming\nrelaxations, and asymptotically achieves zero relaxation error as the partition\nis made finer. We develop a finite partition that attains zero relaxation error\nand use the result to derive a tractable partitioning scheme that minimizes the\nworst-case relaxation error. Experiments using real data show that the\npartitioning procedure is able to issue robustness certificates in cases where\nprior methods fail. Consequently, partition-based certification procedures are\nfound to provide an intuitive, effective, and theoretically justified method\nfor tightening existing convex relaxation techniques.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We describe the class of functions $f: R^n\\to R^m$ which transform a vector\nBrownian Motion into a martingale and use this description to give martingale\ncharacterization of the general measurable solution of the multidimensional\nCauchy functional equation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Shock waves are examples of the far-from-equilibrium behaviour of matter;\nthey are ubiquitous in nature, yet the underlying microscopic mechanisms behind\ntheir formation are not well understood. Here, we study the dynamics of\ndispersive quantum shock waves in a one-dimensional Bose gas, and show that the\noscillatory train forming from a local density bump expanding into a uniform\nbackground is a result of quantum mechanical self-interference. The amplitude\nof oscillations, i.e., the interference contrast, decreases with the increase\nof both the temperature of the gas and the interaction strength due to the\nreduced phase coherence length. Furthermore, we show that vacuum and thermal\nfluctuations can significantly wash out the interference contrast, seen in the\nmean-field approaches, due to shot-to-shot fluctuations in the position of\ninterference fringes around the mean.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Total Generalized Variation (TGV) has recently been introduced as penalty\nfunctional for modelling images with edges as well as smooth variations. It can\nbe interpreted as a \"sparse\" penalization of optimal balancing from the first\nup to the $k$-th distributional derivative and leads to desirable results when\napplied to image denoising, i.e., $L^2$-fitting with TGV penalty. The present\npaper studies TGV of second order in the context of solving ill-posed linear\ninverse problems. Existence and stability for solutions of Tikhonov-functional\nminimization with respect to the data is shown and applied to the problem of\nrecovering an image from blurred and noisy data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the interaction between an incompressible, viscous fluid modeled\nby the dynamic Stokes equation and a multilayered poroelastic structure which\nconsists of a thin, linear, poroelastic plate layer (in direct contact with the\nfree Stokes flow) and a thick Biot layer. The fluid flow and the elastodynamics\nof the multilayered poroelastic structure are fully coupled across a fixed\ninterface through physical coupling conditions (including the\nBeavers-Joseph-Saffman condition), which present mathematical challenges\nrelated to the regularity of associated velocity traces. We prove existence of\nweak solutions to this fluid-structure interaction problem with either (i) a\nlinear, dynamic Biot model, or (ii) a nonlinear quasi-static Biot component,\nwhere the permeability is a nonlinear function of the fluid content (as\nmotivated by biological applications). The proof is based on constructing\napproximate solutions through Rothe's method, and using energy methods and a\nversion of Aubin-Lions compactness lemma (in the nonlinear case) to recover the\nweak solution as the limit of approximate subsequences. We also provide\nuniqueness criteria and show that constructed weak solutions are indeed strong\nsolutions to the coupled problem if one assumes additional regularity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The main aim of our study is to understand the nature of some conventional\nand non-conventional mesonic states by applying effective QFT models. We start\nfrom the relativistic Lagrangians containing a unique $q\\bar{q}$ seed state\nwhich is strongly coupled to the low-masses decay products of the original\nstate. We find out that some states may appear as a dynamically generated\ncompanion poles of the heavier $q\\bar{q}$ mesons. In particular we show that\n$K^*_0(700)$ is a companion pole of the well-known $K^*_0(1430)$ resonance,\n$X(3872)$ emerges as a (virtual) companion pole of $\\chi_{c1}(2P)$, and the\npuzzling $Y(4008)$ is not a real state, but a spurious enhancement which\nappears when studying the state $\\psi(4040)$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Games such as go, chess and checkers have multiple equivalent game states,\ni.e. multiple board positions where symmetrical and opposite moves should be\nmade. These equivalences are not exploited by current state of the art neural\nagents which instead must relearn similar information, thereby wasting\ncomputing time. Group equivariant CNNs in existing work create networks which\ncan exploit symmetries to improve learning, however, they lack the\nexpressiveness to correctly reflect the move embeddings necessary for games. We\nintroduce Finite Group Neural Networks (FGNNs), a method for creating agents\nwith an innate understanding of these board positions. FGNNs are shown to\nimprove the performance of networks playing checkers (draughts), and can be\neasily adapted to other games and learning problems. Additionally, FGNNs can be\ncreated from existing network architectures. These include, for the first time,\nthose with skip connections and arbitrary layer types. We demonstrate that an\nequivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network in\nimage segmentation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The chemical composition of planets is inherited from that of the\nprotoplanetary disk at the time of planet formation. Increasing observational\nevidence suggests that planet formation occurs in less than 1 Myr. This\nmotivates the need for spatially resolved spectral observations of Class I\ndisks, as carried out by the ALMA chemical survey of Disk-Outflow sources in\nTaurus (ALMA-DOT). In the context of ALMA-DOT, we observe the edge-on disk\naround the Class I source IRAS 04302+2247 (the butterfly star) in the 1.3mm\ncontinuum and five molecular lines. We report the first tentative detection of\nmethanol (CH$_3$OH) in a Class I disk and resolve, for the first time, the\nvertical structure of a disk with multiple molecular tracers. The bulk of the\nemission in the CO 2-1, CS 5-4, and o-H$_2$CO 3(1,2)-2(1,1) lines originates\nfrom the warm molecular layer, with the line intensity peaking at increasing\ndisk heights, $z$, for increasing radial distances, $r$. Molecular emission is\nvertically stratified, with CO observed at larger disk heights (aperture\n$z/r\\sim0.41-0.45$) compared to both CS and H$_2$CO, which are nearly cospatial\n($z/r\\sim0.21-0.28$). In the outer midplane, the line emission decreases due to\nmolecular freeze-out onto dust grains (freeze-out layer) by a factor of >100\n(CO) and 15 (CS). The H$_2$CO emission decreases by a factor of only about 2,\nwhich is possibly due to H$_2$CO formation on icy grains, followed by a\nnonthermal release into the gas phase. The inferred [CH$_3$OH]/[H$_2$CO]\nabundance ratio is 0.5-0.6, which is 1-2 orders of magnitude lower than for\nClass 0 hot corinos, and a factor ~2.5 lower than the only other value inferred\nfor a protoplanetary disk (in TW Hya, 1.3-1.7). Additionally, it is at the\nlower edge but still consistent with the values in comets. This may indicate\nthat some chemical reprocessing occurs in disks before the formation of planets\nand comets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the chiral phase transition of the two-flavor Nambu-Jona-Lasinio\n(NJL) model in a rotating sphere, which includes both rotation and finite size\neffects. We find that rotation leads to a suppression of the chiral condensate\nat a finite temperature, while its effects are smaller than the finite size\neffects. Our work can be helpful to study the effects relevant to rotation in\nheavy-ion collisions in a more realistic way.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Casually-taken portrait photographs often suffer from unflattering lighting\nand shadowing because of suboptimal conditions in the environment. Aesthetic\nqualities such as the position and softness of shadows and the lighting ratio\nbetween the bright and dark parts of the face are frequently determined by the\nconstraints of the environment rather than by the photographer. Professionals\naddress this issue by adding light shaping tools such as scrims, bounce cards,\nand flashes. In this paper, we present a computational approach that gives\ncasual photographers some of this control, thereby allowing poorly-lit\nportraits to be relit post-capture in a realistic and easily-controllable way.\nOur approach relies on a pair of neural networks---one to remove foreign\nshadows cast by external objects, and another to soften facial shadows cast by\nthe features of the subject and to add a synthetic fill light to improve the\nlighting ratio. To train our first network we construct a dataset of real-world\nportraits wherein synthetic foreign shadows are rendered onto the face, and we\nshow that our network learns to remove those unwanted shadows. To train our\nsecond network we use a dataset of Light Stage scans of human subjects to\nconstruct input/output pairs of input images harshly lit by a small light\nsource, and variably softened and fill-lit output images of each face. We\npropose a way to explicitly encode facial symmetry and show that our dataset\nand training procedure enable the model to generalize to images taken in the\nwild. Together, these networks enable the realistic and aesthetically pleasing\nenhancement of shadows and lights in real-world portrait images\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper deals with the oncolytic virotherapy model\n\\begin{equation}\\begin{split} \\begin{cases} &u_t = \\Delta u - \\nabla \\cdot\n(u\\nabla v)-uz +\\mu u(1-u),& \\\\[2ex] &v_t = - (u+w)v,& \\\\[2ex] &w_t = D_w\n\\Delta w - w + uz,& \\\\[2ex] &z_t = D_z \\Delta z - z - uz + \\beta w,&\n\\end{cases} \\end{split}\\end{equation} in a bounded domain $\\Omega$ $\\subset$\n$\\Bbb{R}^2$ with smooth boundary, where $\\mu$, $D_w$, $D_z$ and $\\beta$ are\nprescribed positive parameters.\n  For any given suitably regular initial data, the global existence of\nclassical solution to the corresponding homogeneous Neumann initial-boundary\nproblem for a more general model allowing $\\mu=0$ was previously verified in\n$[$Y. Tao $\\&$ M. Winkler, J. Differential Equations $\\mathbf{268}$ (2020),\n4973-4997$]$. This work further shows that whenever $\\mu>0$, the\nabove-mentioned global classical solution to the above equation is uniformly\nbounded; and moreover, if $\\beta<1$, then the solution $(u, v, w, z)$\nstabilizes to the constant equilibrium $(1, 0, 0, 0)$ in the topology\n$L^p(\\Omega)\\times (L^\\infty(\\Omega))^3$ with any $p>1$ in a large time limit.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this note, we consider the capacitated facility location problem when the\ntransportation costs of the instance satisfy the Monge property. We show that a\nstraightforward dynamic program finds the optimal solution when the demands are\npolynomially bounded. When demands are not polynomially bounded, we give a\nfully polynomial-time approximation scheme by adapting an algorithm and\nanalysis of Van Hoesel and Wagelmans.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Over the past thirty years or so the authors have been teaching various\nprogramming for mathematics courses at our respective Universities, as well as\nincorporating computer algebra and numerical computation into traditional\nmathematics courses. These activities are, in some important ways, natural\nprecursors to the use of Artificial Intelligence in Mathematics Education. This\npaper reflects on some of our course designs and experiences and is therefore a\nmix of theory and practice. Underlying both is a clear recognition of the value\nof computer programming for mathematics education. We use this theory and\npractice to suggest good techniques for and to raise questions about the use of\nAI in Mathematics Education.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  After their re-emergence in the last decades, dengue fever and other\nvector-borne diseases are a potential threat to the lives of millions of\npeople. Based on a data set of dengue cases in the Brazilian city of Fortaleza,\ncollected from 2011 to 2016, we study the spatio-temporal characteristics of\ndengue outbreaks to characterize epidemic and non-epidemic years. First, we\nidentify regions that show a high prevalence of dengue cases and mosquito\nlarvae in different years and also analyze their corresponding correlations.\nOur results show that the characteristic correlation length of the epidemic is\nof the order of the system size, suggesting that factors such as citizen\nmobility may play a major role as a drive for spatial spreading of vector-borne\ndiseases. Inspired by this observation, we perform a mean-field estimation of\nthe basic reproduction number and find that our estimated values agree well\nwith the values reported for other regions, pointing towards similar underlying\nspreading mechanisms. These findings provide insights into the spreading\ncharacteristics of dengue in densely populated areas and should be of relevance\nfor the design of improved disease containment strategies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Starshades are a leading technology to enable the detection and spectroscopic\ncharacterization of Earth-like exoplanets. In this paper we report on optical\nexperiments of sub-scale starshades that advance critical starlight suppression\ntechnologies in preparation for the next generation of space telescopes. These\nexperiments were conducted at the Princeton starshade testbed, an 80 m long\nenclosure testing 1/1000th scale starshades at a flight-like Fresnel number. We\ndemonstrate 1e-10 contrast at the starshade's geometric inner working angle\nacross 10% of the visible spectrum, with an average contrast at the inner\nworking angle of 2.0e-10 and contrast floor of 2e-11. In addition to these high\ncontrast demonstrations, we validate diffraction models to better than 35%\naccuracy through tests of intentionally flawed starshades. Overall, this suite\nof experiments reveals a deviation from scalar diffraction theory due to light\npropagating through narrow gaps between the starshade petals. We provide a\nmodel that accurately captures this effect at contrast levels below 1e-10. The\nresults of these experiments demonstrate that there are no optical impediments\nto building a starshade that provides sufficient contrast to detect Earth-like\nexoplanets. This work also sets an upper limit on the effect of unknowns in the\ndiffraction model used to predict starshade performance and set tolerances on\nthe starshade manufacture.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Topology identification (TI) in distribution networks is a challenging task\ndue to the limited measurement resources and therefore the inevitable need to\nuse pseudo-measurements that are often inaccurate. To address this issue, a new\nmethod is proposed in this paper to integrate harmonic synchrophasors into the\nTI problem in order to enhance TI accuracy in distribution networks. In this\nmethod, topology identification is done jointly based on both fundamental\nsynchrophasor measurements and harmonic synchrophasor measurements. This is\ndone by formulating and then solving a mixed-integer linear programming (MILP)\nproblem. Furthermore, an analysis is provided to capture the number of and the\nlocation of harmonic sources and sensors that are needed to ensure full\nobservability. The benefits of the proposed TI scheme are compared against\nthose of the traditional scheme that utilizes only the fundamental\nmeasurements. Finally, through numerical simulations on the IEEE 33-Bus power\nsystem, it is shown that the proposed scheme is considerably accurate compared\nto the traditional scheme in topology identification.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present an effective Hamiltonian theory available for some\nquasi-periodically driven quantum systems which does not need the knowledge of\nthe Fourier frequencies of the control signal. It could also be available for\nsome chaotically driven quantum systems. It is based on the Koopman approach\nwhich generalizes the Floquet approach used with periodically driven systems.\nWe show the properties of the quasi-energy states (eigenvectors of the\neffective Hamiltonian) as quasi-recurrent states of the quantum system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We conjecture a simple set of \"Feynman rules\" for constructing $n$-point\nglobal conformal blocks in any channel in $d$ spacetime dimensions, for\nexternal and exchanged scalar operators for arbitrary $n$ and $d$. The vertex\nfactors are given in terms of Lauricella hypergeometric functions of one, two\nor three variables, and the Feynman rules furnish an explicit power-series\nexpansion in powers of cross-ratios. These rules are conjectured based on\npreviously known results in the literature, which include four-, five- and\nsix-point examples as well as the $n$-point comb channel blocks. We prove these\nrules for all previously known cases, as well as for a seven-point block in a\nnew topology and the even-point blocks in the \"OPE channel.\" The proof relies\non holographic methods, notably the Feynman rules for Mellin amplitudes of\ntree-level AdS diagrams in a scalar effective field theory, and is easily\napplicable to any particular choice of a conformal block.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the equisingularity of a family of function germs\n$\\{f_t\\colon(X_t,0)\\to (\\mathbb{C},0)\\}$, where $(X_t,0)$ are $d$-dimensional\nisolated determinantal singularities. We define the $(d-1)$th polar\nmultiplicity of the fibers $X_t\\cap f_t^{-1}(0)$ and we show how the constancy\nof the polar multiplicities is related to the constancy of the Milnor number of\n$f_t$ and the Whitney equisingularity of the family.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Given a quasi-transitive infinite graph $G$ with volume growth rate ${\\rm\ngr}(G),$ a transient biased electric network $(G,\\, c_1)$ with bias\n$\\lambda_1\\in (0,\\,{\\rm gr}(G))$ and a recurrent biased one $(G,\\, c_2)$ with\nbias $\\lambda_2\\in ({\\rm gr}(G),\\infty).$ Write $G(p)$ for the Bernoulli-$p$\nbond percolation on $G$ defined by the grand coupling. Let $(G,\\, c_1,\\, c_2,\\,\np)$ be the following biased disordered random network: Open edges $e$ in $G(p)$\ntake the conductance $c_1(e)$, and closed edges $g$ in $G(p)$ take the\nconductance $c_2(g)$. Our main results are as follows: (i) On connected\nquasi-transitive infinite graph $G$ with percolation threshold $p_c\\in (0,\\,\n1),$ $(G,\\, c_1,\\, c_2,\\, p)$ has a non-trivial recurrence/transience phase\ntransition such that the threshold $p_{c}^{*}\\in (0,\\, 1)$ is deterministic,\nand almost surely $(G,\\, c_1,\\, c_2,\\, p)$ is recurrent for $p<p_c^*$ and\ntransient for $p>p_c^*.$ There is a non-trivial recurrence/transience phase\ntransition for $(G,\\, c_1,\\, c_2,\\, p)$ with $G$ being a Cayley graph if and\nonly if the corresponding group is not virtually $\\mathbb{Z}$. (ii) On\n$\\mathbb{Z}^d$ for any $d\\geq 1,$ $p_c^{*}= p_c$. And on $d$-regular trees\n$\\mathbb{T}^d$ with $d\\geq 3$, $p_c^{*}=(\\lambda_1\\vee 1) p_c$, and thus\n$p_c^{*}>p_c$ for any $\\lambda_1\\in (1,\\,{\\rm gr}(\\mathbb{T}^d)).$ As a\ncontrast, we also consider phase transition of having unique currents or not\nfor $(\\mathbb{Z}^d,\\, c_1,\\, c_2,\\, p)$ with $d\\geq 2$ and prove that almost\nsurely $(\\mathbb{Z}^2,\\, c_1,\\, c_2,\\, p)$ with $\\lambda_1<1\\leq\\lambda_2$ has\nunique currents for any $p\\in [0,1]$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In an effort to better understand the formation of galaxy groups, we examine\nthe kinematics of a large sample of spectroscopically confirmed X-ray galaxy\ngroups in the Cosmic Evolution Survey (COSMOS) with a high sampling of galaxy\ngroup members up to $z=1$. We compare our results with predictions from the\ncosmological hydrodynamical simulation of {\\sc Horizon-AGN}. Using a\nphase-space analysis of dynamics of groups with halo masses of\n$M_{\\mathrm{200c}}\\sim 10^{12.6}-10^{14.50}M_\\odot$, we show that the brightest\ngroup galaxies (BGG) in low mass galaxy groups ($M_{\\mathrm{200c}}<2 \\times\n10^{13} M_\\odot$) have larger proper motions relative to the group velocity\ndispersion than high mass groups. The dispersion in the ratio of the BGG proper\nvelocity to the velocity dispersion of the group,\n$\\sigma_{\\mathrm{BGG}}/\\sigma_{group}$, is on average $1.48 \\pm 0.13$ for low\nmass groups and $1.01 \\pm 0.09$ for high mass groups. A comparative analysis of\nthe {\\sc Horizon-AGN} simulation reveals a similar increase in the spread of\npeculiar velocities of BGGs with decreasing group mass, though consistency in\nthe amplitude, shape, and mode of the BGG peculiar velocity distribution is\nonly achieved for high mass groups. The groups hosting a BGG with a large\npeculiar velocity are more likely to be offset from the $L_x-\\sigma_{v}$\nrelation; this is probably because the peculiar motion of the BGG is influenced\nby the accretion of new members.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show that, possibly after a compactification of spacetime, constant\nfunctions are local maximizers of the Tomas-Stein adjoint Fourier restriction\ninequality for the cone and paraboloid in every dimension, and for the sphere\nin dimension up to 60. For the cone and paraboloid we work from the PDE\nframework, which enables the use of the Penrose and the Lens transformations,\nwhich map the conjectured optimal functions into constants.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spectral CT has great potential for a variety of clinical applications due to\nthe improved material discrimination with respect to conventional CT. Many\nclinical and preclinical spectral CT systems have two spectral channels for\ndual-energy CT using strategies such as split-filtration, dual-layer detectors,\nor kVp-switching. However, there are emerging clinical imaging applications\nwhich would require three or more spectral sensitivity channels, for example,\nmultiple exogenous contrast agents in a single scan. Spatial-spectral filters\nare a new spectral CT technology which use x-ray beam modulation to offer\ngreater spectral diversity. The device consists of an array of k-edge filters\nwhich divide the x-ray beam into spectrally varied beamlets. This design allows\nfor an arbitrary number of spectral channels; however, traditional two-step\nreconstruction-decomposition schemes are typically not effective because the\nmeasured data for any individual spectral channel is sparse in the projection\ndomain. Instead, we use a one-step model-based material decomposition algorithm\nto iteratively estimate material density images directly from spectral CT data.\nIn this work, we present a prototype spatial-spectral filter integrated with an\nx-ray CT test-bench. The filter is composed of an array of tin, erbium,\ntantalum, and lead filter tiles which spatially modulate the system spectral\nsensitivity pattern. After the system was characterized and modeled, we\nconducted a spectral CT scan of a multi-contrast-enhanced phantom containing\nwater, iodine, and gadolinium solutions. We present the resulting spectral CT\ndata as well as the material density images estimated by model-based material\ndecomposition...\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A partition $\\mathcal{P}$ of a weighted graph $G$ is\n$(\\sigma,\\tau,\\Delta)$-sparse if every cluster has diameter at most $\\Delta$,\nand every ball of radius $\\Delta/\\sigma$ intersects at most $\\tau$ clusters.\nSimilarly, $\\mathcal{P}$ is $(\\sigma,\\tau,\\Delta)$-scattering if instead for\nballs we require that every shortest path of length at most $\\Delta/\\sigma$\nintersects at most $\\tau$ clusters. Given a graph $G$ that admits a\n$(\\sigma,\\tau,\\Delta)$-sparse partition for all $\\Delta>0$, Jia et al. [STOC05]\nconstructed a solution for the Universal Steiner Tree problem (and also\nUniversal TSP) with stretch $O(\\tau\\sigma^2\\log_\\tau n)$. Given a graph $G$\nthat admits a $(\\sigma,\\tau,\\Delta)$-scattering partition for all $\\Delta>0$,\nwe construct a solution for the Steiner Point Removal problem with stretch\n$O(\\tau^3\\sigma^3)$. We then construct sparse and scattering partitions for\nvarious different graph families, receiving many new results for the Universal\nSteiner Tree and Steiner Point Removal problems.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Some properties of the optimal representation of numbers are investigated.\nThis representation, which is to the base-e, is examined for coding of\nintegers. An approximate representation without fractions that we call WF is\nintroduced and compared with base-2 and base-3 representations, which are next\nto base-e in efficiency. Since trees are analogous to number representation, we\nexplore the relevance of the statistical optimality of the base-e system for\nthe understanding of complex system behavior and of social networks. We show\nthat this provides a new theoretical explanation for the nature of the power\nlaw exhibited by many open complex systems. In specific, we show that the power\nlaw distribution most often proposed for such systems has a form that is\nsimilar to that derived from the optimal base-e representation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In $\\Lambda_b^0 \\to \\Lambda_c^+ (\\to \\Lambda^0 \\pi^+) \\tau^- \\bar{\\nu}_\\tau $\ndecay, the solid angle of the final-state particle $\\tau^-$ cannot be\ndetermined precisely since the decay products of the $\\tau^-$ include an\nundetected $\\nu_\\tau$. Therefore, the angular distribution of this decay cannot\nbe measured. In this work, we construct a {\\it measurable} angular distribution\nby considering the subsequent decay $\\tau^- \\to \\pi^- \\nu_\\tau$. The full\ncascade decay is $\\Lambda_b^0 \\to \\Lambda_c^+ (\\to \\Lambda^0 \\pi^+)\\tau^- (\\to\n\\pi^- \\nu_\\tau)\\bar{\\nu}_\\tau$. The three-momenta of the final-state particles\n$\\Lambda^0$, $\\pi^+$, and $\\pi^-$ can be measured. Considering all Lorentz\nstructures of the new physics (NP) effective operators and an unpolarized\ninitial $\\Lambda_b$ state, the five-fold differential angular distribution can\nbe expressed in terms of ten angular observables ${\\cal K}_i (q^2, E_\\pi)$. By\nintegrating over some of the five kinematic parameters, we define a number of\nobservables, such as the $\\Lambda_c$ spin polarization $P_{\\Lambda_c}(q^2)$ and\nthe forward-backward asymmetry of $\\pi^-$ meson $A_{FB}(q^2)$, both of which\ncan be represented by the angular observables $\\widehat{{\\cal K}}_i (q^2)$. We\nprovide numerical results for the entire set of the angular observables\n$\\widehat{{\\cal K}}_i (q^2)$ and $\\widehat{{\\cal K}}_i$ both within the\nStandard Model and in some NP scenarios, which are a variety of best-fit\nsolutions in seven different NP hypotheses. We find that the NP which can\nresolve the anomalies in $\\bar{B} \\to D^{(*)} \\tau^- \\bar{\\nu}_\\tau$ decays has\nobvious effects on the angular observables $\\widehat{{\\cal K}}_i (q^2)$, except\n$\\widehat{{\\cal K}}_{1ss} (q^2)$ and $\\widehat{{\\cal K}}_{1cc} (q^2)$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A search for charged leptons with large impact parameters using 139 fb$^{-1}$\nof $\\sqrt{s} = 13$ TeV $pp$ collision data from the ATLAS detector at the LHC\nis presented, addressing a long-standing gap in coverage of possible new\nphysics signatures. Results are consistent with the background prediction. This\nsearch provides unique sensitivity to long-lived scalar supersymmetric\nlepton-partners (sleptons). For lifetimes of 0.1 ns, selectron, smuon and stau\nmasses up to 720 GeV, 680 GeV, and 340 GeV are respectively excluded at 95%\nconfidence level, drastically improving on the previous best limits from LEP.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although pretrained Transformers such as BERT achieve high accuracy on\nin-distribution examples, do they generalize to new distributions? We\nsystematically measure out-of-distribution (OOD) generalization for seven NLP\ndatasets by constructing a new robustness benchmark with realistic distribution\nshifts. We measure the generalization of previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show that pretrained Transformers'\nperformance declines are substantially smaller. Pretrained transformers are\nalso more effective at detecting anomalous or OOD examples, while many previous\nmodels are frequently worse than chance. We examine which factors affect\nrobustness, finding that larger models are not necessarily more robust,\ndistillation can be harmful, and more diverse pretraining data can enhance\nrobustness. Finally, we show where future work can improve OOD robustness.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Service robots in the future need to execute abstract instructions such as\n\"fetch the milk from the fridge\". To translate such instructions into\nactionable plans, robots require in-depth background knowledge. With regards to\ninteractions with doors and drawers, robots require articulation models that\nthey can use for state estimation and motion planning. Existing frameworks\nmodel articulated connections as abstract concepts such as prismatic, or\nrevolute, but do not provide a parameterized model of these connections for\ncomputation. In this paper, we introduce a novel framework that uses symbolic\nmathematical expressions to model articulated structures -- robots and objects\nalike -- in a unified and extensible manner. We provide a theoretical\ndescription of this framework, and the operations that are supported by its\nmodels, and introduce an architecture to exchange our models in robotic\napplications, making them as flexible as any other environmental observation.\nTo demonstrate the utility of our approach, we employ our practical\nimplementation Kineverse for solving common robotics tasks from state\nestimation and mobile manipulation, and use it further in real-world mobile\nrobot manipulation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We extend a higher-order sum rule proved by B. Simon to matrix valued\nmeasures on the unit circle and their matrix Verblunsky coefficients.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In 1978, Dwight Duffus---editor-in-chief of the journal \"Order\" from 2010 to\n2018 and chair of the Mathematics Department at Emory University from 1991 to\n2005---wrote that \"it is not obvious that $P$ is connected and $P^P$ isomorphic\nto $Q^Q$ implies that $Q$ is connected,\" where $P$ and $Q$ are finite non-empty\nposets. We show that, indeed, under these hypotheses $Q$ is connected and\n$P\\cong Q$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We establish Sobolev type inequalities in the noncommutative settings by\ngeneralizing monotone metrics in the space of quantum states, such as\nmatrix-valued Beckner inequalities. We also discuss examples such as random\ntranspositions and Bernoulli-Laplace models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Multi-task learning aims at solving multiple machine learning tasks at the\nsame time. A good solution to a multi-task learning problem should be\ngeneralizable in addition to being Pareto optimal. In this paper, we provide\nsome insights on understanding the trade-off between Pareto efficiency and\ngeneralization as a result of parameterization in multi-task deep learning\nmodels. As a multi-objective optimization problem, enough parameterization is\nneeded for handling task conflicts in a constrained solution space; however,\nfrom a multi-task generalization perspective, over-parameterization undermines\nthe benefit of learning a shared representation which helps harder tasks or\ntasks with limited training examples. A delicate balance between multi-task\ngeneralization and multi-objective optimization is therefore needed for finding\na better trade-off between efficiency and generalization. To this end, we\npropose a method of under-parameterized self-auxiliaries for multi-task models\nto achieve the best of both worlds. It is task-agnostic and works with other\nmulti-task learning algorithms. Empirical results show that small towers of\nunder-parameterized self-auxiliaries can make big differences in improving\nPareto efficiency in various multi-task applications.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Query optimizers rely on accurate cardinality estimation (CardEst) to produce\ngood execution plans. The core problem of CardEst is how to model the rich\njoint distribution of attributes in an accurate and compact manner. Despite\ndecades of research, existing methods either over simplify the models only\nusing independent factorization which leads to inaccurate estimates, or over\ncomplicate them by lossless conditional factorization without any independent\nassumption which results in slow probability computation. In this paper, we\npropose FLAT, a CardEst method that is simultaneously fast in probability\ncomputation, lightweight in model size and accurate in estimation quality. The\nkey idea of FLAT is a novel unsupervised graphical model, called FSPN. It\nutilizes both independent and conditional factorization to adaptively model\ndifferent levels of attributes correlations, and thus dovetails their\nadvantages. FLAT supports efficient online probability computation in near\nliner time on the underlying FSPN model, provides effective offline model\nconstruction and enables incremental model updates. It can estimate cardinality\nfor both single table queries and multi table join queries. Extensive\nexperimental study demonstrates the superiority of FLAT over existing CardEst\nmethods on well known IMDB benchmarks: FLAT achieves 1 to 5 orders of magnitude\nbetter accuracy, 1 to 3 orders of magnitude faster probability computation\nspeed and 1 to 2 orders of magnitude lower storage cost. We also integrate FLAT\ninto Postgres to perform an end to end test. It improves the query execution\ntime by 12.9% on the benchmark workload, which is very close to the optimal\nresult 14.2% using the true cardinality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Graph Convolutional Networks (GCNs) are state-of-the-art graph based\nrepresentation learning models by iteratively stacking multiple layers of\nconvolution aggregation operations and non-linear activation operations.\nRecently, in Collaborative Filtering (CF) based Recommender Systems (RS), by\ntreating the user-item interaction behavior as a bipartite graph, some\nresearchers model higher-layer collaborative signals with GCNs. These GCN based\nrecommender models show superior performance compared to traditional works.\nHowever, these models suffer from training difficulty with non-linear\nactivations for large user-item graphs. Besides, most GCN based models could\nnot model deeper layers due to the over smoothing effect with the graph\nconvolution operation. In this paper, we revisit GCN based CF models from two\naspects. First, we empirically show that removing non-linearities would enhance\nrecommendation performance, which is consistent with the theories in simple\ngraph convolutional networks. Second, we propose a residual network structure\nthat is specifically designed for CF with user-item interaction modeling, which\nalleviates the over smoothing problem in graph convolution aggregation\noperation with sparse user-item interaction data. The proposed model is a\nlinear model and it is easy to train, scale to large datasets, and yield better\nefficiency and effectiveness on two real datasets. We publish the source code\nat https://github.com/newlei/LRGCCF.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the convergence rate of the Circumcentered-Reflection Method (CRM)\nfor solving the convex feasibility problem and compare it with the Method of\nAlternating Projections (MAP). Under an error bound assumption, we prove that\nboth methods converge linearly, with asymptotic constants depending on a\nparameter of the error bound, and that the one derived for CRM is strictly\nbetter than the one for MAP. Next, we analyze two classes of fairly generic\nexamples. In the first one, the angle between the convex sets approaches zero\nnear the intersection, so that the MAP sequence converges sublinearly, but CRM\nstill enjoys linear convergence. In the second class of examples, the angle\nbetween the sets does not vanish and MAP exhibits its standard behavior, i.e.,\nit converges linearly, yet, perhaps surprisingly, CRM attains superlinear\nconvergence.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Virtualisation first and cloud computing later has led to a consolidation of\nworkload in data centres that also comprises latency-sensitive application\ndomains such as High Performance Computing and telecommunication. These types\nof applications require strict latency guarantees to maintain their Quality of\nService. In virtualised environments with their churn, this demands for\nadaptability and flexibility to satisfy. At the same time, the mere scale of\nthe infrastructures favours commodity (Ethernet) over specialised (Infiniband)\nhardware. For that purpose, this paper introduces a novel traffic management\nalgorithm that combines Rate-limited Strict Priority and Deficit round-robin\nfor latency-aware and fair scheduling respectively. In addition, we present an\nimplementation of this algorithm on the bmv2 P4 software switch by evaluating\nit against standard priority-based and best-effort scheduling.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider a collection of weakly interacting diffusion processes moving in\na two-scale locally periodic environment. We study the large deviations\nprinciple of the empirical distribution of the particles' positions in the\ncombined limit as the number of particles grow to infinity and the time-scale\nseparation parameter goes to zero simultaneously. We make use of weak\nconvergence methods providing a convenient representation for the large\ndeviations rate function, which allow us to characterize the effective\ncontrolled mean field dynamics. In addition, we obtain equivalent\nrepresentations for the large deviations rate function of the form of\nDawson-G\\\"artner which hold even in the case where the diffusion matrix depends\non the empirical measure and when the particles undergo averaging in addition\nto the propagation of chaos.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Charge quantization, or the absence thereof, is a central theme in quantum\ncircuit theory, with dramatic consequences for the predicted circuit dynamics.\nVery recently, the question of whether or not charge should actually be\ndescribed as quantized has enjoyed renewed widespread interest, with however\nseemingly contradictory propositions. Here, we intend to reconcile these\ndifferent approaches, by arguing that ultimately, charge quantization is not an\nintrinsic system property, but instead depends on the spatial resolution of the\ncharge detector. We show that the latter can be directly probed by unique\ngeometric signatures in the correlations of the supercurrent. We illustrate\nthese findings at the example Josephson junction arrays in the superinductor\nregime, where the transported charge appears to be continuous. Finally, we\ncomment on potential consequences of charge quantization beyond superconducting\ncircuits.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this article, we continue our study on universal learning machine by\nintroducing new tools. We first discuss boolean function and boolean circuit,\nand we establish one set of tools, namely, fitting extremum and proper sampling\nset. We proved the fundamental relationship between proper sampling set and\ncomplexity of boolean circuit. Armed with this set of tools, we then introduce\nmuch more effective learning strategies. We show that with such learning\nstrategies and learning dynamics, universal learning can be achieved, and\nrequires much less data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We find that thin peptide films and coatings doped with metallic\nnanoparticles are more stable due to the role of electromagnetic fluctuations.\nIt is shown that for the doped freestanding in vacuum peptide film the Casimir\nattraction becomes larger in magnitude. For dielectric substrates coated with\npeptide films, the nanoparticle doping leads to a wider range of film\nthicknesses where the Casimir pressure is attractive and to larger pressure\nmagnitudes at the points of extremum. The doping of peptide coatings with\nmagnetic nanoparticles preserves all the advantages of nonmagnetic ones and\nsimultaneously imparts superparamagnetic properties to the coating which could\nextend significantly the application areas of bioelectronics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Proposals aimed at measuring the Electric Dipole Moment (EDM) for charged\nparticles require a good understanding of the systematic errors that can\ncontribute to a vertical spin buildup mimicking the EDM signal to be detected.\nIn what follows, a method of averaging emanating from the Bogoliubov-Krylov\nMitropolski method is employed to solve the T-BMT equation and calculate the\nBerry phases arising for the storage ring frozen spin concept. The formalism\nemployed proved to be particularly useful to determine the evolution of the\nspin at the observation point, i.e. at the location of the polarimeter. Several\nselected cases of lattice imperfections were simulated and benchmarked with the\nanalytical estimates. This allowed the proof of the convergence of the\nnumerical simulations and helped gain better understanding of the systematic\nerrors.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study Landau-Zener-St\\\"uckelberg (LZS) interferometry in a cQED\narchitecture under effects of dissipation. To be specific, we consider a\nsuperconducting qubit driven by a dc+ac signal and coupled to a transmission\nline resonator, but our results are valid for general qubit-resonators devices.\nTo take the environment into account, we assume that the resonator is coupled\nto an ohmic quantum bath. The Floquet-Born-Markov master equation is\nnumerically solved to obtain the dynamics of the system for arbitrary amplitude\nof the drive and different time scales. We unveil important differences in the\nresonant patterns between the Strong Coupling and Ultra Strong Coupling regimes\nin the qubit-resonator interaction, which are mainly due to the magnitude of\nphotonic gaps in the energy spectrum of the system. We identify in the LZS\npatterns the contribution of the qubit gap and the photonic gaps, showing that\nfor large driving amplitudes the patterns present a weaving structure due to\nthe combined intercrossing of the different gaps contributions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  t-SNE and hierarchical clustering are popular methods of exploratory data\nanalysis, particularly in biology. Building on recent advances in speeding up\nt-SNE and obtaining finer-grained structure, we combine the two to create\ntree-SNE, a hierarchical clustering and visualization algorithm based on\nstacked one-dimensional t-SNE embeddings. We also introduce alpha-clustering,\nwhich recommends the optimal cluster assignment, without foreknowledge of the\nnumber of clusters, based off of the cluster stability across multiple scales.\nWe demonstrate the effectiveness of tree-SNE and alpha-clustering on images of\nhandwritten digits, mass cytometry (CyTOF) data from blood cells, and\nsingle-cell RNA-sequencing (scRNA-seq) data from retinal cells. Furthermore, to\ndemonstrate the validity of the visualization, we use alpha-clustering to\nobtain unsupervised clustering results competitive with the state of the art on\nseveral image data sets. Software is available at\nhttps://github.com/isaacrob/treesne.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The usual approach to model reduction for parametric partial differential\nequations (PDEs) is to construct a linear space $V_n$ which approximates well\nthe solution manifold $\\mathcal{M}$ consisting of all solutions $u(y)$ with $y$\nthe vector of parameters. This linear reduced model $V_n$ is then used for\nvarious tasks such as building an online forward solver for the PDE or\nestimating parameters from data observations. It is well understood in other\nproblems of numerical computation that nonlinear methods such as adaptive\napproximation, $n$-term approximation, and certain tree-based methods may\nprovide improved numerical efficiency. For model reduction, a nonlinear method\nwould replace the linear space $V_n$ by a nonlinear space $\\Sigma_n$. This idea\nhas already been suggested in recent papers on model reduction where the\nparameter domain is decomposed into a finite number of cells and a linear space\nof low dimension is assigned to each cell.\n  Up to this point, little is known in terms of performance guarantees for such\na nonlinear strategy. Moreover, most numerical experiments for nonlinear model\nreduction use a parameter dimension of only one or two. In this work, a step is\nmade towards a more cohesive theory for nonlinear model reduction. Framing\nthese methods in the general setting of library approximation allows us to give\na first comparison of their performance with those of standard linear\napproximation for any general compact set. We then turn to the study these\nmethods for solution manifolds of parametrized elliptic PDEs. We study a very\nspecific example of library approximation where the parameter domain is split\ninto a finite number $N$ of rectangular cells and where different reduced\naffine spaces of dimension $m$ are assigned to each cell. The performance of\nthis nonlinear procedure is analyzed from the viewpoint of accuracy of\napproximation versus $m$ and $N$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Simulation-based ultrasound training can be an essential educational tool.\nRealistic ultrasound image appearance with typical speckle texture can be\nmodeled as convolution of a point spread function with point scatterers\nrepresenting tissue microstructure. Such scatterer distribution, however, is in\ngeneral not known and its estimation for a given tissue type is fundamentally\nan ill-posed inverse problem. In this paper, we demonstrate a convolutional\nneural network approach for probabilistic scatterer estimation from observed\nultrasound data. We herein propose to impose a known statistical distribution\non scatterers and learn the mapping between ultrasound image and distribution\nparameter map by training a convolutional neural network on synthetic images.\nIn comparison with several existing approaches, we demonstrate in numerical\nsimulations and with in-vivo images that the synthesized images from scatterer\nrepresentations estimated with our approach closely match the observations with\nvarying acquisition parameters such as compression and rotation of the imaged\ndomain.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Convex neural codes are subsets of the Boolean lattice that record the\nintersection patterns of convex sets in Euclidean space. Much work in recent\nyears has focused on finding combinatorial criteria on codes that can be used\nto classify whether or not a code is convex. In this paper we introduce\norder-forcing, a combinatorial tool which recognizes when certain regions in a\nrealization of a code must appear along a line segment between other regions.\nWe use order-forcing to construct novel examples of non-convex codes, and to\nexpand existing families of examples. We also construct a family of codes which\nshows that a dimension bound of Cruz, Giusti, Itskov, and Kronholm (referred to\nas monotonicity of open convexity) is tight in all dimensions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We use the theory of bialgebras to provide the algebraic background for state\nspace realization theorems for input-output maps of control systems. This\nallows us to consider from a common viewpoint classical results about formal\nstate space realizations of nonlinear systems and more recent results involving\nanalysis related to families of trees. If $H$ is a bialgebra, we say that $p\n\\in H^*$ is differentially produced by the algebra $R$ with the augmentation\n$\\epsilon$ if there is right $H$-module algebra structure on $R$ and there\nexists $f \\in R$ satisfying $p(h) = \\epsilon(f \\cdot h)$. We characterize those\n$p \\in H^*$ which are differentially produced.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We provide bounds for the sequence of eigenvalues $\\{\\lambda_i(\\Omega)\\}_i$\nof the Dirichlet problem $$ L_\\Delta u=\\lambda u\\ \\ {\\rm in}\\ \\,\n\\Omega,\\quad\\quad u=0\\ \\ {\\rm in}\\ \\ \\mathbb{R}^N\\setminus \\Omega,$$ where\n$L_\\Delta$ is the logarithmic Laplacian operator with Fourier transform symbol\n$2\\ln |\\zeta|$. The logarithmic Laplacian operator is not positively definitive\nif the volume of the domain is large enough. In this article, we obtain the\nupper and lower bounds for the sum of the first $k$ eigenvalues by extending\nthe Li-Yau method and Kr\\\"oger's method respectively. Moreover, we show the\nlimit of the sum of the first $k$ eigenvalues, which is independent of the\nvolume of the domain. Finally, we discuss the lower and upper bounds of the\n$k$-th principle eigenvalue, the asymptotic behavior of the limit of\neigenvalues.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A recently introduced representation by a set of Wang tiles -- a\ngeneralization of the traditional Periodic Unit Cell based approach -- serves\nas a reduced geometrical model for materials with stochastic heterogeneous\nmicrostructure, enabling an efficient synthesis of microstructural\nrealizations. To facilitate macroscopic analyses with a fully resolved\nmicrostructure generated with Wang tiles, we develop a reduced order modelling\nscheme utilizing pre-computed characteristic features of the tiles. In the\noffline phase, inspired by the computational homogenization, we extract\ncontinuous fluctuation fields from the compressed microstructural\nrepresentation as responses to generalized loading represented by the first-\nand second-order macroscopic gradients. In the online phase, using the ansatz\nof the Generalized Finite Element Method, we combine these fields with a coarse\nfinite element discretization to create microstructure-informed reduced modes\nspecific for a given macroscopic problem. Considering a two-dimensional scalar\nelliptic problem, we demonstrate that our scheme delivers less than a 3% error\nin both the relative $L_2$ and energy norms with only 0.01% of the unknowns\nwhen compared to the fully resolved problem. Accuracy can be further improved\nby locally refining the macroscopic discretization and/or employing more\npre-computed fluctuation fields. Finally, unlike the standard snapshot-based\nreduced-order approaches, our scheme handles significant changes in the\nmacroscopic geometry or loading without the need for recalculating the offline\nphase, because the fluctuation fields are extracted without any prior knowledge\non the macroscopic problem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Zero-shot learning (ZSL) algorithms typically work by exploiting attribute\ncorrelations to be able to make predictions in unseen classes. However, these\ncorrelations do not remain intact at test time in most practical settings and\nthe resulting change in these correlations lead to adverse effects on zero-shot\nlearning performance. In this paper, we present a new paradigm for ZSL that:\n(i) utilizes the class-attribute mapping of unseen classes to estimate the\nchange in target distribution (target shift), and (ii) propose a novel\ntechnique called grouped Adversarial Learning (gAL) to reduce negative effects\nof this shift. Our approach is widely applicable for several existing ZSL\nalgorithms, including those with implicit attribute predictions. We apply the\nproposed technique ($g$AL) on three popular ZSL algorithms: ALE, SJE, and\nDEVISE, and show performance improvements on 4 popular ZSL datasets: AwA2, aPY,\nCUB and SUN. We obtain SOTA results on SUN and aPY datasets and achieve\ncomparable results on AwA2.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Key enzymatic processes in biology use the nonequilibrium error correction\nmechanism called kinetic proofreading to enhance their specificity. Kinetic\nproofreading typically requires several dedicated structural features in the\nenzyme, such as a nucleotide hydrolysis site and multiple enzyme-substrate\nconformations that delay product formation. Such requirements limit the\napplicability and the adaptability of traditional proofreading schemes. Here,\nwe explore an alternative conceptual mechanism of error correction that\nachieves delays between substrate binding and subsequent product formation by\nhaving these events occur at distinct physical locations. The time taken by the\nenzyme-substrate complex to diffuse from one location to another is leveraged\nto discard wrong substrates. This mechanism does not require dedicated\nstructural elements on the enzyme, making it easier to overlook in experiments\nbut also making proofreading tunable on the fly. We discuss how tuning the\nlength scales of enzyme or substrate concentration gradients changes the\nfidelity, speed and energy dissipation, and quantify the performance\nlimitations imposed by realistic diffusion and reaction rates in the cell. Our\nwork broadens the applicability of kinetic proofreading, and sets the stage for\nthe study of spatial gradients as a possible route to specificity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We give a proof of the Bourgain-Milman theorem using complex methods. The\nproof is inspired by Kuperberg's, but considerably shorter.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Order statistics arising from $m$ independent but not identically distributed\nrandom variables are typically constructed by arranging some $X_{1}, X_{2},\n\\ldots, X_{m}$, with $X_{i}$ having distribution function $F_{i}(x)$, in\nincreasing order denoted as $X_{(1)} \\leq X_{(2)} \\leq \\ldots \\leq X_{(m)}$. In\nthis case, $X_{(i)}$ is not necessarily associated with $F_{i}(x)$. Assuming\none can simulate values from each distribution, one can generate such \"non-iid\"\norder statistics by simulating $X_{i}$ from $F_{i}$, for $i=1,2,\\ldots, m$, and\narranging them in order. In this paper, we consider the problem of simulating\nordered values $X_{(1)}, X_{(2)}, \\ldots, X_{(m)}$ such that the marginal\ndistribution of $X_{(i)}$ is $F_{i}(x)$. This problem arises in Bayesian\nprincipal components analysis (BPCA) where the $X_{i}$ are ordered eigenvalues\nthat are a posteriori independent but not identically distributed. We propose a\nnovel coupling-from-the-past algorithm to \"perfectly\" (up to computable order\nof accuracy) simulate such {\\emph{order-constrained non-iid}} order statistics.\nWe demonstrate the effectiveness of our approach for several examples,\nincluding the BPCA problem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Coarse geometry studies metric spaces on the large scale. Our goal here is to\nstudy dynamics from a coarse point of view. To this end we introduce a coarse\nversion of topological entropy, suitable for unbounded metric spaces,\nconsistent with the coarse perspective on such spaces. As is the case with the\nusual topological entropy, the coarse entropy measures the divergence of\norbits. Following Bowen's ideas, we use $(n,\\varepsilon)$-separated or\n$(n,\\varepsilon)$-spanning sets. However, we have to let $\\varepsilon$ go to\ninfinity rather than to zero.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The polylogarithm function is one of the constellation of important\nmathematical functions. It has a long history, and many connections to other\nspecial functions and series, and many applications, for instance in\nstatistical physics. However, the practical aspects of its numerical evaluation\nhave not received the type of comprehensive treatments lavished on its\nsiblings. Only a handful of formal publications consider the evaluation of the\nfunction, and most focus on a specific domain and/or presume arbitrary\nprecision arithmetic will be used. And very little of the literature contains\nany formal validation of numerical performance. In this paper we present an\nalgorithm for calculating polylogarithms for both complex parameter and\nargument and evaluate it thoroughly in comparison to the arbitrary precision\nimplementation in Mathematica. The implementation was created in a new\nscientific computing language Julia, which is ideal for the purpose, but also\nallows us to write the code in a simple, natural manner so as to make it easy\nto port the implementation to other such languages.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the beamforming optimization for an intelligent reflecting surface\n(IRS)-aided full-duplex (FD) communication system in this letter. Specifically,\nwe maximize the sum rate of bi-directional transmissions by jointly optimizing\nthe transmit beamforming and the beamforming of the IRS reflection. A fast\nconverging alternating algorithm is developed to tackle this problem. In each\niteration of the proposed algorithm, the solutions to the transmit beamforming\nand the IRS reflect beamforming are obtained in a semi-closed form and a closed\nform, respectively. Compared to an existing method based on the Arimoto-Blahut\nalgorithm, the proposed method achieves almost the same performance while\nenjoying much faster convergence and lower computational complexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Learning-based control aims to construct models of a system to use for\nplanning or trajectory optimization, e.g. in model-based reinforcement\nlearning. In order to obtain guarantees of safety in this context, uncertainty\nmust be accurately quantified. This uncertainty may come from errors in\nlearning (due to a lack of data, for example), or may be inherent to the\nsystem. Propagating uncertainty forward in learned dynamics models is a\ndifficult problem. In this work we use deep learning to obtain expressive and\nflexible models of how distributions of trajectories behave, which we then use\nfor nonlinear Model Predictive Control (MPC). We introduce a deep quantile\nregression framework for control that enforces probabilistic quantile bounds\nand quantifies epistemic uncertainty. Using our method we explore three\ndifferent approaches for learning tubes that contain the possible trajectories\nof the system, and demonstrate how to use each of them in a Tube MPC scheme. We\nprove these schemes are recursively feasible and satisfy constraints with a\ndesired margin of probability. We present experiments in simulation on a\nnonlinear quadrotor system, demonstrating the practical efficacy of these\nideas.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Wide-bandgap perovskite stannates are of interest for the emergent all-oxide\ntransparent electronic devices due to their unparalleled room temperature\nelectron mobility. Considering the advantage of amorphous material in\nintegrating with non-semiconductor platforms, we herein reported the optical\nand electronic properties in the prototypical stannate, amorphous barium\nstannate (BaSnO3) thin films, which were deposited at room temperature and\nannealed at various temperatures. Despite remaining amorphous status, with\nincreasing the annealing temperature, the defect level within amorphous BaSnO3\nthin films could be suppressed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hydrogen can penetrate reversibly a number of metals, occupy the interstitial\nsites and cause large expansion of the crystal lattice. The question discussed\nhere is whether the kinetics of the structural response matches hydrogen\nabsorption. We show that thin Pd and CoPd films exposed to a relatively rich\nhydrogen atmosphere (4% H2) inflate irreversibly, demonstrate the controllable\nshape memory, and duration of the process can be orders of magnitude longer\nthan hydrogen absorption. The dynamics of the out-of-equilibrium plastic creep\nis well described by the Avrami - type model of the nucleation and lateral\ndomain wall expansion of the swelled sites.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Joint multi-messenger observations with gravitational waves and\nelectromagnetic data offer new insights into the astrophysical studies of\ncompact objects. The third Advanced LIGO and Advanced Virgo observing run began\non April 1, 2019; during the eleven months of observation, there have been 14\ncompact binary systems candidates for which at least one component is\npotentially a neutron star. Although intensive follow-up campaigns involving\ntens of ground and space-based observatories searched for counterparts, no\nelectromagnetic counterpart has been detected. Following on a previous study of\nthe first six months of the campaign, we present in this paper the next five\nmonths of the campaign from October 2019 to March 2020. We highlight two\nneutron star - black hole candidates (S191205ah, S200105ae), two binary neutron\nstar candidates (S191213g and S200213t) and a binary merger with a possible\nneutron star and a \"MassGap\" component, S200115j. Assuming that the\ngravitational-wave candidates are of astrophysical origin and their location\nwas covered by optical telescopes, we derive possible constraints on the matter\nejected during the events based on the non-detection of counterparts. We find\nthat the follow-up observations during the second half of the third observing\nrun did not meet the necessary sensitivity to constrain the source properties\nof the potential gravitational-wave candidate. Consequently, we suggest that\ndifferent strategies have to be used to allow a better usage of the available\ntelescope time. We examine different choices for follow-up surveys to optimize\nsky localization coverage vs.\\ observational depth to understand the likelihood\nof counterpart detection.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the problem of dividing limited resources between a set of agents\narriving sequentially with unknown (stochastic) utilities. Our goal is to find\na fair allocation - one that is simultaneously Pareto-efficient and envy-free.\nWhen all utilities are known upfront, the above desiderata are simultaneously\nachievable (and efficiently computable) for a large class of utility functions.\nIn a sequential setting, however, no policy can guarantee these desiderata\nsimultaneously for all possible utility realizations. A natural online fair\nallocation objective is to minimize the deviation of each agent's final\nallocation from their fair allocation in hindsight. This translates into\nsimultaneous guarantees for both Pareto-efficiency and envy-freeness. However,\nthe resulting dynamic program has state-space which is exponential in the\nnumber of agents. We propose a simple policy, HopeOnline, that instead aims to\n`match' the ex-post fair allocation vector using the current available\nresources and `predicted' histogram of future utilities. We demonstrate the\neffectiveness of our policy compared to other heurstics on a dataset inspired\nby mobile food-bank allocations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Due to the large anomalous Hall effect, magnetic Weyl semimetals can support\nnonreciprocal surface plasmon polariton modes in the absence of an external\nmagnetic field. This implies that magnetic Weyl semimetals can find novel\napplication in (thermal) photonics. In this work, we consider the near-field\nradiative heat transfer between two magnetic Weyl semimetal slabs and show that\nthe heat transfer can be controlled with a relative rotation of the parallel\nslabs. Thanks to the intrinsic nonreciprocity of the surface modes, this\nso-called twisting method does not require surface structuring like periodic\ngratings. The twist-induced control of heat transfer is due to the mismatch of\nthe surface modes from the two slabs with a relative rotation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Prompt and nonprompt productions of exotic multiquark states are studied\nusing the $\\sim$10.4 fb$^{-1}$ data sample collected by the D0 experiment in\nTevatron $p\\bar{p}$ collisions at $\\sqrt{s}$ = 1.96 TeV. The recent D0 results\non the prompt and nonprompt production of the $X(3872)$ and $Z_c^+(3900)$\nstates and the $P_c$ pentaquarks at the 4450 MeV region are reported. Signals\ncorresponding to these states are found in the nonprompt production, whereas\nonly the $X(3872)$ state is seen in the prompt production. The ratio of prompt\nto nonprompt $X(3872)$ production is about three times larger in the D0\nmeasurement than that obtained by the ATLAS experiment at 8 TeV. Theoretically,\nthe production, formation, coalescence, and disassociation processes are\nexpected to be quite different for conventional mesons with a spatial size of\n(0.4-0.8) fm, compact multiquark states such as tetraquarks with a size of a\nfew fm, and spatially extended molecular states with a size of (4-10) fm. They\ncan be differently affected in prompt hadron-hadron collisions where there are\nmany additional particles emitted from the interaction point. Consequently, the\nprompt to nonprompt production ratio of spatially extended exotic states can be\nsuppressed at LHC comparing with the Tevatron conditions, because of large\ndifference in the hadron-hadron collisions particle multiplicity. The prompt\nproduction studies provide an opportunity to better understand the nature of\nexotic states.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose a modular architecture for neuromorphic closed-loop control based\non bistable relaxation oscillator modules consisting of three spiking neurons\neach. Like its biological prototypes, this basic component is robust to\nparameter variation but can be modulated by external inputs. By combining these\nmodules, we can construct a neural state machine capable of generating the\ncyclic or repetitive behaviors necessary for legged locomotion. A concrete case\nstudy for the approach is provided by a modular robot constructed from flexible\nplastic volumetric pixels, in which we produce a forward crawling gait\nentrained to the natural frequency of the robot by a minimal system of twelve\nneurons organized into four modules.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We bound the nuclear dimension of crossed products associated to some partial\nactions of finite groups or $\\mathbb{Z}$ on finite dimensional locally compact\nHausdorff second countable spaces. Our results apply to globalizable partial\nactions, finite group partial actions, minimal partial automorphisms, and\npartial automorphisms acting on zero-dimensional spaces, or a class of one\ndimensional spaces, containing $1$-dimensional CW complexes. This extends work\non global systems by Hirshberg and Wu.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although it is often used in the orthogonal frequency division multiplexing\n(OFDM) systems, application of massive multiple-input multiple-output (MIMO)\nover the orthogonal time frequency space (OTFS) modulation could suffer from\nenormous training overhead in high mobility scenarios. In this paper, we\npropose one uplink-aided high mobility downlink channel estimation scheme for\nthe massive MIMO-OTFS networks. Specifically, we firstly formulate the time\ndomain massive MIMO-OTFS signal model along the uplink and adopt the\nexpectation maximization based variational Bayesian (EM-VB) framework to\nrecover the uplink channel parameters including the angle, the delay, the\nDoppler frequency, and the channel gain for each physical scattering path.\nCorrespondingly, with the help of the fast Bayesian inference, one low complex\napproach is constructed to overcome the bottleneck of the EM-VB. Then, we fully\nexploit the angle, delay and Doppler reciprocity between the uplink and the\ndownlink and reconstruct the angles, the delays, and the Doppler frequencies\nfor the downlink massive channels at the base station. Furthermore, we examine\nthe downlink massive MIMO channel estimation over the delay-Doppler-angle\ndomain. The channel dispersion of the OTFS over the delay-Doppler domain is\ncarefully analyzed. Various numerical examples are presented to confirm the\nvalidity and robustness of the proposed scheme.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present a sparsity-aware deep network for automatic 4D\nfacial expression recognition (FER). Given 4D data, we first propose a novel\naugmentation method to combat the data limitation problem for deep learning.\nThis is achieved by projecting the input data into RGB and depth map images and\nthen iteratively performing randomized channel concatenation. Encoded in the\ngiven 3D landmarks, we also introduce an effective way to capture the facial\nmuscle movements from three orthogonal plans (TOP), the TOP-landmarks over\nmulti-views. Importantly, we then present a sparsity-aware deep network to\ncompute the sparse representations of convolutional features over multi-views.\nThis is not only effective for a higher recognition accuracy but is also\ncomputationally convenient. For training, the TOP-landmarks and sparse\nrepresentations are used to train a long short-term memory (LSTM) network. The\nrefined predictions are achieved when the learned features collaborate over\nmulti-views. Extensive experimental results achieved on the BU-4DFE dataset\nshow the significance of our method over the state-of-the-art methods by\nreaching a promising accuracy of 99.69% for 4D FER.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We report on the fabrication and characterization of nanopatterned dc SQUIDs\nwith grain boundary Josephson junctions based on heteroepitaxially grown\nYBa$_2$Cu$_3$O$_7$ (YBCO)/ SiTrO$_3$ (STO) superlattices on STO bicrystal\nsubstrates. Nanopatterning is performed by Ga focused-ion-beam milling. The\nelectric transport properties and thermal white flux noise of superlattice\nnanoSQUIDs are comparable to single layer YBCO devices on STO bicrystals.\nHowever, we find that the superlattice nanoSQUIDs have more than an order of\nmagnitude smaller low-frequency excess flux noise, with root-mean-square\nspectral density $S_\\Phi^{1/2}\\sim 5-6\\,\\mu\\Phi_0/\\sqrt{\\rm Hz}$ at 1 Hz\n($\\Phi_0$ is the magnetic flux quantum). We attribute this improvement to an\nimproved microstructure at the grain boundaries forming the Josephson junctions\nin our YBCO nanoSQUDs.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this note we prove that the Fourier-Mukai transform $\\Phi_{\\mathcal{U}}$\ninduced by the universal family of the moduli space\n$\\mathcal{M}_{\\mathbb{P}^2}(4,1,3)$ is not fully faithful.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Aims: Our objective is the optical and near-infrared spectroscopic\ncharacterisation of 2MASS J0249-0557 c, a recently discovered young planetary\nmass companion to the $\\beta$ Pictoris member 2MASS J0249-0557. Methods: Using\nthe Visible and Infrared Survey Telescope for Astronomy (VISTA) Hemisphere\nSurvey (VHS) and the Two Micron All Sky Survey (2MASS) data, we independently\nidentified the companion 2MASS J0249-0557 c. We obtained low-resolution optical\nspectroscopy of this object using the Optical System for Imaging and\nlow-intermediate-Resolution Integrated Spectroscopy (OSIRIS) spectrograph at\nthe Gran Telescopio Canarias (GTC), and near-infrared spectroscopy using the\nSon of Isaac (SofI) spectrograph on the New Technology Telescope (NTT).\nResults: We classified 2MASS J0249-0557 c with a spectral type of L2.5$\\pm$0.5\nin the optical and L3$\\pm$1 in the near-infrared. We identified spectroscopic\nindicators of youth that are compatible with the age of the $\\beta$ Pictoris\nmoving group. We also detect a strong H$\\alpha$ emission, with a pEW of\n-90$^{+20}_{-40}$A, which seems persistent in time. This indicates strong\nchromospheric activity or disk accretion. Although many M-type brown dwarfs\nhave strong H$\\alpha$ emission, this target is one of the very few L-type\nplanetary mass objects in which this strong H$\\alpha$ emission has been\ndetected. Lithium absorption at 6708 A is observed with pEW $\\lesssim$ 5A. We\nalso computed the binding energy of 2MASS J0249-0557 c and obtained an\n(absolute) upper limit of $U=(-8.8\\pm4.4) 10^{32}$ J. Conclusions: Similarly to\nother young brown dwarfs and isolated planetary mass objects, strong H$\\alpha$\nemission is also present in young planetary mass companions at ages of some\ndozen million years. We also found that 2MASS J0249-0557 c is one of the wide\nsubstellar companions with the lowest binding energy known to date.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Magnetic droplets are strongly nonlinear and localized spin-wave solitons\nthat can be formed in current-driven nanocontacts. Here, we propose a simple\nway to launch droplets in an inhomogeneous nanoscopic waveguide. We use the\ndrift motion of a droplet and show that in a system with broken translational\nsymmetry, the droplet acquires a linear momentum and propagates. We find that\nthe droplet velocity can be tuned via the strength of the break in symmetry and\nthe size of the nanocontact. In addition, we demonstrate that the launched\ndroplet can propagate up to several micrometers in a realistic system with\nreasonable damping. Finally, we demonstrate how an annihilating droplet\ndelivers its momentum to a highly nonreciprocal spin-wave burst with a tunable\nwave vector with nanometer wavelengths. Such a propagating droplet can be used\nas a moveable spin-wave source in nanoscale magnonic networks. The presented\nmethod enables full control of the spin-wave emission direction, which can\nlargely extend the freedom to design integrated magnonic circuits with a single\nspin-wave source.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We have designed and fabricated an integrated two-stage SQUID amplifier,\nrequiring only one bias line and one flux setpoint line. From the biasing\nviewpoint the two stages are connected in series while from the signal\npropagation viewpoint the stages are cascaded. A proof-of principle\ndemonstration at T = 4.2 K is presented.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We show the correlation between nanoscale structural heterogeneity and\nmechanical property and glass forming ability of Zr-based metallic glasses\n(MGs). Detailed parameters of medium range ordering (MRO) that constitutes the\nstructural heterogeneity, including the type, size, and volume fraction of MRO\ndomains determined using 4-dimensional scanning transmission electron\nmicroscopy, directly correlate with the ductility and glass forming ability of\nZr-Cu-Co-Al MGs. Mesoscale deformation simulation incorporating the\nexperimentally determined MRO confirms that the diverse types and sizes of MRO\ncan significantly influence the MGs' mechanical behavior.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Light propagation on a two-dimensional curved surface embedded in a\nthree-dimensional space has attracted increasing attention as an analog model\nof four-dimensional curved spacetime in laboratory. Despite recent developments\nin modern cosmology on the dynamics and evolution of the universe,\ninvestigation of nonlinear dynamics of light in non-Euclidean geometry is still\nscarce and remains challenging. Here, we study classical and wave chaotic\ndynamics on a family of surfaces of revolution by considering its equivalent\nconformally transformed flat billiard, with nonuniform distribution of\nrefractive index. This equivalence is established by showing how these two\nsystems have the same equations and the same dynamics. By exploring the\nPoincar\\'{e} surface of section, the Lyapunov exponent and the statistics of\neigenmodes and eigenfrequency spectrum in the transformed inhomogeneous table\nbilliard, we find that the degree of chaos is fully controlled by a single\ngeometric parameter of the curved surface. A simple interpretation of our\nfindings in transformed billiards, the \"fictitious force\", allows to extend our\nprediction to other class of curved surfaces. This powerful analogy between two\na prior unrelated systems not only brings forward a novel approach to control\nthe degree of chaos, but also provides potentialities for further studies and\napplications in various fields, such as billiards design, optical fibers, or\nlaser microcavities.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Net length is a key proxy metric for optimizing timing and power across\nvarious stages of a standard digital design flow. However, the bulk of net\nlength information is not available until cell placement, and hence it is a\nsignificant challenge to explicitly consider net length optimization in design\nstages prior to placement, such as logic synthesis. This work addresses this\nchallenge by proposing a graph attention network method with customization,\ncalled Net2, to estimate individual net length before cell placement. Its\naccuracy-oriented version Net2a achieves about 15% better accuracy than several\nprevious works in identifying both long nets and long critical paths. Its fast\nversion Net2f is more than 1000 times faster than placement while still\noutperforms previous works and other neural network techniques in terms of\nvarious accuracy metrics.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum fluctuations give rise to Casimir forces between two parallel\nconducting plates, the magnitude of which increases monotonically as the\nseparation decreases. By introducing nanoscale gratings to the surfaces, recent\nadvances have opened opportunities for controlling the Casimir force in complex\ngeometries. Here, we measure the Casimir force between two rectangular gratings\nin regimes not accessible before. Using an on-chip detection platform, we\nachieve accurate alignment between the two gratings so that they interpenetrate\nas the separation is reduced. Just before interpenetration occurs, the measured\nCasimir force is found to have a geometry dependence that is much stronger than\nprevious experiments, with deviations from the proximity force approximation\nreaching a factor of ~500. After the gratings interpenetrate each other, the\nCasimir force becomes non-zero and independent of displacement. This work shows\nthat the presence of gratings can strongly modify the Casimir force to control\nthe interaction between nanomechanical components.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we address computation of the degree $\\mathop{\\rm deg Det} A$\nof Dieudonn\\'e determinant $\\mathop{\\rm Det} A$ of \\[ A = \\sum_{k=1}^m A_k x_k\nt^{c_k}, \\] where $A_k$ are $n \\times n$ matrices over a field $\\mathbb{K}$,\n$x_k$ are noncommutative variables, $t$ is a variable commuting with $x_k$,\n$c_k$ are integers, and the degree is considered for $t$. This problem\ngeneralizes noncommutative Edmonds' problem and fundamental combinatorial\noptimization problems including the weighted linear matroid intersection\nproblem. It was shown that $\\mathop{\\rm deg Det} A$ is obtained by a discrete\nconvex optimization on a Euclidean building. We extend this framework by\nincorporating a cost scaling technique, and show that $\\mathop{\\rm deg Det} A$\ncan be computed in time polynomial of $n,m,\\log_2 C$, where $C:= \\max_k |c_k|$.\nWe give a polyhedral interpretation of $\\mathop{\\rm deg Det}$, which says that\n$\\mathop{\\rm deg Det} A$ is given by linear optimization over an integral\npolytope with respect to objective vector $c = (c_k)$. Based on it, we show\nthat our algorithm becomes a strongly polynomial one. We apply this result to\nan algebraic combinatorial optimization problem arising from a symbolic matrix\nhaving $2 \\times 2$-submatrix structure.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It is commonly known that the Fokker-Planck equation is exactly solvable only\nfor some particular systems, usually with time-independent drift coefficients.\nTo extend the class of solvable problems, we use the intertwining relations of\nSUSY Quantum Mechanics but in new - asymmetric - form. It turns out that this\nform is just useful for solution of Fokker-Planck equation. As usual,\nintertwining provides a partnership between two different systems both\ndescribed by Fokker-Planck equation. Due to the use of an asymmetric kind of\nintertwining relations with a suitable ansatz, we managed to obtain a new class\nof analytically solvable models. What is important, this approach allows us to\ndeal with the drift coefficients depending on both variables, $x,$ and $t.$ An\nillustrating example of the proposed construction is given explicitly.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this work, we extend the work of gravastars to analyze the role of\nelectromagnetic field in $f(R,T)$ gravity. We consider the irrotational\ncylindrically symmetric geometry and established the $f(R,T)$ field equations\nand conservation laws. After considering charged exterior geometry, the\nmathematical quantities for evaluating Israel junction conditions are being\ncalculated. The mass of the gravastar-like cylindrical structure is calculated\nthrough the equations of motion at the hypersurface in the presence of an\nelectromagnetic field. The behavior of electric charge on the length of the\nthin shell, energy content, and entropy of gravastar is being studied\ngraphically. We concluded that charge has an important role in the length of\nthe thin shell, energy content, and entropy of gravastar.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We introduce the complete box-ball system (cBBS), which is an integrable\ncellular automaton on 1D lattice associated with the quantum group\n$U_q(\\widehat{sl}_n)$. Compared with the conventional $(n-1)$-color BBS, it\nenjoys a remarkable simplification that scattering of solitons is totally\ndiagonal. We also submit the cBBS to randomized initial conditions and study\nits non-equilibrium behavior by thermodynamic Bethe ansatz and generalized\nhydrodynamics. Excellent agreement is demonstrated between theoretical\npredictions and numerical simulation on the density plateaux generated from\ndomain wall initial conditions including their diffusive broadening.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Solar arrays are the primary energy source of the satellite. In this paper, a\nmetamaterial absorber for solar arrays with simultaneous high optical\ntransparency and broadband microwave absorption is presented. By tailoring the\nreflection response of meta-atoms, 85% absorption performance from 6.8GHz to\n18GHz is obtained. In the meantime, by employing transparent substrates,\nincluding indium tin oxide (ITO) film and anti-reflection glass, a maximum of\n87% light transmittance is achieved. The absorptivity of the proposed\nmetamaterial absorber is simulated and measured experimentally. Light\ntransmittance and the effect of transparent metamaterial absorber on the\nconversion efficiency of the solar array have also been measured. These results\nfully demonstrate the reliability of our design for solar arrays, which also\nmeet the requirements of structural strength, atomic oxygen erosion resistance,\nweight limitation, etc.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It has been empirically observed that defense mechanisms designed to protect\nneural networks against $\\ell_\\infty$ adversarial examples offer poor\nperformance against $\\ell_2$ adversarial examples and vice versa. In this paper\nwe conduct a geometrical analysis that validates this observation. Then, we\nprovide a number of empirical insights to illustrate the effect of this\nphenomenon in practice. Then, we review some of the existing defense mechanism\nthat attempts to defend against multiple attacks by mixing defense strategies.\nThanks to our numerical experiments, we discuss the relevance of this method\nand state open questions for the adversarial examples community.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Hyperspectral unmixing is an important remote sensing task with applications\nincluding material identification and analysis. Characteristic spectral\nfeatures make many pure materials identifiable from their visible-to-infrared\nspectra, but quantifying their presence within a mixture is a challenging task\ndue to nonlinearities and factors of variation. In this paper, spectral\nvariation is considered from a physics-based approach and incorporated into an\nend-to-end spectral unmixing algorithm via differentiable programming. The\ndispersion model is introduced to simulate realistic spectral variation, and an\nefficient method to fit the parameters is presented. Then, this dispersion\nmodel is utilized as a generative model within an analysis-by-synthesis\nspectral unmixing algorithm. Further, a technique for inverse rendering using a\nconvolutional neural network to predict parameters of the generative model is\nintroduced to enhance performance and speed when training data is available.\nResults achieve state-of-the-art on both infrared and visible-to-near-infrared\n(VNIR) datasets, and show promise for the synergy between physics-based models\nand deep learning in hyperspectral unmixing in the future.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The scarcity of comprehensive up-to-date studies on evaluation metrics for\ntext summarization and the lack of consensus regarding evaluation protocols\ncontinue to inhibit progress. We address the existing shortcomings of\nsummarization evaluation methods along five dimensions: 1) we re-evaluate 14\nautomatic evaluation metrics in a comprehensive and consistent fashion using\nneural summarization model outputs along with expert and crowd-sourced human\nannotations, 2) we consistently benchmark 23 recent summarization models using\nthe aforementioned automatic evaluation metrics, 3) we assemble the largest\ncollection of summaries generated by models trained on the CNN/DailyMail news\ndataset and share it in a unified format, 4) we implement and share a toolkit\nthat provides an extensible and unified API for evaluating summarization models\nacross a broad range of automatic metrics, 5) we assemble and share the largest\nand most diverse, in terms of model types, collection of human judgments of\nmodel-generated summaries on the CNN/Daily Mail dataset annotated by both\nexpert judges and crowd-source workers. We hope that this work will help\npromote a more complete evaluation protocol for text summarization as well as\nadvance research in developing evaluation metrics that better correlate with\nhuman judgments.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the complex Gaussian as well as non-Gaussian distributed\nrandom analytical and entire functions (complex entire random field) and\ncalculate their domain of definiteness (radius of convergence) as well as some\nimportant characteristics: order and type. As a consequence we deduce that all\nthe mentioned characteristics, under very natural conditions, are deterministic\n(non-random) with probability one and we calculate them. Moreover we exhibit\nsome examples to show the exactness of the obtained results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Compared with the conventional hand-crafted approaches, the deep learning\nbased methods have achieved tremendous performance improvements by training\nexquisitely crafted fancy networks over large-scale training sets. However, do\nwe really need large-scale training set for salient object detection (SOD)? In\nthis paper, we provide a deeper insight into the interrelationship between the\nSOD performances and the training sets. To alleviate the conventional demands\nfor large-scale training data, we provide a feasible way to construct a novel\nsmall-scale training set, which only contains 4K images. Moreover, we propose a\nnovel bi-stream network to take full advantage of our proposed small training\nset, which is consisted of two feature backbones with different structures,\nachieving complementary semantical saliency fusion via the proposed gate\ncontrol unit. To our best knowledge, this is the first attempt to use a\nsmall-scale training set to outperform state-of-the-art models which are\ntrained on large-scale training sets; nevertheless, our method can still\nachieve the leading state-of-the-art performance on five benchmark datasets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Methods of topological data analysis have been successfully applied in a wide\nrange of fields to provide useful summaries of the structure of complex data\nsets in terms of topological descriptors, such as persistence diagrams. While\nthere are many powerful techniques for computing topological descriptors, the\ninverse problem, i.e., recovering the input data from topological descriptors,\nhas proved to be challenging. In this article we study in detail the\nTopological Morphology Descriptor (TMD), which assigns a persistence diagram to\nany tree embedded in Euclidean space, and a sort of stochastic inverse to the\nTMD, the Topological Neuron Synthesis (TNS) algorithm, gaining both theoretical\nand computational insights into the relation between the two. We propose a new\napproach to classify barcodes using symmetric groups, which provides a concrete\nlanguage to formulate our results. We investigate to what extent the TNS\nrecovers a geometric tree from its TMD and describe the effect of different\ntypes of noise on the process of tree generation from persistence diagrams. We\nprove moreover that the TNS algorithm is stable with respect to specific types\nof noise.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Considering the thermodynamics of bosons in a lattice described by the\nBose-Hubbard Hamiltonian, we report the occurrence of anomalous double peaks in\ntheir specific heat dependence on temperature. This feature, usually associated\nwith a high geometrical frustration, can also be a consequence of a purely\nenergetic competition. By employing self-energy functional calculations\ncombined with finite-temperature perturbation theory, we propose a mechanism\nbased on ground-state degeneracies expressed as residual entropies. A general\ndecomposition of the specific heat regarding all possible transitions between\nthe system's eingenvalues provides an insight into the nature of each maximum.\nFurthermore, we address how the model parameters modify the structure of these\npeaks based on its spectral properties and atom-atom correlation function.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We wish to automatically predict the \"speediness\" of moving objects in\nvideos---whether they move faster, at, or slower than their \"natural\" speed.\nThe core component in our approach is SpeedNet---a novel deep network trained\nto detect if a video is playing at normal rate, or if it is sped up. SpeedNet\nis trained on a large corpus of natural videos in a self-supervised manner,\nwithout requiring any manual annotations. We show how this single, binary\nclassification network can be used to detect arbitrary rates of speediness of\nobjects. We demonstrate prediction results by SpeedNet on a wide range of\nvideos containing complex natural motions, and examine the visual cues it\nutilizes for making those predictions. Importantly, we show that through\npredicting the speed of videos, the model learns a powerful and meaningful\nspace-time representation that goes beyond simple motion cues. We demonstrate\nhow those learned features can boost the performance of self-supervised action\nrecognition, and can be used for video retrieval. Furthermore, we also apply\nSpeedNet for generating time-varying, adaptive video speedups, which can allow\nviewers to watch videos faster, but with less of the jittery, unnatural motions\ntypical to videos that are sped up uniformly.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we present a topology optimization (TO) framework to enable\nautomated design of mechanical components while ensuring the result can be\nmanufactured using multi-axis machining. Although TO improves the part's\nperformance, the as-designed model is often geometrically too complex to be\nmachined and the as-manufactured model can significantly vary due to machining\nconstraints that are not accounted for during TO. In other words, many of the\noptimized design features cannot be accessed by a machine tool without\ncolliding with the part (or fixtures). The subsequent post-processing to make\nthe part machinable with the given setup requires trial-and-error without\nguarantees on preserving the optimized performance. Our proposed approach is\nbased on the well-established accessibility analysis formulation using\nconvolutions in configuration space that is extensively used in spatial\nplanning and robotics. We define an 'inaccessibility measure field' (IMF) over\nthe design domain to identify non-manufacturable features and quantify their\ncontribution to non-manufacturability. The IMF is used to penalize the\nsensitivity field of performance objectives and constraints to prevent\nformation of inaccessible regions. Unlike existing discrete formulations, our\nIMF provides a continuous spatial field that is desirable for TO convergence.\nOur approach applies to arbitrary geometric complexity of the part, tools, and\nfixtures, and is highly parallelizable on multi-core architecture. We\ndemonstrate the effectiveness of our framework on benchmark and realistic\nexamples in 2D and 3D. We also show that it is possible to directly construct\nmanufacturing plans for the optimized designs based on the accessibility\ninformation.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider the isomorphism problem for hypergraphs taking as input two\nhypergraphs over the same set of vertices $V$ and a permutation group $\\Gamma$\nover domain $V$, and asking whether there is a permutation $\\gamma \\in \\Gamma$\nthat proves the two hypergraphs to be isomorphic. We show that for input\ngroups, all of whose composition factors are isomorphic to a subgroup of the\nsymmetric group on $d$ points, this problem can be solved in time\n$(n+m)^{O((\\log d)^{c})}$ for some absolute constant $c$ where $n$ denotes the\nnumber of vertices and $m$ the number of hyperedges. In particular, this gives\nthe currently fastest isomorphism test for hypergraphs in general. The previous\nbest algorithm for this problem due to Schweitzer and Wiebking (STOC 2019) runs\nin time $n^{O(d)}m^{O(1)}$.\n  As an application of this result, we obtain, for example, an algorithm\ntesting isomorphism of graphs excluding $K_{3,h}$ ($h \\geq 3$) as a minor in\ntime $n^{O((\\log h)^{c})}$. In particular, this gives an isomorphism test for\ngraphs of Euler genus at most $g$ running in time $n^{O((\\log g)^{c})}$.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Compact DC high-voltage photo-electron guns are able to meet the\nsophisticated demands of high-current applications such as energy recovery\nlinacs. A main design parameter for such sources is the electric field\nstrength, which depends on the electrode geometry and is limited by the field\nemission threshold of the electrode material. In order to minimize the maximum\nfield strength for optimal gun operation, isogeometric analysis (IGA) can be\nused to exploit the axisymmetric geometry and describe its cross section by\nnon-uniform rational B-splines, the control points of which are the parameters\nto be optimized. This computationally efficient method is capable of describing\nCAD-generated geometries using open source software (GeoPDEs, NLopt, Octave)\nand it can simplify the step from design to simulation. We will present the\nmathematical formulation, the software workflow, and the results of an\nIGA-based shape optimization for a planned high-voltage upgrade of the DC\nphotogun teststand Photo-CATCH at TU Darmstadt. The software builds on a\ngeneral framework for isogeometric analysis and allows for easy adaptations to\nother geometries or quantities of interest. Simulations assuming a bias voltage\nof -300 kV yielded maximum field gradients of 9.06 MV/m on the surface of an\ninverted insulator electrode and below 3 MV/m on the surface of the\nphotocathode.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  To maximize the benefits of LTE cellular networks, careful and proper\nplanning is needed. This requires the use of accurate propagation models to\nquantify the path loss required for base station deployment. Deployed LTE\nnetworks in Ghana can barely meet the desired 100Mbps throughput leading to\ncustomer dissatisfaction. Network operators rely on transmission planning tools\ndesigned for generalized environments that come with already embedded\npropagation models suited to other environments. A challenge therefore to\nGhanaian transmission Network planners will be choosing an accurate and precise\npropagation model that best suits the Ghanaian environment. Given this,\nextensive LTE path loss measurements at 800MHz and 2600MHz were taken in\nselected urban and suburban environments in Ghana and compared with 6 commonly\nused propagation models. Improved versions of the Ericson, SUI, and ECC-33\ndeveloped in this study predict more precisely the path loss in Ghanaian\nenvironments compared with commonly used propagation models.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The space of the solutions of the differential equations resulting from\nconsidering matter fluids of scalar field type or perfect fluid in\nEinstein-aether theory is analyzed. The Einstein-aether theory of gravity\nconsists of General Relativity coupled to a vector field of unit time type,\ncalled the aether. In this effective theory, Lorentz invariance is violated,\nbut locality and covariance are preserved in the presence of the vector field.\nFor the mathematical formulation of the models, the 1 + 3 formalism is used\nthat allows writing field equations for spherically symmetric inhomogeneous\nmetrics as a system of partial differential equations in two variables. Using\nthe homothetic diagonal formulation, the Partial differential equations can be\nwritten as ordinary differential equations plus algebraic constraints, using\nthe fact that the metric adapts to homothetic symmetry. The resulting equations\nare very similar to those of the models with homogeneous hypersurfaces. This\nallows the qualitative study of the solutions using techniques of local theory\nof dynamic systems. Analytical results are verified numerically.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The specific heat Cp and other properties of glasses (ranging from amorphous\nsolids to disordered crystals) at low temperatures, are well known to be\nmarkedly different from those in fully-ordered crystals. For decades, this\nqualitative, and even quantitative, universal behavior of glasses has been\nthoroughly studied. However, a clear understanding of its origin and\nmicroscopic nature, needless to say a closed theory, is still lacking. To shed\nlight on this matter, I review the situation in this work, mainly by compiling\nand discussing measured low-temperature Cp data of many glasses and disordered\ncrystals, as well as highlighting a few exceptions to that \"universality rule\".\nThus, one can see that, in contrast to other low-temperature properties of\nglasses, the magnitude of the \"glassy\" Cp excess at low temperature is far from\nbeing universal. Even worse, some molecular crystals without a clear sign of\ndisorder exhibit linear coefficients in Cp larger than those found in many\namorphous solids, whereas a few of the latter show negligible values.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Physics-informed neural networks are developed to characterize the state of\ndynamical systems in a random environment. The neural network approximates the\nprobability density function (pdf) or the characteristic function (chf) of the\nstate of these systems which satisfy the Fokker-Planck equation or an\nintegro-differential equation under Gaussian and/or Poisson white noises. We\nexamine analytically and numerically the advantages and disadvantages of\nsolving each type of differential equation to characterize the state. It is\nalso demonstrated how prior information of the dynamical system can be\nexploited to design and simplify the neural network architecture. Numerical\nexamples show that: 1) the neural network solution can approximate the target\nsolution even for partial integro-differential equations and system of PDEs\ndescribing the time evolution of the pdf/chf, 2) solving either the\nFokker-Planck equation or the chf differential equation using neural networks\nyields similar pdfs of the state, and 3) the solution to these differential\nequations can be used to study the behavior of the state for different types of\nrandom forcings.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A recent palette sparsification theorem of Assadi, Chen, and Khanna [SODA'19]\nstates that in every $n$-vertex graph $G$ with maximum degree $\\Delta$,\nsampling $O(\\log{n})$ colors per each vertex independently from $\\Delta+1$\ncolors almost certainly allows for proper coloring of $G$ from the sampled\ncolors. Besides being a combinatorial statement of its own independent\ninterest, this theorem was shown to have various applications to design of\nalgorithms for $(\\Delta+1)$ coloring in different models of computation on\nmassive graphs such as streaming or sublinear-time algorithms.\n  In this paper, we further study palette sparsification problems:\n  * We prove that for $(1+\\varepsilon) \\Delta$ coloring, sampling only\n$O_{\\varepsilon}(\\sqrt{\\log{n}})$ colors per vertex is sufficient and necessary\nto obtain a proper coloring from the sampled colors.\n  * A natural family of graphs with chromatic number much smaller than\n$(\\Delta+1)$ are triangle-free graphs which are $O(\\frac{\\Delta}{\\ln{\\Delta}})$\ncolorable. We prove that sampling $O(\\Delta^{\\gamma} + \\sqrt{\\log{n}})$ colors\nper vertex is sufficient and necessary to obtain a proper\n$O_{\\gamma}(\\frac{\\Delta}{\\ln{\\Delta}})$ coloring of triangle-free graphs.\n  * We show that sampling $O_{\\varepsilon}(\\log{n})$ colors per vertex is\nsufficient for proper coloring of any graph with high probability whenever each\nvertex is sampling from a list of $(1+\\varepsilon) \\cdot deg(v)$ arbitrary\ncolors, or even only $deg(v)+1$ colors when the lists are the sets\n$\\{1,\\ldots,deg(v)+1\\}$.\n  Similar to previous work, our new palette sparsification results naturally\nlead to a host of new and/or improved algorithms for vertex coloring in\ndifferent models including streaming and sublinear-time algorithms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We investigate the stability and stabilization concepts for infinite\ndimensional time fractional differential linear systems in Hilbert spaces with\nCaputo derivatives. Firstly, based on a family of operators generated by\nstrongly continuous semigroups and on a probability density function, we\nprovide sufficient and necessary conditions for the exponential stability of\nthe considered class of systems. Then, by assuming that the system dynamics is\nsymmetric and uniformly elliptic and by using the properties of the\nMittag-Leffler function, we provide sufficient conditions that ensure strong\nstability. Finally, we characterize an explicit feedback control that\nguarantees the strong stabilization of a controlled Caputo time fractional\nlinear system through a decomposition approach. Some examples are presented\nthat illustrate the effectiveness of our results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present the derivation of conserved tensors associated to higher-order\nsymmetries in the higher derivative Maxwell Abelian gauge field theories. In\nour model, the wave operator of the higher derived theory is a $n$-th order\npolynomial expressed in terms of the usual Maxwell operator. Any symmetry of\nthe primary wave operator gives rise to a collection of independent\nhigher-order symmetries of the field equations which thus leads to a series of\nindependent conserved quantities of derived system. In particular, by the\nextension of Noether's theorem, the spacetime translation invariance of the\nMaxwell primary operator results in the series of conserved second-rank tensors\nwhich includes the standard canonical energy-momentum tensors. Although this\ncanonical energy is unbounded from below, by introducing a set of parameters,\nthe other conserved tensors in the series can be bounded which ensure the\nstability of the higher derivative dynamics. In addition, with the aid of\nauxiliary fields, we successfully obtain the relations between the roots\ndecomposition of characteristic polynomial of the wave operator and the\nconserved energy-momentum tensors within the context of another equivalent\nlower-order representation. Under the certain conditions, the 00-component of\nthe linear combination of these conserved quantities is bounded and by this\nreason, the original derived theory is considered stable. Finally, as an\ninstructive example, we discuss the third-order derived system and analyze\nextensively the stabilities in different cases of roots decomposition.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  An increasing need to analyse event-centric cross-lingual information calls\nfor innovative user interaction models that assist users in crossing the\nlanguage barrier. However, datasets that reflect user interaction traces in\ncross-lingual settings required to train and evaluate the user interaction\nmodels are mostly missing. In this paper, we present the EventKG+Click dataset\nthat aims to facilitate the creation and evaluation of such interaction models.\nEventKG+Click builds upon the event-centric EventKG knowledge graph and\nlanguage-specific information on user interactions with events, entities, and\ntheir relations derived from the Wikipedia clickstream.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A large part of the literature on learning disentangled representations\nfocuses on variational autoencoders (VAE). Recent developments demonstrate that\ndisentanglement cannot be obtained in a fully unsupervised setting without\ninductive biases on models and data. However, Khemakhem et al., AISTATS, 2020\nsuggest that employing a particular form of factorized prior, conditionally\ndependent on auxiliary variables complementing input observations, can be one\nsuch bias, resulting in an identifiable model with guarantees on\ndisentanglement. Working along this line, we propose a novel VAE-based\ngenerative model with theoretical guarantees on identifiability. We obtain our\nconditional prior over the latents by learning an optimal representation, which\nimposes an additional strength on their regularization. We also extend our\nmethod to semi-supervised settings. Experimental results indicate superior\nperformance with respect to state-of-the-art approaches, according to several\nestablished metrics proposed in the literature on disentanglement.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Although deep neural networks (DNNs) have achieved success in many\napplication fields, it is still vulnerable to imperceptible adversarial\nexamples that can lead to misclassification of DNNs easily. To overcome this\nchallenge, many defensive methods are proposed. Indeed, a powerful adversarial\nexample is a key benchmark to measure these defensive mechanisms. In this\npaper, we propose a novel method (TEAM, Taylor Expansion-Based Adversarial\nMethods) to generate more powerful adversarial examples than previous methods.\nThe main idea is to craft adversarial examples by minimizing the confidence of\nthe ground-truth class under untargeted attacks or maximizing the confidence of\nthe target class under targeted attacks. Specifically, we define the new\nobjective functions that approximate DNNs by using the second-order Taylor\nexpansion within a tiny neighborhood of the input. Then the Lagrangian\nmultiplier method is used to obtain the optimize perturbations for these\nobjective functions. To decrease the amount of computation, we further\nintroduce the Gauss-Newton (GN) method to speed it up. Finally, the\nexperimental result shows that our method can reliably produce adversarial\nexamples with 100% attack success rate (ASR) while only by smaller\nperturbations. In addition, the adversarial example generated with our method\ncan defeat defensive distillation based on gradient masking.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Motivated by a recent experiment [J. H. Han, et. al., Phys. Rev. Lett.122,\n065303 (2019)], we investigate many-body physics of interacting fermions in a\nsynthetic Hall tube, using state-of-the-art density-matrix\nrenormalization-group numerical method. Since the inter-leg couplings of this\nsynthetic Hall tube generate an interesting spin-tensor Zeeman field, exotic\ntopological and magnetic properties occur. Especially, four new quantum phases,\nsuch as nontopological spin-vector and -tensor paramagnetic insulators, and\ntopological and nontopological spin-mixed paramagnetic insulators, are\npredicted by calculating entanglement spectrum, entanglement entropies, energy\ngaps, and local magnetic orders with 3 spin-vectors and 5 spin-tensors.\nMoreover, the topologically magnetic phase transitions induced by the\ninteraction as well as the inter-leg couplings are also revealed. Our results\npave a new way to explore many-body (topological) states induced by both the\nspiral spin-vector and -tensor Zeeman fields.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A natural embodiment for multifunctional materials combining energy-storing\ncapabilities and structural mechanical properties are layered structures,\nsimilar to both laminate structural composites and electrochemical energy\nstorage devices. A structural composite with integrated electric double layer\ncapacitive storage is produced by resin infusion of a lay up including woven\nglass fabric used as mechanical reinforcement, carbon nanotube non-woven\nfabrics as electrodes/current collectors and a polymer electrolyte. The\nenergy-storing layer is patterned with holes, which after integration form\nresin plugs for mechanical interconnection between layers, similar to rivets.\nFinite element modelling is used to optimise rivet shape and areal density on\ninterlaminar shear properties. Galvanostatic charge discharge tests during\nthree point bending show no degradation of properties after large deflections\nor repeated load/unload cycling at 3.5 V.This mechanical tolerance is a\nconsequence of the elimination of metallic current collectors and the effective\nintegration of multifunctional materials, as observed by electron microscopy\nand X-ray computed tomography. In contrast, control samples with metallic\ncurrent collectors, analogous to embedded devices, rapidly degrade upon\nrepeated bending.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Engineered electromagnetic fields in plasmonic nanopores enable enhanced\noptical detection and their use in single molecule sequencing. Here, a\nplasmonic nanopore prepared in a thick nanoporous film is used to investigate\nthe interaction between the metal and a long-chain double strand DNA molecule.\nWe discuss how the matrix of nanoporous metal can interact with the molecule\nthanks to: i) transient aspecific interactions between the porous surface and\nDNA and ii) optical forces exerted by the localized field in a metallic\nnanostructure. A duration of interaction up to tens of milliseconds enables to\ncollect high signal-to-noise Raman vibrations allowing an easy label-free\nreading of information from the DNA molecule. Moreover, in order to further\nincrease the event of detection rate, we tested a polymeric porous hydrogel\nplaced beneath the solid-state membrane. This approach enables a slowdown of\nthe molecule diffusion, thus increasing the number of detected interactions by\na factor of about 20.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Continuous path keyboard input has higher inherent ambiguity than standard\ntapping, because the path trace may exhibit not only local\novershoots/undershoots (as in tapping) but also, depending on the user,\nsubstantial mid-path excursions. Deploying a robust solution thus requires a\nlarge amount of high-quality training data, which is difficult to\ncollect/annotate. In this work, we address this challenge by using GANs to\naugment our training corpus with user-realistic synthetic data. Experiments\nshow that, even though GAN-generated data does not capture all the\ncharacteristics of real user data, it still provides a substantial boost in\naccuracy at a 5:1 GAN-to-real ratio. GANs therefore inject more robustness in\nthe model through greatly increased word coverage and path diversity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This workshop paper reports on an ongoing mixed-methods study on the two\narguably most popular social network sites, Facebook and Twitter, for the same\nusers. The overarching goal of the study is to shed light into the nuances of\nsocial media selection and cross-platform use by combining survey data about\nparticipants' motivations with usage data collected via API extraction. We\ndescribe the set-up of the study and focus our discussion on the challenges and\ninsights relating to participant recruiting and data collection, handling and\ndimensionalizing usage data, and comparing usage data across sites.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study simply connected Lie groups $G$ for which the hull-kernel topology\nof the primitive ideal space $\\text{Prim}(G)$ of the group $C^*$-algebra\n$C^*(G)$ is $T_1$, that is, the finite subsets of $\\text{Prim}(G)$ are closed.\nThus, we prove that $C^*(G)$ is AF-embeddable. To this end, we show that if $G$\nis solvable and its action on the centre of $[G, G]$ has at least one imaginary\nweight, then $\\text{Prim}(G)$ has no nonempty quasi-compact open subsets. We\nprove in addition that connected locally compact groups with $T_1$ ideal spaces\nare strongly quasi-diagonal.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We completely calculate the $RO(G)$-graded coefficients of ordinary\nequivariant cohomology where $G$ is the dihedral group of order $2p$ for a\nprime $p>2$ both with constant and Burnside ring coefficients. The authors\nfirst proved it for $p=3$ and then the second author generalized it to\narbitrary $p$. These are the first such calculations for a non-abelian group.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the computation of equilibria in prediction markets in perhaps the\nmost fundamental special case with two players and three trading opportunities.\nTo do so, we show equivalence of prediction market equilibria with those of a\nsimpler signaling game with commitment introduced by Kong and Schoenebeck\n(2018). We then extend their results by giving computationally efficient\nalgorithms for additional parameter regimes. Our approach leverages a new\nconnection between prediction markets and Bayesian persuasion, which also\nreveals interesting conceptual insights.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The integrability nature of a nonparaxial nonlinear Schr\\\"odinger (NNLS)\nequation, describing the propagation of ultra-broad nonparaxial beams in a\nplanar optical waveguide, is studied by employing the Painlev\\'e singularity\nstructure analysis. Our study shows that the NNLS equation fails to satisfy the\nPainlev\\'e test. Nevertheless, we construct one bright solitary wave solution\nfor the NNLS equation by using the Hirota's direct method. Also, we numerically\ndemonstrate the stable propagation of the obtained bright solitary waves even\nin the presence of an external perturbation in a form of white noise. We then\nnumerically investigate the coherent interaction dynamics of two and three\nbright solitary waves. Our study reveals interesting energy switching among the\ncolliding solitary waves due to the nonparaxiality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a novel approach for RANSAC-based computation of the fundamental\nmatrix based on epipolar homography decomposition. We analyze the geometrical\nmeaning of the decomposition-based representation and show that it directly\ninduces a consecutive sampling strategy of two independent sets of\ncorrespondences. We show that our method guarantees a minimal number of\nevaluated hypotheses with respect to current minimal approaches, on the\ncondition that there are four correspondences on an image line. We validate our\napproach on real-world image pairs, providing fast and accurate results.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Since the classification of COVID-19 as a global pandemic, there have been\nmany attempts to treat and contain the virus. Although there is no specific\nantiviral treatment recommended for COVID-19, there are several drugs that can\npotentially help with symptoms. In this work, we mined a large twitter dataset\nof 424 million tweets of COVID-19 chatter to identify discourse around drug\nmentions. While seemingly a straightforward task, due to the informal nature of\nlanguage use in Twitter, we demonstrate the need of machine learning alongside\ntraditional automated methods to aid in this task. By applying these\ncomplementary methods, we are able to recover almost 15% additional data,\nmaking misspelling handling a needed task as a pre-processing step when dealing\nwith social media data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Solution processable two-dimensional (2D) materials have provided an ideal\nplatform for both fundamental studies and wearable electronic applications.\nApart from graphene and 2D dichalcogenides, IVA-VI monochalcogenides (MMCs) has\nemerged recently as a promising candidate for next generation electronic\napplications. However, the dispersion behavior, which is crucial for the\nquality, solubility and stability of MMCs, has been quite unexplored. Here, the\nexfoliation and the dispersion behavior of Germanium (II) monosulfide (GeS) and\nTin (II) monosulfide (SnS) nanosheets has been investigated in a wide range of\norganic solvents. Nine different organic solvents were examined and analyzed,\nconsidering the solvent polarity, surface tension, and Hansen solubility\nparameters. A significant yield of isolated GeS and SnS flakes, namely ~16.4\nand ~23.08 {\\mu}g/ml in 2-propanol and N-Methyl-2-pyrrolidone respectively were\nattained. The isolated flakes are few-layers nanosheets with lateral sizes over\na few hundreds of nanometers. The MMCs colloids exhibit long-term stability,\nsuggesting the MMCs applicability for scalable solution processable printed\nelectronic device applications.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spatially coupled serially concatenated codes (SC-SCCs) are a class of\nspatially coupled turbo-like codes, which have a close-to-capacity performance\nand low error floor. In this paper we investigate the impact of coupling\nmemory, block length, decoding window size, and number of iterations on the\nperformance, complexity, and latency of SC-SCCs. Several design tradeoffs are\npresented to see the relation between these parameters in a wide range. Also,\nour analysis provides design guidelines for SC-SCCs in different scenarios to\nmake the code design independent of block length. As a result, block length and\ncoupling memory can be exchanged flexibly without changing the latency and\ncomplexity. Also, we observe that the performance of SC-SCCs is improved with\nrespect to the uncoupled ensembles for a fixed latency and complexity.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Deep neural networks has become the first choice for researchers working on\nalgorithmic aspects of learning-to-rank. Unfortunately, it is not trivial to\nfind the optimal setting of hyper-parameters that achieves the best ranking\nperformance. As a result, it becomes more and more difficult to develop a new\nmodel and conduct a fair comparison with prior methods, especially for\nnewcomers. In this work, we propose PT-Ranking, an open-source project based on\nPyTorch for developing and evaluating learning-to-rank methods using deep\nneural networks as the basis to construct a scoring function. On one hand,\nPT-Ranking includes many representative learning-to-rank methods. Besides the\ntraditional optimization framework via empirical risk minimization, adversarial\noptimization framework is also integrated. Furthermore, PT-Ranking's modular\ndesign provides a set of building blocks that users can leverage to develop new\nranking models. On the other hand, PT-Ranking supports to compare different\nlearning-to-rank methods based on the widely used datasets (e.g., MSLR-WEB30K,\nYahoo!LETOR and Istella LETOR) in terms of different metrics, such as\nprecision, MAP, nDCG, nERR. By randomly masking the ground-truth labels with a\nspecified ratio, PT-Ranking allows to examine to what extent the ratio of\nunlabelled query-document pairs affects the performance of different\nlearning-to-rank methods. We further conducted a series of demo experiments to\nclearly show the effect of different factors on neural learning-to-rank\nmethods, such as the activation function, the number of layers and the\noptimization strategy.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Inverse Heusler alloy Mn2CoAl thin films, known as a spin-gapless\nsemiconductor (SGS), grown by three different methods: ultra-high vacuum\nmagnetron spattering, Ar-ion beam assisted sputtering, and molecular beam\nepitaxy, are investigated by comparing their electric transport properties,\nmicrostructures and atomic-level structures. Of the samples, the Mn2CoAl thin\nfilm grown by MBE consists of Mn- and Co-rich phases, the structures of which\nare determined to be the L21B-type and disordered L21-type, respectively,\naccording to anomalous XRD analysis. None of them forms the XA-type structure\nexpected for SGS Heusler alloy, although they all show SGS characteristics. We\nsuggest, to validate SGS characteristics, it is necessary to extract not only\nmagnetic and electric transport properties but also information about\nmicrostructures and atomic-scale structures of the films including defects such\nas atomic swap.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Geometric mean market makers (G3Ms), such as Uniswap and Balancer, comprise a\npopular class of automated market makers (AMMs) defined by the following rule:\nthe reserves of the AMM before and after each trade must have the same\n(weighted) geometric mean. This paper extends several results known for\nconstant-weight G3Ms to the general case of G3Ms with time-varying and\npotentially stochastic weights. These results include the returns and\nno-arbitrage prices of liquidity pool (LP) shares that investors receive for\nsupplying liquidity to G3Ms. Using these expressions, we show how to create\nG3Ms whose LP shares replicate the payoffs of financial derivatives. The\nresulting hedges are model-independent and exact for derivative contracts whose\npayoff functions satisfy an elasticity constraint. These strategies allow LP\nshares to replicate various trading strategies and financial contracts,\nincluding standard options. G3Ms are thus shown to be capable of recreating a\nvariety of active trading strategies through passive positions in LP shares.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The recently discovered two-dimensional (2D) layered semiconductor MoSi2N4\nhas aroused great interest due to its unique 2D material characteristics. In\nthis Letter, we found that differences in the structural details for MoSi2N4\nmay lead to differences in the intensity of second harmonic generation (SHG)\nand its response to strain. Accordingly, SHG can be used as a simple technique\nto identify the structural details of this system. We further calculated the\nSHG effects of MoSi2N4 derivatives and investigated their strain-regulation\nmechanism, especially including the anomalous SHG responses under strain for\nMoSi2P4 and MoGe2P4, differing from other known 2D materials. The studies may\nhave forward-looking significance for the research of nonlinear optics and\noptoelectronics in this novel 2D material system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The physics of astrophysical jets can be divided into three regimes: (i)\nengine and launch (ii) propagation and collimation, (iii) dissipation and\nparticle acceleration. Since astrophysical jets comprise a huge range of scales\nand phenomena, practicality dictates that most studies of jets intentionally or\ninadvertently focus on one of these regimes, and even therein, one body of work\nmay be simply boundary condition for another. We first discuss long standing\npersistent mysteries that pertain the physics of each of these regimes,\nindependent of the method used to study them. This discussion makes contact\nwith frontiers of plasma astrophysics more generally. While observations\ntheory, and simulations, and have long been the main tools of the trade, what\nabout laboratory experiments? Jet related experiments have offered controlled\nstudies of specific principles, physical processes, and benchmarks for\nnumerical and theoretical calculations. We discuss what has been done to date\non these fronts. Although experiments have indeed helped us to understand\ncertain processes, proof of principle concepts, and benchmarked codes, they\nhave yet to solved an astrophysical jet mystery on their own. A challenge is\nthat experimental tools used for jet-related experiments so far, are typically\nnot machines originally designed for that purpose, or designed with specific\nastrophysical mysteries in mind. This presents an opportunity for a different\nway of thinking about the development of future platforms: start with the\nastrophysical mystery and build an experiment to address it.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Self-testing is a procedure for characterizing quantum resources with the\nminimal level of trust. Up to now it has been used as a device-independent\ncertification tool for particular quantum measurements, channels, and pure\nentangled states. In this work we introduce the concept of self-testing more\ngeneral entanglement structures. More precisely, we present the first\nself-tests of an entangled subspace - the five-qubit code and the toric code.\nWe show that all quantum states maximally violating a suitably chosen Bell\ninequality must belong to the corresponding code subspace, which remarkably\nincludes also mixed states.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents a method for local motion planning in unstructured\nenvironments with static and moving obstacles, such as humans. Given a\nreference path and speed, our optimization-based receding-horizon approach\ncomputes a local trajectory that minimizes the tracking error while avoiding\nobstacles. We build on nonlinear model-predictive contouring control (MPCC) and\nextend it to incorporate a static map by computing, online, a set of convex\nregions in free space. We model moving obstacles as ellipsoids and provide a\ncorrect bound to approximate the collision region, given by the Minkowsky sum\nof an ellipse and a circle. Our framework is agnostic to the robot model. We\npresent experimental results with a mobile robot navigating in indoor\nenvironments populated with humans. Our method is executed fully onboard\nwithout the need of external support and can be applied to other robot\nmorphologies such as autonomous cars.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study electrical and thermoelectrical properties for a double quantum dot\nsystem. We consider the cases of both single-level and multilevel quantum dots\nwhatever the way they are coupled, either in a series or in a parallel\narrangement. The calculations are performed by using the nonequilibrium Green\nfunction theory. In the case of a single-level double quantum dot, the problem\nis exactly solvable whereas for a multilevel double quantum dot, an analytical\nsolution is obtained in the limit of energy-independent hopping integrals. { We\npresent a detailed discussion about} the dependences of electrical conductance,\nzero-frequency charge susceptibility and Seebeck coefficient on the gate\nvoltages applied to the dots, allowing us to derive the charge stability\ndiagram. The findings are in agreement with the experimental observations\nnotably with the occurrence of successive sign changes of the Seebeck\ncoefficient when varying the gate voltages. We interpret the results in terms\nof the bonding and antibonding states produced by the level anticrossing effect\nwhich occurs in the presence of a finite interdot coupling. We show that at\nequilibrium the boundary lines between the domains with different dot\noccupancies in the charge stability diagram, take place when the bonding and\nantibonding state levels are aligned with the chemical potentials in the leads.\nFinally the total dot occupancy is found to be considerably reduced in the case\nin parallel compared with the case in series, { whenever} the level energies in\neach dot are equal. We interpret this dip as a direct manifestation of the\ninterference effects occurring in the presence of the two electronic\ntransmission paths provided by each dot.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Motivated mainly by applications to partial differential equations with\nrandom coefficients, we introduce a new class of Monte Carlo estimators, called\nToeplitz Monte Carlo (TMC) estimator for approximating the integral of a\nmultivariate function with respect to the direct product of an identical\nunivariate probability measure. The TMC estimator generates a sequence\n$x_1,x_2,\\ldots$ of i.i.d. samples for one random variable, and then uses\n$(x_{n+s-1},x_{n+s-2}\\ldots,x_n)$ with $n=1,2,\\ldots$ as quadrature points,\nwhere $s$ denotes the dimension. Although consecutive points have some\ndependency, the concatenation of all quadrature nodes is represented by a\nToeplitz matrix, which allows for a fast matrix-vector multiplication. In this\npaper we study the variance of the TMC estimator and its dependence on the\ndimension $s$. Numerical experiments confirm the considerable efficiency\nimprovement over the standard Monte Carlo estimator for applications to partial\ndifferential equations with random coefficients, particularly when the\ndimension $s$ is large.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Attaining accurate average structural properties in a molecular simulation\nshould be considered a prerequisite if one aims to elicit meaningful insights\ninto a system's behavior. For charged surfaces in contact with an electrolyte\nsolution, an obvious example is the density profile of ions along the direction\nnormal to the surface. Here we demonstrate that, in the slab geometry typically\nused in simulations, imposing an electric displacement field $D$ determines the\nintegrated surface charge density of adsorbed ions at charged interfaces. This\nallows us to obtain macroscopic surface charge densities irrespective of the\nslab thickness used in our simulations. We also show that the commonly used\nYeh-Berkowitz method and the 'mirrored slab' geometry both impose vanishing\nintegrated surface charge density. We present results both for relatively\nsimple rocksalt (111) interfaces, and the more complex case of kaolinite's\nbasal faces in contact with aqueous electrolyte solution.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Human brain activity generates scalp potentials (electroencephalography EEG),\nintracranial potentials (iEEG), and external magnetic fields\n(magnetoencephalography MEG), all capable of being recorded, often\nsimultaneously, for use in research and clinical applications. The so-called\nforward problem is the modeling of these fields at their sensors for a given\nputative neural source configuration. While early generations modeled the head\nas a simple set of isotropic spheres, today s ubiquitous magnetic resonance\nimaging (MRI) data allows detailed descriptions of head compartments with\nassigned isotropic and anisotropic conductivities. In this paper, we present a\ncomplete pipeline, integrated into the Brainstorm software, that allows users\nto generate an individual and accurate head model from the MRI and then\ncalculate the electromagnetic forward solution using the finite element method\n(FEM). The head model generation is performed by the integration of the latest\ntools for MRI segmentation and FEM mesh generation. The final head model is\ndivided into five main compartments: white matter, grey matter, CSF, skull, and\nscalp. For the isotropic compartments, widely-used default conductivity values\nare assigned. For the brain tissues, we use the process of the effective medium\napproach (EMA) to estimate anisotropic conductivity tensors from\ndiffusion-weighted imaging (DWI) data. The FEM electromagnetic calculations are\nperformed by the DUNEuro library, integrated into Brainstorm and accessible\nwith a user-friendly graphical interface. This integrated pipeline, with full\ntutorials and example data sets freely available on the Brainstorm website,\ngives the neuroscience community easy access to advanced tools for\nelectromagnetic modeling using FEM.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider generic optimal Bayesian inference, namely, models of signal\nreconstruction where the posterior distribution and all hyperparameters are\nknown. Under a standard assumption on the concentration of the free energy, we\nshow how replica symmetry in the strong sense of concentration of all\nmultioverlaps can be established as a consequence of the Franz-de Sanctis\nidentities; the identities themselves in the current setting are obtained via a\nnovel perturbation coming from exponentially distributed \"side-observations\" of\nthe signal. Concentration of multioverlaps means that asymptotically the\nposterior distribution has a particularly simple structure encoded by a random\nprobability measure (or, in the case of binary signal, a non-random probability\nmeasure). We believe that such strong control of the model should be key in the\nstudy of inference problems with underlying sparse graphical structure (error\ncorrecting codes, block models, etc) and, in particular, in the rigorous\nderivation of replica symmetric formulas for the free energy and mutual\ninformation in this context.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We formulate the conditions for the generalized fields in the space with\nadditional commuting Weyl spinor coordinates which define the infinite\nhalf-integer spin representation of the four-dimensional Poincar\\'e group.\nUsing this formulation we develop the BRST approach and derive the Lagrangian\nfor the half-integer infinite spin fields.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Program synthesis is the task of automatically generating a program\nconsistent with a given specification. A natural way to specify programs is to\nprovide examples of desired input-output behavior, and many current program\nsynthesis approaches have achieved impressive results after training on\nrandomly generated input-output examples. However, recent work has discovered\nthat some of these approaches generalize poorly to data distributions different\nfrom that of the randomly generated examples. We show that this problem applies\nto other state-of-the-art approaches as well and that current methods to\ncounteract this problem are insufficient. We then propose a new, adversarial\napproach to control the bias of synthetic data distributions and show that it\noutperforms current approaches.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The astrophysical importance of the Kerr spacetime cannot be overstated. Of\nthe currently known exact solutions to the Einstein field equations, the Kerr\nspacetime stands out in terms of its direct applicability to describing\nastronomical black hole candidates. In counterpoint, purely mathematically,\nthere is an old classical result of differential geometry, due to Darboux, that\nall 3-manifolds can have their metrics recast into diagonal form. In the case\nof the Kerr spacetime the Boyer-Lindquist coordinates provide an explicit\nexample of a diagonal spatial 3-metric. Unfortunately, as we demonstrate\nherein, Darboux diagonalization of the spatial 3-slices of the Kerr spacetime\nis incompatible with simultaneously putting the Kerr metric into unit-lapse\nform while retaining manifest axial symmetry. This no-go theorem is somewhat\nreminiscent of the no-go theorem to the effect that the spatial 3-slices of the\nKerr spacetime cannot be chosen to be conformally flat.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Quantum confinement of graphene Dirac-like electrons in artificially crafted\nnanometer structures is a long sought goal that would provide a strategy to\nselectively tune the electronic properties of graphene, including bandgap\nopening or quantization of energy levels However, creating confining structures\nwith nanometer precision in shape, size and location, remains as an\nexperimental challenge, both for top-down and bottom-up approaches. Moreover,\nKlein tunneling, offering an escape route to graphene electrons, limits the\nefficiency of electrostatic confinement. Here, a scanning tunneling microscope\n(STM) is used to create graphene nanopatterns, with sub-nanometer precision, by\nthe collective manipulation of a large number of H atoms. Individual graphene\nnanostructures are built at selected locations, with predetermined orientations\nand shapes, and with dimensions going all the way from 2 nanometers up to 1\nmicron. The method permits to erase and rebuild the patterns at will, and it\ncan be implemented on different graphene substrates. STM experiments\ndemonstrate that such graphene nanostructures confine very efficiently graphene\nDirac quasiparticles, both in zero and one dimensional structures. In graphene\nquantum dots, perfectly defined energy band gaps up to 0.8 eV are found, that\nscale as the inverse of the dots linear dimension, as expected for massless\nDirac fermions\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The conductance quantization and shot noise below the first conductance\nplateau $G_0 = 2e^2/h$ are measured in a quantum point contact fabricated in a\nGaAs/AlGaAs tunnel-coupled double quantum well. From the conductance\nmeasurement, we observe a clear quantized conductance plateau at $0.5G_0$ and a\nsmall minimum in the transconductance at $0.7 G_0$. Spectroscopic\ntransconductance measurement reveals three maxima inside the first diamond,\nthus suggesting three minima in the dispersion relation for electric subbands.\nShot noise measurement shows that the Fano factor behavior is consistent with\nthis observation. We propose a model that relates these features to a\nwavenumber directional split subband due to a strong Rashba spin--orbit\ninteraction that is induced by the center barrier potential gradient of the\ndouble-layer sample.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Image denoising or artefact removal using deep learning is possible in the\navailability of supervised training dataset acquired in real experiments or\nsynthesized using known noise models. Neither of the conditions can be\nfulfilled for nanoscopy (super-resolution optical microscopy) images that are\ngenerated from microscopy videos through statistical analysis techniques. Due\nto several physical constraints, supervised dataset cannot be measured. Due to\nnon-linear spatio-temporal mixing of data and valuable statistics of\nfluctuations from fluorescent molecules which compete with noise statistics,\nnoise or artefact models in nanoscopy images cannot be explicitly learnt.\nTherefore, such problem poses unprecedented challenges to deep learning. Here,\nwe propose a robust and versatile simulation-supervised training approach of\ndeep learning auto-encoder architectures for the highly challenging nanoscopy\nimages of sub-cellular structures inside biological samples. We show the proof\nof concept for one nanoscopy method and investigate the scope of\ngeneralizability across structures, noise models, and nanoscopy algorithms not\nincluded during simulation-supervised training. We also investigate a variety\nof loss functions and learning models and discuss the limitation of existing\nperformance metrics for nanoscopy images. We generate valuable insights for\nthis highly challenging and unsolved problem in nanoscopy, and set the\nfoundation for application of deep learning problems in nanoscopy for life\nsciences.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The magnetar SGR J1745-2900 discovered at parsecs distance from the Milky Way\ncentral black hole, Sagittarius A*, represents the closest pulsar to a\nsupermassive black hole ever detected. Furthermore, its intriguing radio\nemission has been used to study the environment of the black hole, as well as\nto derive a precise position and proper motion for this object. The discovery\nof SGR J1745-2900 has opened interesting debates about the number, age and\nnature of pulsars expected in the Galactic center region. In this work, we\npresent extensive X-ray monitoring of the outburst of SGR J1745-2900 using the\nChandra X-ray Observatory, the only instrument with the spatial resolution to\ndistinguish the magnetar from the supermassive black hole (2.4\" angular\ndistance). It was monitored from its outburst onset in April 2013 until August\n2019, collecting more than fifty Chandra observations for a total of more than\n2.3 Ms of data. Soon after the outburst onset, the magnetar emission settled\nonto a purely thermal emission state that cooled from a temperature of about\n0.9 to 0.6 keV over 6 years. The pulsar timing properties showed at least two\nchanges in the period derivative, increasing by a factor of about 4 during the\noutburst decay. We find that the long-term properties of this outburst\nchallenge current models for the magnetar outbursts.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Ramsey Theory deals with avoiding certain patterns. When constructing an\ninstance that avoids one pattern, it is observed that other patterns emerge.\nFor example, repetition emerges when avoiding arithmetic progression (Van der\nWaerden numbers), while reflection emerges when avoiding monochromatic\nsolutions of $a+b=c$ (Schur numbers). We exploit observed patterns when\ncoloring a grid while avoiding monochromatic rectangles. Like many problems in\nRamsey Theory, this problem has a rapidly growing search space that makes\ncomputer search difficult. Steinbach et al. obtained a solution of an 18 by 18\ngrid with 4 colors by enforcing a rotation symmetry. However, that symmetry is\nnot suitable for 5 colors.\n  In this article, we will encode this problem into propositional logic and\nenforce so-called internal symmetries, which preserves satisfiability, to guide\nSAT-solving. We first observe patterns with 2 and 3 colors, among which the\n\"shift pattern\" can be easily generalized and efficiently encoded. Using this\npattern, we obtain a new solution of the 18 by 18 grid that is non-isomorphic\nto the known solution. We further analyze the pattern and obtain necessary\nconditions to further trim down the search space. We conclude with our attempts\non finding a 5-coloring of a 26 by 26 grid, as well as further open problems on\nthe shift pattern.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Many two-dimensional (2D) semiconductors represented by transition metal\ndichalcogenides have tunable optical bandgaps in the visible or near IR-range\nstanding as a promising candidate for optoelectronic devices. Despite this\npotential, however, their photoreactions are not well understood or\ncontroversial in the mechanistic details. In this work, we report a unique\nthickness-dependent photoreaction sensitivity and a switchover between two\ncompeting reaction mechanisms in atomically thin chromium thiophosphate\n(CrPS4), a two-dimensional antiferromagnetic semiconductor. CrPS4 showed a\nthreshold power density 2 orders of magnitude smaller than that for MoS2\nobeying a photothermal reaction route. In addition, reaction cross section\nquantified with Raman spectroscopy revealed distinctive power dependences in\nthe low and high power regimes. On the basis of optical in situ thermometric\nmeasurements and control experiments against O2, water, and photon energy, we\nproposed a photochemical oxidation mechanism involving singlet O2 in the low\npower regime with a photothermal route for the other. We also demonstrated a\nhighly effective encapsulation with Al2O3 as a protection against the\ndestructive photoinduced and ambient oxidations.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Data augmentation uses artificially-created examples to support supervised\nmachine learning, adding robustness to the resulting models and helping to\naccount for limited availability of labelled data. We apply and evaluate a\nsynthetic data approach to relationship classification in digital libraries,\ngenerating artificial books with relationships that are common in digital\nlibraries but not easier inferred from existing metadata. We find that for\nclassification on whole-part relationships between books, synthetic data\nimproves a deep neural network classifier by 91%. Further, we consider the\nability of synthetic data to learn a useful new text relationship class from\nfully artificial training data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Systems that require high-throughput and fault tolerance, such as key-value\nstores and databases, are looking to persistent memory to combine the\nperformance of in-memory systems with the data-consistent fault-tolerance of\nnonvolatile stores. Persistent memory devices provide fast bytea-ddressable\naccess to non-volatile memory. We analyze the design space when integrating\npersistent memory into in-memory key value stores and quantify performance\ntradeoffs between throughput, latency, and and recovery time. Previous works\nhave explored many design choices, but did not quantify the tradeoffs. We\nimplement persistent memory support in Redis and Memcached, adapting the data\nstructures of each to work in two modes: (1) with all data in persistent memory\nand (2) a hybrid mode that uses persistent memory for key/value data and\nnon-volatile memory for indexing and metadata. Our experience reveals three\nactionable design principles that hold in Redis and Memcached, despite their\nvery different implementations. We conclude that the hybrid design increases\nthroughput and decreases latency at a minor cost in recovery time and code\ncomplexity\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Object point cloud classification has drawn great research attention since\nthe release of benchmarking datasets, such as the ModelNet and the ShapeNet.\nThese benchmarks assume point clouds covering complete surfaces of object\ninstances, for which plenty of high-performing methods have been developed.\nHowever, their settings deviate from those often met in practice, where, due to\n(self-)occlusion, a point cloud covering partial surface of an object is\ncaptured from an arbitrary view. We show in this paper that performance of\nexisting point cloud classifiers drops drastically under the considered\nsingle-view, partial setting; the phenomenon is consistent with the observation\nthat semantic category of a partial object surface is less ambiguous only when\nits distribution on the whole surface is clearly specified. To this end, we\nargue for a single-view, partial setting where supervised learning of object\npose estimation should be accompanied with classification. Technically, we\npropose a baseline method of Pose-Accompanied Point cloud classification\nNetwork (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns\nintermediate pose transformations for equivariant features defined on vector\nfields, which makes the subsequent classification easier (ideally) in the\ncategory-level, canonical pose. By adapting existing ModelNet40 and ScanNet\ndatasets to the single-view, partial setting, experiment results can verify the\nnecessity of object pose estimation and superiority of our PAPNet to existing\nclassifiers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Despite breakthrough performance, modern learning models are known to be\nhighly vulnerable to small adversarial perturbations in their inputs. While a\nwide variety of recent \\emph{adversarial training} methods have been effective\nat improving robustness to perturbed inputs (robust accuracy), often this\nbenefit is accompanied by a decrease in accuracy on benign inputs (standard\naccuracy), leading to a tradeoff between often competing objectives.\nComplicating matters further, recent empirical evidence suggest that a variety\nof other factors (size and quality of training data, model size, etc.) affect\nthis tradeoff in somewhat surprising ways. In this paper we provide a precise\nand comprehensive understanding of the role of adversarial training in the\ncontext of linear regression with Gaussian features. In particular, we\ncharacterize the fundamental tradeoff between the accuracies achievable by any\nalgorithm regardless of computational power or size of the training data.\nFurthermore, we precisely characterize the standard/robust accuracy and the\ncorresponding tradeoff achieved by a contemporary mini-max adversarial training\napproach in a high-dimensional regime where the number of data points and the\nparameters of the model grow in proportion to each other. Our theory for\nadversarial training algorithms also facilitates the rigorous study of how a\nvariety of factors (size and quality of training data, model\noverparametrization etc.) affect the tradeoff between these two competing\naccuracies.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Advances of quantum control technology have led to nearly perfect\nsingle-qubit control of nuclear spins and atomic hyperfine ground states. In\ncontrast, quantum control of strong optical transitions, even for free atoms,\nare far from being perfect. Developments of such quantum control appears to be\nbottlenecked by available laser technology for generating isolated,\nsub-nanosecond optical waveforms with sub-THz programming bandwidth. Here we\npropose a simple and robust method for the desired pulse shaping, based on\nprecisely stacking multiple delayed picosecond pulses. Our proof-of-principal\ndemonstration leads to arbitrarily shapeable optical waveforms with 30~GHz\nbandwidth and $100~$ps duration. We confirm the stability of the waveforms by\ninterfacing the pulses with laser-cooled atoms, resulting in ``super-resolved''\nspectroscopic signals. This pulse shaping method may open exciting perspectives\nin quantum optics, and for fast laser cooling and atom interferometry with\nmode-locked lasers.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the formation of a room temperature magnon Bose-Einstein condensate\n(BEC) in nanoscopic systems and demonstrate that its lifetime is influenced by\nthe spatial confinement. We predict how dipolar interactions and nonlinear\nmagnon scattering assist in the generation of a metastable magnon BEC in\nenergy-quantized nanoscopic devices. We verify our prediction by a full\nnumerical simulation of the Landau-Lifshitz-Gilbert equation and demonstrate\nthe generation of magnon BEC in confined insulating magnets of yttrium iron\ngarnet. We directly map out the nonlinear magnon scattering processes behind\nthis phase transition to show how fast quantized thermalization channels allow\nthe BEC formation in confined structures. Based on our results, we discuss a\nnew mechanism to manipulate the BEC lifetime in nanoscaled systems. Our study\ngreatly extends the freedom to study the dynamics of magnon BEC in realistic\nsystems and to design integrated circuits for BEC-based applications at room\ntemperature.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  With the recent developments in artificial intelligence and machine learning,\nanomalies in network traffic can be detected using machine learning approaches.\nBefore the rise of machine learning, network anomalies which could imply an\nattack, were detected using well-crafted rules. An attacker who has knowledge\nin the field of cyber-defence could make educated guesses to sometimes\naccurately predict which particular features of network traffic data the\ncyber-defence mechanism is looking at. With this information, the attacker can\ncircumvent a rule-based cyber-defense system. However, after the advancements\nof machine learning for network anomaly, it is not easy for a human to\nunderstand how to bypass a cyber-defence system. Recently, adversarial attacks\nhave become increasingly common to defeat machine learning algorithms. In this\npaper, we show that even if we build a classifier and train it with adversarial\nexamples for network data, we can use adversarial attacks and successfully\nbreak the system. We propose a Generative Adversarial Network(GAN)based\nalgorithm to generate data to train an efficient neural network based\nclassifier, and we subsequently break the system using adversarial attacks.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A majority of methods for video frame interpolation compute bidirectional\noptical flow between adjacent frames of a video, followed by a suitable warping\nalgorithm to generate the output frames. However, approaches relying on optical\nflow often fail to model occlusions and complex non-linear motions directly\nfrom the video and introduce additional bottlenecks unsuitable for widespread\ndeployment. We address these limitations with FLAVR, a flexible and efficient\narchitecture that uses 3D space-time convolutions to enable end-to-end learning\nand inference for video frame interpolation. Our method efficiently learns to\nreason about non-linear motions, complex occlusions and temporal abstractions,\nresulting in improved performance on video interpolation, while requiring no\nadditional inputs in the form of optical flow or depth maps. Due to its\nsimplicity, FLAVR can deliver 3x faster inference speed compared to the current\nmost accurate method on multi-frame interpolation without losing interpolation\naccuracy. In addition, we evaluate FLAVR on a wide range of challenging\nsettings and consistently demonstrate superior qualitative and quantitative\nresults compared with prior methods on various popular benchmarks including\nVimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR\nfor video frame interpolation can serve as a useful self-supervised pretext\ntask for action recognition, optical flow estimation, and motion magnification.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We study the physical property of pair density wave (PDW) and fluctuating\nPDW, and use it to build an effective theory of the strongly interacting\npseudogap phase in cuprate high temperature superconductors. In Chapter2, we\nstudy how Fulde-Ferrell state, the simplest form of PDW, responds to incident\nlight. The collective motion of the condensate plays a key role; gauge\ninvariance guides us to the correct result. From Chapter 3 to Chapter 7, we\nconstruct a pseudogap metallic state by considering quantum fluctuating PDW. We\nanalyze a recent scanning tunneling microscope (STM) discovery of period-8\ndensity waves in the vortex halo of the d-wave superconductor. We put it in the\ncontext of the broader pseudogap phenomenology, and compare the experimental\nresults with various PDW-driven models and a charge density wave (CDW) driven\nmodel. We propose experiments to distinguish these different models. We present\nthe Bogoliubov bands of PDW. We discuss fluctuating PDW from the general\nperspective of fluctuating superconductivity. We discuss how Bogoliubov bands\nevolve when the superconducting order parameter is fluctuating. We compare\ntheoretical predictions with existing experiments on angle-resolved\nphotoemission spectroscopy (ARPES), infrared conductivity, diamagnetism, and\nlattice symmetry breaking. The material presented here is based on\nRef.[38,40,41].\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove that if $P$ is a set of $n$ points in $\\mathbb{C}^2$, then either\nthe points in $P$ determine $\\Omega(n^{1-\\epsilon})$ complex distances, or $P$\nis contained in a line with slope $\\pm i$. If the latter occurs then each pair\nof points in $P$ have complex distance 0.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Water resource management is of crucial societal and economic importance,\nrequiring a strong capacity for anticipating environmental change. Progress in\nphysical process knowledge, numerical methods and computational power, allows\nus to address hydro-environmental problems of growing complexity. Modeling of\nriver and marine flows is no exception. With the increase in IT resources,\nenvironmental modeling is evolving to meet the challenges of complex real-world\nproblems. This paper presents a new distributed Application Programming\nInterface (API) of the open source TELEMAC-MASCARET system to run\nhydro-environmental simulations with the help of the interoperability concept.\nUse of the API encourages and facilitates the combination of worldwide\nreference environmental libraries with the hydro-informatic system.\nConsequently, the objective of the paper is to promote the interoperability\nconcept for studies dealing with such issues as uncertainty propagation, global\nsensitivity analysis, optimization, multi-physics or multi-dimensional\ncoupling. To illustrate the capability of the API, an operational problem for\nimproving the navigation capacity of the Gironde Estuary is presented. The API\npotential is demonstrated in a re-calibration context. The API is used for a\nmultivariate sensitivity analysis to quickly reveal the most influential\nparameters which can then be optimally calibrated with the help of a data\nassimilation technique.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We prove a sharp Sobolev inequality on manifolds with nonnegative Ricci\ncurvature. Moreover, we prove a Michael-Simon inequality for submanifolds in\nmanifolds with nonnegative sectional curvature. Both inequalities depend on the\nasymptotic volume ratio of the ambient manifold.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Spectra of 76 known dwarf novae from the LAMOST survey were presented. Most\nof the objects were observed in quiescence, and about 16 systems have typical\noutburst spectra. 36 of these systems were observed by SDSS, and most of their\nspectra are similar to the SDSS spectra. 2 objects, V367 Peg and V537 Peg, are\nthe first to observe their spectra. The spectrum of V367 Peg shows a\ncontribution from a M-type donor and its spectral type could be estimated as\nM3-5 by combining its orbital period. The signature of white dwarf spectrum can\nbe seen clearly in four low-accretion-rate WZ Sge stars. Other special spectral\nfeatures worthy of further observations are also noted and discussed. We\npresent a LAMOST spectral atlas of outbursting dwarf novae. 6 objects have the\nfirst outburst spectra, and the others were also compared with the published\noutburst spectra. We argue that these data will be useful for further\ninvestigation of the accretion disc properties. The HeII $\\lambda$4686 emission\nline can be found in the outburst spectra of seven dwarf novae. These objects\nare excellent candidates for probing the spiral asymmetries of accretion disc.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The development of new assessment methods for the performance of automated\nvehicles is essential to enable the deployment of automated driving\ntechnologies, due to the complex operational domain of automated vehicles. One\ncontributing method is scenario-based assessment in which test cases are\nderived from real-world road traffic scenarios obtained from driving data.\nGiven the complexity of the reality that is being modeled in these scenarios,\nit is a challenge to define a structure for capturing these scenarios. An\nintensional definition that provides a set of characteristics that are deemed\nto be both necessary and sufficient to qualify as a scenario assures that the\nscenarios constructed are both complete and intercomparable.\n  In this article, we develop a comprehensive and operable definition of the\nnotion of scenario while considering existing definitions in the literature.\nThis is achieved by proposing an object-oriented framework in which scenarios\nand their building blocks are defined as classes of objects having attributes,\nmethods, and relationships with other objects. The object-oriented approach\npromotes clarity, modularity, reusability, and encapsulation of the objects. We\nprovide definitions and justifications of each of the terms. Furthermore, the\nframework is used to translate the terms in a coding language that is publicly\navailable.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we study integrated estimation and control of soft robots. A\nsignificant challenge in deploying closed loop controllers is reliable\nproprioception via integrated sensing in soft robots. Despite the considerable\nadvances accomplished in fabrication, modelling, and model-based control of\nsoft robots, integrated sensing and estimation is still in its infancy. To that\nend, this paper introduces a new method of estimating the degree of curvature\nof a soft robot using a stretchable sensing skin. The skin is a spray-coated\npiezoresistive sensing layer on a latex membrane. The mapping from the strain\nsignal to the degree of curvature is estimated by using a recurrent neural\nnetwork. We investigate uni-directional bending as well as bi-directional\nbending of a single-segment soft robot. Moreover, an adaptive controller is\ndeveloped to track the degree of curvature of the soft robot in the presence of\ndynamic uncertainties. Subsequently, using the integrated soft sensing skin, we\nexperimentally demonstrate successful curvature tracking control of the soft\nrobot.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper presents advancement towards making an efficient and viable wheel\nchair control system based on brain computer interface via electro-oculogram\n(EOG) signals. The system utilizes the movement of eye as the element of\npurpose for controlling the movement of the wheel chair. Skin-surface\nelectrodes are placed over skin for the purpose of acquiring the\nelectro-oculogram signal and with the help of differential amplifier the\nbio-potential is measured between the reference and the point of interest,\nafterwards these obtained low voltage pulses are amplified, then passed through\na sallen-key filter for noise removal and smoothening. These pulses are then\ncollected on to the micro-controller; based on these pulses motor is switched\nto move in either right or left direction. A prototype system was developed and\ntested. The system showed promising results. The test conducted showed 99.5%\nefficiency of movement in correct direction.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Traditional logical equivalence checking (LEC) which plays a major role in\nentire chip design process faces challenges of meeting the requirements\ndemanded by the many emerging technologies that are based on logic models\ndifferent from standard complementary metal oxide semiconductor (CMOS). In this\npaper, we propose a LEC framework to be employed in the verification process of\nbeyond-CMOS circuits. Our LEC framework is compatible with existing CMOS\ntechnologies, but, also able to check features and capabilities that are unique\nto beyond-CMOS technologies. For instance, the performance of some emerging\ntechnologies benefits from ultra-deep pipelining and verification of such\ncircuits requires new models and algorithms. We, therefore, present the\nMulti-Cycle Input Dependency (MCID) circuit model which is a novel model\nrepresentation of design to explicitly capture the dependency of primary\noutputs of the circuit on sequences of internal signals and inputs. Embedding\nthe proposed circuit model and several structural checking modules, the process\nof verification can be independent of the underlying technology and signaling.\nWe benchmark the proposed framework on post-synthesis rapid single-flux-quantum\n(RSFQ) netlists. Results show a comparative verification time of RSFQ circuit\nbenchmark including 32-bit Kogge-Stone adder, 16-bit integer divider, and\nISCAS'85 circuits with respect to ABC tool for similar CMOS circuits.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We complete the classification of globally generated vector bundles with\nsmall $c_1$ on projective spaces by treating the case $c_1 = 5$ on\n$\\mathbb{P}^n$, $n \\geq 4$ (the case $c_1 \\leq 3$ has been considered by Sierra\nand Ugaglia, while the cases $c_1 = 4$ on any projective space and $c_1 = 5$ on\n$\\mathbb{P}^2$ and $\\mathbb{P}^3$ have been studied in two of our previous\npapers). It turns out that there are very few indecomposable bundles of this\nkind: besides some obvious examples there are, roughly speaking, only the\n(first twist of the) rank 5 vector bundle which is the middle term of the monad\ndefining the Horrocks bundle of rank 3 on $\\mathbb{P}^5$, and its restriction\nto $\\mathbb{P}^4$. We recall, in an appendix, from our preprint\n[arXiv:1805.11336], the main results allowing the classification of globally\ngenerated vector bundles with $c_1 = 5$ on $\\mathbb{P}^3$. Since there are many\nsuch bundles, a large part of the main body of the paper is occupied with the\nproof of the fact that, except for the simplest ones, they do not extend to\n$\\mathbb{P}^4$ as globally generated vector bundles.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We consider Proof Complexity in light of the unusual binary encoding of\ncertain combinatorial principles. We contrast this Proof Complexity with the\nnormal unary encoding in several refutation systems, based on Resolution and\nInteger Linear Programming. Please consult the article for the full abstract.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We present a deterministic fully dynamic algorithm to answer $c$-edge\nconnectivity queries on pairs of vertices in $n^{o(1)}$ worst case update and\nquery time for any positive integer $c = (\\log n)^{o(1)}$ for a graph with $n$\nvertices. Previously, only polylogarithmic and $O(\\sqrt{n})$ worst case update\ntime fully dynamic algorithms were known for answering $1$, $2$ and $3$-edge\nconnectivity queries respectively [Henzinger and King 1995, Frederikson 1997,\nGalil and Italiano 1991].\n  Our result extends the $c$-edge connectivity vertex sparsifier [Chalermsook\net al. 2021] to a multi-level sparsification framework. As our main technical\ncontribution, we present a novel update algorithm for the multi-level $c$-edge\nconnectivity vertex sparsifier with subpolynomial update time.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we investigate the time evolution an accreting magneto-fluid\nwith finite conductivity. For the case of a thin disk, the fluid equations\nalong with Maxwell equations are derived in a simplified, one-dimensional model\nthat neglects the latitudinal dependence of the flow. The finite electrical\nconductivity is taken into account for the plasma through Ohm law; however, the\nshear viscous stress is neglected, as well as the self-gravity of the disk. In\norder to solve the integrated equations that govern the dynamical behaviour of\nthe magneto-fluid, we have used a self-similar solution. We introduce two\ndimensionless variables, $S_0$ and $\\epsilon_\\rho$, which show the magnitude of\nelectrical conductivity and the density behaviour with time, respectively. The\neffect of each of these on the structure of the disk is studied. While the\npressure is obtained simply by solving an ordinary differential equation, the\ndensity, the magnetic field, the radial velocity and the rotational velocity\nare presented analytically. The solutions show that the $S_0$ and\n$\\epsilon_\\rho$ parameters affect the radial thickness of the disk. Also, the\nradial velocity and gas pressure are more sensitive to electrical conductivity\nin the inner regions of disk. Moreover, the $\\epsilon_\\rho$ parameter has a\nmore significant effect on physical quantities in small radii.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper we propose a multi-task sequence prediction system, based on\nrecurrent neural networks and used to annotate on multiple levels an Arabizi\nTunisian corpus. The annotation performed are text classification,\ntokenization, PoS tagging and encoding of Tunisian Arabizi into CODA* Arabic\northography. The system is learned to predict all the annotation levels in\ncascade, starting from Arabizi input. We evaluate the system on the TIGER\nGerman corpus, suitably converting data to have a multi-task problem, in order\nto show the effectiveness of our neural architecture. We show also how we used\nthe system in order to annotate a Tunisian Arabizi corpus, which has been\nafterwards manually corrected and used to further evaluate sequence models on\nTunisian data. Our system is developed for the Fairseq framework, which allows\nfor a fast and easy use for any other sequence prediction problem.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The famous hoop conjecture by Thorne has been claimed to be\\ violated in\ncurved spacetimes coupled to linear electrodynamics. Hod \\cite{Hod:2018} has\nrecently refuted this claim by clarifying the status and validity of the\nconjecture appropriately interpreting the gravitational mass parameter $M$.\nHowever, it turns out that partial violations of the conjecture might seemingly\noccur also in the well known regular curved spacetimes of gravity coupled to\n\\textit{nonlinear electrodynamic}s. Using the interpretation of $M$ in a\ngeneric form accommodating nonlinear electrodynamic coupling, we illustrate a\nnovel extension that the hoop conjecture is \\textit{not} violated even in such\ncurved spacetimes. We introduce a Hod function summarizing the hoop conjecture\nand find that it surprisingly encapsulates the transition regimes between\n\"horizon and no horizon\" across the critical values determined essentially by\nthe concerned curved geometries.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Next-basket recommendation (NBR) is prevalent in e-commerce and retail\nindustry. In this scenario, a user purchases a set of items (a basket) at a\ntime. NBR performs sequential modeling and recommendation based on a sequence\nof baskets. NBR is in general more complex than the widely studied sequential\n(session-based) recommendation which recommends the next item based on a\nsequence of items. Recurrent neural network (RNN) has proved to be very\neffective for sequential modeling and thus been adapted for NBR. However, we\nargue that existing RNNs cannot directly capture item frequency information in\nthe recommendation scenario.\n  Through careful analysis of real-world datasets, we find that {\\em\npersonalized item frequency} (PIF) information (which records the number of\ntimes that each item is purchased by a user) provides two critical signals for\nNBR. But, this has been largely ignored by existing methods. Even though\nexisting methods such as RNN based methods have strong representation ability,\nour empirical results show that they fail to learn and capture PIF. As a\nresult, existing methods cannot fully exploit the critical signals contained in\nPIF. Given this inherent limitation of RNNs, we propose a simple item frequency\nbased k-nearest neighbors (kNN) method to directly utilize these critical\nsignals. We evaluate our method on four public real-world datasets. Despite its\nrelative simplicity, our method frequently outperforms the state-of-the-art NBR\nmethods -- including deep learning based methods using RNNs -- when patterns\nassociated with PIF play an important role in the data.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The underscreened Kondo lattice consisting of a single twofold degenerate\nconduction band and a CEF split 4f-electron quasi-quartet has non-conventional\nquasiparticle dispersions obtained from the constrained mean-field theory. An\nadditional genuinely heavy band is found in the main hybridization band gap of\nthe upper and lower hybridzed bands whose heavy effective mass is controlled by\nthe CEF splitting. Its presence should profoundly influence the dynamical\noptical and magnetic response functions. In the former, the onset of the\noptical conductivity is not the main hybridisation energy but the much lower\nKondo energy scale which appears in the direct transitions to the additional\nheavy band. The dynamical magnetic response is also strongly modified by the\nin-gap heavy band which can lead to unconventional resonant excitations that\nmay be interpreted as coherent CEF-Kondo lattice magnetic exciton bands. Their\ninstability at low temperature signifies the onset of induced excitonic\nmagnetism in the underscreened Kondo lattice.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In the last few decades, lots of universal relations between different global\nphysical quantities of neutron stars have been proposed to constrain the\nunobservable or hard to be observed properties of neutron stars. But few of\nthem are related to the gravitational redshift or the gravitational binding\nenergy, especially for the fast rotating neutron stars. Here we will focus on\nthe universal relations related to these two quantities. Based on 11 equations\nof state (EOSs) from the predictions of microscopic nuclear many-body theories\nfor normal or hybrid neutron stars, we proposed a set of new quasi-universal\nrelations under three rotating cases: static, general rotating and Keplerian\nrotating. These new quasi-universal relations provide a potential way to\nconstrain or estimate the unobservable or hard to be observed properties of\nneutron stars.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, a sub-optimal boundary control strategy for a free boundary\nproblem is investigated. The model is described by a non-smooth\nconvection-diffusion equation. The control problem is addressed by an\ninstantaneous strategy based on the characteristics method. The resulting time\nindependent control problems are formulated as function space optimization\nproblems with complementarity constraints. At each time step, the existence of\nan optimal solution is proved and first-order optimality conditions with\nregular Lagrange multipliers are derived for a penalized-regularized version.\nThe performance of the overall approach is illustrated by numerical examples.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  We propose an atomistic model for correlated particle dynamics in liquids and\nglasses predicting both slow stretched-exponential relaxation (SER) and fast\ncompressed-exponential relaxation (CER). The model is based on the key concept\nof elastically interacting local relaxation events. SER is related to slowing\ndown of dynamics of local relaxation events as a result of this interaction,\nwhereas CER is related to the avalanche-like dynamics in the low-temperature\nglass state. The model predicts temperature dependence of SER and CER seen\nexperimentally and recovers the simple, Debye, exponential decay at high\ntemperature. Finally, we reproduce SER to CER crossover across the glass\ntransition recently observed in metallic glasses.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $k$ be a number field and $G$ be a finite group. Let\n$\\mathfrak{F}_{k}^{G}(Q)$ be the family of number fields $K$ with absolute\ndiscriminant $D_K$ at most $Q$ such that $K/k$ is normal with Galois group\nisomorphic to $G$. If $G$ is the symmetric group $S_n$ or any transitive group\nof prime degree, then we unconditionally prove that for all\n$K\\in\\mathfrak{F}_k^G(Q)$ with at most $O_{\\epsilon}(Q^{\\epsilon})$ exceptions,\nthe $L$-functions associated to the faithful Artin representations of\n$\\mathrm{Gal}(K/k)$ have a region of holomorphy and non-vanishing commensurate\nwith predictions by the Artin conjecture and the generalized Riemann\nhypothesis. This result is a special case of a more general theorem. As\napplications, we prove that:\n  1) there exist infinitely many degree $n$ $S_n$-fields over $\\mathbb{Q}$\nwhose class group is as large as the Artin conjecture and GRH imply, settling a\nquestion of Duke;\n  2) for a prime $p$, the periodic torus orbits attached to the ideal classes\nof almost all totally real degree $p$ fields $F$ over $\\mathbb{Q}$\nequidistribute on\n$\\mathrm{PGL}_p(\\mathbb{Z})\\backslash\\mathrm{PGL}_p(\\mathbb{R})$ with respect\nto Haar measure;\n  3) for each $\\ell\\geq 2$, the $\\ell$-torsion subgroups of the ideal class\ngroups of almost all degree $p$ fields over $k$ (resp. almost all degree $n$\n$S_n$-fields over $k$) are as small as GRH implies; and\n  4) an effective variant of the Chebotarev density theorem holds for almost\nall fields in such families.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Security metrics present the security level of a system or a network in both\nqualitative and quantitative ways. In general, security metrics are used to\nassess the security level of a system and to achieve security goals. There are\na lot of security metrics for security analysis, but there is no systematic\nclassification of security metrics that are based on network reachability\ninformation. To address this, we propose a systematic classification of\nexisting security metrics based on network reachability information. Mainly, we\nclassify the security metrics into host-based and network-based metrics. The\nhost-based metrics are classified into metrics ``without probability\" and \"with\nprobability\", while the network-based metrics are classified into \"path-based\"\nand \"non-path based\". Finally, we present and describe an approach to develop\ncomposite security metrics and it's calculations using a Hierarchical Attack\nRepresentation Model (HARM) via an example network. Our novel classification of\nsecurity metrics provides a new methodology to assess the security of a system.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we study the initial-boundary value problem of a repulsion\nKeller--Segel system with a logarithmic sensitivity modeling the reinforced\nrandom walk. By establishing an energy-dissipation identity, we prove the\nexistence of classical solutions in two dimensions as well as existence of weak\nsolutions in the three-dimensional setting. Moreover, it is shown that the weak\nsolutions enjoys an eventual regularity property, i.e., it becomes regular\nafter certain time $T>0$. An exponential convergence rate toward the spatially\nhomogeneous steady states is obtained as well. We adopt a new approach\ndeveloped recently by the author \\cite{J19} to study the eventual regularity.\nThe argument is based on observation of the exponential stability of constant\nsolutions in scaling-invariant spaces together with certain dissipative\nproperty of the global solutions in the same spaces.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper proposes an efficient FDTD technique for determining\nelectromagnetic fields interacting with a finite-sized 2D and 3D periodic\nstructures. The technique combines periodic boundary conditions---modelling\nfields away from the edges of the structure---with independent simulations of\nfields near the edges of the structure. It is shown that this algorithm\nefficiently determines the size of a periodic structure necessary for fields to\nconverge to the infinitely-periodic case. Numerical validations of the\ntechnique illustrate the savings concomitant with the algorithm.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we are interested in studying two properties related to the\ndenseness of the operators which attain their numerical radius: the\nBishop-Phelps-Bollob\\'as point and operator properties for numerical radius\n(BPBpp-nu and BPBop-nu, respectively). We prove that every Banach space with\nmicro-transitive norm and second numerical index strictly positive satisfy the\nBPBpp-nu and that, if the numerical index of $X$ is 1, only one-dimensional\nspaces enjoy it. On the other hand, we show that the BPBop-nu is a very\nrestrictive property: under some general assumptions, it holds only for\none-dimensional spaces. We also consider two weaker properties, the local\nversions of BPBpp-nu and BPBop-nu, where the $\\eta$ which appears in their\ndefinition does not depend just on $\\epsilon > 0$ but also on a state $(x,\nx^*)$ or on a numerical radius one operator $T$. We address the relation\nbetween the local BPBpp-nu and the strong subdifferentiability of the norm of\nthe space $X$. We show that finite dimensional spaces and $c_0$ are examples of\nBanach spaces satisfying the local BPBpp-nu, and we exhibit an example of a\nBanach space with strongly subdifferentiable norm failing it. We finish the\npaper by showing that finite dimensional spaces satisfy the local BPBop-nu and\nthat, if $X$ has strictly positive numerical index and has the approximation\nproperty, this property is equivalent to finite dimensionality.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper proposes a robust self-triggered distributed model predictive\ncontrol (DMPC) scheme for a family of Discrete-Time linear systems with local\n(uncoupled) and global (coupled) constraints. To handle the additive\ndisturbance, tube-based method is proposed for the satisfaction of local state\nand control constraints. Meanwhile, A special form of constraints tightening is\ngiven to guarantee the global coupled constraints. The self-triggering\nmechanism help reduce the computation burden by skip insignificant iteration\nsteps, which determine a certain sampling instants to solve the DMPC\noptimization problem in parallel ways. The DMPC optimization problem is\nconstructed as a dual form, and solved distributedly based on the Alternative\nDirection Multiplier Method (ADMM) with some known simplifications. Recursive\nfeasibility and input-to-state stability of the closed-loop system are shown,\nthe performance of proposed scheme is demonstrated by a simulation example.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In quantum scattering theory, there exists a relationship between the\ndifference in the scattering phase shifts at threshold and infinity and the\nnumber of bound states, which is established by the Levinson theorem. The\npresence of Castillejo, Dalitz and Dyson poles in the scattering amplitude, as\nwell as Jaffe and Low primitives, corresponding to zeros of $D$ function on the\nunitary cut, modify the Levinson theorem. The asymptotic value of the\nscattering phase shift is shown to be determined by the number of bound states,\nthe number of Castillejo, Dalitz and Dyson poles, and the number of primitives.\nSome consequences of the generalized theorem with respect to properties of\nnucleon-nucleon interactions are discussed.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Public satellite missions are commonly bound to a trade-off between spatial\nand temporal resolution as no single sensor provides fine-grained acquisitions\nwith frequent coverage. This hinders their potential to assist vegetation\nmonitoring or humanitarian actions, which require detecting rapid and detailed\nterrestrial surface changes. In this work, we probe the potential of deep\ngenerative models to produce high-resolution optical imagery by fusing products\nwith different spatial and temporal characteristics. We introduce a dataset of\nco-registered Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat\nsurface reflectance time series and demonstrate the ability of our generative\nmodel to blend coarse daily reflectance information into low-paced finer\nacquisitions. We benchmark our proposed model against state-of-the-art\nreflectance fusion algorithms.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The boundaries of cyber-physical systems (CPS) and the Internet of Things\n(IoT) are converging together day by day to introduce a common platform on\nhybrid systems. Moreover, the combination of artificial intelligence (AI) with\nCPS creates a new dimension of technological advancement. All these\nconnectivity and dependability are creating massive space for the attackers to\nlaunch cyber attacks. To defend against these attacks, intrusion detection\nsystem (IDS) has been widely used. However, emerging CPS technologies suffer\nfrom imbalanced and missing sample data, which makes the training of IDS\ndifficult. In this paper, we propose a generative adversarial network (GAN)\nbased intrusion detection system (G-IDS), where GAN generates synthetic\nsamples, and IDS gets trained on them along with the original ones. G-IDS also\nfixes the difficulties of imbalanced or missing data problems. We model a\nnetwork security dataset for an emerging CPS using NSL KDD-99 dataset and\nevaluate our proposed model's performance using different metrics. We find that\nour proposed G-IDS model performs much better in attack detection and model\nstabilization during the training process than a standalone IDS.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Atomic collapse can be observed in graphene because of its large \"effective\"\nfine structure constant, which enables this phenomenon to occur for an impurity\ncharge as low as $Z_c\\sim 1-2$. Here, we investigate the effect of the\nsublattice symmetry on molecular collapse in two spatially separated charge\ntunable vacancies, that are located on the same (A-A type) or different (A-B\ntype) sublattices. We find that the broken sublattice symmetry: (1) does not\naffect the location of the main bonding and anti-bonding molecular collapse\npeaks, (2) but shifts the position of the satellite peaks, because they are a\nconsequence of the breaking of the local sublattice symmetry, and (3) there are\nvacancy characteristic collapse peaks that only occur for A-B type vacancies,\nwhich can be employed to distinguish them experimentally from the A-A type. As\nthe charge, energy, and separation distance increase, the additional collapse\nfeatures merge with the main molecular collapse peaks. We show that the spatial\ndistribution around the vacancy site of the collapse states allows us to\ndifferentiate the molecular from the frustrated collapse.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This paper investigates the problem of finding an optimal nonbinary index\nassignment from (M) quantization levels of a maximum entropy scalar quantizer\nto (M)-PSK symbols transmitted over a symmetric memoryless channel with\nadditive noise following decreasing probability density function (such as the\nAWGN channel) so as to minimize the channel mean-squared distortion. The\nso-called zigzag mapping under maximum-likelihood (ML) decoding was known to be\nasymptotically optimal, but the problem of determining the optimal index\nassignment for any given signal-to-noise ratio (SNR) is still open. Based on a\ngeneralized version of the Hardy-Littlewood convolution-rearrangement\ninequality, we prove that the zigzag mapping under ML decoding is optimal for\nall SNRs. It is further proved that the same optimality results also hold under\nminimum mean-square-error (MMSE) decoding. Numerical results are presented to\nverify our optimality results and to demonstrate the performance gain of the\noptimal (M)-ary index assignment over the state-of-the-art binary counterpart\nfor the case of (8)-PSK over the AWGN channel.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  This study investigates the efficiency of some select stock markets. Using an\nimproved wavelet estimator of long range dependence, we show evidence of long\nmemory in the stock returns of some emerging Asian economies. However,\ndeveloped markets of Europe and the United States did not exhibit long memory\nthereby confirming the efficiency of developed stock markets. On the other\nhand, emerging Asian markets are found to be less efficient as long memory is\nmore pronounced in these markets.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A variety of simulation methodologies have been used for modeling\nreaction-diffusion dynamics -- including approaches based on Differential\nEquations (DE), the Stochastic Simulation Algorithm (SSA), Brownian Dynamics\n(BD), Green's Function Reaction Dynamics (GFRD), and variations thereon -- each\noffering trade-offs with respect to the ranges of phenomena they can model,\ntheir computational tractability, and the difficulty of fitting them to\nexperimental measurements. Here, we develop a multiscale approach combining\nefficient SSA-like sampling suitable for well-mixed systems with aspects of the\nslower but space-aware GFRD model, assuming as with GFRD that reactions occur\nin a spatially heterogeneous environment that must be explicitly modeled. Our\nmethod extends the SSA approach in two major ways. First, we sample bimolecular\nassociation reactions following diffusive motion with a time-dependent reaction\npropensity. Second, reaction locations are sampled from within overlapping\ndiffusion spheres describing the spatial probability densities of individual\nreactants. We show the approach to provide efficient simulation of spatially\nheterogeneous biochemistry in comparison to alternative methods via application\nto a Michaelis-Menten model.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Let $G=(V,E)$ be a connected graph, where $V=\\{v_1, v_2, \\cdots, v_n\\}$ and\n$m=|E|$. $d_i$ will denote the degree of vertex $v_i$ of $G$, and\n$\\Delta=\\max_{1\\leq i \\leq n} d_i$. The ABC matrix of $G$ is defined as\n$M(G)=(m_{ij})_{n \\times n}$, where $m_{ij}=\\sqrt{(d_i + d_j -2)/(d_i d_j)}$ if\n$v_i v_j \\in E$, and 0 otherwise. The largest eigenvalue of $M(G)$ is called\nthe ABC spectral radius of $G$, denoted by $\\rho_{ABC}(G)$. Recently, this\ngraph invariant has attracted some attentions. We prove that $\\rho_{ABC}(G)\n\\leq \\sqrt{\\Delta+(2m-n+1)/\\Delta -2}$. As an application, the unique tree with\n$n \\geq 4$ vertices having second largest ABC spectral radius is determined.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Volatile species in protoplanetary discs can undergo a phase change from\nvapour to solid. These \"snow-lines\" can play vital roles in planet formation at\nall scales, from dust coagulation to planetary migration. In the outer regions\nof protoplanetary discs, the temperature profile is set by the absorption of\nreprocessed stellar light by the solids. Further, the temperature profile sets\nthe distribution of solids through sublimation and condensation at various\nsnow-lines. Hence the snow-line position depends on the temperature profile and\nvice-versa. We show that this coupling can be thermally unstable, such that a\npatch of the disc at a snow-line will produce either run-away sublimation or\ncondensation. This thermal instability arises at moderate optical depths, where\nheating by absorption of reprocessed stellar light from the disc's atmosphere\nis optically thick, yet cooling is optically thin. Since volatiles in the solid\nphase drift much faster than volatiles in the vapour phase, this thermal\ninstability results in a limit-cycle. The snow-line progressively moves in,\ncondensing volatiles, before receding, as the volatiles sublimate. Using\nnumerical simulations, we study the evolution of the CO snow-line. We find the\nCO snow-line is thermally unstable under typical disc conditions and evolves\ninwards from $\\sim50$ to $\\sim30$~AU on timescales from 1,000-10,000 years. The\nCO snow-line spends between $\\sim 10\\%-50\\%$ of its time at smaller\nseparations, where the exact value is sensitive to the total opacity and\nturbulent viscosity. The evolving snow-line also creates ring-like structures\nin the solid distribution interior to the snow-line. Multiple ring-like\nstructures created by moving snow-lines could potentially explain the\nsub-structures seen in many {\\it ALMA} images.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  In this paper, we analyze a transportation game first introduced by Fotakis,\nGourv\\`es, and Monnot in 2017, where players want to be transported to a common\ndestination as quickly as possible and, in order to achieve this goal, they\nhave to choose one of the available buses. We introduce a sequential version of\nthis game and provide bounds for the Sequential Price of Stability and the\nSequential Price of Anarchy in both metric and non-metric instances,\nconsidering three social cost functions: the total traveled distance by all\nbuses, the maximum distance traveled by a bus, and the sum of the distances\ntraveled by all players (a new social cost function that we introduce).\nFinally, we analyze the Price of Stability and the Price of Anarchy for this\nnew function in simultaneous transportation games.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  It was shown recently that standard resummation of logarithms of $Q/p_T$ can\nbe supplemented with the resummation of logarithmic contributions at large\n$x=Q^2/s$ in the case of a colourless final state such as Higgs produced via\ngluon fusion or the production of a lepton pair via Drell--Yan mechanism. Such\nan improved transverse momentum resummation takes into account soft emissions\nthat are emitted at very small angles. We report on recent phenomenological\nstudies of a combined threshold-improved $p_T$ and threshold resummation\nformalism to the Higgs boson produced at the LHC where small-$p_T$ and\nthreshold logarithms are resummed up to NNLL and NNLL* respectively. We show\nthat the effect of the modified $p_T$ resummation yields a faster perturbative\nconvergence in the small-$p_T$ region while the effect of the threshold one\nimproves the agreement with fixed-order calculations in the medium and\nlarge-$p_T$ regions.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Based on the strongly lensed gravitational waves (GWs) from compact binary\ncoalescence, we propose a new strategy to examine the fluid shear viscosity of\ndark matter (DM) in the gravitational wave domain, i.e., whether a GW\nexperiences the damping effect when it propagates in DM fluid with nonzero\nshear viscosity. By assuming that the dark matter self-scatterings are\nefficient enough for the hydrodynamic description to be valid, our results\ndemonstrate that future ground-based Einstein Telescope (ET) and satellite GW\nobservatory (Big Bang Observer; BBO) may succeed in detecting any dark matter\nself-interactions at the scales of galaxies and clusters.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A fundamental result in modern quantum chaos theory is the\nMaldacena-Shenker-Stanford upper bound on the growth of out-of-time-order\ncorrelators, whose infinite-temperature limit is related to the operator-space\nentanglement entropy of the evolution operator. Here we show that, for\none-dimensional quantum cellular automata (QCA), there exists a lower bound on\nquantum chaos quantified by such entanglement entropy. This lower bound is\nequal to twice the index of the QCA, which is a topological invariant that\nmeasures the chirality of information flow, and holds for all the R\\'enyi\nentropies, with its strongest R\\'enyi-$\\infty$ version being tight. The\nrigorous bound rules out the possibility of any sublinear entanglement growth\nbehavior, showing in particular that many-body localization is forbidden for\nunitary evolutions displaying nonzero index. Since the R\\'enyi entropy is\nmeasurable, our findings have direct experimental relevance. Our result is\nrobust against exponential tails which naturally appear in quantum dynamics\ngenerated by local Hamiltonians.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  A basic diagnostic of entanglement in mixed quantum states is known as the\npartial transpose and the corresponding entanglement measure is called the\nlogarithmic negativity. Despite the great success of logarithmic negativity in\ncharacterizing bosonic many-body systems, generalizing the partial transpose to\nfermionic systems remained a technical challenge until recently when a new\ndefinition that accounts for the Fermi statistics was put forward. In this\npaper, we propose a way to generalize the partial transpose to anyons with\n(non-Abelian) fractional statistics based on the apparent similarity between\nthe partial transpose and the braiding operation. We then define the anyonic\nversion of the logarithmic negativity and show that it satisfies the standard\nrequirements such as monotonicity to be an entanglement measure. In particular,\nwe elucidate the properties of the anyonic logarithmic negativity by computing\nit for a toy density matrix of a pair of anyons within various categories. We\nconjecture that the subspace of states with a vanishing logarithmic negativity\nis a set of measure zero in the entire space of anyonic states, in contrast\nwith the ordinary qubit systems where this subspace occupies a finite volume.\nWe prove this conjecture for multiplicity-free categories.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Functional Connectivity (FC) matrices measure the regional interactions in\nthe brain and have been widely used in neurological brain disease\nclassification. However, a FC matrix is neither a natural image which contains\nshape and texture information, nor a vector of independent features, which\nrenders the extracting of efficient features from matrices as a challenging\nproblem. A brain network, also named as connectome, could forma a graph\nstructure naturally, the nodes of which are brain regions and the edges are\ninterregional connectivity. Thus, in this study, we proposed novel graph\nconvolutional networks (GCNs) to extract efficient disease-related features\nfrom FC matrices. Considering the time-dependent nature of brain activity, we\ncomputed dynamic FC matrices with sliding-windows and implemented a graph\nconvolution based LSTM (long short term memory) layer to process dynamic\ngraphs. Moreover, the demographics of patients were also used to guide the\nclassification. However, unlike in conventional methods where personal\ninformation, i.e., gender and age were added as extra inputs, we argue that\nthis kind of approach may not actually improve the classification performance,\nfor such personal information given in dataset was usually balanced\ndistributed. In this paper, we proposed to utilize the demographic information\nas extra outputs and to share parameters among three networks predicting\nsubject status, gender and age, which serve as assistant tasks. We tested the\nperformance of the proposed architecture in ADNI II dataset to classify\nAlzheimer's disease patients from normal controls. The classification accuracy,\nsensitivity and specificity reach 0.90, 0.92 and 0.89 on ADNI II dataset.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  The robust Chinese remainder theorem (CRT) has been recently proposed for\nrobustly reconstructing a large nonnegative integer from erroneous remainders.\nIt has found many applications in signal processing, including phase unwrapping\nand frequency estimation under sub-Nyquist sampling. Motivated by the\napplications in multidimensional (MD) signal processing, in this paper we\npropose the MD-CRT and robust MD-CRT for integer vectors. Specifically, by\nrephrasing the abstract CRT for rings in number-theoretic terms, we first\nderive the MD-CRT for integer vectors with respect to a general set of integer\nmatrix moduli, which provides an algorithm to uniquely reconstruct an integer\nvector from its remainders, if it is in the fundamental parallelepiped of the\nlattice generated by a least common right multiple of all the moduli. For some\nspecial forms of moduli, we present explicit reconstruction formulae. Moreover,\nwe derive the robust MD-CRT for integer vectors when the remaining integer\nmatrices of all the moduli left divided by their greatest common left divisor\n(gcld) are pairwise commutative and coprime. Two different reconstruction\nalgorithms are proposed, and accordingly, two different conditions on the\nremainder error bound for the reconstruction robustness are obtained, which are\nrelated to a quarter of the minimum distance of the lattice generated by the\ngcld of all the moduli or the Smith normal form of the gcld.\n\n\n###\n\n", "completion": " 20\n"}
{"prompt": "  Building on recent developments in electronic-structure methods, we define\nand calculate the flexoelectric response of two-dimensional (2D) materials\nfully from first principles. In particular, we show that the open-circuit\nvoltage response to a flexural deformation is a fundamental linear-response\nproperty of the crystal that can be calculated within the primitive unit cell\nof the flat configuration. Applications to graphene, silicene, phosphorene, BN\nand transition-metal dichalcogenide monolayers reveal that two distinct\ncontributions exist, respectively of purely electronic and lattice-mediated\nnature. Within the former, we identify a key $metric$ term, consisting in the\nquadrupolar moment of the unperturbed charge density. We propose a simple\ncontinuum model to connect our findings with the available experimental\nmeasurements of the converse flexoelectric effect.\n\n\n###\n\n", "completion": " 20\n"}
