{"prompt": "  In this paper, a 3D patch-based fully dense and fully convolutional network\n(FD-FCN) is proposed for fast and accurate segmentation of subcortical\nstructures in T1-weighted magnetic resonance images. Developed from the seminal\nFCN with an end-to-end learning-based approach and constructed by newly\ndesigned dense blocks including a dense fully-connected layer, the proposed\nFD-FCN is different from other FCN-based methods and leads to an outperformance\nin the perspective of both efficiency and accuracy. Compared with the U-shaped\narchitecture, FD-FCN discards the upsampling path for model fitness. To\nalleviate the problem of parameter explosion, the inputs of dense blocks are no\nlonger directly passed to subsequent layers. This architecture of FD-FCN brings\na great reduction on both memory and time consumption in training process.\nAlthough FD-FCN is slimmed down, in model competence it gains better capability\nof dense inference than other conventional networks. This benefits from the\nconstruction of network architecture and the incorporation of redesigned dense\nblocks. The multi-scale FD-FCN models both local and global context by\nembedding intermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. In addition,\ndense blocks are rebuilt to enlarge the receptive fields without significantly\nincreasing parameters, and spectral coordinates are exploited for spatial\ncontext of the original input patch. The experiments were performed over the\nIBSR dataset, and FD-FCN produced an accurate segmentation result of overall\nDice overlap value of 89.81% for 11 brain structures in 53 seconds, with at\nleast 3.66% absolute improvement of dice accuracy than state-of-the-art 3D\nFCN-based methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We revisit spatially flat FLRW cosmology in light of recent advances in\nstandard model relativistic fluid dynamics. Modern fluid dynamics requires the\npresence of curvature-matter terms in the energy-momentum tensor for\nconsistency. These terms are linear in the Ricci scalar and tensor, such that\nthe corresponding cosmological model is referred to as ``Ricci cosmology''. No\ncosmological constant is included, there are no inflaton fields, bulk viscosity\nis assumed to be zero and we only employ standard Einstein gravity. Analytic\nsolutions to Ricci cosmology are discussed, and we find that it is possible to\nsupport an early-time inflationary universe using only well-known ingredients\nfrom the Standard Model of physics and geometric properties of space-time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using different sensors in an autonomous vehicle (AV) can provide multiple\nconstraints to optimize AV location estimation. In this paper, we present a\nlow-cost GPS-assisted LiDAR state estimation system for AVs. Firstly, we\nutilize LiDAR to obtain highly precise 3D geometry data. Next, we use an\ninertial measurement unit (IMU) to correct point cloud misalignment caused by\nincorrect place recognition. The estimated LiDAR odometry and IMU measurement\nare then jointly optimized. We use a lost-cost GPS instead of a real-time\nkinematic (RTK) module to refine the estimated LiDAR-inertial odometry. Our\nlow-cost GPS and LiDAR complement each other, and can provide highly accurate\nvehicle location information. Moreover, a low-cost GPS is much cheaper than an\nRTK module, which reduces the overall AV sensor cost. Our experimental results\ndemonstrate that our proposed GPS-aided LiDAR-inertial odometry system performs\nvery accurately. The accuracy achieved when processing a dataset collected in\nan industrial zone is approximately 0.14 m.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose conceptually novel thermopile structures for the anisotropic\nmagneto-Peltier effect (AMPE) to enhance its heating/cooling power. The\ncross-shaped thermopile, one of the representative AMPE-based thermopile\nstructures, consists of four L-shaped ferromagnetic metals arranged in a\ncross-shaped configuration, which allows concentration of the AMPE-induced\ntemperature modulation at the center of the cross structure. The AMPE-based\nthermopile does not require the use of any complicated junctions comprising\ndifferent materials, enabling the design of compact and versatile temperature\ncontrollers for nanoscale devices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  There is an extensive history of scholarship into what constitutes a \"basic\"\ncolor term, as well as a broadly attested acquisition sequence of basic color\nterms across many languages, as articulated in the seminal work of Berlin and\nKay (1969). This paper employs a set of diverse measures on massively\ncross-linguistic data to operationalize and critique the Berlin and Kay color\nterm hypotheses. Collectively, the 14 empirically-grounded computational\nlinguistic metrics we design---as well as their aggregation---correlate\nstrongly with both the Berlin and Kay basic/secondary color term partition\n(gamma=0.96) and their hypothesized universal acquisition sequence. The\nmeasures and result provide further empirical evidence from computational\nlinguistics in support of their claims, as well as additional nuance: they\nsuggest treating the partition as a spectrum instead of a dichotomy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The manufacturing cost of quantum cascade lasers is still a major bottleneck\nfor the adoption of this technology for chemical sensing. The integration of\nMid-Infrared sources on Si substrate based on CMOS technology paves the way for\nhigh-volume low-cost fabrication. Furthermore, the use of Si-based fabrication\nplatform opens the way to the co-integration of QCL Mid-InfraRed sources with\nSiGe-based waveguides, allowing realization of optical sensors fully integrated\non planar substrate. We report here the fabrication and the characterization of\nDFB-QCL sources using top metal grating approach working at 7.4 microns fully\nimplemented on our 200 mm CMOS pilot line. These QCL featured threshold current\ndensity of 2.5 kA/cm2 and a linewidth of 0.16 cm-1 with a high fabrication\nyield. This approach paves the way toward a Mid-IR spectrometer at the silicon\nchip level.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We derive in detail the equations of motion for the tensorial modes of\nprimordial metric perturbations in the Coupled-Scalar-Tachyon Bounce Universe.\nWe solve for the gravitational wave equations in the pre-bounce contraction and\nthe post-bounce expansion epochs. To match the solutions of the tensor\nperturbations, we idealise the bounce process yet retaining the essential\nphysical properties of the bounce universe. We put forward two matching\nconditions: one ensures the continuity of the gravitational wave functions and\nthe other respects the symmetric nature of the bounce dynamics. The matching\nconditions connect the two independent modes of gravitational waves solutions\nbefore and after the bounce. We further analyse the scale dependence and time\ndependence of the gravitational waves spectra in the bounce universe and\ncompare them with the primordial spectrum in the single field inflation\nscenario. We discuss the implications to early universe physics and present\nmodel independent observational signatures extracted from the bounce universe.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  H-abstraction reactions occurring on electron ionization mass spectrometry\n(EI-MS) are a long-standing and crucial topic in MS research. Yet some critical\nrelevant mechanisms are controversial and ambiguous, and information about the\nEI-induced H-abstraction reactions of halogenated organic compounds (HOCs) is\ncompletely in the dark. This study provides a systematic investigation of\nH-abstraction reactions of HOCs taking place on EI source using\n13C6-hexachlorobenzene (13C6-HCB) and 13C6-hexabromobenzene (13C6-HBB) as\nexemplary compounds by gas chromatography high resolution mass spectrometry\n(GC-HRMS). The H-abstraction efficiencies were evaluated with the MS signal\nintensity ratios of ions with H-abstraction relative to the corresponding\noriginal ions (without H-abstraction). Ion source temperatures, EI energies and\nnumbers of heavy isotope atoms (37Cl or 81Br) of isotopologues were\ninvestigated in terms of their effects on the H-abstraction efficiencies. The\nH-abstraction efficiencies of individual isotopologues generally decreased from\nthe first to the last isotopologues of respective ions, and those of individual\nions were different from each other, with the highest values of 0.017 and 0.444\nfor 13C6-HCB and 13C6-HBB, respectively. The overall H-abstraction efficiencies\ninvolving all measured ions of 13C6-HCB and 13C6-HBB were 0.004 and 0.128,\nrespectively. EI energies and emission currents could impact the H-abstraction\nefficiencies. The H-abstraction reactions were inferred to belong to\nion-molecule reactions. Some strategies were proposed for eliminating or\nalleviating the interference triggered by the H-abstraction reactions on EI-MS\nin identification of halogenated organic pollutants (HOPs). Our findings\nprovide a better understanding for the EI-induced H-abstraction reactions of\nHOCs, and may benefit identification of HOPs in environmental analysis,\nespecially for novel HOPs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents a literature review on recent applications and design\naspects of the intelligent reflecting surface (IRS) in the future wireless\nnetworks. Conventionally, the network optimization has been limited to\ntransmission control at two endpoints, i.e., end users and network controller.\nThe fading wireless channel is uncontrollable and becomes one of the main\nlimiting factors for performance improvement. The IRS is composed of a large\narray of scattering elements, which can be individually configured to generate\nadditional phase shifts to the signal reflections. Hence, it can actively\ncontrol the signal propagation properties in favor of signal reception, and\nthus realize the notion of a smart radio environment. As such, the IRS's phase\ncontrol, combined with the conventional transmission control, can potentially\nbring performance gain compared to wireless networks without IRS. In this\nsurvey, we first introduce basic concepts of the IRS and the realizations of\nits reconfigurability. Then, we focus on applications of the IRS in wireless\ncommunications. We overview different performance metrics and analytical\napproaches to characterize the performance improvement of IRS-assisted wireless\nnetworks. To exploit the performance gain, we discuss the joint optimization of\nthe IRS's phase control and the transceivers' transmission control in different\nnetwork design problems, e.g.,~rate maximization and power minimization\nproblems. Furthermore, we extend the discussion of IRS-assisted wireless\nnetworks to some emerging use cases. Finally, we highlight important practical\nchallenges and future research directions for realizing IRS-assisted wireless\nnetworks in beyond 5G communications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper is concerned with the Cauchy problem of the modified Kawahara\nequation (posed on $\\mathbb T$), which is well-known as a model of\ncapillary-gravity waves in an infinitely long canal over a flat bottom in a\nlong wave regime \\cite{Hasimoto1970}. We show in this paper some well-posedness\nresults, mainly the \\emph{global well-posedness} in $L^2(\\mathbb T)$. The proof\nbasically relies on the idea introduced in Takaoka-Tsutsumi's works\n\\cite{TT2004, NTT2010}, which weakens the non-trivial resonance in the cubic\ninteractions (a kind of smoothing effect) for the local result, and the global\nwell-posedness result immediately follows from $L^2$ conservation law. An\nimmediate application of Takaoka-Tsutsumi's idea is available only in\n$H^s(\\mathbb T)$, $s > 0$, due to the lack of $L^4$-Strichartz estimate for\narbitrary $L^2$ data, a slight modification, thus, is needed to attain the\nlocal well-posedness in $L^2(\\mathbb T)$. This is the first low regularity\n(global) well-posedness result for the periodic modified Kwahara equation, as\nfar as we know. A direct interpolation argument ensures the \\emph{unconditional\nuniqueness} in $H^s(\\mathbb T)$, $s > \\frac12$, and as a byproduct, we show the\nweak ill-posedness below $H^{\\frac12}(\\mathbb T)$, in the sense that the flow\nmap fails to be uniformly continuous.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we explore techniques centered around periodic sampling of\nmodel weights that provide convergence improvements on gradient update methods\n(vanilla \\acs{SGD}, Momentum, Adam) for a variety of vision problems\n(classification, detection, segmentation). Importantly, our algorithms provide\nbetter, faster and more robust convergence and training performance with only a\nslight increase in computation time. Our techniques are independent of the\nneural network model, gradient optimization methods or existing optimal\ntraining policies and converge in a less volatile fashion with performance\nimprovements that are approximately monotonic. We conduct a variety of\nexperiments to quantify these improvements and identify scenarios where these\ntechniques could be more useful.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Consider $F$ an element of the second Wiener chaos with variance one. In full\ngenerality, we show that, for every integer $p\\ge 1$, there exists $\\eta_p>0$\nsuch that if $\\kappa_4(F)<\\eta_p$ then the Malliavin derivative of $F$ admits a\nnegative moment of order $p$. This entails that any sequence of random\nvariables in the second Wiener chaos converging in distribution to a\nnon--degenerated Gaussian is getting more regular as its distribution is\ngetting close to the normal law. This substantially generalizes some recent\nfindings contained in \\cite{hu2014convergence,hu2015density,nourdin2016fisher}\nwhere analogous statements were given with additional assumptions which we are\nable to remove here. Moreover, we provide a multivariate version of this\nTheorem.\n  Our main contribution concerns the case of the third Wiener chaos which is\nnotoriously more delicate as one cannot anymore decompose the random variables\ninto a linear combination of i.i.d. random variables. We still prove that the\nsame phenomenon of regularization along central convergence occurs.\nUnfortunately, we are not able to provide a statement as strong as the previous\none, but we can show that the usual non--degeneracy estimates of the Malliavin\nderivative given by the Carbery-Wright inequality can be improved by a factor\nthree. Our proof introduces new techniques such that a specific Malliavin\ngradient enabling us to encode the distribution of the Malliavin derivative by\nthe spectrum of some Gaussian matrix. This allows us to revisit the fourth\nmoment phenomenon in terms of the behavior of its spectral radius.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Three-dimensional control is considered in the flow past a backward-facing\nstep (BFS). The BFS flow at Reynolds number $Re=500$ (defined with the step\nheight and the maximum inlet velocity) is two-dimensional and linearly stable\nbut increasingly receptive to disturbances, with a potential for amplification\nas the recirculation length increases. We compute optimal spanwise-periodic\ncontrol (steady wall blowing/suction or wall deformation) for decreasing the\nrecirculation length, based on a second-order sensitivity analysis. Results\nshow that wall-normal velocity control is always more efficient than\nwall-tangential control. The most efficient spanwise wavelength for the optimal\ncontrol depends on the location: $\\beta=0.6$ on the upper wall and $\\beta=1$ on\nthe upstream part of the lower wall. The linear amplification of the optimal\ncontrol resembles the maximum linear gain, which confirms the link between\nrecirculation length and amplification potential in this flow. Sensitivity\npredictions for blowing/suction amplitudes up to $O(10^{-3})$ and wall\ndeformation amplitudes up to $O(10^{-2})$ are in good agreement with\nthree-dimensional direct numerical simulations.\n  For larger wall deformation amplitudes, the flow becomes unsteady. This study\nillustrates how the concept of second-order sensitivity and the associated\noptimization method allow for a systematic exploration of the best candidates\nfor spanwise-periodic control.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Diffusion of heat in metals is a fundamental process which is crucial for a\nvariety of applications of metal nanostructures. Surprisingly, however, {\\em\nultrafast} heat diffusion received only limited attention so far. Here, we show\nthat heat diffusion can be made faster than $e-ph$ energy transfer rate, in\nwhich case, it dominates the spatio-temporal dynamics of the temperature. This\nenables the metals to overcome the conventional limitations of the nonlinear\noptical response of materials - it can be simultaneously fast and strong. As a\nspecific example, we identify the underlying (femtosecond and few picosecond)\ntime scales responsible for the generation and erasure of optically-induced\ntransient Bragg gratings in thin metal films. Further, we show that heat\ndiffusion gives rise to a significant nonlocal thermo-optic nonlinearity - it\naffects also the nonlinear optical response such that the overall change of the\npermittivity (hence, reflectivity of the transient grating) has a significant\ndependence also on the illumination period rather than only on the illumination\nintensity.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Radiochemical experiments for low-energy solar-neutrino detection have been\nmaking headlines by exploiting the isotopes \\iso{Cl}{37} and \\iso{Ga}{71}. Such\na very low-threshold measurement of this type can also be performed using\n\\iso{Tl}{205}, which has been considered for decades for this purpose. A unique\nfeature of this detector nucleus is the integration is the solar-neutrino flux\nover millions of years owing to its long-living daughter \\iso{Pb}{205}. In this\nstudy we have calculated for the first time the cross section for the\ncharged-current solar-neutrino scattering off \\iso{Tl}{205}. Taking into\naccount the solar-model-predicted neutrino fluxes and the electron-neutrino\nsurvival probabilities, a solar-neutrino capture rate of 62.2 $\\pm 8.6$ SNU is\ndetermined, a value significantly smaller than in previous estimates.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A main theoretical interest in biology and physics is to identify the\nnonlinear dynamical system (DS) that generated observed time series. Recurrent\nNeural Networks (RNNs) are, in principle, powerful enough to approximate any\nunderlying DS, but in their vanilla form suffer from the exploding vs.\nvanishing gradients problem. Previous attempts to alleviate this problem\nresulted either in more complicated, mathematically less tractable RNN\narchitectures, or strongly limited the dynamical expressiveness of the RNN.\nHere we address this issue by suggesting a simple regularization scheme for\nvanilla RNNs with ReLU activation which enables them to solve long-range\ndependency problems and express slow time scales, while retaining a simple\nmathematical structure which makes their DS properties partly analytically\naccessible. We prove two theorems that establish a tight connection between the\nregularized RNN dynamics and its gradients, illustrate on DS benchmarks that\nour regularization approach strongly eases the reconstruction of DS which\nharbor widely differing time scales, and show that our method is also en par\nwith other long-range architectures like LSTMs on several tasks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article we consider a certain sub class of Integer Equal Flow\nproblem, which are known NP hard [8]. Currently there exist no direct solutions\nfor the same. It is a common problem in various inventory management systems.\nHere we discuss a local minima solution which uses projection of the convex\nspaces to resolve the equal flows and turn the problem into a known linear\ninteger programming or constraint satisfaction problem which have reasonable\nknown solutions and can be effectively solved using simplex or other standard\noptimization strategies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper considers estimation of large dynamic factor models with common\nand idiosyncratic trends by means of the Expectation Maximization algorithm,\nimplemented jointly with the Kalman smoother. We show that, as the\ncross-sectional dimension $n$ and the sample size $T$ diverge to infinity, the\ncommon component for a given unit estimated at a given point in time is\n$\\min(\\sqrt n,\\sqrt T)$-consistent. The case of local levels and/or local\nlinear trends trends is also considered. By means of a MonteCarlo simulation\nexercise, we compare our approach with estimators based on principal component\nanalysis.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This work focuses on the understanding of the production mechanism of\nnegative-ions on surface in low pressure plasmas of H2/D2. The negative ions\nare produced on a HOPG sample (Highly Oriented Pyrolitic Graphite) negatively\nbiased with respect to plasma potential. The negative ions created under the\npositive ion bombardment are accelerated towards the plasma, self-extracted and\ndetected according to their energy and mass by a mass spectrometer placed in\nfront of the sample. The shape of the measured Negative-Ion Energy Distribution\nFunction (NIEDF) strongly differs from the NIEDF of the ions emitted by the\nsample because of the limited acceptance angle of the mass spectrometer. To get\ninformation on the production mechanisms, we propose a method to obtain the\ndistribution functions in energy and angle (NIEADFs) of the negative-ions\nemitted by the sample. It is based on an a priori determination of the NIEADF\nand on an a posteriori validation of the choice by comparison of the modelled\nand experimental NIEDFs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The steady flow of spherical particles in a rectangular bin is studied using\nthe Discrete Element Method (DEM) for different flow rates of the particles\nfrom the bin, in the slow flow regime. The flow has two non-zero velocity\ncomponents and is more complex than the widely studied unidirectional shear\nflows. The objective of the study is to characterize, in detail, the local\nrheology of the flowing material. The flow is shown to be nearly constant\ndensity, with a symmetric stress tensor and the principal directions of the\nstress and rate of strain tensors nearly colinear. The local rheology is\nanalyzed using a coordinate transformation which enables direct computation of\nthe viscosity and components of the pressure assuming the granular material to\nbe a generalized Newtonian fluid. The scaled viscosity, fluctuation velocity\nand volume fraction are shown to follow power law relations with the inertial\nnumber, a scaled shear rate, and data for different flow rates collapse to a\nsingle curve in each case. Results for flow of the particles on an inclined\nsurface, presented for comparison, are similar to those for the bin flow, but\nwith a lower viscosity and a higher solid fraction due to layering of the\nparticles. The in plane normal stresses are nearly equal and slightly larger\nthan the third component. All three normal stresses correlate well with the\ncorresponding fluctuation velocity components. Based on the empirical\ncorrelations obtained, a continuum model is presented for computation of\ngranular flows.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we consider open-boundary conditions at high temperatures, as\nthey can potentially be of help to measure the topological susceptibility. In\nparticular, we measure the extent of the boundary effects at $T=1.5T_c$ and\n$T=2.7T_c$. In the first case, it is larger than at $T=0$ while we find it to\nbe smaller in the second case. The length of this \"boundary zone\" is controlled\nby the screening masses. We use this fact to measure the scalar and\npseudo-scalar screening masses at these two temperatures. We observe a mass gap\nat $T=1.5T_c$ but not at $T=2.7T_c$. Finally, we use our pseudo-scalar channel\nanalysis to estimate the topological susceptibility. The results at $T=1.5T_c$\nare in good agreement with the literature. At $T=2.7T_c$, they appear to suffer\nfrom topological freezing, impeding us from providing a precise determination\nof the topological susceptibility. It still provides us with a lower bound,\nwhich is already in mild tension with some of the existing results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We derive the recent star formation histories of 23 active dwarf galaxies\nusing HST observations from the Legacy Extragalactic UV Survey (LEGUS). We\napply a color-magnitude diagram fitting technique using two independent sets of\nstellar models, PARSEC-COLIBRI and MIST. Despite the non-negligible recent\nactivity, none of the 23 star forming dwarfs show enhancements in the last 100\nMyr larger than three times the 100-Myr-average. The unweighted mean of the\nindividual SFHs in the last 100 Myr is also consistent with a rather constant\nactivity, irrespective of the atomic gas fraction. We confirm previous results\nthat for dwarf galaxies the CMD-based average star formation rates (SFRs) are\ngenerally higher than the FUV-based SFR. For half of the sample, the\n60-Myr-average CMD-based SFR is more than two times the FUV SFR. In contrast,\nwe find remarkable agreement between the 10-Myr-average CMD-based SFR and the\nH${\\alpha}$-based SFR. Finally, using core helium burning stars of intermediate\nmass we study the pattern of star formation spatial progression over the past\n60 Myr, and speculate on the possible triggers and connections of the star\nformation activity with the environment in which these galaxies live.\nApproximately half of our galaxies show spatial progression of star formation\nin the last 60 Myr, and/or very recent diffuse and off-center activity compared\nto RGB stars.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Probability generating functions for first passage times of Markov chains are\nfound using the method of collective marks. A system of equations is found\nwhich can be used to obtain moments of the first passage times.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Slime mould plasmodia can adjust their behaviour in response to chemical\ntrails left by themselves and other Physarum plasmodia. This simple feedback\nprocess increases their foraging efficiency. We still do not know whether other\nfactors influence plasmodium behaviour in realistic competition settings. Here\nwe designed a competition experiment where two plasmodia had to find one food\nsource in a common environment. As previously shown, the time it took plasmodia\nto find food depended on their hunger motivation. However, the time it took a\nplasmodium to start looking for food depended on its motivation and the\nmotivation of its competitor. Plasmodia always initiated foraging quicker if\nthey were in the presence of a competitor and the quickest if they were hungry\nand in the presence of a satiated competitor. The time it took to arrive to the\nfood was not influenced by whether they were alone or with a competitor.\nUltimately, this complex competition response benefited the hungry plasmodia as\nthey had a 4:1 chance of finding the food first. The sensory ecology of\nPhysarum polycephalum is more complex than previously thought and yields\ncomplex behaviour in a simple organism.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article we investigate in detail the possibility of accounting for\nthe $b\\to s\\ell^+\\ell^-$ and $(g-2)_\\mu$ anomalies via loop contributions\ninvolving with new scalars and fermions. For this purpose, we first write down\nthe most general Lagrangian which can generate the desired effects and then\ncalculate the generic expressions for all relevant $b\\to s$ Wilson\ncoefficients. Here we extend previous analysis by allowing that the new\nparticles can also couple to right-handed Standard Model (SM) fermions as\npreferred by recent $b\\to s\\ell^+\\ell^-$ data and the anomalous magnetic moment\nof the muon. In the second part of this article we illustrate this generic\napproach for a UV complete model in which we supplement the Standard Model by a\n$4^{\\rm th}$ generation of vector-like fermions and a real scalar field. This\nmodel allows one to coherently address the observed anomalies in $b\\to\ns\\ell^+\\ell^-$ transitions and in $a_\\mu$ without violating the bounds from\nother observables (in particular $B_s -\\bar B_s$ mixing) or LHC searches. In\nfact, we find that our global fit to this model, after the recent experimental\nupdates, is very good and prefers couplings to right-handed SM fermions,\nshowing the importance of our generic setup and calculation performed in the\nfirst part of the article.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study combinatorial auctions with interdependent valuations. In such\nsettings, each agent $i$ has a private signal $s_i$ that captures her private\ninformation, and the valuation function of every agent depends on the entire\nsignal profile, ${\\bf s}=(s_1,\\ldots,s_n)$. The literature in economics shows\nthat the interdependent model gives rise to strong impossibility results, and\nidentifies assumptions under which optimal solutions can be attained. The\ncomputer science literature provides approximation results for simple\nsingle-parameter settings (mostly single item auctions, or matroid feasibility\nconstraints). Both bodies of literature focus largely on valuations satisfying\na technical condition termed {\\em single crossing} (or variants thereof).\n  We consider the class of {\\em submodular over signals} (SOS) valuations\n(without imposing any single-crossing type assumption), and provide the first\nwelfare approximation guarantees for multi-dimensional combinatorial auctions,\nachieved by universally ex-post IC-IR mechanisms. Our main results are: $(i)$\n4-approximation for any single-parameter downward-closed setting with\nsingle-dimensional signals and SOS valuations; $(ii)$ 4-approximation for any\ncombinatorial auction with multi-dimensional signals and {\\em separable}-SOS\nvaluations; and $(iii)$ $(k+3)$- and $(2\\log(k)+4)$-approximation for any\ncombinatorial auction with single-dimensional signals, with $k$-sized signal\nspace, for SOS and strong-SOS valuations, respectively. All of our results\nextend to a parameterized version of SOS, $d$-SOS, while losing a factor that\ndepends on $d$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With growing popularity, LoRa networks are pivotally enabling Long Range\nconnectivity to low-cost and power-constrained user equipments (UEs). Due to\nits wide coverage area, a critical issue is to effectively allocate wireless\nresources to support potentially massive UEs in the cell while resolving the\nprominent near-far fairness problem for cell-edge UEs, which is challenging to\naddress due to the lack of tractable analytical model for the LoRa network and\nits practical requirement for low-complexity and low-overhead design. To\nachieve massive connectivity with fairness, we investigate the problem of\nmaximizing the minimum throughput of all UEs in the LoRa network, by jointly\ndesigning high-level policies of spreading factor (SF) allocation, power\ncontrol, and duty cycle adjustment based only on average channel statistics and\nspatial UE distribution. By leveraging on the Poisson rain model along with\ntailored modifications to our considered LoRa network, we are able to account\nfor channel fading, aggregate interference and accurate packet overlapping, and\nstill obtain a tractable and yet accurate closed-form formula for the packet\nsuccess probability and hence throughput. We further propose an iterative\nbalancing (IB) method to allocate the SFs in the cell such that the overall\nmax-min throughput can be achieved within the considered time period and cell\narea. Numerical results show that the proposed scheme with optimized design\ngreatly alleviates the near-far fairness issue, and significantly improves the\ncell-edge throughput.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Although ADAM is a very popular algorithm for optimizing the weights of\nneural networks, it has been recently shown that it can diverge even in simple\nconvex optimization examples. Several variants of ADAM have been proposed to\ncircumvent this convergence issue. In this work, we study the ADAM algorithm\nfor smooth nonconvex optimization under a boundedness assumption on the\nadaptive learning rate. The bound on the adaptive step size depends on the\nLipschitz constant of the gradient of the objective function and provides safe\ntheoretical adaptive step sizes. Under this boundedness assumption, we show a\nnovel first order convergence rate result in both deterministic and stochastic\ncontexts. Furthermore, we establish convergence rates of the function value\nsequence using the Kurdyka-Lojasiewicz property.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Atomically thin films of III-VI post-transition metal chalcogenides (InSe and\nGaSe) form an interesting class of two-dimensional semiconductor that feature\nstrong variations of their band gap as a function of the number of layers in\nthe crystal [1-4] and, specifically for InSe, an earlier predicted crossover\nfrom a direct gap in the bulk [5,6] to a weakly indirect band gap in monolayers\nand bilayers [7-11]. Here, we apply angle resolved photoemission spectroscopy\nwith submicrometer spatial resolution ($\\mu$ARPES) to visualise the\nlayer-dependent valence band structure of mechanically exfoliated crystals of\nInSe. We show that for 1 layer and 2 layer InSe the valence band maxima are\naway from the $\\mathbf{\\Gamma}$-point, forming an indirect gap, with the\nconduction band edge known to be at the $\\mathbf{\\Gamma}$-point. In contrast,\nfor six or more layers the bandgap becomes direct, in good agreement with\ntheoretical predictions. The high-quality monolayer and bilayer samples enables\nus to resolve, in the photoluminescence spectra, the band-edge exciton (A) from\nthe exciton (B) involving holes in a pair of deeper valence bands, degenerate\nat $\\mathbf{\\Gamma}$, with the splitting that agrees with both $\\mu$ARPES data\nand the results of DFT modelling. Due to the difference in symmetry between\nthese two valence bands, light emitted by the A-exciton should be predominantly\npolarised perpendicular to the plane of the two-dimensional crystal, which we\nhave verified for few-layer InSe crystals.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Since its discovery in the last century, quantum entanglement has challenged\nsome of our most cherished classical views, such as locality and reality.\nToday, the second quantum revolution is in full swing and promises to\nrevolutionize areas such as computation, communication, metrology, and imaging.\nHere, we review conceptual and experimental advances in complex entangled\nsystems involving many multilevel quantum particles. We provide an overview of\nthe latest technological developments in the generation and manipulation of\nhigh-dimensionally entangled photonic systems encoded in various discrete\ndegrees of freedom such as path, transverse spatial modes or time/frequency\nbins. This overview should help to transfer various physical principles for the\ngeneration and manipulation from one to another degree of freedom and thus\ninspire new technical developments. We also show how purely academic questions\nand curiosity led to new technological applications. Here fundamental research\nprovides the necessary knowledge for coming technologies such as a prospective\nquantum internet or the quantum teleportation of all information stored in a\nquantum system. Finally, we discuss some important problems in the area of\nhigh-dimensional entanglement and give a brief outlook on possible future\ndevelopments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This work proposes and investigates a new model of the rotating rigid body\nbased on the non-twisting frame. Such a frame consists of three mutually\northogonal unit vectors whose rotation rate around one of the three axis\nremains zero at all times and thus, is represented by a nonholonomic\nrestriction. Then, the corresponding Lagrange-D'Alembert equations are\nformulated by employing two descriptions, the first one relying on rotations\nand a splitting approach, and the second one relying on constrained directors.\nFor vanishing external moments, we prove that the new model possesses\nconservation laws, i.e., the kinetic energy and two nonholonomic momenta that\nsubstantially differ from the holonomic momenta preserved by the standard rigid\nbody model. Additionally, we propose a new specialization of a class of\nenergy-momentum integration schemes that exactly preserves the kinetic energy\nand the nonholonomic momenta replicating the continuous counterpart. Finally,\nwe present numerical results that show the excellent conservation properties as\nwell as the accuracy for the time-discretized governing equations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this report, we present nanoelectromechanical resonators fabricated with\nthin exfoliated crystals of a high-T$_c$ cuprate superconductor\nBi$_2$Sr$_2$Ca$_1$Cu$_2$O$_{8+\\delta}$. The mechanical readout is performed by\ncapacitively coupling their motion to a coplanar waveguide microwave cavity\nfabricated with a superconducting alloy of molybdenum-rhenium. We demonstrate\nmechanical frequency tunability with external dc-bias voltage, and quality\nfactors up to 36600. Our spectroscopic and time-domain measurements show that\nmechanical dissipation in these systems is limited by the contact resistance\narising from resistive outer layers. The temperature dependence of dissipation\nindicates the presence of tunneling states, further suggesting that their\nintrinsic performance could be as good as other two-dimensional atomic crystals\nsuch as graphene.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $G_1$ and $G_2$ be compact Lie groups, $X_1 \\in \\mathfrak{g}_1$, $X_2 \\in\n\\mathfrak{g}_2$ and consider the operator \\begin{equation*} L_{aq} = X_1 +\na(x_1)X_2 + q(x_1,x_2), \\end{equation*} where $a$ and $q$ are\nultradifferentiable functions in the sense of Komatsu, and $a$ is real-valued.\nWe characterize completely the global hypoellipticity and the global\nsolvability of $L_{aq}$ in the sense of Komatsu. For this, we present a\nconjugation between $L_{aq}$ and a constant-coefficient operator that preserves\nthese global properties in Komatsu classes. We also present examples of\nglobally hypoelliptic and globally solvable operators on $\\mathbb{T}^1\\times\n\\mathbb{S}^3$ and $\\mathbb{S}^3\\times \\mathbb{S}^3$ in the sense of Komatsu. In\nparticular, we give examples of differential operators which are not globally\n$C^\\infty$-solvable, but are globally solvable in Gevrey spaces.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Possible connections between central black-hole (BH) growth and host-galaxy\ncompactness have been found observationally, which may provide insight into\nBH-galaxy coevolution: compact galaxies might have large amounts of gas in\ntheir centers due to their high mass-to-size ratios, and simulations predict\nthat high central gas density can boost BH accretion. However, it is not yet\nclear if BH growth is fundamentally related to the compactness of the host\ngalaxy, due to observational degeneracies between compactness, stellar mass\n($M_\\bigstar$), and star formation rate (SFR). To break these degeneracies, we\ncarry out systematic partial-correlation studies to investigate the dependence\nof sample-averaged BH accretion rate ($\\rm \\overline{BHAR}$) on the compactness\nof host galaxies, represented by the surface-mass density, $\\Sigma_\\rm e$, or\nthe projected central surface-mass density within 1 kpc, $\\Sigma_1$. We utilize\n8842 galaxies with H < 24.5 in the five CANDELS fields at z = 0.5-3. We find\nthat $\\rm \\overline{BHAR}$ does not significantly depend on compactness when\ncontrolling for SFR or $M_\\bigstar$ among bulge-dominated galaxies and galaxies\nthat are not dominated by bulges, respectively. However, when testing is\nconfined to star-forming galaxies at z = 0.5-1.5, we find that the $\\rm\n\\overline{BHAR}$-$\\Sigma_1$ relation is not simply a secondary manifestation of\na primary $\\rm \\overline{BHAR}$-$M_\\bigstar$ relation, which may indicate a\nlink between BH growth and the gas density within the central 1 kpc of\ngalaxies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The polarizability of electrons occupying the lowest subband of spatial\nquantization in CdTe/Cd$_x$Hg$_{1-x}$Te/CdTe quantum wells is calculated. It is\nshown that polarizability in the quantum well without cadmium is negative,\ni.e., the displacement of an electron in an electric field applied\nperpendicularly to the quantum well plane is opposite to the force acting on\nit. The negative polarizability of 2D electrons can reduce the dielectric\nconstant of quantum wells by up to $(10-15)$ percent.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the rigidity problem for the logarithmic Sobolev inequality on\nweighted Riemannian manifolds satisfying $\\mathrm{Ric}_{\\infty} \\ge K>0$.\nAssuming equality holds, we show that the $1$-dimensional Gaussian space is\nnecessarily split off, similarly to the rigidity results of Cheng--Zhou on the\nspectral gap as well as Morgan on the isoperimetric inequality. The key\ningredient of the proof is the needle decomposition method introduced on\nRiemannian manifolds by Klartag. We also present several related open problems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Consider an arbitrary closed, countably $n$-rectifiable set in a strictly\nconvex $(n+1)$-dimensional domain, and suppose that the set has finite\n$n$-dimensional Hausdorff measure and the complement is not connected. Starting\nfrom this given set, we show that there exists a non-trivial Brakke flow with\nfixed boundary data for all times. As $t \\uparrow \\infty$, the flow\nsequentially converges to non-trivial solutions of Plateau's problem in the\nsetting of stationary varifolds.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report a numerical study of the diffusiophoresis of short polymers using\nnon-equilibrium molecular dynamics simulations. More precisely, we consider\npolymer chains in a fluid containing a solute which has a concentration\ngradient, and examine the variation of the induced diffusiophoretic velocity of\nthe polymer chains as the interaction between the monomer and the solute is\nvaried. We find that there is a non-monotonic relation between the\ndiffusiophoretic mobility and the strength of the monomer-solute interaction.\nIn addition we find a weak dependence of the mobility on the length of the\npolymer chain, which shows clear difference from the diffusiophoresis of a\nsolid particle. Interestingly, the hydrodynamic flow through the polymer is\nmuch less screened than for pressure driven flows.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a generative model and an inference scheme for epidemic processes\non dynamic, adaptive contact networks. Network evolution is formulated as a\nlink-Markovian process, which is then coupled to an individual-level stochastic\nSIR model, in order to describe the interplay between epidemic dynamics on a\nnetwork and network link changes. A Markov chain Monte Carlo framework is\ndeveloped for likelihood-based inference from partial epidemic observations,\nwith a novel data augmentation algorithm specifically designed to deal with\nmissing individual recovery times under the dynamic network setting. Through a\nseries of simulation experiments, we demonstrate the validity and flexibility\nof the model as well as the efficacy and efficiency of the data augmentation\ninference scheme. The model is also applied to a recent real-world dataset on\ninfluenza-like-illness transmission with high-resolution social contact\ntracking records.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the performance of different methodologies that measure the\ntime lag between broad-line and continuum variations in reverberation mapping\ndata using simulated light curves that probe a range of cadence, time baseline,\nand signal-to-noise ratio in the flux measurements. We compare three\nwidely-adopted lag measuring methods: the Interpolated Cross-Correlation\nFunction (ICCF), the z-transformed Discrete Correlation Function (ZDCF) and the\nMCMC code JAVELIN, for mock data with qualities typical of multi-object\nspectroscopic reverberation mapping (MOS-RM) surveys that simultaneously\nmonitor hundreds of quasars. We quantify the overall lag detection efficiency,\nthe rate of false detections, and the quality of lag measurements for each of\nthese methods and under different survey designs (e.g., observing cadence and\ndepth) using mock quasar light curves. Overall JAVELIN and ICCF outperform ZDCF\nin essentially all tests performed. Compared with ICCF, JAVELIN produces higher\nquality lag measurements, is capable of measuring more lags with timescales\nshorter than the observing cadence, is less susceptible to seasonal gaps and\nS/N degradation in the light curves, and produces more accurate lag\nuncertainties. We measure the Hbeta broad-line region size-luminosity (R-L)\nrelation with each method using the simulated light curves to assess the impact\nof selection effects of the design of MOS-RM surveys. The slope of the R-L\nrelation measured by JAVELIN is the least biased among the three methods, and\nis consistent across different survey designs. These results demonstrate a\nclear preference for JAVELIN over the other two non-parametric methods for\nMOS-RM programs, particularly in the regime of limited light curve quality as\nexpected from most MOS-RM programs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  It has always been a research hotspot to use geographic information to assist\nthe navigation of unmanned aerial vehicles. In this paper, a road-network-based\nlocalization method is proposed. We match roads in the measurement images to\nthe reference road vector map, and realize successful localization on areas as\nlarge as a whole city. The road network matching problem is treated as a point\ncloud registration problem under two-dimensional projective transformation, and\nsolved under a hypothesise-and-test framework. To deal with the projective\npoint cloud registration problem, a global projective invariant feature is\nproposed, which consists of two road intersections augmented with the\ninformation of their tangents. We call it two road intersections tuple. We\ndeduce the closed-form solution for determining the alignment transformation\nfrom a pair of matching two road intersections tuples. In addition, we propose\nthe necessary conditions for the tuples to match. This can reduce the candidate\nmatching tuples, thus accelerating the search to a great extent. We test all\nthe candidate matching tuples under a hypothesise-and-test framework to search\nfor the best match. The experiments show that our method can localize the\ntarget area over an area of 400 within 1 second on a single cpu.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We generalize a combinatorial formula of Douady from the main cardioid to\nother hyperbolic components $H$ of the Mandelbrot set, constructing an explicit\npiecewise linear map which sends the set of angles of external rays landing on\n$H$ to the set of angles of external rays landing on the real axis.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Blazars are a subclass of AGN and flaring in multi-TeV gamma-ray seems to be\nthe major activity in high energy blazars a subgrup of blazars. Flaring is also\nunpredictable and switches between quiescent and active states involving\ndifferent time scales and fluxes. While in some high energy blazars a strong\ntemporal correlation between X-ray and multi-TeV gamma-ray has been observed,\noutbursts in some other have no low energy counterparts and explanation of such\nextreme activity needs to be addressed through different mechanisms as it is\nnot understood well. The extragalactic background light (EBL) plays an\nimportant role in the observation of these high energy gamma-rays as it\nattenuates through pair production of electron-positron and also changes the\nspectral shape of the high energy photons. In the context of the photohadronic\nmodel and taking EBL correction into account, flaring can be explained very\nwell. In a series of papers we have developed this model to explain multi-TeV\nflaring events form many blazars. Here in this review, the photohadronic model\nis discussed and applied to explain the multi-TeV flaring from nearby high\nenergy blazars: Markarian 421, Markarian 501 and 1ES1959+650.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Brain Tumor Segmentation from magnetic resonance imaging (MRI) is a critical\ntechnique for early diagnosis. However, rather than having complete four\nmodalities as in BraTS dataset, it is common to have missing modalities in\nclinical scenarios. We design a brain tumor segmentation algorithm that is\nrobust to the absence of any modality. Our network includes a\nchannel-independent encoding path and a feature-fusion decoding path. We use\nself-supervised training through channel dropout and also propose a novel\ndomain adaptation method on feature maps to recover the information from the\nmissing channel. Our results demonstrate that the quality of the segmentation\ndepends on which modality is missing. Furthermore, we also discuss and\nvisualize the contribution of each modality to the segmentation results. Their\ncontributions are along well with the expert screening routine.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work we construct a low-order nonconforming approximation method for\nlinear elasticity problems supporting general meshes and valid in two and three\nspace dimensions. The method is obtained by hacking the Hybrid High-Order\nmethod, that requires the use of polynomials of degree $k\\ge1$ for stability.\nSpecifically, we show that coercivity can be recovered for $k=0$ by introducing\na novel term that penalises the jumps of the displacement reconstruction across\nmesh faces. This term plays a key role in the fulfillment of a discrete Korn\ninequality on broken polynomial spaces, for which a novel proof valid for\ngeneral polyhedral meshes is provided. Locking-free error estimates are derived\nfor both the energy- and the $L^2$-norms of the error, that are shown to\nconvergence, for smooth solutions, as $h$ and $h^2$, respectively (here, $h$\ndenotes the meshsize). A thorough numerical validation on a complete panel of\ntwo- and three-dimensional test cases is provided.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper studies the boundary feedback stabilization of a class of diagonal\ninfinite-dimensional boundary control systems. In the studied setting, the\nboundary control input is subject to a constant delay while the open loop\nsystem might exhibit a finite number of unstable modes. The proposed control\ndesign strategy consists in two main steps. First, a finite-dimensional\nsubsystem is obtained by truncation of the original Infinite-Dimensional System\n(IDS) via modal decomposition. It includes the unstable components of the\ninfinite-dimensional system and allows the design of a finite-dimensional delay\ncontroller by means of the Artstein transformation and the pole-shifting\ntheorem. Second, it is shown via the selection of an adequate Lyapunov function\nthat 1) the finite-dimensional delay controller successfully stabilizes the\noriginal infinite-dimensional system; 2) the closed-loop system is\nexponentially Input-to-State Stable (ISS) with respect to distributed\ndisturbances. Finally, the obtained ISS property is used to derive a small gain\ncondition ensuring the stability of an IDS-ODE interconnection.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The effect of short-range disorder in nodal line semimetals is studied by\nnumerically exact means. For arbitrary small disorder, a novel semimetallic\nphase is unveiled for which the momentum-space amplitude of the ground-state\nwave function is concentrated around the nodal line and follows a multifractal\ndistribution. At a critical disorder strength, a semimetal to compressible\nmetal transition occurs, coinciding with a multi- to single-fractality\ntransition. The universality class of this critical point is characterized by\nthe correlation length and dynamical exponents. At considerably higher\ndisorder, an Anderson metal-insulator transition takes place. Our results show\nthat the nature of the semimetallic phase in non-clean samples is fundamentally\ndifferent from a clean nodal semimetal.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the Courr\\`{e}ge theorem in the context of linear operators\n$A$ that satisfy the positive maximum principle on a space of continuous\nfunctions over a symmetric space. Applications are given to Feller--Markov\nprocesses. We also introduce Gangolli operators, which satisfy the positive\nmaximum principle, and generalise the form associated with the generator of a\nL\\'{e}vy process on a symmetric space. When the space is compact, we show that\nGangolli operators are pseudo--differential operators having scalar symbols.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we have studied the holographic subregion complexity for\nboosted black brane for strip like subsystem. The holographic subregion\ncomplexity has been computed for a subsystem chosen along and perpendicular to\nthe boost direction. We have observed that there is an asymmetry in the result\ndue to the boost parameter which can be attributed to the asymmetry in the\nholographic entanglement entropy. The Fisher information metric and the\nfidelity susceptibility have also been computed using bulk dual prescriptions.\nIt is observed that the two metrics computed holographically are not related\nfor both the pure black brane as well as the boosted black brane. This is one\nof the main findings in this paper and the holographic results have been\ncompared with the results available in the quantum information literature where\nit is known that the two distances are related to each other in general.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the quantization of a simple model of antisymmetric tensor field\nwith spontaneous Lorentz violation in curved spacetime. We evaluate the 1-loop\ncorrections at first order of metric perturbation, using a general covariant\neffective action approach. We revisit the issue of quantum equivalence, and\nfind that it holds for non-Lorentz-violating modes but breaks down for Lorentz\nviolating modes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This survey of topological cyclic homology is a chapter in the Handbook on\nHomotopy Theory. We give a brief introduction to topological cyclic homology\nand the cyclotomic trace map following Nikolaus-Scholze, followed by a proof of\nB\\\"okstedt periodicity that closely resembles B\\\"okstedt's original unpublished\nproof. We explain the extension of B\\\"{o}kstedt periodicity by\nBhatt-Morrow-Scholze from perfect fields to perfectoid rings and use this to\ngive a purely p-adic proof of Bott periodicity. Finally, we evaluate the\ncofiber of the assembly map in p-adic topological cyclic homology for the\ncyclic group of order p and a perfectoid ring of coefficients.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we present Shift Convolution Network (ShiftConvNet) to provide\nmatching capability between two feature maps for stereo estimation. The\nproposed method can speedily produce a highly accurate disparity map from\nstereo images. A module called shift convolution layer is proposed to replace\nthe traditional correlation layer to perform patch comparisons between two\nfeature maps. By using a novel architecture of convolutional network to learn\nthe matching process, ShiftConvNet can produce better results than\nDispNet-C[1], also running faster with 5 fps. Moreover, with a proposed auto\nshift convolution refine part, further improvement is obtained. The proposed\napproach was evaluated on FlyingThings 3D. It achieves state-of-the-art results\non the benchmark dataset. Codes will be made available at github.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the baseline design of the International Linear Collider (ILC) an\nundulator-based source is foreseen for the positron source in order to match\nthe physics requirements. The baseline parameters are optimized for the ILC at\nsqrt(s)=500 GeV, that means an electron drive beam of 250 GeV. Precision\nmeasurements in the Higgs sector, however, require measurements at sqrt(s)=250\nGeV, i.e. running with the electron drive beam only at 125 GeV, which imposes a\nchallenge for achieving a high yield. Therefore the baseline undulator\nparameters have to be optimized as much as possible within their technical\nperformances. In this bachelor thesis we therefore present a theoretical study\non the radiation spectra of a helical undulator, based on the equation for the\nradiated synchrotron energy spectral density per solid angle per electron in\nthe relativistic, far-field and point-like charge approximation. From this\nstarting point the following undulator properties are examined: the deposited\npower in the undulator vessel, which can disrupt the functionality of the\nundulator magnets, the protective property of a mask on this disturbances and\nthe number of positrons produced by the synchrotron radiation in a Ti6Al4V\ntarget. Those quantities were evaluated for various values for parameters as\nundulator period, undulator length and magnetic flux in order to find optimal\nbaseline parameter sets for sqrt(s)=250 GeV.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Two-dimensional triangular-lattice materials with spin-1/2 are perfect\nplatforms for investigating quantum frustrated physics with spin fluctuations.\nHere we report the structure, magnetization, heat capacity and inelastic\nneutron scattering (INS) results on cesium ytterbium diselenide, CsYbSe$_2$.\nThere is no long-range magnetic order down to 0.4 K at zero field. The\ntemperature dependent magnetization, $M(T)$, reveals an easy-plane magnetic\nanisotropy. A maximum is found in $M(T)$ around \\emph{T}$\\sim$1.5 K when\nmagnetic field $H$ is applied in the $ab$ plane, indicating the short-range\ninteraction. The low-temperature isothermal magnetization $M(H)$ shows a\none-third plateau of the estimated saturation moment, that is characteristic of\na two-dimensional frustrated triangular lattice. Heat capacity shows\nfield-induced long-range magnetic order for both $H||c$ and $H||ab$ directions.\nThe broad peak in heat capacity and highly damped INS magnetic excitation at\n$T$=2 K suggests strong spin fluctuations. The dispersive in-plane INS,\ncentered at the (1/3 1/3 0) point, and the absence of dispersion along $c$\ndirection suggests 120$^{\\circ}$ non-collinear 2D-like spin correlations. All\nthese results indicate that the two-dimensional frustrated material CsYbSe$_2$\ncan be in proximity to the triangular-lattice quantum spin liquid. We propose\nan experimental low-temperature $H$-$T$ phase diagram for CsYbSe$_2$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Due to high binding energy and oscillator strength, excitons in thin flakes\nof transition metal dichalcogenides constitute a perfect foundation for\nrealizing a strongly coupled light-matter system. In this paper we investigate\nmono- and few-layer WSe$_2$ flakes encapsulated in hexagonal boron nitride and\nincorporated into a planar dielectric cavity. We use an open cavity design\nwhich provides tunability of the cavity mode energy by as much as 150 meV. We\nobserve a strong coupling regime between the cavity photons and the neutral\nexcitons in direct-bandgap monolayer WSe$_2$, as well as in few-layer WSe$_2$\nflakes exhibiting indirect bandgap. We discuss the dependence of the exciton's\noscillator strength and resonance linewidth on the number of layers and predict\nthe exciton-photon coupling strength.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We implement machine learning algorithms to nuclear data. These algorithms\nare purely data driven and generate models that are capable to capture\nintricate trends. Gradient boosted trees algorithm is employed to generate a\ntrained model from existing nuclear data, which is used for prediction for data\nof damping parameter, shell correction energies, quadrupole deformation,\npairing gaps, level densities and giant dipole resonance for large number of\nnuclei. We, in particular, predict level density parameter for superheavy\nelements which is of great current interest. The predictions made by the\nmachine learning algorithm is found to have standard deviation from 0.00035 to\n0.73.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Complex or co-existing diseases are commonly treated using drug combinations,\nwhich can lead to higher risk of adverse side effects. The detection of\npolypharmacy side effects is usually done in Phase IV clinical trials, but\nthere are still plenty which remain undiscovered when the drugs are put on the\nmarket. Such accidents have been affecting an increasing proportion of the\npopulation (15% in the US now) and it is thus of high interest to be able to\npredict the potential side effects as early as possible. Systematic\ncombinatorial screening of possible drug-drug interactions (DDI) is challenging\nand expensive. However, the recent significant increases in data availability\nfrom pharmaceutical research and development efforts offer a novel paradigm for\nrecovering relevant insights for DDI prediction. Accordingly, several recent\napproaches focus on curating massive DDI datasets (with millions of examples)\nand training machine learning models on them. Here we propose a neural network\narchitecture able to set state-of-the-art results on this task---using the type\nof the side-effect and the molecular structure of the drugs alone---by\nleveraging a co-attentional mechanism. In particular, we show the importance of\nintegrating joint information from the drug pairs early on when learning each\ndrug's representation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The work is devoted to the theoretical and experimental study of quantum\nstates of light conditionally prepared by subtraction of a random number of\nphotons from the initial multimode thermal state. A fixed number of photons is\nsubtracted from a multimode quantum state, but only a subsystem of a lower\nnumber of modes is registered, in which the number of subtracted photons turns\nout to be a non-fixed random variable. It is shown that the investigation of\nmultiphoton subtracted multimode thermal states provides a direct study of the\nfundamental quantum-statistical properties of bosons using a simple\nexperimental implementation. The developed experimental setup plays a role of a\nspecific boson lototron, which is based on the fundamental link between the\nstatistics of boson systems and the Polya distribution. It is shown that the\ncalculation of the photon number distribution based on the Polya urn scheme is\nequivalent to a calculation using statistical weights for boson systems. A\nmathematical model based on the composition of the Polya distribution and\nthermal state is developed and verified. The experimental results are in a good\nagreement with the developed theory.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Hybrid quantum information processing combines the advantages of discrete and\ncontinues variable protocols by realizing protocols consisting of photon\ncounting and homodyne measurements. However, the mode structure of pulsed\nsources and the properties of the detection schemes often require the use\noptical filters in order to combine both detection methods in a common\nexperiment. This limits the efficiency and the overall achievable squeezing of\nthe experiment. In our work, we use photon subtraction to implement the\ndistillation of pulsed squeezed states originating from a genuinely spatially\nand temporally single-mode parametric down-conversion source in non-linear\nwaveguides. Due to the distillation, we witness an improvement of\n$0.17~\\mathrm{dB}$ from an initial squeezing value of $-1.648 \\pm\n0.002~\\mathrm{dB}$, while achieving a purity of $0.58$, and confirm the\nnon-Gaussianity of the distilled state via the higher-order cumulants. With\nthis, we demonstrate the source's suitability for scalable hybrid quantum\nnetwork applications with pulsed quantum light.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Hybrid quantum systems exhibiting coupled optical, spin, and mechanical\ndegrees of freedom can serve as a platform for sensing, or as a bus to mediate\ninteractions between qubits with disparate energy scales. These systems are\nalso creating opportunities to test foundational ideas in quantum mechanics,\nincluding direct observations of the quantum regime in macroscopic objects.\nHere, we make use of angular momentum conservation to study the dynamics of a\npair of paramagnetic centers featuring different spin numbers in the presence\nof a properly tuned external magnetic field. We examine the interplay between\noptical excitation, spin evolution, and mechanical motion, and theoretically\nshow that in the presence of continuous optical illumination, inter-spin\ncross-relaxation must induce rigid rotation of the host crystal. The system\ndynamics is robust to scattering of spin-polarized phonons, a result we build\non to show this form of angular momentum transfer should be observable using\nstate-of-the-art torsional oscillators or trapped nanoparticles.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In Two Higgs Doublet Models (2HDMs) shaped by some unbroken symmetry,\nimposing perturbativity requirements on the quartic couplings can imply that\nthe allowed masses of all the fundamental scalars are bounded from above. This\nimportant property is analysed in detail for the only two realistic 2HDMs with\nan exact symmetry, the case with $\\mathbb{Z}_2$ symmetry and the case with CP\nsymmetry. It is also noticeable that one exception arises in each case: when\nthe vacuum is assumed to respect the imposed symmetry, a decoupling regime can\nnevertheless appear without violating perturbativity requirements. In both\nmodels with an exact symmetry and no decoupling regime, soft symmetry breaking\nterms can however lead to a decoupling regime: the possibility that this regime\nmight be unnatural, since it requires some fine tuning, is also analysed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Tent and Ziegler proved that the automorphism group of the Urysohn sphere is\nsimple and that the automorphism group of the Urysohn space is simple modulo\nbounded automorphisms. A key component of their proof is the definition of a\nstationary independence relation (SIR). In this paper we prove that the\nexistence of a SIR satisfying some extra axioms is enough to prove simplicity\nof the automorphism group of a countable structure. The extra axioms are chosen\nwith applications in mind, namely homogeneous structures which admit a\n\"metric-like amalgamation\", for example all primitive 3-constrained metrically\nhomogeneous graphs of finite diameter from Cherlin's list.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We developed a versatile integrated control and readout instrument for\nexperiments with superconducting quantum bits (qubits), based on a\nfield-programmable gate array (FPGA) platform. Using this platform, we perform\nmeasurement-based, closed-loop feedback operations with $428 \\, \\mathrm{ns}$\nplatform latency. The feedback capability is instrumental in realizing active\nreset initialization of the qubit into the ground state in a time much shorter\nthan its energy relaxation time $T_1$. We show experimental results\ndemonstrating reset of a fluxonium qubit with $99.4\\,\\%$ fidelity, using a\nreadout-and-drive pulse sequence approximately $1.5 \\, \\mathrm{\\mu s}$ long.\nCompared to passive ground state initialization through thermalization, with\nthe time constant given by $T_1 = ~ 80 \\, \\mathrm{\\mu s}$, the use of the\nFPGA-based platform allows us to improve both the fidelity and the time of the\nqubit initialization by an order of magnitude.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We address the problem of semantic correspondence, that is, establishing a\ndense flow field between images depicting different instances of the same\nobject or scene category. We propose to use images annotated with binary\nforeground masks and subjected to synthetic geometric deformations to train a\nconvolutional neural network (CNN) for this task. Using these masks as part of\nthe supervisory signal offers a good compromise between semantic flow methods,\nwhere the amount of training data is limited by the cost of manually selecting\npoint correspondences, and semantic alignment ones, where the regression of a\nsingle global geometric transformation between images may be sensitive to\nimage-specific details such as background clutter. We propose a new CNN\narchitecture, dubbed SFNet, which implements this idea. It leverages a new and\ndifferentiable version of the argmax function for end-to-end training, with a\nloss that combines mask and flow consistency with smoothness terms.\nExperimental results demonstrate the effectiveness of our approach, which\nsignificantly outperforms the state of the art on standard benchmarks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a data-driven deep neural algorithm for detecting deceptive\nwalking behavior using nonverbal cues like gaits and gestures. We conducted an\nelaborate user study, where we recorded many participants performing tasks\ninvolving deceptive walking. We extract the participants' walking gaits as\nseries of 3D poses. We annotate various gestures performed by participants\nduring their tasks. Based on the gait and gesture data, we train an LSTM-based\ndeep neural network to obtain deep features. Finally, we use a combination of\npsychology-based gait, gesture, and deep features to detect deceptive walking\nwith an accuracy of 88.41%. This is an improvement of 10.6% over handcrafted\ngait and gesture features and an improvement of 4.7% and 9.2% over classifiers\nbased on the state-of-the-art emotion and action classification algorithms,\nrespectively. Additionally, we present a novel dataset, DeceptiveWalk, that\ncontains gaits and gestures with their associated deception labels. To the best\nof our knowledge, ours is the first algorithm to detect deceptive behavior\nusing non-verbal cues of gait and gesture.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Partial Label Learning (PLL) aims to learn from the data where each training\nexample is associated with a set of candidate labels, among which only one is\ncorrect. The key to deal with such problem is to disambiguate the candidate\nlabel sets and obtain the correct assignments between instances and their\ncandidate labels. In this paper, we interpret such assignments as\ninstance-to-label matchings, and reformulate the task of PLL as a matching\nselection problem. To model such problem, we propose a novel Graph Matching\nbased Partial Label Learning (GM-PLL) framework, where Graph Matching (GM)\nscheme is incorporated owing to its excellent capability of exploiting the\ninstance and label relationship. Meanwhile, since conventional one-to-one GM\nalgorithm does not satisfy the constraint of PLL problem that multiple\ninstances may correspond to the same label, we extend a traditional one-to-one\nprobabilistic matching algorithm to the many-to-one constraint, and make the\nproposed framework accommodate to the PLL problem. Moreover, we also propose a\nrelaxed matching prediction model, which can improve the prediction accuracy\nvia GM strategy. Extensive experiments on both artificial and real-world data\nsets demonstrate that the proposed method can achieve superior or comparable\nperformance against the state-of-the-art methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Massive young stellar objects (MYSOs) have recently been shown to drive jets\nwhose particles can interact with either the magnetic fields of the jet or\nambient medium to emit non-thermal radiation. We report a search for\nnon-thermal radio emission from a sample of 15 MYSOs to establish the\nprevalence of the emission in the objects. We used their spectra across the L-,\nC- and Q-bands along with spectral index maps to characterise their emission.\nWe find that about 50% of the sources show evidence for non-thermal emission\nwith 40% showing clear non-thermal lobes, especially sources of higher\nbolometric luminosity. The common or IRAS names of the sources that manifest\nnon-thermal lobes are; V645Cyg, IRAS 22134+5834, NGC 7538 IRS 9, IRAS\n23262+640, AFGL 402d and AFGL 490. All the central cores of the sources are\nthermal with corresponding mass-loss rates that lie in the range 3X10^{-7} to\n7X10^{-6} solar masses per year. Given the presence of non-thermal lobes in\nsome of the sources and the evidence of non-thermal emission from some spectral\nindex maps, it seems that magnetic fields play a significant role in the jets\nof massive protostars. Also noted is that some of the sources show evidence of\nbinarity and variability.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In queueing theory, Lorden's inequality can be used for bounds estimation of\nthe moments of backward and forward renewal times. Two random variables called\nbackwards renewal time and forward renewal time for this process are defined.\nLorden's inequality it's true for the renewal process, so expectations of\nbackward and forward renewal times are bounded by the relation of expectation\nof moment of the random variable for any fixed moment of time, where random\nvariables are i.i.d. We generalised and proved a similar result for dependent\nrandom variables with finite expectations, some constant C and integrable\nfunction Q(s): if X is not independent and have absolutely continuous\ndistribution function which satisfies some boundary conditions, then the\nanalogue of Lorden's inequality for renewal process is true. In August 2021\nreviewed version is uploaded.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The three-dimensional structure of the gas flow around a planet is thought to\ninfluence the accretion of both gas and solid materials. In particular, the\noutflow in the mid-plane region may prevent the accretion of the solid\nmaterials and delay the formation of super-Earths' cores. However, it is not\nyet understood how the nature of the flow field and outflow speed change as a\nfunction of the planetary mass. In this study, we investigate the dependence of\ngas flow around a planet embedded in a protoplanetary disc on the planetary\nmass. Assuming an isothermal, inviscid gas disc, we perform three-dimensional\nhydrodynamical simulations on the spherical polar grid, which has a planet\nlocated at its centre. We find that gas enters the Bondi or Hill sphere at high\nlatitudes and exits through the mid-plane region of the disc regardless of the\nassumed dimensionless planetary mass $m=R_{\\rm Bondi}/H$, where $R_{\\rm Bondi}$\nand $H$ are the Bondi radius of the planet and disc scale height, respectively.\nThe altitude from where gas predominantly enters the envelope varies with the\nplanetary mass. The outflow speed can be expressed as $|u_{\\rm\nout}|=\\sqrt{3/2}mc_{\\rm s}$ $(R_{\\rm Bondi}\\leq R_{\\rm Hill})$ or $|u_{\\rm\nout}|=\\sqrt{3/2}(m/3)^{1/3} c_{\\rm s}$ ($R_{\\rm Bondi}\\geq R_{\\rm Hill}$),\nwhere $c_{\\rm s}$ is the isothermal sound speed and $R_{\\rm Hill}$ is the Hill\nradius. The outflow around a planet may reduce the accretion of dust and\npebbles onto the planet when $m\\gtrsim\\sqrt{\\rm St}$, where St is the Stokes\nnumber. Our results suggest that the flow around proto-cores of super-Earths\nmay delay their growth and, consequently, help them to avoid runaway gas\naccretion within the lifetime of the gas disc.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For random $d$-regular graphs on $N$ vertices with $1 \\ll d \\ll N^{2/3}$, we\ndevelop a $d^{-1/2}$ expansion of the local eigenvalue distribution about the\nKesten-McKay law up to order $d^{-3}$. This result is valid up to the edge of\nthe spectrum. It implies that the eigenvalues of such random regular graphs are\nmore rigid than those of Erd\\H{o}s-R\\'enyi graphs of the same average degree.\nAs a first application, for $1 \\ll d \\ll N^{2/3}$, we show that all nontrivial\neigenvalues of the adjacency matrix are with very high probability bounded in\nabsolute value by $(2 + o(1)) \\sqrt{d - 1}$. As a second application, for\n$N^{2/9} \\ll d \\ll N^{1/3}$, we prove that the extremal eigenvalues are\nconcentrated at scale $N^{-2/3}$ and their fluctuations are governed by\nTracy-Widom statistics. Thus, in the same regime of $d$, $52\\%$ of all\n$d$-regular graphs have second-largest eigenvalue strictly less than $2 \\sqrt{d\n- 1}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Rotating radio transients (RRATs) are peculiar astronomical objects whose\nemission mechanism remains under investigation. In this paper, we present\nobservations of three RRATs, J1538+2345, J1854+0306 and J1913+1330, observed\nwith the Five-hundred-meter Aperture Spherical radio Telescope (FAST).\nSpecifically, we analyze the mean pulse profiles and temporal flux density\nevolutions of the RRATs. Owing to the high sensitivity of FAST, the derived\nburst rates of the three RRATs are higher than those in previous reports. RRAT\nJ1854+0306 exhibited a time-dynamic mean pulse profile, whereas RRAT J1913+1330\nshowed distinct radiation and nulling segments on its pulse intensity trains.\nThe mean pulse profile variation with frequency is also studied for RRAT\nJ1538+2345 and RRAT J1913+1330, and the profiles at different frequencies could\nbe well fitted with a cone-core model and a conal-beam model, respectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Uniform bin width histograms are widely used so this data graphic should\nrepresent data as correctly as possible. Method of moments based on familiar\nmean, variance and Fisher-Pearson skewness cure this problem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider harmonic sections of a bundle over the complement of a\ncodimension 2 submanifold in a Riemannian manifold, which can be thought of as\nmultivalued harmonic functions. We prove a result to the effect that these are\nstable under small deformations of the data. The proof is an application of a\nversion of the Nash-Moser implicit function theorem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  U-statistics constitute a large class of estimators, generalizing the\nempirical mean of a random variable $X$ to sums over every $k$-tuple of\ndistinct observations of $X$. They may be used to estimate a regular functional\n$\\theta(P_{X})$ of the law of $X$. When a vector of covariates $Z$ is\navailable, a conditional U-statistic may describe the effect of $z$ on the\nconditional law of $X$ given $Z=z$, by estimating a regular conditional\nfunctional $\\theta(P_{X|Z=\\cdot})$. We prove concentration inequalities for\nconditional U-statistics. Assuming a parametric model of the conditional\nfunctional of interest, we propose a regression-type estimator based on\nconditional U-statistics. Its theoretical properties are derived, first in a\nnon-asymptotic framework and then in two different asymptotic regimes. Some\nexamples are given to illustrate our methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The mechanical properties of Mg-Al alloys are greatly influenced by the\ncomplex intermetallic phase Mg$_{17}$Al$_{12}$, which is the most dominant\nprecipitate found in this alloy system. The interaction of basal edge and\n30$^\\text{o}$ dislocations with Mg$_{17}$Al$_{12}$ precipitates is studied by\nmolecular dynamics and statics simulations, varying the inter-precipitate\nspacing ($L$), and size ($D$), shape and orientation of the precipitates. The\ncritical resolved shear stress $\\tau_c$ to pass an array of precipitates\nfollows the usual $\\ln((1/D + 1/L)^{-1})$ proportionality. In all cases but the\nsmallest precipitate, the dislocations pass the obstacles by depositing\ndislocation segments in the disordered interphase boundary rather than shearing\nthe precipitate or leaving Orowan loops in the matrix around the precipitate.\nAn absorbed dislocation increases the stress necessary for a second dislocation\nto pass the precipitate also by absorbing dislocation segments into the\nboundary. Replacing the precipitate with a void of identical size and shape\ndecreases the critical passing stress and work hardening contribution while an\nartificially impenetrable Mg$_{17}$Al$_{12}$ precipitate increases both. These\ninsights will help improve mesoscale models of hardening by incoherent\nparticles.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Nonstationarity is a major challenge in analyzing spatial data. For example,\ndaily precipitation measurements may have increased variability and decreased\nspatial smoothness in areas with high mean rainfall. Common nonstationary\ncovariance models introduce parameters specific to each location, giving a\nhighly-parameterized model which is difficult to fit. We develop a\nnonstationary spatial model that uses the mean to determine the covariance in a\nregion, resulting in a far simpler, albeit more specialized, model. We explore\ninferential and predictive properties of the model under various simulated data\nsituations. We show that this model in certain circumstances improves\npredictions compared to a standard stationary spatial model. We further propose\na computationally efficient approximation that has comparable predictive\naccuracy. We also develop a test for nonstationary data and show it reliably\nidentifies nonstationarity. We apply these methods to daily precipitation in\nPuerto Rico.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we derive the geodesic equation for massive particles and light\nfor the black spindle spacetime. The solution for light can be formulated in\nterms of the Weierstra{\\ss} {\\wp}-, {\\sigma}- and {\\zeta}-function, whereas a\npart of the solutions for massive particles is given in terms of derivatives of\nthe Kleinian {\\sigma}-function. We analyze the possible orbit types using\nparametric diagrams and effective potentials. Furthermore we visualize the\norbits in a coordinate system, where the spindle-like topology of the horizon\nis visible.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Black holes and wormholes are solutions of Einstein's field equations, both\nof which, from afar, look like a central mass. We show here that although at\nlarge distances both behave like Newtonian objects, close to the event horizon\nor to the throat, black holes and wormholes have different tidal effects on\nstars, due to their respective geometries. We quantify this difference by a\nnumerical procedure in the Schwarzschild black hole and the exponential\nwormhole backgrounds, and compare the peak fallback rates of tidal debris in\nthese geometries. The tidal disruption rates in these backgrounds are also\ncomputed. It is shown that these quantities are a few times higher for\nwormholes, compared to the black hole cases.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We model the ultraviolet spectra of the Seyfert 1 galaxy NGC~5548 obtained\nwith the Hubble Space Telescope during the 6-month reverberation-mapping\ncampaign in 2014. Our model of the emission from NGC 5548 corrects for\noverlying absorption and deblends the individual emission lines. Using the\nmodeled spectra, we measure the response to continuum variations for the\ndeblended and absorption-corrected individual broad emission lines, the\nvelocity-dependent profiles of Ly$\\alpha$ and C IV, and the narrow and broad\nintrinsic absorption features. We find that the time lags for the corrected\nemission lines are comparable to those for the original data. The\nvelocity-binned lag profiles of Ly$\\alpha$ and C IV have a double-peaked\nstructure indicative of a truncated Keplerian disk. The narrow absorption lines\nshow delayed response to continuum variations corresponding to recombination in\ngas with a density of $\\sim 10^5~\\rm cm^{-3}$. The high-ionization narrow\nabsorption lines decorrelate from continuum variations during the same period\nas the broad emission lines. Analyzing the response of these absorption lines\nduring this period shows that the ionizing flux is diminished in strength\nrelative to the far-ultraviolet continuum. The broad absorption lines\nassociated with the X-ray obscurer decrease in strength during this same time\ninterval. The appearance of X-ray obscuration in $\\sim\\,2012$ corresponds with\nan increase in the luminosity of NGC 5548 following an extended low state. We\nsuggest that the obscurer is a disk wind triggered by the brightening of NGC\n5548 following the decrease in size of the broad-line region during the\npreceding low-luminosity state.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Denotational models should provide an opportunity for the revision of current\npractices seen in the manuals of programming languages. New styles should on\none hand base on denotational models but on the other - do not assume that\ntoday readers are acquainted in this field. A manual should, therefore, provide\nsome basic knowledge and notation needed to understand the definition of a\nprogramming language written in a new style. At the same time - I strongly\nbelieve that - it should be written for professional programmers rather than\nfor amateurs. The role of a manual is not to teach the skills of programming.\nSuch textbooks are, of course, necessary, but they should tell the readers what\nthe programming is about rather than the technicalities of a concrete language.\nThe paper contains an example of a manual for a virtual programming language\nLingua developed in our project.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we propose a class of numerical schemes for solving semilinear\nHamilton-Jacobi-Bellman-Isaacs (HJBI) boundary value problems which arise\nnaturally from exit time problems of diffusion processes with controlled drift.\nWe exploit policy iteration to reduce the semilinear problem into a sequence of\nlinear Dirichlet problems, which are subsequently approximated by a multilayer\nfeedforward neural network ansatz. We establish that the numerical solutions\nconverge globally in the $H^2$-norm, and further demonstrate that this\nconvergence is superlinear, by interpreting the algorithm as an inexact Newton\niteration for the HJBI equation. Moreover, we construct the optimal feedback\ncontrols from the numerical value functions and deduce convergence. The\nnumerical schemes and convergence results are then extended to HJBI boundary\nvalue problems corresponding to controlled diffusion processes with oblique\nboundary reflection. Numerical experiments on the stochastic Zermelo navigation\nproblem are presented to illustrate the theoretical results and to demonstrate\nthe effectiveness of the method.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the dependence of mechanical conformations of graphene sheets\nlocated on flat substrates on the density of unilateral (one-side) attachment\nof hydrogen, fluorine or chlorine atoms to them. It is shown that chemically\nmodified graphene sheet can take four main forms on a flat substrate: the form\nof a flat sheet located parallel to the surface of the substrate, the form of\nconvex sheet partially detached from the substrate with bent edges adjacent to\nthe substrate, and the forms of single and double rolls on the substrate. On\nthe surface of crystalline graphite, the flat form of the sheet is lowest in\nenergy for hydrogenation density p <0.21, fluorination density p<0.20 and\nchlorination density p<0.16. The surface of crystalline nickel has higher\nadsorption energy for graphene monolayer and the flat form of chemically\nmodified sheet on such substrate is lowest in energy for hydrogenation density\np<0.47, fluorination density p<0.30 and chlorination density p<0.21. The flat\nshape of the graphene sheet remains basic on a substrate also when molecular\ngroups CH3, CH2-CH3 or rings C6H5 are one-side attached to its outer surface.\nAt the attachment density p=1/6 (one group per 6 sheet atoms) the sheet becomes\nthe nanocarpet the basis of which is formed by a sheet of graphene and the pile\nof which is formed by the attached molecular groups forming a tightly packed\nregular lattice. The addition of hydroxyl groups OH with attachment density\np=1/4 leads to the formation of hexagonal lattices of hydroxyl groups on the\nouter surface of graphene sheet on a substrate. In this lattice, the groups can\nform various configurations of hydrogen bonds, which turns the chemically\nmodified sheet into a multistable system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Second and third harmonic generation in the opaque region of a GaAs wafer is\nexperimentally observed both in transmission and reflection. These harmonic\ncomponents can propagate through an opaque material as long as the pump is\ntuned to a region of transparency or semi-transparency, and correspond to the\ninhomogeneous solutions of Maxwell's equations with nonlinear polarization\nsources. We show that measurement of the angular and polarization dependence of\nthe observed harmonic components allows one to infer the different nonlinear\nmechanisms that trigger these processes, including bulk nonlinearity, magnetic\nLorentz and surface contributions. Experimental results are compared with a\ndetailed numerical model that takes into account these different effects.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent works on adversarial examples for image classification focus on\ndirectly modifying pixels with minor perturbations. The small perturbation\nrequirement is imposed to ensure the generated adversarial examples being\nnatural and realistic to humans, which, however, puts a curb on the attack\nspace thus limiting the attack ability and transferability especially for\nsystems protected by a defense mechanism. In this paper, we propose the novel\nconcepts of structure patterns and structure-aware perturbations that relax the\nsmall perturbation constraint while still keeping images natural. The key idea\nof our approach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Built upon\nthese concepts, we propose a \\emph{structure-preserving attack (SPA)} for\ngenerating natural adversarial examples with extremely high transferability.\nEmpirical results on the MNIST and the CIFAR10 datasets show that SPA exhibits\nstrong attack ability in both the white-box and black-box setting even defenses\nare applied. Moreover, with the integration of PGD or CW attack, its attack\nability escalates sharply under the white-box setting, without losing the\noutstanding transferability inherited from SPA.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Biochemical systems that express certain chemical species of interest at the\nsame level at any positive equilibrium are called \"absolute concentration\nrobust\" (ACR). These species behave in a stable, predictable way, in the sense\nthat their expression is robust with respect to sudden changes in the species\nconcentration, regardless the new positive equilibrium reached by the system.\nSuch a property has been proven to be fundamentally important in certain gene\nregulatory networks and signaling systems. In the present paper, we\nmathematically prove that a well-known class of ACR systems studied by Shinar\nand Feinberg in 2010 hides an internal integral structure. This structure\nconfers these systems with a higher degree of robustness that what was\npreviously unknown. In particular, disturbances much more general than sudden\nchanges in the species concentrations can be rejected, and robust perfect\nadaptation is achieved. Significantly, we show that these properties are\nmaintained when the system is interconnected with other chemical reaction\nnetworks. This key feature enables design of insulator devices that are able to\nbuffer the loading effect from downstream systems - a crucial requirement for\nmodular circuit design in synthetic biology.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The main aim of photometrical observations of the asteroid 3200 Phaethon was\nsearching for its low-level cometary activity (possible coma and/or dust tail)\nin the pre-perihelion passage. We performed observational runs with telescopes\nranging from 0.61-m to 2-m and BVR color imaging. Three longer photometric\nseries were used for modeling of the 3D shape of Phaethon. The color indices\nand size of the asteroid were estimated.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The construction by Du et al. (2019) implies that even if a learner is given\nlinear features in $\\mathbb R^d$ that approximate the rewards in a bandit with\na uniform error of $\\epsilon$, then searching for an action that is optimal up\nto $O(\\epsilon)$ requires examining essentially all actions. We use the\nKiefer-Wolfowitz theorem to prove a positive result that by checking only a few\nactions, a learner can always find an action that is suboptimal with an error\nof at most $O(\\epsilon \\sqrt{d})$. Thus, features are useful when the\napproximation error is small relative to the dimensionality of the features.\nThe idea is applied to stochastic bandits and reinforcement learning with a\ngenerative model where the learner has access to $d$-dimensional linear\nfeatures that approximate the action-value functions for all policies to an\naccuracy of $\\epsilon$. For linear bandits, we prove a bound on the regret of\norder $\\sqrt{dn \\log(k)} + \\epsilon n \\sqrt{d} \\log(n)$ with $k$ the number of\nactions and $n$ the horizon. For RL we show that approximate policy iteration\ncan learn a policy that is optimal up to an additive error of order $\\epsilon\n\\sqrt{d}/(1 - \\gamma)^2$ and using $d/(\\epsilon^2(1 - \\gamma)^4)$ samples from\na generative model. These bounds are independent of the finer details of the\nfeatures. We also investigate how the structure of the feature set impacts the\ntradeoff between sample complexity and estimation error.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the boundedness problem for unions of conjunctive regular path\nqueries with inverses (UC2RPQs). This is the problem of, given a UC2RPQ,\nchecking whether it is equivalent to a union of conjunctive queries (UCQ). We\nshow the problem to be ExpSpace-complete, thus coinciding with the complexity\nof containment for UC2RPQs. As a corollary, when a UC2RPQ is bounded, it is\nequivalent to a UCQ of at most triple-exponential size, and in fact we show\nthat this bound is optimal. We also study better behaved classes of UC2RPQs,\nnamely acyclic UC2RPQs of bounded thickness, and strongly connected UCRPQs,\nwhose boundedness problem are, respectively, PSpace-complete and\n$\\Pi^p_2$-complete. Most upper bounds exploit results on limitedness for\ndistance automata, in particular extending the model with alternation and\ntwo-wayness, which may be of independent interest.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Spin-orbit interactions in optics traditionally describe an influence of the\npolarization degree of freedom of light on its spatial properties. The most\nprominent example is the generation of a spin-dependent optical vortex upon\nfocusing or scattering of a circularly polarized plane-wave by a nanoparticle,\nconverting spin to orbital angular momentum of light. Here, we present a\nmechanism of conversion of orbital-to-spin angular momentum of light upon\nscattering of a linearly polarized vortex beam by a spherical silicon\nnanoparticle. We show that focused linearly polarized Laguerre-Gaussian beams\nof first order ($\\ell = \\pm 1$) exhibit an $\\ell$-dependent spatial\ndistribution of helicity density in the focal volume. By using a dipolar\nscatterer the helicity density can be manipulated locally, while influencing\nglobally the spin and orbital angular momentum of the beam. Specifically, the\nscattered light can be purely circularly polarized with the handedness\ndepending on the orbital angular momentum of the incident beam. We corroborate\nour findings with theoretical calculations and an experimental demonstration.\nOur work sheds new light on the global and local properties of helicity\nconservation laws in electromagnetism.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the production of a colourless system at next-to-leading order in\nthe strong coupling constant $\\alpha_s$. We impose a transverse-momentum\ncutoff, qtcut, on the colourless final state and we compute the power\ncorrections for the inclusive cross section in the cutoff, up to the fourth\npower.\n  The study of the dependence of the cross section on qtcut allows for an\nunderstanding of its behaviour at the boundaries of the phase space, giving\nhints on the structure at all orders in $\\alpha_s$ and on the identification of\nuniversal patterns. The knowledge of such power corrections is also a required\ningredient in order to reduce the dependence on the transverse-momentum cutoff\nof the QCD cross sections at higher orders, when the qt-subtraction method is\napplied.\n  We present analytic results for both Drell--Yan vector boson and Higgs boson\nproduction in gluon fusion and we illustrate a process-independent procedure\nfor the calculation of the all-order power corrections in the cutoff. In order\nto show the impact of the power-correction terms, we present selected numerical\nresults and discuss how the residual dependence on qtcut affects the total\ncross section for Drell--Yan Z production and Higgs boson production via gluon\nfusion at the LHC.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We address the intensively studied extended bosonic Hubbard model (EBHM) with\ntruncation of the on-site Hilbert space to the three lowest occupation states\nn=0,1,2 in frames of the S=1 pseudospin formalism. Similar model was recently\nproposed to describe the charge degree of freedom in a model high-Tc cuprate\nwith the on-site Hilbert space reduced to the three effective valence centers,\nnominally Cu^{1+;2+;3+} . With small corrections the model becomes equivalent\nto a strongly anisotropic S=1 quantum magnet in an external magnetic field. We\nhave applied a generalized mean-field approach and quantum Monte-Carlo\ntechnique for the model 2D S=1 system with a two-particle transport to find the\nground state phase with its evolution under deviation from half-filling.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A high theoretical efficiency of 47.2% was achieved by a novel combination of\nIn0.51Ga0.49P, GaAs, In0.24Ga0.76As and In0.19Ga0.81Sb subcell layers in a\nsimulated quadruple junction solar cell under 1 sun concentration. The\nelectronic bandgap of these materials are 1.9 eV, 1.42 eV, 1.08 eV and 0.55 eV\nrespectively. This unique arrangement enables the cell absorb photons from\nultraviolet to deep infrared wavelengths of the sunlight. Emitter and base\nthicknesses of the subcells and doping levels of the materials were optimized\nto maintain the same current in all the four junctions and to obtain the\nhighest conversion efficiency. The short-circuit current density, open circuit\nvoltage and fill factor of the solar cell are 14.7 mA/cm2, 3.38 V and 0.96\nrespectively. In our design, we considered 1 sun, AM 1.5 global solar spectrum.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a novel objective function for learning robust deep\nrepresentations of data based on information theory. Data is projected into a\nfeature-vector space such that the mutual information of all subsets of\nfeatures relative to the supervising signal is maximized. This objective\nfunction gives rise to robust representations by conserving available\ninformation relative to supervision in the face of noisy or unavailable\nfeatures. Although the objective function is not directly tractable, we are\nable to derive a surrogate objective function. Minimizing this surrogate loss\nencourages features to be non-redundant and conditionally independent relative\nto the supervising signal. To evaluate the quality of obtained solutions, we\nhave performed a set of preliminary experiments that show promising results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Translationnally invariant bidimensional magnetic Laplacians are considered.\nUsing an improved version of the harmonic approximation, we establish the\nabsence of point spectrum under various assumptions on the behavior of the\nmagnetic field.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Kernel functions may be used in robotics for comparing different poses of a\nrobot, such as in collision checking, inverse kinematics, and motion planning.\nThese comparisons provide distance metrics often based on joint space\nmeasurements and are performed hundreds or thousands of times a second,\ncontinuously for changing environments. Few examples exist in creating new\nkernels, despite their significant effect on computational performance and\nrobustness in robot control and planning. We introduce a new kernel function\nbased on forward kinematics (FK) to compare robot manipulator configurations.\nWe integrate our new FK kernel into our proxy collision checker, Fastron, that\npreviously showed significant speed improvements to collision checking and\nmotion planning. With the new FK kernel, we realize a two-fold speedup in proxy\ncollision check speed, 8 times less memory, and a boost in classification\naccuracy from 75% to over 95% for a 7 degrees-of-freedom robot arm compared to\nthe previously-used radial basis function kernel. Compared to state-of-the-art\ngeometric collision checkers, with the FK kernel, collision checks are now 9\ntimes faster. To show the broadness of the approach, we apply Fastron FK in\nOMPL across a wide variety of motion planners, showing unanimously faster robot\nplanning.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have evaluated the performance of a Ce-doped fused-silica fiber as\nwavelength shifter coupled to a CeF$_{3}$ crystal using electron beams at CERN.\nThe pulse shape and collection efficiency were measured using irradiated (100\nkGy) and un-irradiated fibers. In addition, we evaluated the light yield of\nvarious Ce-doped fibers and explored the possibility of using them in the\nfuture, including for precision timing applications in a high-luminosity\ncollider environment.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We observe and analyze tunable relaxation of a pure spin current by an\nantiferromagnet in spin-valves. This is achieved by carefully controlling the\nangle between a resonantly excited ferromagnetic layer pumping the spin current\nand the N\\'eel vector of the antiferromagnetic layer. The effect is observed as\nan angle-dependent spin-pumping contribution to the ferromagnetic resonance\nlinewidth. An interplay between spin-mixing conductance and, often disregarded,\nlongitudinal spin conductance is found to underlie our observations, which is\nin agreement with a recent prediction for related ferromagnetic spin valves.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We establish a family of sharp Sobolev trace inequalities involving the\n$W^{k,2}(\\mathbb{R}_+^{n+1},y^a)$-norm. These inequalities are closely related\nto the realization of fractional powers of the Laplacian on\n$\\mathbb{R}^n=\\partial\\mathbb{R}_+^{n+1}$ as generalized Dirichlet-to-Neumann\noperators associated to powers of the weighted Laplacian in upper half space,\ngeneralizing observations of Caffarelli--Silvestre and of Yang.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We review some current ideas about tripartite entanglement, the case\nrepresenting the next level of complexity beyond the simplest one (though far\nfrom trivial), namely the bipartite. This kind of entanglement has an essential\nrole in the understanding of foundations of quantum mechanics. Also, it allows\nseveral applications in the fields of quantum information processing and\nquantum computing. In this paper, we make a revision about the main\nfoundational aspects of tripartite entanglement and we discuss the possibility\nof using it as a resource to execute quantum protocols. We present some\nexamples of quantum protocols in detail.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The first digit law, also known as Benford's law or the significant digit\nlaw, is an empirical phenomenon that the leading digit of numbers from real\nworld sources favors small ones in a form $\\log(1+{1}/{d})$, where $d=1, 2,\n..., 9$. Such a law keeps elusive for over one hundred years because it was\nobscure whether this law is due to the logical consequence of the number system\nor some mysterious mechanism of the nature. We provide a simple and elegant\nproof of this law from the application of the Laplace transform, which is an\nimportant tool of mathematical methods in physics. We reveal that the first\ndigit law is originated from the basic property of the number system, thus it\nshould be attributed as a basic mathematical knowledge for wide applications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The General Antiparticle Spectrometer (GAPS) is an Antarctic balloon-borne\nmission to indirectly search for dark matter through sensitive observation of\ncosmic antiparticles. The first flight is planned for late 2021. GAPS is the\nfirst experiment optimized specifically for detection of low-energy (< 0.25\nGeV/n) antideuterons, which are recognized as distinctive signals from dark\nmatter annihilation or decay in the Galactic halo. To achieve high sensitivity\nto cosmic antinuclei in this low-energy range, GAPS uses a novel particle\nidentification method based on exotic atom capture and decay. The GAPS\ninstrument consists of ten planes of 1440 10 cm-diameter, 2.5 mm-thick, 8-strip\nlithium drifted silicon (Si(Li)) detectors, which constitutes the tracker,\nsurrounded by a plastic scintillator time-of-flight system. A new fabrication\ntechnique has been developed to satisfy the stringent requirements of the\nmission. In this contribution, we describe the front-end electronics of the\ntracker of GAPS. The system is composed of front-end ASICs and power supplies.\nThe ASICs provide readout and digitization of the signal (with an 11-bit ADC)\nin a wide dynamic range (10 keV - 100 MeV). Every ASIC has 32 channels and\nperforms the readout for 4 detectors, for a total amount of 11520 channels. The\nASIC analog front-end is based on a dynamic compression technique to handle a\nlarge range of signal amplitudes and features a low noise performance,\nachieving the required 4 keV resolution at low energies. The power system\nsupplies both bias voltages for the Si(Li) detectors and low voltages for the\nelectronics. 36th\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Vehicle-to-vehicle (V2V) communication plays a pivotal role in intelligent\ntransport systems (ITS) with cellular-vehicle to everything (C-V2X) and IEEE\n802.11p being the two competing enabling technologies. This paper presents\nmulti-dimensional discrete-time Markov chain (DTMC) based models to study the\nmedium access control (MAC) layer performance of the IEEE 802.11p standard and\nC-V2X Mode 4. These models are coupled with an appropriate DTMC based queuing\nmodel, and traffic generators for periodic cooperative awareness messages\n(CAMs) and event-driven decentralized environmental notification messages\n(DENMs). Closed-form solutions for the steady-state probabilities of the models\nare obtained, which are then utilized to derive expressions for several key\nperformance metrics. An application for a highway scenario is presented to\nprovide numerical results and to draw insights on the performance. In\nparticular, a performance comparison between IEEE 802.11p and C-V2X Mode 4 in\nterms of the average delay, the collision probability, and the channel\nutilization is presented. The results show that IEEE 802.11p is superior in\nterms of average delay, whereas C-V2X Mode 4 excels in collision resolution.\nThe paper also includes design insights on possible future MAC layer\nperformance enhancements of both standards.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We spectroscopically investigate a pathway for the conversion of\n$^{23}\\textrm{Na}^{39}\\textrm{K}$ Feshbach molecules into rovibronic ground\nstate molecules via STImulated Raman Adiabatic Passage (STIRAP). Using\nphotoassociation spectroscopy from the diatomic scattering threshold in the\n$a^3\\Sigma^+$ potential, we locate the resonantly mixed electronically excited\nintermediate states $|B^1\\Pi, v=8\\rangle$ and $|c^3\\Sigma^+, v=30\\rangle$\nwhich, due to their singlet-triplet admixture, serve as an ideal bridge between\npredominantly $a^3\\Sigma^+$ Feshbach molecules and pure $X^1\\Sigma^+$ ground\nstate molecules. We investigate their hyperfine structure and present a simple\nmodel to determine the singlet-triplet coupling of these states. Using\nAutler-Townes spectroscopy, we locate the rovibronic ground state of the\n$^{23}\\textrm{Na}^{39}\\textrm{K}$ molecule ($|X^1\\Sigma^+, v=0, N=0\\rangle$)\nand the second rotationally excited state $N=2$ to unambiguously identify the\nground state. We also extract the effective transition dipole moment from the\nexcited to the ground state. Our investigations result in a fully characterized\nscheme for the creation of ultracold bosonic $^{23}\\textrm{Na}^{39}\\textrm{K}$\nground state molecules.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Existing deep neural networks, say for image classification, have been shown\nto be vulnerable to adversarial images that can cause a DNN misclassification,\nwithout any perceptible change to an image. In this work, we propose shock\nabsorbing robust features such as binarization, e.g., rounding, and group\nextraction, e.g., color or shape, to augment the classification pipeline,\nresulting in more robust classifiers. Experimentally, we show that augmenting\nML models with these techniques leads to improved overall robustness on\nadversarial inputs as well as significant improvements in training time. On the\nMNIST dataset, we achieved 14x speedup in training time to obtain 90%\nadversarial accuracy com-pared to the state-of-the-art adversarial training\nmethod of Madry et al., as well as retained higher adversarial accuracy over a\nbroader range of attacks. We also find robustness improvements on traffic sign\nclassification using robust feature augmentation. Finally, we give theoretical\ninsights for why one can expect robust feature augmentation to reduce\nadversarial input space\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A well-known theorem of J.E. Hutchinson states that if an iterated function\nsystem consists of similarity transformations and satisfies the open set\ncondition then its attractor supports a self-similar measure with Hausdorff\ndimension equal to the similarity dimension. In this article we prove the\nfollowing result which may be regarded as a form of partial converse: if an\niterated function system consists of invertible affine transformations whose\nlinear parts do not preserve a common invariant subspace, and its attractor\nsupports a self-affine measure with Hausdorff dimension equal to the affinity\ndimension, then the system necessarily consists of similarity transformations.\nWe obtain this result by showing that the equilibrium measures of an affine\niterated function system are never Bernoulli measures unless the system either\nis reducible or consists of similarity transformations. The proof builds on\nearlier results in the thermodynamic formalism of affine iterated function\nsystems due to Bochi, Feng, K\\\"aenm\\\"aki, Shmerkin and the first named author\nand also relies on the work of Benoist on the spectral properties of\nZariski-dense subsemigroups of reductive linear groups.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The impact of surface vacancies and single adatoms on the magnetic properties\nof tetragonal {\\bf{L1}$_{0}$} CoPt thin films is investigated from first\nprinciples. We consider Co and Fe single adatoms deposited on a Pt-terminated\nthin film while a Pt adatom is assumed to be supported by a Co-terminated film.\nThe vacancy is injected in the top-surface layer of the films with both types\nof termination. After finding the most stable location of the defects, we\ndiscuss their magnetic properties tight to those of the substrate and\ninvestigate the magnetic crystalline anisotropy energy (MAE). Previous\nsimulations [Brahimi et al. J. Phys.: Condens. Matter. \\textbf{28}, 496002\n(2016)] predicted a large out-of-plane surface MAE for the Pt-terminated CoPt\nfilms (4 meV per f.u.) in contrast to in-plane surface MAE for Co-terminated\nfilms (-1 meV per f.u.). Here, we find that the surface MAE is significantly\nmodified upon the presence of the atomic defects. All investigated defects\ninduce an in-plane MAE, which is large enough for Fe adatom and Pt vacancy to\nswitch the surface MAE from out-of-plane to in-plane for the Pt-terminated\nfilms. Interestingly, among the investigated defects Pt vacancy has the largest\neffect on the MAE in contrast to Co vacancy, which induced the smallest but\nstill significant effect. This behavior is explained in terms of the orbital\nmoment anisotropy of the thin films.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Tensor network states and parton wave functions are two pivotal methods for\nstudying quantum many-body systems. This work connects these two subjects as we\ndemonstrate that a variety of parton wave functions, such as projected Fermi\nsea and projected fermionic or bosonic paired states, can be represented\nexactly as tensor networks. The results can be compressed into matrix product\nstates with moderate bond dimensions so various physical quantities can be\ncomputed efficiently. For the projected Fermi sea, we develop an excellent\ncompression scheme with high fidelity using maximally localized Wannier\norbitals. Numerical calculations on two parton wave functions demonstrate that\nour method exceeds commonly adopted Monte Carlo methods in some aspects. It\nproduces energy and correlation function with very high accuracy that is\ndifficult to achieve using Monte Carlo method. The entanglement measures that\nwere almost impossible to compute before can also be obtained easily using our\nmethod.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A method of approximating the inverse Radon transform on the plane by\nintegrating against a smooth kernel is investigated. For piecewise smooth\nintegrable functions, convergence theorems are proven and Gibbs phenomena are\nruled out. Geometric properties of the kernel and their implications for\ncomputer implementation are discussed. Suggestions are made for applications\nand an example is presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The flow taking place in the rear part of the fuselage during the emergency\nlanding on water is investigated experimentally in realistic conditions. To\nthis aim, tests on a double curvature specimen have been performed at\nhorizontal velocities ranging from 21 m/s to 45 m/s. Tests data highlight\ndifferent cavitation and/or ventilation modalities which are highly dependent\non the horizontal velocity, with substantial variations in the flow features\noccurring with velocity variations of few meters per second. For the specimen\nconsidered here, the inception of the cavitation is found at about 30 m/s,\nconfirming that scaled model tests performed at small horizontal velocities are\nunable to capture the hydrodynamics correctly. By comparing pressure data,\nunderwater movies and force measurements, it is shown that the transition from\ncavitation to ventilation condition has a significant effect of the\nlongitudinal distribution of the loading which, together with inertia,\naerodynamic loads and engine thrust, governs the aircraft dynamics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Quasi-one-dimensional lattice systems such as flux ladders with artificial\ngauge fields host rich quantum-phase diagrams that have attracted great\ninterest. However, so far, most of the work on these systems has concentrated\non zero-temperature phases while the corresponding finite-temperature regime\nremains largely unexplored. The question if and up to which temperature\ncharacteristic features of the zero-temperature phases persist is relevant in\nexperimental realizations. We investigate a two-leg ladder lattice in a uniform\nmagnetic field and concentrate our study on chiral edge currents and\nmomentum-distribution functions, which are key observables in ultracold\nquantum-gas experiments. These quantities are computed for hard-core bosons as\nwell as noninteracting bosons and spinless fermions at zero and finite\ntemperatures. We employ a matrix-product-state based purification approach for\nthe simulation of strongly interacting bosons at finite temperatures and\nanalyze finite-size effects. Our main results concern the\nvortex-fluid-to-Meissner crossover of strongly interacting bosons. We\ndemonstrate that signatures of the vortex-fluid phase can still be detected at\nelevated temperatures from characteristic finite-momentum maxima in the\nmomentum-distribution functions, while the vortex-fluid phase leaves weaker\nfingerprints in the local rung currents and the chiral edge current. In order\nto determine the range of temperatures over which these signatures can be\nobserved, we introduce a suitable measure for the contrast of these maxima. The\nresults are condensed into a finite-temperature crossover diagram for hard-core\nbosons.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The paper presents a model of lateral phase separation in a two component\nmaterial surface. The resulting fourth order nonlinear PDE can be seen as a\nCahn-Hilliard equation posed on a time-dependent surface. Only elementary\ntangential calculus and the embedding of the surface in $\\mathbb{R}^3$ are used\nto formulate the model, thereby facilitating the development of a fully\nEulerian discretization method to solve the problem numerically. A hybrid\nmethod, finite difference in time and trace finite element in space, is\nintroduced and stability of its semi-discrete version is proved. The method\navoids any triangulation of the surface and uses a surface-independent\nbackground mesh to discretize the equation. Thus, the method is capable of\nsolving the Cahn-Hilliard equation numerically on implicitly defined surfaces\nand surfaces undergoing strong deformations and topological transitions. We\nassess the approach on a set of test problems and apply it to model spinodal\ndecomposition and pattern formation on colliding surfaces. Finally, we consider\nthe phase separation on a sphere splitting into two droplets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Despite its widespread use in fiber optics, encoding quantum information in\nphotonic time-bin states is usually considered impractical for free-space\nquantum communication as turbulence-induced spatial distortion impedes the\nanalysis of time-bin states at the receiver. Here, we demonstrate quantum key\ndistribution using time-bin photonic states distorted by turbulence and\ndepolarization during free-space transmission. Utilizing a novel analyzer\napparatus, we observe stable quantum bit error ratios of 5.32 %, suitable for\ngenerating secure keys, despite significant wavefront distortions and\npolarization fluctuations across a 1.2 km channel. This shows the viability of\ntime-bin quantum communication over long-distance free-space channels, which\nwill simplify direct fiber/free-space interfaces and enable new approaches for\npractical free-space quantum communication over multi-mode, turbulent, or\ndepolarizing channels.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Features of low dimensional magnetism resulting from a square-net arrangement\nof Co atoms in trirutile CoTa$_2$O$_6$ is studied in the present work by means\nof density functional theory and is compared with the experimental results of\nspecific heat and neutron diffraction. The small total energy differences\nbetween the ferromagnetic (FM) and antiferromagnetic (AFM) configuration of\nCoTa$_2$O$_6$ shows that competing magnetic ground states exist, with the\npossibility of transition from FM to AFM phase at low temperature. Our\ncalculation further suggests the semi-conducting behavior for CoTa$_2$O$_6$\nwith a band gap of $\\sim$0.41 eV. The calculated magnetic anisotropy energy is\n$\\sim$2.5 meV with its easy axis along the [100] (in-plane) direction. Studying\nthe evolution of magnetism in Co$_{1-x}$Mg$_x$Ta$_2$O$_6$ (x = 0, 0.1, 0.3,\n0.5, 0.7 and 1). it is found that the sharp AFM transition exhibited by\nCoTa$_2$O$_6$ at $T_N$ = 6.2 K in its heat capacity vanishes with Mg-dilution,\nindicating the obvious effect of weakening the superexchange pathways of Co.\nThe current specific heat study reveals the robust nature of $T_N$ for\nCoTa$_2$O$_6$ in applied magnetic fields. Clear indication of short-range\nmagnetism is obtained from the magnetic entropy, however, diffuse components\nare absent in neutron diffraction data. At $T_N$, CoTa$_2$O$_6$ enters a\nlong-range ordered magnetic state which can be described using a propagation\nvector, (1/4, 1/4, 0). Upon Mg-dilution at $x \\geq$0.1, the long-range ordered\nmagnetism is destroyed. The present results should motivate an investigation of\nmagnetic excitations in this low-dimensional anisotropic magnet.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The state-of-the-art models for medical image segmentation are variants of\nU-Net and fully convolutional networks (FCN). Despite their success, these\nmodels have two limitations: (1) their optimal depth is apriori unknown,\nrequiring extensive architecture search or inefficient ensemble of models of\nvarying depths; and (2) their skip connections impose an unnecessarily\nrestrictive fusion scheme, forcing aggregation only at the same-scale feature\nmaps of the encoder and decoder sub-networks. To overcome these two\nlimitations, we propose UNet++, a new neural architecture for semantic and\ninstance segmentation, by (1) alleviating the unknown network depth with an\nefficient ensemble of U-Nets of varying depths, which partially share an\nencoder and co-learn simultaneously using deep supervision; (2) redesigning\nskip connections to aggregate features of varying semantic scales at the\ndecoder sub-networks, leading to a highly flexible feature fusion scheme; and\n(3) devising a pruning scheme to accelerate the inference speed of UNet++. We\nhave evaluated UNet++ using six different medical image segmentation datasets,\ncovering multiple imaging modalities such as computed tomography (CT), magnetic\nresonance imaging (MRI), and electron microscopy (EM), and demonstrating that\n(1) UNet++ consistently outperforms the baseline models for the task of\nsemantic segmentation across different datasets and backbone architectures; (2)\nUNet++ enhances segmentation quality of varying-size objects -- an improvement\nover the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design)\noutperforms the original Mask R-CNN for the task of instance segmentation; and\n(4) pruned UNet++ models achieve significant speedup while showing only modest\nperformance degradation. Our implementation and pre-trained models are\navailable at https://github.com/MrGiovanni/UNetPlusPlus.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Motivated by the endeavors of Li Xiang and You-Gen Shen on naked\nsingularities, we investigate the validity of the cosmic censorship conjecture\nin the context of generalized uncertainty principle. In particular, upon\nconsidering both linear and quadratic terms of momentum in the uncertainty\nprinciple, we first compute the entropy of a massless charged black hole in de\nSitter spacetime at a given modified temperature. Then, we compute the\ncorresponding modified cosmological radius and express the black hole electric\ncharge in terms of this modified cosmological radius and, thus, in terms of the\ngeneralized uncertainty principle parameter. Finally, we examine whether such a\nsystem will end up being a naked singularity or it might be protected by the\ncosmic censorship conjecture, and how that might be related to the possible\nexistence of massless charged particles.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Video understanding usually requires expensive computation that prohibits its\ndeployment, yet videos contain significant spatiotemporal redundancy that can\nbe exploited. In particular, operating directly on the motion vectors and\nresiduals in the compressed video domain can significantly accelerate the\ncompute, by not using the raw videos which demand colossal storage capacity.\nExisting methods approach this task as a multiple modalities problem. In this\npaper we are approaching the task in a completely different way; we are looking\nat the data from the compressed stream as a one unit clip and propose that the\nresidual frames can replace the original RGB frames from the raw domain.\nFurthermore, we are using teacher-student method to aid the network in the\ncompressed domain to mimic the teacher network in the raw domain. We show\nexperiments on three leading datasets (HMDB51, UCF1, and Kinetics) that\napproach state-of-the-art accuracy on raw video data by using compressed data.\nOur model MFCD-Net outperforms prior methods in the compressed domain and more\nimportantly, our model has 11X fewer parameters and 3X fewer Flops,\ndramatically improving the efficiency of video recognition inference. This\napproach enables applying neural networks exclusively in the compressed domain\nwithout compromising accuracy while accelerating performance.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Objective: We hypothesized that prenatal stress (PS) exerts lasting impact on\nfetal heart rate (fHR). We sought to validate the presence of such PS signature\nin fHR by measuring coupling between maternal HR (mHR) and fHR. Study design:\nProspective observational cohort study in stressed group (SG) mothers with\ncontrols matched for gestational age during screening at third trimester using\nCohen Perceived Stress Scale (PSS) questionnaire with PSS-10 equal or above 19\nclassified as SG. Women with PSS-10 less than 19 served as control group (CG).\nSetting: Klinikum rechts der Isar of the Technical University of Munich.\nPopulation: Singleton 3rd trimester pregnant women. Methods: Transabdominal\nfetal electrocardiograms (fECG) were recorded. We deployed a signal processing\nalgorithm termed bivariate phase-rectified signal averaging (BPRSA) to quantify\ncoupling between mHR and fHR resulting in a fetal stress index (FSI). Maternal\nhair cortisol was measured at birth. Differences were assumed to be significant\nfor p value less than 0.05. Main Outcome Measures: Differences for FSI between\nboth groups. Results: We screened 1500 women enrolling 538 of which 16.5 %\nshowed a PSS-10 score equal or above 19 at 34+0 weeks. Fifty five women\neventually comprised the SG and n=55 served as CG. Median PSS was 22.0 (IQR\n21.0-24.0) in the SG and 9.0 (6.0-12.0) in the CG, respectively. Maternal hair\ncortisol was higher in SG than CG at 86.6 (48.0-169.2) versus 53.0 (34.4-105.9)\npg/mg. At 36+5 weeks, FSI was significantly higher in fetuses of stressed\nmothers when compared to controls [0.43 (0.18-0.85) versus 0.00 (-0.49-0.18)].\nConclusion: Our findings show a persistent effect of PS affecting fetuses in\nthe last trimester.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  High-energy cosmic neutrinos carry unique information about the most\nenergetic non-thermal sources in the Universe. This white paper describes the\noutstanding astrophysics questions that neutrino astronomy can address in the\ncoming decade. A companion white paper discusses how the observation of cosmic\nneutrinos can address open questions in fundamental physics. Detailed\nmeasurements of the diffuse neutrino flux, measurements of neutrinos from point\nsources, and multi-messenger observations with neutrinos will enable the\ndiscovery and characterization of the most energetic sources in the Universe.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Adversarial training is by far the most successful strategy for improving\nrobustness of neural networks to adversarial attacks. Despite its success as a\ndefense mechanism, adversarial training fails to generalize well to unperturbed\ntest set. We hypothesize that this poor generalization is a consequence of\nadversarial training with uniform perturbation radius around every training\nsample. Samples close to decision boundary can be morphed into a different\nclass under a small perturbation budget, and enforcing large margins around\nthese samples produce poor decision boundaries that generalize poorly.\nMotivated by this hypothesis, we propose instance adaptive adversarial training\n-- a technique that enforces sample-specific perturbation margins around every\ntraining sample. We show that using our approach, test accuracy on unperturbed\nsamples improve with a marginal drop in robustness. Extensive experiments on\nCIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our\nproposed approach.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this papers we investigate the g-frame and Bessel g-sequence related to a\nlinear bounded operator $K$ in Hilbert $C^{\\ast}$-module and we establish some\nresults.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $G(q)$ be a finite group of Lie type over a field with $q$ elements,\nwhere $q$ is a prime power. The Green functions of $G(q)$, as defined by\nDeligne and Lusztig, are known in \\textit{almost} all cases by work of\nBeynon--Spaltenstein, Lusztig und Shoji. Open cases exist for groups of\nexceptional type ${^2\\!E}_6$, $E_7$, $E_8$ in small characteristics. We propose\na general method for dealing with these cases, which procedes by a reduction to\nthe case where $q$ is a prime and then uses computer algebra techniques. In\nthis way, all open cases in type ${^2\\!E}_6$, $E_7$ are solved, as well as at\nleast one particular open case in type $E_8$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study random sequential adsorption of particles from pool onto a one\ndimensional substrate following ballistic deposition rules, with separate\nnucleation and growth processes occurring simultaneously. Nucleation describes\nthe formation of point-sized seeds, and after a seed is sown, it acts as an\nattractor and grows in size by the addition of grains of a fixed-sized. At each\ntime step either an already-nucleated seed can increase in size, or a new seed\nmay be nucleated. We incorporate a parameter $m$, to describe the relative\nrates of growth to nucleation. We solve the model analytically to obtain gap\nsize distribution function and a general expression for the jamming coverage as\na function of $m$. We show that the jamming coverage $\\theta(m)$ reaches its\nmaximum value $\\theta(m)=1$ in the limit $m\\rightarrow \\infty$ following a\npower-law $\\theta(\\infty) - \\theta(m) \\sim m^{-1/2}$. We also perform extensive\nMonte Carlo simulation and find excellent agreement between analytic and\nnumerical results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Excitable pulses are among the most widespread dynamical patterns that occur\nin many different systems, ranging from biological cells to chemical reactions\nand ecological populations. Traditionally, the mutual annihilation of two\ncolliding pulses is regarded as their prototypical signature. Here we show that\ncolliding excitable pulses may exhibit soliton-like crossover and pulse\nnucleation if the system obeys a mass conservation constraint. In contrast to\nprevious observations in systems without mass conservation, these alternative\ncollision scenarios are robustly observed over a wide range of parameters. We\ndemonstrate our findings using a model of intracellular actin waves since, on\ntime scales of wave propagations over the cell scale, cells obey the\nconservation of actin monomers. The results provide a key concept to understand\nthe ubiquitous occurrence of actin waves in cells, suggesting why they are so\ncommon, and why their dynamics is robust and long-lived.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper infers from a generalized Picone identity the uniqueness of the\nstable positive solution for a class of semilinear equations of superlinear\nindefinite type, as well as the uniqueness and global attractivity of the\ncoexistence state in two generalized diffusive prototypes of the symbiotic and\ncompeting species models of Lotka-Volterra. The optimality of these uniqueness\ntheorems reveals the tremendous strength of the Picone identity.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Large-scale {\\it in vitro} drug sensitivity screens are an important tool in\npersonalized oncology to predict the effectiveness of potential cancer drugs.\nThe prediction of the sensitivity of cancer cell lines to a panel of drugs is a\nmultivariate regression problem with high-dimensional heterogeneous multi-omics\ndata as input data and with potentially strong correlations between the outcome\nvariables which represent the sensitivity to the different drugs. We propose a\njoint penalized regression approach with structured penalty terms which allow\nus to utilize the correlation structure between drugs with group-lasso-type\npenalties and at the same time address the heterogeneity between omics data\nsources by introducing data-source-specific penalty factors to penalize\ndifferent data sources differently. By combining integrative penalty factors\n(IPF) with tree-guided group lasso, we create the IPF-tree-lasso method. We\npresent a unified framework to transform more general IPF-type methods to the\noriginal penalized method. Because the structured penalty terms have multiple\nparameters, we demonstrate how the interval-search Efficient Parameter\nSelection via Global Optimization (EPSGO) algorithm can be used to optimize\nmultiple penalty parameters efficiently. Simulation studies show that\nIPF-tree-lasso can improve the prediction performance compared to other\nlasso-type methods, in particular for heterogenous data sources. Finally, we\nemploy the new methods to analyse data from the Genomics of Drug Sensitivity in\nCancer project.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Face aging is of great importance for cross-age recognition and\nentertainment-related applications. Recently, conditional generative\nadversarial networks (cGANs) have achieved impressive results for face aging.\nExisting cGANs-based methods usually require a pixel-wise loss to keep the\nidentity and background consistent. However, minimizing the pixel-wise loss\nbetween the input and synthesized images likely resulting in a ghosted or\nblurry face. To address this deficiency, this paper introduces an Attention\nConditional GANs (AcGANs) approach for face aging, which utilizes attention\nmechanism to only alert the regions relevant to face aging. In doing so, the\nsynthesized face can well preserve the background information and personal\nidentity without using the pixel-wise loss, and the ghost artifacts and\nblurriness can be significantly reduced. Based on the benchmarked dataset\nMorph, both qualitative and quantitative experiment results demonstrate\nsuperior performance over existing algorithms in terms of image quality,\npersonal identity, and age accuracy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper considers and proposes some algorithms to compute the mean\ncurvature flow under topological changes. Instead of solving the fully\nnonlinear partial differential equations based on the level set approach, we\npropose some minimization algorithms based on the phase field approach. It is\nwell known that zero-level set of the Allen-Cahn equation approaches the mean\ncurvature flow before the onset of the topological changes; however, there are\nfew papers systematically studying the evolution of the mean curvature flow\nunder the topological changes. There are three main contributions of this\npaper. First, in order to consider various random initial conditions, we design\nseveral benchmark problems with topological changes, and we find different\npatterns of the evolutions of the solutions can be obtained if the interaction\nlength (width of the interface) is slightly changed, which is different from\nthe problems without topological changes. Second, we propose an energy\npenalized minimization algorithm which works very well for these benchmark\nproblems, and thus furthermore, for the problems with random initial\nconditions. Third, we propose a multilevel minimization algorithm. This\nalgorithm is shown to be more tolerant of the unsatisfying initial guess when\nthere are and there are no topological changes in the evolutions of the\nsolutions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Accurate approximation of the sampling distribution of nonparametric kernel\ndensity estimators is crucial for many statistical inference problems. Since\nthese estimators have complex asymptotic distributions, bootstrap methods are\noften used for this purpose. With i.i.d. observations, a large literature\nexists concerning optimal bootstrap methods which achieve the fastest possible\nconvergence rate of the bootstrap estimator of the sampling distribution of the\nkernel density estimator. With dependent data, such an optimality theory is an\nimportant open problem. We establish a general theory of optimality of the\nblock bootstrap for kernel density estimation under weak dependence assumptions\nwhich are satisfied by many important time series models. We propose a unified\nframework for a theoretical study of a rich class of bootstrap methods which\ninclude as special cases subsampling, Kunsch's moving block bootstrap, Hall's\nunder-smoothing (UNS) as well as approaches incorporating no (NBC) or explicit\nbias correction (EBC). Moreover, we consider their accuracy under a broad\nspectrum of choices of the bandwidth $h$, which include as an important special\ncase the MSE-optimal choice, as well as other under-smoothed choices. Under\neach choice of $h$, we derive the optimal tuning parameters and compare optimal\nperformances between the main subclasses (EBC, NBC, UNS) of the bootstrap\nmethods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Gravitational lensing is a powerful tool for quantifying the mass content and\ndistribution in distant galaxies. By using milliarcsecond angular resolution\nobservations of radio-loud gravitationally lensed sources it is also possible\nto detect and quantify small deviations from a smooth mass density\ndistribution, which can be due to low mass substructures in the lensing galaxy.\nWe present high-resolution global VLBI observations of the gravitationally\nlensed radio source MG J0751+2716 (at z = 3.2), that shows evidence of both\ncompact and extended structure (core-jet morphology) across several\ngravitational arcs. These data provide a wealth of observational constraints\nthat are used to determine the inner (baryonic and dark matter) mass profile of\na group of galaxies and also investigate the smoothness of the dark matter\ndistribution on mas-scales, which is sensitive to possible structures of\n$10^{6-7}$ M$_{\\odot}$ within the lensing halo or along the line-of-sight. Our\nlens modelling finds evidence for astrometric anomalies in this system, which\nsuggest presence of extra mass structure in the lens model. To date this kind\nof detailed studies of gravitational lensing systems like MG J0751+2716 has\nbeen limited by the currently small sample of radio-loud gravitational lenses.\nIn this context, we also present a new pilot gravitational lens search in the\nVLBI survey mJIVE-20, in perspective of future surveys with the next generation\nof radio interferometers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper investigates the symbol error probability~(SEP) of point-to-point\nmassive multiple-input multiple-output (MIMO) systems using equally likely PAM,\nPSK, and square QAM signallings in the presence of transmitter correlation. The\nreceiver has perfect knowledge of the channel coefficients, while the\ntransmitter only knows first- and second-order channel statistics. With a\nzero-forcing~(ZF) detector implemented at the receiver side, we design and\nderive closed-form expressions of the optimal precoders at the transmitter that\nminimizes the average SEP over channel statistics for various modulation\nschemes. We then unveil some nice structures on the resulting minimum average\nSEP expressions, which naturally motivate us to explore the use of two useful\nmathematical tools to systematically study their asymptotic behaviors. The\nfirst tool is the Szeg\\\"o's theorem on large Hermitian Toeplitz matrices and\nthe second tool is the well-known limit: $\\lim_{x\\to\\infty}(1+1/x)^x=e$. The\napplication of these two tools enables us to attain very simple expressions of\nthe SEP limits as the number of the transmitter antennas goes to infinity. A\nmajor advantage of our asymptotic analysis is that the asymptotic SEP converges\nto the true SEP when the number of antennas is moderately large. As such, the\nobtained expressions can serve as effective SEP approximations for massive MIMO\nsystems even when the number of antennas is not very large. For the widely used\nexponential correlation model, we derive closed-form expressions for the SEP\nlimits of both optimally precoded and uniformly precoded systems. Extensive\nsimulations are provided to demonstrate the effectiveness of our asymptotic\nanalysis and compare the performance limit of optimally precoded and uniformly\nprecoded systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper studies a multi-hop decode-and-forward (DF) simultaneous wireless\ninformation and power transfer (SWIPT) system where a source sends data to a\ndestination with the aid of multi-hop relays which do not depend on an external\nenergy source. To this end, we apply power splitting (PS) based SWIPT relaying\nprotocol so that the relays can harvest energy from the received signals from\nthe previous hop to reliably forward the information of the source to the\ndestination. We aim to solve two optimization problems relevant to our system\nmodel. First, we minimize the transmit power at the source under the individual\nquality-of-service (QoS) threshold constraints of the relays and the\ndestination nodes by optimizing PS ratios at the relays. The second is to\nmaximize the minimum system achievable rate by optimizing the PS ratio at each\nrelay. Based on convex optimization techniques, the globally optimal PS ratio\nsolution is obtained in closed-form for both problems. By setting the QoS\nthreshold constraint the same for each node for the source transmit power\nproblem, we discovered that either the minimum source transmit power or the\nmaximum system throughput can be found using the same approach. Numerical\nresults demonstrate the superiority of the proposed optimal SWIPT PS design\nover conventional fixed PS ratio schemes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We discuss the deformed function algebra of a simply connected reductive Lie\ngroup G over the complex numbers using a basis consisting of matrix elements of\nfinite dimensional representations. This leads to a preferred deformation,\nmeaning one where the structure constants of comultiplication are unchanged.\nThe structure constants of multiplication are controlled by quantum 3j symbols.\nWe then discuss connections earlier work on preferred deformations that\ninvolved Schur-Weyl duality.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Different from other multiple top-quark productions, triple top-quark\nproduction requires the presence of both flavor violating neutral interaction\nand flavor conserving neutral interaction. We describe the interaction of\ntriple top-quarks and up-quark in terms of two dimension-6 operators; one can\nbe induced by a new heavy vector resonance, the other by a scalar resonance.\nCombining same-sign top-quark pair production and four top-quark production, we\nexplore the potential of the 13 TeV LHC on searching for the triple top-quark\nproduction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Companies like Google and Microsoft run billions of auctions every day to\nsell advertising opportunities. Any change to the rules of these auctions can\nhave a tremendous effect on the revenue of the company and the welfare of the\nadvertisers and the users. Therefore, any change requires careful evaluation of\nits potential impacts. Currently, such impacts are often evaluated by running\nsimulations or small controlled experiments. This, however, misses the\nimportant factor that the advertisers respond to changes. Our goal is to build\na theoretical framework for predicting the actions of an agent (the advertiser)\nthat is optimizing her actions in an uncertain environment. We model this\nproblem using a variant of the multi-armed bandit setting where playing an arm\nis costly. The cost of each arm changes over time and is publicly observable.\nThe value of playing an arm is drawn stochastically from a static distribution\nand is observed by the agent and not by us. We, however, observe the actions of\nthe agent. Our main result is that assuming the agent is playing a strategy\nwith a regret of at most $f(T)$ within the first $T$ rounds, we can learn to\nplay the multi-armed bandits game (without observing the rewards) in such a way\nthat the regret of our selected actions is at most $O(k^4(f(T)+1)\\log(T))$,\nwhere $k$ is the number of arms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent years have seen a boom in interest in machine learning systems that\ncan provide a human-understandable rationale for their predictions or\ndecisions. However, exactly what kinds of explanation are truly\nhuman-interpretable remains poorly understood. This work advances our\nunderstanding of what makes explanations interpretable under three specific\ntasks that users may perform with machine learning systems: simulation of the\nresponse, verification of a suggested response, and determining whether the\ncorrectness of a suggested response changes under a change to the inputs.\nThrough carefully controlled human-subject experiments, we identify\nregularizers that can be used to optimize for the interpretability of machine\nlearning systems. Our results show that the type of complexity matters:\ncognitive chunks (newly defined concepts) affect performance more than variable\nrepetitions, and these trends are consistent across tasks and domains. This\nsuggests that there may exist some common design principles for explanation\nsystems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we study the asymptotics of the Korteweg--de Vries (KdV)\nequation with steplike initial data, which leads to shock waves, in the middle\nregion between the dispersive tail and the soliton region, as $t \\rightarrow\n\\infty$. In our previous work we have dealt with this question, but failed to\nobtain uniform estimates in $x$ and $t$ because of the previously unknown\nsingular behaviour of the matrix model solution. The main goal of this paper is\nto close this gap. We present an alternative approach to the usual argument\ninvolving a small norm Riemann--Hilbert (R-H) problem, which is based instead\non Fredholm index theory for singular integral operators. In particular, we\navoid the construction of a global model matrix solution, which would be\nsingular for arbitrary large $x$ and $t$, and utilize only the symmetric model\nvector solution, which always exists and is unique.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Next-generation radio surveys will yield an unprecedented amount of data,\nwarranting analysis by use of machine learning techniques. Convolutional neural\nnetworks are the deep learning technique that has proven to be the most\nsuccessful in classifying image data. Capsule networks are a more recently\ndeveloped technique that use capsules comprised of groups of neurons, that\ndescribe properties of an image including the relative spatial locations of\nfeatures. The current work explores the performance of different capsule\nnetwork architectures against simpler convolutional neural network\narchitectures, in reproducing the classifications into the classes of\nunresolved, FRI and FRII morphologies. We utilise images from a LOFAR survey\nwhich is the deepest, wide-area radio survey to date, revealing more complex\nradio-source structures compared to previous surveys, presenting further\nchallenges for machine learning algorithms. The 4- and 8-layer convolutional\nnetworks attain an average precision of 93.3% and 94.3% respectively, compared\nto 89.7% obtained with the capsule network, when training on original and\naugmented images. Implementing transfer learning achieves a precision of 94.4%,\nthat is within the confidence interval of the 8-layer convolutional network.\nThe convolutional networks always outperform any variation of the capsule\nnetwork, as they prove to be more robust to the presence of noise in images.\nThe use of pooling appears to allow more freedom for the intra-class\nvariability of radio galaxy morphologies, as well as reducing the impact of\nnoise.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $\\A$ be an abelian category having enough projective objects and enough\ninjective objects. We prove that if $\\A$ admits an additive generating object,\nthen the extension dimension and the weak resolution dimension of $\\A$ are\nidentical, and they are at most the representation dimension of $\\A$ minus two.\nBy using it, for a right Morita ring $\\La$, we establish the relation between\nthe extension dimension of the category $\\mod \\La$ of finitely generated right\n$\\Lambda$-modules and the representation dimension as well as the right global\ndimension of $\\Lambda$. In particular, we give an upper bound for the extension\ndimension of $\\mod \\Lambda$ in terms of the projective dimension of certain\nclass of simple right $\\Lambda$-modules and the radical layer length of\n$\\Lambda$. In addition, we investigate the behavior of the extension dimension\nunder some ring extensions and recollements.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Rhythmic activity has been associated with a wide range of cognitive\nprocesses. Previous studies have shown that spike-timing-dependent plasticity\ncan facilitate the transfer of rhythmic activity downstream the information\nprocessing pathway. However, STDP has also been known to generate strong\nwinner-take-all like competitions between subgroups of correlated synaptic\ninputs. Consequently, one might expect that STDP would induce strong\ncompetition between different rhythmicity channels thus preventing the\nmultiplexing of information across different frequency channels. This study\nexplored whether STDP facilitates the multiplexing of information across\nmultiple frequency channels, and if so, under what conditions. We investigated\nthe STDP dynamics in the framework of a model consisting of two competing\nsubpopulations of neurons that synapse in a feedforward manner onto a single\npostsynaptic neuron. Each sub-population was assumed to oscillate in an\nindependent manner and in a different frequency band. To investigate the STDP\ndynamics, a mean field Fokker-Planck theory was developed in the limit of the\nslow learning rate. Surprisingly, our theory predicted limited interactions\nbetween the different sub-groups. Our analysis further revealed that the\ninteraction between these channels was mainly mediated by the shared component\nof the mean activity. Next, we generalized these results beyond the simplistic\nmodel using numerical simulations. We found that for a wide range of\nparameters, the system converged to a solution in which the post-synaptic\nneuron responded to both rhythms. Nevertheless, all the synaptic weights\nremained dynamic and did not converge to a fixed point. These findings imply\nthat STDP can support the multiplexing of rhythmic information and demonstrate\nhow functionality can be retained in the face of continuous remodeling of all\nthe synaptic weights.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A range of nonlinear wave structures, including Langmuir waves, unipolar\nelectric fields and bipolar electric fields, are often observed in association\nwith whistler-mode chorus waves in the near-Earth space. We demonstrate that\nthe three seemingly different nonlinear wave structures originate from the same\nnonlinear electron trapping process by whistler-mode chorus waves. The ratio of\nthe Landau resonant velocity to the electron thermal velocity controls the type\nof nonlinear wave structures that will be generated.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Stochastic multi-armed bandits form a class of online learning problems that\nhave important applications in online recommendation systems, adaptive medical\ntreatment, and many others. Even though potential attacks against these\nlearning algorithms may hijack their behavior, causing catastrophic loss in\nreal-world applications, little is known about adversarial attacks on bandit\nalgorithms. In this paper, we propose a framework of offline attacks on bandit\nalgorithms and study convex optimization based attacks on several popular\nbandit algorithms. We show that the attacker can force the bandit algorithm to\npull a target arm with high probability by a slight manipulation of the rewards\nin the data. Then we study a form of online attacks on bandit algorithms and\npropose an adaptive attack strategy against any bandit algorithm without the\nknowledge of the bandit algorithm. Our adaptive attack strategy can hijack the\nbehavior of the bandit algorithm to suffer a linear regret with only a\nlogarithmic cost to the attacker. Our results demonstrate a significant\nsecurity threat to stochastic bandits.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a fast and robust framework to prepare non-classical states of a\nbosonic mode exploiting a coherent exchange of excitations with a two-level\nsystem ruled by a Jaynes-Cummings interaction mechanism. Our protocol, which is\nbuilt on shortcuts to adiabaticity, allows for the generation of arbitrary Fock\nstates of the bosonic mode, as well as coherent quantum superpositions of a\nSchr\\\"odinger cat-like form. In addition, we show how to obtain a class of\nphoton-shifted states where the vacuum population is removed, a result akin to\nphoton addition, but displaying more non-classicality than standard\nphoton-added states. Owing to the ubiquity of the spin-boson interaction that\nwe consider, our proposal is amenable for implementations in state-of-the-art\nexperiments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper describes the UM-IU@LING's system for the SemEval 2019 Task 6:\nOffensEval. We take a mixed approach to identify and categorize hate speech in\nsocial media. In subtask A, we fine-tuned a BERT based classifier to detect\nabusive content in tweets, achieving a macro F1 score of 0.8136 on the test\ndata, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C,\nwe used a linear SVM with selected character n-gram features. For subtask C,\nour system could identify the target of abuse with a macro F1 score of 0.5243,\nranking it 27th out of 65 submissions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  There has been a recent surge of interest in modeling neural networks (NNs)\nas Gaussian processes. In the limit of a NN of infinite width the NN becomes\nequivalent to a Gaussian process. Here we demonstrate that for an ensemble of\nlarge, finite, fully connected networks with a single hidden layer the\ndistribution of outputs at initialization is well described by a Gaussian\nperturbed by the fourth Hermite polynomial for weights drawn from a symmetric\ndistribution. We show that the scale of the perturbation is inversely\nproportional to the number of units in the NN and that higher order terms decay\nmore rapidly, thereby recovering the Edgeworth expansion. We conclude by\nobserving that understanding how this perturbation changes under training would\nreveal the regimes in which the Gaussian process framework is valid to model NN\nbehavior.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce QuESTlink, pronounced \"quest link\", an open-source Mathematica\npackage which efficiently emulates quantum computers. By integrating with the\nQuantum Exact Simulation Toolkit (QuEST), QuESTlink offers a high-level,\nexpressive and usable interface to a high-performance, hardware-accelerated\nemulator. Requiring no installation, QuESTlink streamlines the powerful\nanalysis capabilities of Mathematica into the study of quantum systems, even\nutilising remote multicore and GPU hardware. We demonstrate the use of\nQuESTlink to concisely and efficiently simulate several quantum algorithms, and\npresent some comparative benchmarking against core QuEST.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we study the vector-valued analogues of several inequalities\nfor the Fourier transform. In particular, we consider the inequalities of\nHausdorff--Young, Hardy--Littlewood, Paley, Pitt, Bochkarev and Zygmund. The\nPitt inequalities include the Hausdorff--Young and Hardy--Littlewood\ninequalities and state that the Fourier transform is bounded from\n$L^p(\\mathbb{R}^d,|\\cdot|^{\\beta p})$ into $L^q(\\mathbb{R}^d,|\\cdot|^{-\\gamma\nq})$ under certain condition on $p,q,\\beta$ and $\\gamma$. Vector-valued\nanalogues are derived under geometric conditions on the underlying Banach space\nsuch as Fourier type and related geometric properties. Similar results are\nderived for $\\mathbb{T}^d$ and $\\mathbb{Z}^d$ by a transference argument. We\nprove sharpness of our results by providing elementary examples on\n$\\ell^p$-spaces. Moreover, connections with Rademacher (co)type are discussed\nas well.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose the pure natural quintessential inflation model which is motivated\nby Witten's conjecture, where the axion couples to pure Yang-Mills $SU(N)$\ngauge field at large N limit. This modifies the standard cosine potential which\nis presented in the natural inflation, making it compatible with current CMB\ndata. Our model gives a successful inflation as well as acceleration at the\nlate times by quintessence inflaton field($\\phi$). Here the inflaton field is\nresponsible for inflation, after that the field enters into peculiar type of\nreheating and then they act as dynamical dark energy field which follows the\nsame inflation potential and same model parameters. The dynamical field slowly\nrolls until the Hubble drops to mass of the quintessence field and it reaches\nthe current dark energy field value. Here the dark energy scale is field\ndependent. Our quintessence model follows thawing frozen approach therefore the\nfrozen quintessence field evolves with respect to cosmic time from initial\nfield value($\\phi_{i}$) to present non-zero minimum field value($\\phi_{0}$).\nThe obtained field value turned into ultra-light and it satisfies the present\ndark energy density which is $V(\\phi_{0}) \\approx \\Lambda_{\\text{DE}}^{4} =\n(2.3 \\times 10^{-3}\\text{eV})^{4}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The evolution of a closed two-dimensional surface driven by both mean\ncurvature flow and a reaction--diffusion process on the surface is formulated\ninto a system, which couples the velocity law not only to the surface partial\ndifferential equation but also to the evolution equations for the geometric\nquantities, namely the normal vector and the mean curvature on the surface. Two\nalgorithms are considered for the obtained system. Both methods combine surface\nfinite elements as a space discretisation and linearly implicit backward\ndifference formulae for time integration. Based on our recent results for mean\ncurvature flow, one of the algorithms directly admits a convergence proof for\nits full discretisation in the case of finite elements of polynomial degree at\nleast two and backward difference formulae of orders two to five. Numerical\nexamples are provided to support and complement the theoretical convergence\nresults (demonstrating the convergence properties of the method without error\nestimate), and demonstrate the effectiveness of the methods in simulating a\nthree-dimensional tumour growth model.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Thin nanomaterials are key constituents of modern quantum technologies and\nmaterials research. Identifying specimens of these materials with properties\nrequired for the development of state of the art quantum devices is usually a\ncomplex and lengthy human task. In this work we provide a neural-network driven\nsolution that allows for accurate and efficient scanning, data-processing and\nsample identification of experimentally relevant two-dimensional materials. We\nshow how to approach classification of imperfect imbalanced data sets using an\niterative application of multiple noisy neural networks. We embed the trained\nclassifier into a comprehensive solution for end-to-end automatized data\nprocessing and sample identification.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Insulating uniaxial room-temperature ferromagnets are a prerequisite for\ncommonplace spin wave-based devices, the obstacle in contemporary ferromagnets\nbeing the coupling of ferromagnetism with large conductivity. We show that the\nuniaxial $A^{1+2x}$Ti$^{4+}$$_{1-x}$O$_3$ (ATO), $A=$Ni$^{2+}$,Co$^{2+}$ and\n$0.6<x \\leq 1$, thin films are electrically insulating ferromagnets already at\nroom-temperature. The octahedra network of the ATO and ilmenite structures are\nsimilar yet different octahedra-filling proved to be a route to switch from the\nantiferromagnetic to ferromagnetic regime. Octahedra can continuously be filled\nup to $x=1$, or vacated $(-0.24<x<0)$ in the ATO structure. TiO-layers, which\nseparate the ferromagnetic (Ni,Co)O-layers and intermediate the\nantiferromagnetic coupling between the ferromagnetic layers in the NiTiO$_3$\nand CoTiO$_3$ ilmenites, can continuously be replaced by (Ni,Co)O-layers to\nconvert the ATO-films to ferromagnetic insulator with abundant direct cation\ninteractions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present the first detailed study of the spatially-resolved dust continuum\nemission of simulated galaxies at 1<z<5. We run the radiative transfer code\nSKIRT on a sample of submillimeter-bright galaxies drawn from the Feedback in\nRealistic Environments (FIRE) project. These simulated galaxies reach Milky Way\nmasses by z=2. Our modelling provides predictions for the full rest-frame\nfar-ultraviolet-to-far-infrared spectral energy distributions of these\nsimulated galaxies, as well as 25-pc-resolution maps of their emission across\nthe wavelength spectrum. The derived morphologies are notably different in\ndifferent wavebands, with the same galaxy often appearing clumpy and extended\nin the far-ultraviolet yet an ordered spiral at far-infrared wavelengths. The\nobserved-frame 870-$\\mu$m half-light radii of our FIRE-2 galaxies are\n~0.5-4kpc, consistent with existing ALMA observations of galaxies with\nsimilarly high redshifts and stellar masses. In both simulated and observed\ngalaxies, the dust continuum emission is generally more compact than the cold\ngas and the dust mass, but more extended than the stellar component. The most\nextreme cases of compact dust emission seem to be driven by particularly\ncompact recent star-formation, which generates steep dust temperature\ngradients. Our results confirm that the spatial extent of the dust continuum\nemission is sensitive to both the dust mass and SFR distributions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For p=5,7, we show that a subset \\(E \\subset \\F_p^3\\) is spectral if and only\nif E tiles \\(\\F_p^3\\) by translation. Additionally, we give an alternate proof\nthat the conjecture holds for p=3.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, an evolutionary art project is presented where images are\napproximated by transparent, overlapping and geometric shapes of different\ntypes, e.g., polygons, circles, lines. Genotypes representing features and\norder of the geometric shapes are evolved with a fitness function that has the\ncorresponding pixels of an input image as a target goal. A\ngenotype-to-phenotype mapping is therefore applied to render images, as the\nchosen genetic representation is indirect, i.e., genotypes do not include\npixels but a combination of shapes with their properties. Different\ncombinations of shapes, quantity of shapes, mutation types and populations are\ntested. The goal of the work herein is twofold: (1) to approximate images as\nprecisely as possible with evolved indirect encodings, (2) to produce visually\nappealing results and novel artistic styles.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider a Fog Radio Access Network (F-RAN) with a Base Band Unit (BBU) in\nthe cloud and multiple cache-enabled enhanced Remote Radio Heads (eRRHs). The\nsystem aims at delivering contents on demand with minimal average latency from\na time-varying library of popular contents. Information about uncached\nrequested files can be transferred from the cloud to the eRRHs by following\neither backhaul or fronthaul modes. The backhaul mode transfers fractions of\nthe requested files, while the fronthaul mode transmits quantized baseband\nsamples as in Cloud-RAN (C-RAN). The backhaul mode allows the caches of the\neRRHs to be updated, which may lower future delivery latencies. In contrast,\nthe fronthaul mode enables cooperative C-RAN transmissions that may reduce the\ncurrent delivery latency. Taking into account the trade-off between current and\nfuture delivery performance, this paper proposes an adaptive selection method\nbetween the two delivery modes to minimize the long-term delivery latency.\nAssuming an unknown and time-varying popularity model, the method is based on\nmodel-free Reinforcement Learning (RL). Numerical results confirm the\neffectiveness of the proposed RL scheme.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Near-ambient pressure XPS and STM experiments are performed to study the\nintercalation of oxygen and nitrogen at different partial gas pressures and\ndifferent temperatures in the graphene/Ni/Ir(111) system of different\nmorphologies. We performed detailed experiments on the investigation of the\nchemical state and topography of graphene, before and after gas intercalation,\ndepending on the amount of pre-intercalated Ni in graphene/Ir(111). It is found\nthat only oxygen can be intercalated under graphene in all considered cases,\nindicating the role of the intra-molecular bonding strength and possibility of\ngas molecules dissociation on different metallic surfaces on the principal\npossibility and on the mechanism of intercalation of different species under\ngraphene.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We provide a general framework for characterizing the trade-off between\naccuracy and robustness in supervised learning. We propose a method and define\nquantities to characterize the trade-off between accuracy and robustness for a\ngiven architecture, and provide theoretical insight into the trade-off.\nSpecifically we introduce a simple trade-off curve, define and study an\ninfluence function that captures the sensitivity, under adversarial attack, of\nthe optima of a given loss function. We further show how adversarial training\nregularizes the parameters in an over-parameterized linear model, recovering\nthe LASSO and ridge regression as special cases, which also allows us to\ntheoretically analyze the behavior of the trade-off curve. In experiments, we\ndemonstrate the corresponding trade-off curves of neural networks and how they\nvary with respect to factors such as number of layers, neurons, and across\ndifferent network structures. Such information provides a useful guideline to\narchitecture selection.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent efforts show that neural networks are vulnerable to small but\nintentional perturbations on input features in visual classification tasks. Due\nto the additional consideration of connections between examples (\\eg articles\nwith citation link tend to be in the same class), graph neural networks could\nbe more sensitive to the perturbations, since the perturbations from connected\nexamples exacerbate the impact on a target example. Adversarial Training (AT),\na dynamic regularization technique, can resist the worst-case perturbations on\ninput features and is a promising choice to improve model robustness and\ngeneralization. However, existing AT methods focus on standard classification,\nbeing less effective when training models on graph since it does not model the\nimpact from connected examples.\n  In this work, we explore adversarial training on graph, aiming to improve the\nrobustness and generalization of models learned on graph. We propose Graph\nAdversarial Training (GraphAT), which takes the impact from connected examples\ninto account when learning to construct and resist perturbations. We give a\ngeneral formulation of GraphAT, which can be seen as a dynamic regularization\nscheme based on the graph structure. To demonstrate the utility of GraphAT, we\nemploy it on a state-of-the-art graph neural network model --- Graph\nConvolutional Network (GCN). We conduct experiments on two citation graphs\n(Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness\nof GraphAT which outperforms normal training on GCN by 4.51% in node\nclassification accuracy. Codes are available via:\nhttps://github.com/fulifeng/GraphAT.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  {\\it Caenorhabditis elegans} nematode worms are the only animals with the\nknown detailed neural connectivity diagram, well characterized genomics, and\nrelatively simple quantifiable behavioral output. With this in mind, many\nresearchers view this animal as the best candidate for a systems biology\napproach, where one can integrate molecular and cellular knowledge to gain\nglobal understanding of worm's behavior. This work reviews some research in\nthis direction, emphasizing computational perspective, and points out some\nsuccesses and challenges to meet this lofty goal.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With growing security threats to the evolving wireless systems, protecting\nuser privacy becomes progressively challenging. Even if the transmitted\ninformation is encrypted and the potential wiretap channel is physically\nlimited (e.g. through information-theoretic security approaches), the raw data\nitself, such as transmitter position and transmission pattern, could expose\nconfidential information. In this context, covert communication that intends to\nhide the existence of transmission from an observant adversary by exploiting\nthe physical characteristics of the wireless medium has been actively\ninvestigated. However, existing covertness techniques ineluctably consume\nadditional resources such as bandwidth and energy, which burdens system\ndeployment. In view of this concern, we propose an intelligent reflecting\nsurface (IRS)-based approach to enhance communication covertness. The core idea\nis making use of a smartly controlled metasurface to reshape undesirable\npropagation conditions which could divulge secret messages. To facilitate the\nunderstanding of the proposed idea, we first provide an overview of the\nstate-of-the-art covert communication techniques. Then, we introduce the\nfundamentals of IRS and elaborate on how an IRS can be integrated to benefit\ncommunication covertness. We also demonstrate a case study of the joint\nconfiguration of the IRS and the legitimate transmitter, which is of pivotal\nimportance in designing an IRS-enhanced covert communication system. Finally,\nwe shed light on some open research directions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We develop a systematic approach for constructing symmetry-based indicators\nof a topological classification for superconducting systems. The topological\ninvariants constructed in this work form a complete set of symmetry-based\nindicators that can be computed from knowledge of the Bogoliubov-de Gennes\nHamiltonian on high-symmetry points in Brillouin zone. After excluding\ntopological invariants corresponding to the phases without boundary signatures,\nwe arrive at natural generalization of symmetry-based indicators [H. C. Po, A.\nVishwanath, and H. Watanabe, Nature Comm. 8, 50 (2017)] to Hamiltonians of\nBogoliubov-de Gennes type.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The past few years have seen rapid progress in the development of service\nrobots. Universities and companies alike have launched major research efforts\ntoward the deployment of ambitious systems designed to aid human operators\nperforming a variety of tasks. These robots are intended to make those who may\notherwise need to live in assisted care facilities more independent, to help\nworkers perform their jobs, or simply to make life more convenient. Service\nrobots provide a powerful platform on which to study Artificial Intelligence\n(AI) and Human-Robot Interaction (HRI) in the real world. Research sitting at\nthe intersection of AI and HRI is crucial to the success of service robots if\nthey are to fulfill their mission.\n  This symposium seeks to highlight research enabling robots to effectively\ninteract with people autonomously while modeling, planning, and reasoning about\nthe environment that the robot operates in and the tasks that it must perform.\nAI-HRI deals with the challenge of interacting with humans in environments that\nare relatively unstructured or which are structured around people rather than\nmachines, as well as the possibility that the robot may need to interact\nnaturally with people rather than through teach pendants, programming, or\nsimilar interfaces.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present geometric numerical integrators for contact flows that stem from a\ndiscretization of Herglotz' variational principle. First we show that the\nresulting discrete map is a contact transformation and that any contact map can\nbe derived from a variational principle. Then we discuss the backward error\nanalysis of our variational integrators, including the construction of a\nmodified Lagrangian. Throughout the paper we use the damped harmonic oscillator\nas a benchmark example to compare our integrators to their symplectic\nanalogues.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the effects of a magnetic field on the thermodynamics of a\nneutron system at finite density and temperature. Our main motivation is to\ndeepen the understanding of the physics of a class of neutron stars known as\nmagnetars, which exhibit extremely strong magnetic fields. Taking into account\ntwo facts, (i) the existence of a pressure anisotropy in the presence of a\nmagnetic field and (ii) that the quantum field theory contribution to the\npressure is non-negligible, we show that the maximum value that the inner\nmagnetic field of a star can reach while being in agreement with the\nmagnetohydrostatic equilibrium between the gravitational and matter pressures\nbecomes $10^{17}$ G, an order of magnitude smaller than the previous value\nobtained through the scalar virial theorem; that the magnetic field has a\nnegligible effect on the neutron system's equation of state; that the system's\nmagnetic susceptibility increases with the temperature; and that the specific\nheat $C_V$ does not significantly change with the magnetic field in the range\nof temperatures characteristic of protoneutron stars.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Rossby waves play an important role in mediating the angular momentum of\nrotating spherical fluids, creating weather on Earth and tuning exoplanet\norbits in distant stellar systems (Ogilvie 2014). Their recent discovery in the\nsolar convection zone provides an exciting opportunity to appreciate the\ndetailed astrophysics of Rossby waves. Large-scale Rossby waves create subtle\ndrifts in acoustic oscillations in the convection zone, which we measure using\nhelioseismology to image properties of Rossby waves in the interior. We analyze\n20 years of space-based observations, from 1999 to 2018, to measure Rossby-mode\nfrequencies, line-widths, and amplitudes. Spatial leakage affects the\nmeasurements of normal model coupling and complicates the analysis of\nseparating out specific harmonic degree and the azimuthal number of features on\nthe Sun. Here we demonstrate a novel approach to overcome this difficulty and\ntest it by performing synthetic tests. We find that the root-mean-square\nvelocity of the modes is of the order of 0.5 m/s at the surface.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Hawking radiation is one essential property of quantum black hole. It results\nin the information loss paradox, and give important clue to the unification of\nquantum mechanics and general relativity. In the previous works, the boundary\nscalar field on the horizon of black holes were used to give the microstates of\nBTZ black holes and Kerr black holes. They are account for the\nBekenstein-Hawking entropy. In this paper, we show that the Hawking radiation\ncan also be derived from those scalar fields. Actually the Hawking radiation\nare superposition of thermal radiation of right/left sector at different\ntemperatures. We also discuss the impact on the information loss paradox.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the regularization dependence of the Nambu-Jona--Lasinio model (NJL)\npredictions for some properties of magnetized quark matter at zero temperature\n(and baryonic density) in the mean field approximation. The model parameter\ndependence for each regularization procedure is also analyzed in detail. We\ncalculate the average and difference of the quark condensates using different\nregularization methods and compare with recent lattice results. In this\ncontext, the reliability of the different regularization procedures is\ndiscussed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The current study yielded a number of important findings. We managed to build\na neural network that achieved an accuracy score of 91 per cent in classifying\ntroll and genuine tweets. By means of regression analysis, we identified a\nnumber of features that make a tweet more susceptible to correct labelling and\nfound that they are inherently present in troll tweets as a special type of\ndiscourse. We hypothesised that those features are grounded in the\nsociolinguistic limitations of troll writing, which can be best described as a\ncombination of two factors: speaking with a purpose and trying to mask the\npurpose of speaking. Next, we contended that the orthogonal nature of these\nfactors must necessarily result in the skewed distribution of many different\nlanguage parameters of troll messages. Having chosen as an example distribution\nof the topics and vocabulary associated with those topics, we showed some very\npronounced distributional anomalies, thus confirming our prediction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Language is a popular resource to mine speakers' attitude bias, supposing\nthat speakers' statements represent their bias on concepts. However, psychology\nstudies show that people's explicit bias in statements can be different from\ntheir implicit bias in mind. Although both explicit and implicit bias are\nuseful for different applications, current automatic techniques do not\ndistinguish them. Inspired by psychological measurements of explicit and\nimplicit bias, we develop an automatic language-based technique to reproduce\npsychological measurements on large population. By connecting each\npsychological measurement with the statements containing the certain\ncombination of special words, we derive explicit and implicit bias by\nunderstanding the sentiment of corresponding category of statements. Extensive\nexperiments on English and Chinese serious media (Wikipedia) and non-serious\nmedia (social media) show that our method successfully reproduce the\nsmall-scale psychological observations on large population and achieve new\nfindings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Reinforcement learning algorithms use correlations between policies and\nrewards to improve agent performance. But in dynamic or sparsely rewarding\nenvironments these correlations are often too small, or rewarding events are\ntoo infrequent to make learning feasible. Human education instead relies on\ncurricula--the breakdown of tasks into simpler, static challenges with dense\nrewards--to build up to complex behaviors. While curricula are also useful for\nartificial agents, hand-crafting them is time consuming. This has lead\nresearchers to explore automatic curriculum generation. Here we explore\nautomatic curriculum generation in rich, dynamic environments. Using a\nsetter-solver paradigm we show the importance of considering goal validity,\ngoal feasibility, and goal coverage to construct useful curricula. We\ndemonstrate the success of our approach in rich but sparsely rewarding 2D and\n3D environments, where an agent is tasked to achieve a single goal selected\nfrom a set of possible goals that varies between episodes, and identify\nchallenges for future work. Finally, we demonstrate the value of a novel\ntechnique that guides agents towards a desired goal distribution. Altogether,\nthese results represent a substantial step towards applying automatic task\ncurricula to learn complex, otherwise unlearnable goals, and to our knowledge\nare the first to demonstrate automated curriculum generation for\ngoal-conditioned agents in environments where the possible goals vary between\nepisodes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An analytical method is proposed for computing the low-Reynolds-number\nhydrodynamic mobility function of a small colloidal particle asymmetrically\nmoving inside a large spherical elastic cavity, the membrane of which is\nendowed with resistance toward shear and bending. In conjunction with the\nresults obtained in the first part [Daddi-Moussa-Ider, L\\\"{o}wen, and Gekle,\nEur. Phys. J. E 41, 104 (2018)], in which the axisymmetric motion normal to the\nsurface of an elastic cavity is investigated, the general motion for an\narbitrary force direction can be addressed. The elastohydrodynamic problem is\nformulated and solved using the classic method of images through expressing the\nhydrodynamic flow fields as a multipole expansion involving higher-order\nderivatives of the free-space Green's function. In the quasi-steady limit, we\ndemonstrate that the particle self-mobility function of a particle moving\ntangent to the surface of the cavity is larger than that predicted inside a\nrigid stationary cavity of equal size. This difference is justified by the fact\nthat a stationary rigid cavity introduces additional hindrance to the\ntranslational motion of the encapsulated particle, resulting in a reduction of\nits hydrodynamic mobility. Furthermore, the motion of the cavity is\ninvestigated, revealing that the translational pair (composite) mobility, which\nlinearly couples the velocity of the elastic cavity to the force exerted on the\nsolid particle, is solely determined by membrane shear properties. Our\nanalytical predictions are favorably compared with fully-resolved computer\nsimulations based on a completed-double-layer boundary integral method.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $f$ be a $C^1$-diffeomorphism and $\\mu$ be a hyperbolic ergodic\n$f$-invariant Borel probability measure with positive measure-theoretic\nentropy. Assume that the Oseledec splitting $$T_xM=E_1(x) \\oplus\\cdots\\oplus\nE_s(x) \\oplus E_{s+1}(x) \\oplus\\cdots\\oplus E_l(x) $$ is dominated on the\nOseledec basin $\\Gamma$. We give extensions of Katok's Horseshoes construction.\nMoreover there is a dominated splitting corresponding to Oseledec subspace on\nhorseshoes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the Witten-Reshetikhin-Turaev invariants or Chern-Simons\npartition function at or around roots of unity $q=e^{2\\pi i \\frac{1}{K}}$ with\nrational level $K=\\frac{r}{s}$ where $r$ and $s$ are coprime integers. From the\nexact expression for the $G=SU(2)$ Witten-Reshetikhin-Turaev invariants of\nSeifert manifolds at other roots of unity obtained by Lawrence and Rozansky, we\nprovide an expected form of the structure of the Witten-Reshetikhin-Turaev\ninvariants in terms of the homological blocks at other roots of unity. Also, we\ndiscuss the asymptotic expansion of knot invariants around roots of unity where\nwe take a limit different from the standard limit in the volume conjecture.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With substantial evidence of glassy behavior in the phase diagram of high\n$T_c$ superconductors and its co-existence with superconductivity, we attempt\nto answer the question: what are the properties of a superconducting state\nwhere the force driving cooper pairing becomes dissipative? We find that when\nthe bosonic mediator is local, dissipation acts to reduce the superconducting\ncritical temperature ($T_c$). On the other hand, contrary to na\\\"{i}ve\nexpectations, $T_c$ behaves non-monotonically with dissipation for a non-local\nmediator -- weakly dissipative bosons at different energy scales act coherently\nto give rise to an increase in $T_c$ and eventually destroy superconductivity\nwhen the dissipation exceeds a critical value. The critical value occurs when\ndissipative effects become comparable to the energy scale associated with the\nspatial stiffness of the mediator, at which point, $T_c$ acquires a maximum. We\noutline consequences of our results to recent proton irradiation experiments\n(M. Leroux et al.,~\\cite{Welp2018}) on the cuprate superconductor\nLa$_{2-x}$Ba$_x$CuO$_4$ (LBCO) which observe a disorder induced increase in\n$T_c$ even when the transition temperature of the proximate charge density wave\n(CDW) is unaffected by the presence of irradiation. Our mechanism is a novel\nway to raise $T_c$ that does not require a `tug-of-war' -type scenario between\ntwo competing phases.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we propose to apply the recently developed Koopman operator\ntechniques to explore the global phase space of a nonlinear system from\ntime-series data. In particular, we address the problem of identifying various\ninvariant subsets of a phase space from the spectral properties of the\nassociated Koopman operator constructed from time-series data. Moreover, in the\ncase when the system evolution is known locally in various invariant subspaces,\nthen a phase space stitching result is proposed that can be applied to identify\na global Koopman operator. A biological system, bistable toggle switch and a\nsecond-order nonlinear system example are considered to illustrate the proposed\nresults. The construction of this global Koopman operator is very helpful in\nexperimental works as multiple experiments can't be run at the same time\nstarting from several initial conditions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Asteroseismology has undergone a profound transformation as a scientific\nfield following the CoRoT and Kepler space missions. The latter is now yielding\nthe first measurements of latitudinal differential rotation obtained directly\nfrom oscillation frequencies. Differential rotation is a fundamental mechanism\nof the stellar dynamo effect. Our goal is to measure the amount of differential\nrotation in the solar analogues 16 Cyg A and B, which are the components of a\nbinary system. These stars are the brightest observed by Kepler and have\ntherefore been extensively observed, with exquisite precision on their\noscillation frequencies. We modelled the acoustic power spectrum of 16 Cyg A\nand B using a model that takes into account the contribution of differential\nrotation to the rotational frequency splitting. The estimation was carried out\nin a Bayesian setting. We then inverted these results to obtain the rotation\nprofile of both stars under the assumption of a solar-like functional form. We\nobserve that the magnitude of latitudinal differential rotation has a strong\nchance of being solar-like for both stars, their rotation rates being higher at\nthe equator than at the pole. The measured latitudinal differential rotation,\ndefined as the difference of rotation rate between the equator and the pole, is\n$320\\pm269$ nHz and $440^{+363}_{-383}$ nHz for 16 Cyg A and B, respectively,\nconfirming that the rotation rates of these stars are almost solar-like. Their\nequatorial rotation rates are $535\\pm75$ nHz and $565_{-129}^{+150}$ nHz. Our\nresults are in good agreement with measurements obtained from\nspectropolarimetry, spectroscopy, and photometry. We present the first\nconclusive measurement of latitudinal differential rotation for solar\nanalogues. Their rotational profiles are very close to those of the Sun. These\nresults depend weakly on the uncertainties of the stellar parameters.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Networked control system (NCS) refer to a set of control loops that are\nclosed over a communication network. In this article, the joint operation of\ncontrol and networking for NCS is investigated wherein the network serves the\nsensor-to-controller communication links for multiple stochastic linear\ntime-invariant (LTI) sub-systems. The sensors sample packets based on the\nobserved plant state, which they send over a shared multi-hop network. The\nnetwork has limited communication resources, which need to be assigned to\ncompeting links to support proper control loop operation. In this set-up, we\nformulate an optimization problem to minimize the weighted-sum\nlinear-quadratic-Gaussian (LQG) cost of all loops, taking into account the\nadmissible sampling, control, congestion control and scheduling policies. Under\nsome mild assumptions on the sampling frequencies of the control loops and the\ncommunication network, we find the joint optimal solution to be given by a\ncertainty equivalence control with threshold-based sampling policy, as well as\na back-pressure type scheduler with a simple pass-through congestion control.\nThe interface between network and control loops is identified to be the buffer\nstate of the sensor node, which can be interpreted as network price for\nsampling a packet from control perspective. We validate our theoretical claims\nby simulating NCSs comprising of multiple LTI stochastic control loops\ncommunicating over a two-hop cellular network.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents a physics-based data-driven method to learn predictive\nreduced-order models (ROMs) from high-fidelity simulations, and illustrates it\nin the challenging context of a single-injector combustion process. The method\ncombines the perspectives of model reduction and machine learning. Model\nreduction brings in the physics of the problem, constraining the ROM\npredictions to lie on a subspace defined by the governing equations. This is\nachieved by defining the ROM in proper orthogonal decomposition (POD)\ncoordinates, which embed the rich physics information contained in solution\nsnapshots of a high-fidelity computational fluid dynamics (CFD) model. The\nmachine learning perspective brings the flexibility to use transformed physical\nvariables to define the POD basis. This is in contrast to traditional model\nreduction approaches that are constrained to use the physical variables of the\nhigh-fidelity code. Combining the two perspectives, the approach identifies a\nset of transformed physical variables that expose quadratic structure in the\ncombustion governing equations and learns a quadratic ROM from transformed\nsnapshot data. This learning does not require access to the high-fidelity model\nimplementation. Numerical experiments show that the ROM accurately predicts\ntemperature, pressure, velocity, species concentrations, and the limit-cycle\namplitude, with speedups of more than five orders of magnitude over\nhigh-fidelity models. Our ROM simulation is shown to be predictive 200% past\nthe training interval. Moreover, ROM-predicted pressure traces accurately match\nthe phase of the pressure signal and yield good approximations of the\nlimit-cycle amplitude.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The $\\Z_{2^s}$-additive codes are subgroups of $\\Z^n_{2^s}$, and can be seen\nas a generalization of linear codes over $\\Z_2$ and $\\Z_4$. A $\\Z_{2^s}$-linear\ncode is a binary code which is the Gray map image of a $\\Z_{2^s}$-additive\ncode. We consider $\\Z_{2^s}$-additive simplex codes of type $\\alpha$ and\n$\\beta$, which are a generalization over $\\Z_{2^s}$ of the binary simplex\ncodes. These $\\Z_{2^s}$-additive simplex codes are related to the\n$\\Z_{2^s}$-additive Hadamard codes. In this paper, we use this relationship to\nestablish the kernel of their binary images, under the Gray map, the\n$\\Z_{2^s}$-linear simplex codes. Similar results can be obtained for the binary\nGray map image of $\\Z_{2^s}$-additive MacDonald codes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Can neural networks learn to solve partial differential equations (PDEs)? We\ninvestigate this question for two (systems of) PDEs, namely, the Poisson\nequation and the steady Navier--Stokes equations. The contributions of this\npaper are five-fold. (1) Numerical experiments show that small neural networks\n(< 500 learnable parameters) are able to accurately learn complex solutions for\nsystems of partial differential equations. (2) It investigates the influence of\nrandom weight initialization on the quality of the neural network approximate\nsolution and demonstrates how one can take advantage of this non-determinism\nusing ensemble learning. (3) It investigates the suitability of the loss\nfunction used in this work. (4) It studies the benefits and drawbacks of\nsolving (systems of) PDEs with neural networks compared to classical numerical\nmethods. (5) It proposes an exhaustive list of possible directions of future\nwork.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we explore the partitioning attacks on the Bitcoin network,\nwhich is shown to exhibit spatial bias, and temporal and logical diversity.\nThrough data-driven study we highlight: 1) the centralization of Bitcoin nodes\nacross autonomous systems, indicating the possibility of BGP attacks, 2)the\nnon-uniform consensus among nodes, that can be exploited to partition the\nnetwork, and 3)the diversity in the Bitcoin software usage that can lead to\nprivacy attacks. Atop the prior work, which focused on spatial partitioning,\nour work extends the analysis of the Bitcoin network to understand the temporal\nand logical effects on the robustness of the Bitcoin network.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We describe the current performance of the Subaru Coronagraphic Extreme\nAdaptive Optics (SCExAO) instrument on the Subaru telescope on Maunakea, Hawaii\nand present early science results for SCExAO coupled with the CHARIS integral\nfield spectrograph. SCExAO now delivers H band Strehl ratios up to $\\sim$ 0.9\nor better, extreme AO corrections for optically faint stars, and planet-to-star\ncontrasts rivaling that of GPI and SPHERE. CHARIS yield high signal-to-noise\ndetections and 1.1--2.4 $\\mu m$ spectra of benchmark directly-imaged companions\nlike HR 8799 cde and kappa And b that clarify their atmospheric properties. We\nalso show how recently published as well as unpublished observations of LkCa 15\nlead to a re-evaluation of its claimed protoplanets. Finally, we briefly\ndescribe plans for a SCExAO-focused direct imaging campaign to directly image\nand characterize young exoplanets, planet-forming disks, and (later) mature\nplanets in reflected light.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Variational representations of quantum states abound and have successfully\nbeen used to guess ground-state properties of quantum many-body systems. Some\nare based on partial physical insight (Jastrow, Gutzwiller projected, and\nfractional quantum Hall states, for instance), and others operate as a black\nbox that may contain information about the underlying structure of entanglement\nand correlations (tensor networks, neural networks) and offer the advantage of\na large set of variational parameters that can be efficiently optimized.\nHowever, using variational approaches to study excited states and, in\nparticular, calculating the excitation spectrum, remains a challenge. We\npresent a variational method to calculate the dynamical properties and spectral\nfunctions of quantum many-body systems in the frequency domain, where the\nGreen's function of the problem is encoded in the form of a restricted\nBoltzmann machine (RBM). We introduce a natural gradient descent approach to\nsolve linear systems of equations and use Monte Carlo to obtain the dynamical\ncorrelation function. In addition, we propose a strategy to regularize the\nresults that improves the accuracy dramatically. As an illustration, we study\nthe dynamical spin structure factor of the one dimensional $J_1-J_2$ Heisenberg\nmodel. The method is general and can be extended to other variational forms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Entanglement offers substantial advantages in quantum information processing,\nbut loss and noise hinder its applications in practical scenarios. Although it\nhas been well known for decades that the classical communication capacity over\nlossy and noisy bosonic channels can be significantly enhanced by entanglement,\nno practical encoding and decoding schemes are available to realize any\nentanglement-enabled advantage. Here, we report structured encoding and\ndecoding schemes for such an entanglement-assisted communication scenario.\nSpecifically, we show that phase encoding on the entangled two-mode squeezed\nvacuum state saturates the entanglement-assisted classical communication\ncapacity over a very noisy channel and overcomes the fundamental limit of\ncovert communication without entanglement assistance. We then construct\nreceivers for optimum hypothesis testing protocols under discrete phase\nmodulation and for optimum noisy phase estimation protocols under continuous\nphase modulation. Our results pave the way for entanglement-assisted\ncommunication and sensing in the radio-frequency and microwave spectral ranges.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using a general parameterization of two-body scattering amplitude, we\nsystematically analyze the corresponding data on $X(3872)$, more explicitly,\nthe CDF data on inclusive $p\\bar{p}$ scattering to $J/\\psi \\pi^+\\pi^-$, and the\nBelle and BaBar data on $B$ decays to $K\\, J/\\psi \\pi^+\\pi^-$ and $K\nD\\bar{D}^{*0}$ around the $D^0\\bar{D}^{*0}$ threshold. We achieve a good\nreproduction of data and find that the $X(3872)$ can be interpreted as a bound\nand/or virtual state, or even higher-order (double or triple) virtual sate\npole. The latter point was not realized previously in the literature. % The\nlatter point has not been realized in other literatures. As a result, the\ncompositeness of the $X(3872)$ can vary largely from nearly 0 to 1. More\nhigher-precision data is needed to discriminate its pole structure and nature.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Scaling theory predicts complete localization in $d=2$ in quantum systems\nbelonging to orthogonal class (i.e. with time-reversal symmetry and\nspin-rotation symmetry). The conductance $g$ behaves as $g \\sim exp(-L/l)$ with\nsystem size $L$ and localization length $l$ in the strong disorder limit.\nHowever, classical systems can always have metallic states in which Ohm's law\nshows a constant $g$ in $d=2$. We study a two-dimensional quantum percolation\nmodel by controlling dephasing effects. The numerical investigation of $g$ aims\nat simulating a quantum-to-classical percolation evolution. An unexpected\nmetallic phase, where $g$ increases with $L$, generates immense interest before\nthe system becomes completely classical. Furthermore, the analysis of the\nscaling plot of $g$ indicates a metal-insulator crossover.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we propose two cryptosystems based on group rings and existing\ncryptosystem. First one is Elliptic ElGamal type group ring public key\ncryptosystem whose security is greater than security of cryptosystems based on\nelliptic curves discrete logarithmic problem (ECDLP). Second is ElGamal type\ngroup ring public key cryptosystem, which is analogous to ElGamal public key\ncryptosystem but has comparatively greater security. Examples are also given\nfor both the proposed cryptosystems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper studies delayed synchronization of continuous-time multi-agent\nsystems (MAS) in the presence of unknown nonuniform communication delays. A\ndelay-free transformation is developed based on a communication network which\nis a directed spanning tree, which can transform the original MAS to a new one\nwithout delays. By using this transformation, we design a static protocol for\nfull-state coupling and a dynamic protocol for delayed state synchronization\nfor homogeneous MAS via full- and partial-state coupling. Meanwhile, the\ndelayed output synchronization is also studied for heterogeneous MAS, which is\nachieved by using a low-gain and output regulation based dynamic protocol\ndesign via the delay-free transformation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Observations recently suggest $4.4\\sigma$ discrepancy between Hubble constant\nmeasured locally and that inferred from cosmic microwave background in standard\ncosmology. Either standard cosmology or local measurement might expect\nsomething new. We investigate the possibility that the value of Hubble\nconstants might be affected by observers' motional status in the local\nmeasurement. Using adapted coordinate for geodesic observers in FLRW space-time\nand constraints inferred from observation of cosmic shear, we find that the\nmotional status of reference frame could contribute to about $1.1 \\pm 0.3\\%$\ndiscrepancy of these Hubble constants. Namely, the difference is $\\Delta H_0=\n0.81\\pm0.22$ km s$^{-1}$ Mpc$^{-1}$, if the locally measured Hubble constant is\n$ 74$ km s$^{-1}$ Mpc$^{-1}$. This effect seems not negligible as an\nuncertainty for local measurement of Hubble constant\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The high pseudorapidity ($\\eta$) region of the Compact Muon Solenoid (CMS)\nmuon system is covered by Cathode Strip Chambers only and lacks redundant\ncoverage despite the fact that it is a challenging region for muons in terms of\nbackgrounds and momentum resolution. During the annual Year-End Technical Stops\n2022 & 2023, two new layers of improved Resistive Plate Chambers (iRPC) will be\nadded, RE3/1 & RE4/1, which will completely cover the region of $1.8 < |\\eta| <\n2.4$ in the endcap. Thus, the additional new chambers will lead to increase\nefficiency for both trigger and offline reconstruction in the difficult region\nwhere the background is the highest and the magnetic field is the lowest within\nthe muon system. The extended RPC system will improve the performance and the\nrobustness of the muon trigger. The final design of iRPC chambers and the\nconcept to integrate and install them in the CMS muon system have been\nfinalized. In this report, the main results demonstrating the implementation\nand installation of the new iRPC detectors in the CMS muon system at high\n$|\\eta|$ region will be presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose and analyze a finite element approximation of the relaxed\nCahn-Hilliard equation with singular single-well potential of Lennard-Jones\ntype and degenerate mobility that is energy stable and nonnegativity\npreserving. The Cahn-Hilliard model has recently been applied to model\nevolution and growth for living tissues: although the choices of degenerate\nmobility and singular potential are biologically relevant, they induce\ndifficulties regarding the design of a numerical scheme. We propose a finite\nelement scheme and we show that it preserves the physical bounds of the\nsolutions thanks to an upwind approach adapted to the finite element method.\nMoreover, we show well-posedness, energy stability properties, and convergence\nof solutions of the numerical scheme. Finally, we validate our scheme by\npresenting numerical simulations in one and two dimensions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Ballinger et al. have determined the list of all prism manifolds that are\npossibly realizable by Dehn surgeries on knots in $S^3$. In this paper, we\nexplicitly find braid words of primitive/Seifert-fibered knots on which surface\nslope surgeries yield all the prism manifolds listed above. This completes the\nsolution to the prism manifold realization problem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Millimeter wave (mmWave) communications are evolving as a promising\ntechnology to meet the ever increasing data rate requirements. However, high\ndirectivity and severe path loss make it vulnerable to blockages, which could\nbe frequent in indoor or urban environments. To address this issue, intelligent\nreflecting surfaces (IRSs) are introduced to provide additional adjustable\nreflected paths. Most prior works assume that elements of IRSs have an infinite\nphase resolution, which is difficult to be realized in practical systems. In\nthis paper, IRSs with low-resolution phase shifters are considered. We aim to\nmaximize the receive signal power at the user by jointly optimizing discrete\nphase shifts of IRSs and the transmit beamforming vector at the base station\nfor mmWave downlink systems. An analytical near-optimal solution is developed\nby exploiting some important characteristics of mmWave channels. Our\ntheoretical analysis reveals that low-resolution phase shifters can still\nachieve a receive signal power that increases quadratically with the number of\nreflecting elements. Simulation results are provided to corroborate our\nanalysis and show the effectiveness of the proposed solution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $K$ be a locally compact hypergroup with a left invariant Haar measure.\nWe show that the Liouville property and amenability are equivalent for $K$ when\nit is second countable. Suppose that $\\sigma$ is a non-degenerate probability\nmeasure on $K$, we show that there is no non-trivial $\\sigma$-harmonic function\nwhich is continuous and vanishing at infinity. Using this, we prove that the\nspace $H_\\sigma^p(K)$ of all $\\sigma$-harmonic $L^p$-functions, is trivial for\nall $1\\leq p<\\infty$. Further, it is shown that $H_\\sigma^\\infty(K)$ contains\nonly constant functions if and only if it is a subalgebra of $L^\\infty(K)$. In\nthe case where $\\sigma$ is adapted and $K$ is compact, we show that\n$H_\\sigma^p(K)={\\mathbb C}1$ for all $1\\leq p\\leq\\infty$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The strength of obfuscated software has increased over the recent years.\nCompiler based obfuscation has become the de facto standard in the industry and\nrecent papers also show that injection of obfuscation techniques is done at the\ncompiler level. In this paper we discuss a generic approach for deobfuscation\nand recompilation of obfuscated code based on the compiler framework LLVM. We\nshow how binary code can be lifted back into the compiler intermediate language\nLLVM-IR and explain how we recover the control flow graph of an obfuscated\nbinary function with an iterative control flow graph construction algorithm\nbased on compiler optimizations and SMT solving. Our approach does not make any\nassumptions about the obfuscated code, but instead uses strong compiler\noptimizations available in LLVM and Souper Optimizer to simplify away the\nobfuscation. Our experimental results show that this approach can be effective\nto weaken or even remove the applied obfuscation techniques like constant\nunfolding, certain arithmetic-based opaque expressions, dead code insertions,\nbogus control flow or integer encoding found in public and commercial\nobfuscators. The recovered LLVM-IR can be further processed by custom\ndeobfuscation passes that are now applied at the same level as the injected\nobfuscation techniques or recompiled with one of the available LLVM backends.\nThe presented work is implemented in a deobfuscation tool called SATURN.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Ground state and radial excitation mass spectra of bottomonium states are\ncomputed using the Instanton Induced $q \\bar{q}$ Potential obtained from\nInstanton Liquid Model for QCD vacuum. The Schrodinger equation is solved for\ntwo body problem using variational method. Hyperfine interactions are\nincorporated to remove the degeneracy between spin singlet and spin triplet\nstates of $S$ wave masses. Spin-orbit and tensor interactions through one gluon\nexchange has been added to the spin average masses of P and D wave states. The\nvector decay constants and pseudoscalar decay constants are also estimated\nusing the Van Royan Weiskopff formula within non-relativistic limit. The\ndi-leptonic, di-gamma, di-gluon decays of the bottomonium states are predicted\nwithout and with QCD corrections up to the lowest order using numerical values\nof the wave function at origin. Tri-gluon decays of respective $S$, $P$ and $D$\nstates are also studied. Other annihilation decay channels of bottomonium\nstates into $\\gamma gg$ , $\\gamma \\gamma \\gamma$ , $q\\bar{q} + g $ are also\ncomputed. The radiative transitions of first order like electric dipole\ntransitions (E1) and magnetic dipole transitions (M1) are estimated. Our\nestimations of the mass spectra and decay properties for bottomonium found to\nbe in excellent agreement with the average experimental values reported by\nparticle data group.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the space of solutions to certain $A$-hypergeometric\n$\\mathscr{D}$-modules, which were defined and studied by Gelfand, Kapranov, and\nZelevinsky. We show that the solution space can be identified with certain\nrelative cohomology group of the toric variety determined by $A$, which\ngeneralizes the results of Huang, Lian, Yau, and Zhu. As a corollary, we also\nprove the existence of rank one points for Calabi--Yau complete intersections\nin toric varieties.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce a new pre-trainable generic representation for visual-linguistic\ntasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the\nsimple yet powerful Transformer model as the backbone, and extends it to take\nboth visual and linguistic embedded features as input. In it, each element of\nthe input is either of a word from the input sentence, or a region-of-interest\n(RoI) from the input image. It is designed to fit for most of the\nvisual-linguistic downstream tasks. To better exploit the generic\nrepresentation, we pre-train VL-BERT on the massive-scale Conceptual Captions\ndataset, together with text-only corpus. Extensive empirical analysis\ndemonstrates that the pre-training procedure can better align the\nvisual-linguistic clues and benefit the downstream tasks, such as visual\ncommonsense reasoning, visual question answering and referring expression\ncomprehension. It is worth noting that VL-BERT achieved the first place of\nsingle model on the leaderboard of the VCR benchmark. Code is released at\n\\url{https://github.com/jackroos/VL-BERT}.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The security of cyber-physical systems, from self-driving cars to medical\ndevices, depends on their underlying multi-hop wireless networks. Yet, the lack\nof trusted central infrastructures and limited nodes' resources make securing\nthese networks challenging. Recent works on key pre-distribution schemes, where\nnodes communicate over encrypted overlay paths, provide an appealing solution\nbecause of their distributed, computationally light-weight nature. Alas, these\nschemes share a glaring security vulnerability: the two ends of every overlay\nlink can decrypt---and potentially modify and alter---the message. Plus, the\nlonger overlay paths impose traffic overhead and increase latency.\n  We present a novel routing mechanism, KPsec, to address these issues. KPsec\ndeploys multiple disjoint paths and an initial key-exchange phase to secure\nend-to-end communications. After the initial key-exchange phase, traffic in\nKPsec follows the shortest paths and, in contrast to key pre-distribution\nschemes, intermediate nodes cannot decrypt it. We measure the security and\nperformance of KPsec as well as three state-of-the-art key pre-distribution\nschemes using a real 10-node testbed and large-scale simulations. Our\nexperiments show that, in addition to its security benefits, KPsec results in\n$5-15\\%$ improvement in network throughput, up to $75\\%$ reduction in latency,\nand an order of magnitude reduction in energy consumption.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we study the well-posedness for a coupled PDE/ODE system\ndescribing the interaction of several massive point vortices moving within a\nvorticity backgound in a 2D ideal incompressible fluid. The points are driven\nby the velocity induced by the background vorticity, by the other vortices, and\nby a Kutta-Joukowski-type lift force creating an additional gyroscopic effect.\nThis system reduces to the so-called vortex-wave system, introduced by\nMarchioro and Pulvirenti, when the point vortices are massless. On the one\nhand, we establish existence of a weak solution before the first collision. We\nshow moreover that the background vorticity is transported by the flow\nassociated to the total velocity field. On the other hand, we establish\nuniqueness in the case where the vorticity is initially constant in a\nneighborhood of the points vortices. When all the densities of the point\nvortices have the same sign, no collision occurs in finite time and our results\nare then global in time. Our proofs strongly rely on the definition of a\nsuitable energy functional.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Individual chemically active drops suspended in a surfactant solution were\nobserved to self-propel spontaneously with straight, helical, or chaotic\ntrajectories. To elucidate how these drops can exhibit such strikingly\ndifferent dynamics and `decide' what to do, we propose a minimal axisymmetric\nmodel of a spherical active drop, and show that simple and linear interface\nproperties can lead to both steady self-propulsion of the droplet as well as\nchaotic behavior. The model includes two different mobility mechanisms, namely,\ndiffusiophoresis and the Marangoni effect, that convert self-generated\ngradients of surfactant concentration into the flow at the droplet surface. In\nturn, surface-driven flow initiates surfactant advection that is the only\nnonlinear mechanism and, thus, the only source of dynamical complexity in our\nmodel. Numerical investigation of the fully-coupled hydrodynamic and advection\ndiffusion problems reveals that strong advection (e.g., large droplet size) may\ndestabilize a steadily self-propelling drop; once destabilized, the droplet\nspontaneously stops and a symmetric extensile flow emerges. If advection is\nstrengthened even further in comparison with molecular diffusion, the droplet\nmay perform chaotic oscillations. Our results indicate that the thresholds of\nthese instabilities depend heavily on the balance between diffusiophoresis and\nthe Marangoni effect. Using linear stability analysis, we demonstrate that\ndiffusiophoresis promotes the onset of high-order modes of monotonic\ninstability of the motionless drop. We argue that diffusiophoresis has a\nsimilar effect on the instabilities of a moving drop.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The main aim of this paper is to develop a framelet representation of the\ntensor nuclear norm for third-order tensor completion. In the literature, the\ntensor nuclear norm can be computed by using tensor singular value\ndecomposition based on the discrete Fourier transform matrix, and tensor\ncompletion can be performed by the minimization of the tensor nuclear norm\nwhich is the relaxation of the sum of matrix ranks from all Fourier transformed\nmatrix frontal slices. These Fourier transformed matrix frontal slices are\nobtained by applying the discrete Fourier transform on the tubes of the\noriginal tensor. In this paper, we propose to employ the framelet\nrepresentation of each tube so that a framelet transformed tensor can be\nconstructed. Because of framelet basis redundancy, the representation of each\ntube is sparsely represented. When the matrix slices of the original tensor are\nhighly correlated, we expect the corresponding sum of matrix ranks from all\nframelet transformed matrix frontal slices would be small, and the resulting\ntensor completion can be performed much better. The proposed minimization model\nis convex and global minimizers can be obtained. Numerical results on several\ntypes of multi-dimensional data (videos, multispectral images, and magnetic\nresonance imaging data) have tested and shown that the proposed method\noutperformed the other testing methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using the formulation of electrodynamics in rotating media, we put into\nexplicit quantitative form the effect of rotation on interference and\ndiffraction patterns as observed in the rotating medium's rest-frame. As a\nparadigm experiment we focus the interference generated by a linear array of\nsources in a homogeneous medium. The interference is distorted due to rotation;\nthe maxima now follow curved trajectories. Unlike the classical Sagnac effect\nin which the rotation induced phase is independent of the refraction index $n$,\nhere the maxima bending increases when $n$ decreases, suggesting that\n$\\epsilon$-near-zero metamaterials can enhance optical gyroscopes and\nrotation-induced non-reciprocal devices. This result is counter intuitive as\none may expect that a wave that travels faster would bend less. The apparent\ncontradiction is clarified via the Minkowski momentum picture for a\nquasi-particle model of the interference that introduces the action of a\nCoriolis force, and by the Abraham picture of the wave-only momentum. our\nresults may also shed light on the Abraham-Minkowski controversy as examined in\nnon-inertial electrodynamics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a convex optimization to reduce the impact of sensor falsification\nattacks in linear time invariant systems controlled by observer-based feedback.\nWe accomplish this by finding optimal observer and controller gain matrices\nthat minimize the size of the reachable set of attack-induced states. To avoid\ntrivial solutions, we integrate a covariance-based $\\|H\\|_2$ closed-loop\nperformance constraint, for which we develop a novel linearization for this\ntypically nonlinear, non-convex problem. We demonstrate the effectiveness of\nthis linear matrix inequality framework through a numerical case study.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Modern power systems are at risk of largely reducing the inertia of\ngeneration assets and prone to experience extreme dynamics. The consequence is\nthat, during electromechanical transients triggered by large contingencies,\ntransmission of electrical power may take place in a broad spectrum well beyond\nthe single fundamental component. Traditional modeling approaches rely on the\nphasor representation derived from the Fourier Transform (FT) of the signal\nunder analysis. During large transients, though, FT-based analysis may fail to\naccurately identify the fundamental component parameters, in terms of\namplitude, frequency and phase. In this paper, we propose an alternative\napproach relying on the Hilbert Transform (HT), that, in view of the\npossibility to identify the whole spectrum, enables the tracking of signal\ndynamics. We compare FT- and HT-based approaches during representative\noperating conditions, i.e., amplitude modulations, frequency ramps and step\nchanges, in synthetic and real-world datasets. We further validate the\napproaches using a contingency analysis on the IEEE 39-bus.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The observation of chirality is ubiquitous in nature. Contrary to intuition,\nthe population of opposite chiralities is surprisingly asymmetric at\nfundamental levels. Examples range from parity violation in the subatomic weak\nforce to the homochirality in essential biomolecules. The ability to achieve\nchirality-selective synthesis (chiral induction) is of great importance in\nstereochemistry, molecular biology and pharmacology. In condensed matter\nphysics, a crystalline electronic system is geometrically chiral when it lacks\nany mirror plane, space inversion center or roto-inversion axis. Typically, the\ngeometrical chirality is predefined by a material's chiral lattice structure,\nwhich is fixed upon the formation of the crystal. By contrast, a particularly\nunconventional scenario is the gyrotropic order, where chirality spontaneously\nemerges across a phase transition as the electron system breaks the relevant\nsymmetries of an originally achiral lattice. Such a gyrotropic order, proposed\nas the quantum analogue of the cholesteric liquid crystals, has attracted\nsignificant interest. However, to date, a clear observation and manipulation of\nthe gyrotropic order remain challenging. We report the realization of optical\nchiral induction and the observation of a gyrotropically ordered phase in the\ntransition-metal dichalcogenide semimetal $1T$-TiSe$_2$. We show that shining\nmid-infrared circularly polarized light near the critical temperature leads to\nthe preferential formation of one chiral domain. As a result, we are able to\nobserve an out-of-plane circular photogalvanic current, whose direction depends\non the optical induction. Our study provides compelling evidence for the\nspontaneous emergence of chirality in the correlated semimetal TiSe$_2$. Such\nchiral induction provides a new way of optical control over novel orders in\nquantum materials.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For many machine learning algorithms, predictive performance is critically\naffected by the hyperparameter values used to train them. However, tuning these\nhyperparameters can come at a high computational cost, especially on larger\ndatasets, while the tuned settings do not always significantly outperform the\ndefault values. This paper proposes a recommender system based on meta-learning\nto identify exactly when it is better to use default values and when to tune\nhyperparameters for each new dataset. Besides, an in-depth analysis is\nperformed to understand what they take into account for their decisions,\nproviding useful insights. An extensive analysis of different categories of\nmeta-features, meta-learners, and setups across 156 datasets is performed.\nResults show that it is possible to accurately predict when tuning will\nsignificantly improve the performance of the induced models. The proposed\nsystem reduces the time spent on optimization processes, without reducing the\npredictive performance of the induced models (when compared with the ones\nobtained using tuned hyperparameters). We also explain the decision-making\nprocess of the meta-learners in terms of linear separability-based hypotheses.\nAlthough this analysis is focused on the tuning of Support Vector Machines, it\ncan also be applied to other algorithms, as shown in experiments performed with\ndecision trees.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep neural networks (DNNs) often suffer from \"catastrophic forgetting\"\nduring incremental learning (IL) --- an abrupt degradation of performance on\nthe original set of classes when the training objective is adapted to a newly\nadded set of classes. Existing IL approaches tend to produce a model that is\nbiased towards either the old classes or new classes, unless with the help of\nexemplars of the old data. To address this issue, we propose a\nclass-incremental learning paradigm called Deep Model Consolidation (DMC),\nwhich works well even when the original training data is not available. The\nidea is to first train a separate model only for the new classes, and then\ncombine the two individual models trained on data of two distinct set of\nclasses (old classes and new classes) via a novel double distillation training\nobjective. The two existing models are consolidated by exploiting publicly\navailable unlabeled auxiliary data. This overcomes the potential difficulties\ndue to the unavailability of original training data. Compared to the\nstate-of-the-art techniques, DMC demonstrates significantly better performance\nin image classification (CIFAR-100 and CUB-200) and object detection (PASCAL\nVOC 2007) in the single-headed IL setting.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce \\TEOBiResumSM{}, a nonspinning inspiral-merger-ringdown waveform\nmodel built within the effective one body (EOB) framework that includes\ngravitational waveform modes beyond the dominant quadrupole $(\\ell,|m|) =\n(2,2)$. The model incorporates: (i) an improved Pad\\'e resummation of the\nfactorized waveform amplitudes $\\rho_{\\ell m}^{\\rm orb}$ entering the\nEOB-resummed waveform where the 3PN, mass-ratio dependent, terms are hybridized\nwith test-mass limit terms up to 6PN relative order for most of the multipoles\nup to $\\ell=6$ included; (ii) an improved determination of the effective 5PN\nfunction $a_6^c(\\nu)$ entering the EOB interaction potential done using the\nmost recent, error-controlled, nonspinning numerical relativity (NR) waveforms\nfrom the Simulating eXtreme Spacetimes (SXS) collaboration; and (iii) a\nNR-informed phenomenological description of the multipolar ringdown. Such\nrepresentation stems from 19 NR waveforms with mass ratios up to $m_1/m_2=18$\nas well as test-mass waveform data, although it does not incorporate\nmode-mixing effects. The NR-completed higher modes through merger and ringdown\nconsidered here are: $(\\ell,|m|) = \\lbrace (2,1), (3,3), (3,2),(3,1),(4,4),\n(4,3),(4,2), (4,1),(5,5)\\rbrace$. For simplicity, the other subdominant modes,\nup to $\\ell=8$, are approximated by the corresponding, purely analytical,\nfactorized and resummed EOB waveform. To attempt an estimate of (some of) the\nunderlying analytic uncertainties of the model, we also contrast the effect of\nthe 6PN-hybrid Pad\\'e-resummed $\\rho_{\\ell m}$'s with the standard $3^{+2}$PN,\nTaylor-expanded, ones used in previous EOB works. The maximum unfaithfulness\n$\\bar{F}$ against the SXS waveforms including all NR-completed modes up to\n$\\ell=m=5$ is always $\\lesssim 2\\%$ for binaries with total mass $M$ as $50\nM_{\\odot} \\leq M \\lesssim 200 M_{\\odot}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We develop a quantum theory of magnetic skyrmions and antiskyrmions in a\nspin-1/2 Heisenberg magnet with frustrating next-nearest neighbor interactions.\nUsing exact diagonalization we show numerically that a quantum skyrmion exists\nas a stable many-magnon bound state and investigate its quantum numbers. We\nthen derive a phenomenological Schr\\\"odinger equation for the quantum skyrmion\nand its internal degrees of freedom. We find that quantum skyrmions have highly\nunusual properties. Their bandwidth is exponentially small and arises from\ntunneling processes between skyrmion and antiskyrmion. The bandstructure\nchanges both qualitatively and quantitatively when a single spin is added or\nremoved from the quantum skyrmion, reflecting a locking of angular momentum and\nspin quantum numbers characteristic for skyrmions. Additionally, while for weak\nforces the quantum skyrmion is accelerated parallel to the force, it moves in a\nperpendicular direction for stronger fields.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The detection of the binary neutron star (BNS) merger, GW170817, was the\nfirst success story of multi-messenger observations of compact binary mergers.\nThe inferred merger rate along with the increased sensitivity of the\nground-based gravitational-wave (GW) network in the present LIGO/Virgo, and\nfuture LIGO/Virgo/KAGRA observing runs, strongly hints at detection of binaries\nwhich could potentially have an electromagnetic (EM) counterpart. A rapid\nassessment of properties that could lead to a counterpart is essential to aid\ntime-sensitive follow-up operations, especially robotic telescopes. At minimum,\nthe possibility of counterparts require a neutron star (NS). Also, the tidal\ndisruption physics is important to determine the remnant matter post merger,\nthe dynamics of which could result in the counterparts. The main challenge,\nhowever, is that the binary system parameters such as masses and spins\nestimated from the real time, GW template-based searches are often dominated by\nstatistical and systematic errors. Here, we present an approach that uses\nsupervised machine-learning to mitigate such selection effects to report\npossibility of counterparts based on presence of a NS component, and presence\nof remnant matter post merger in real time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The number of times that we can access a system to extract information via\nquantum metrology is always finite, and possibly small, and realistic amounts\nof prior knowledge tend to be moderate. Thus theoretical consistency demands a\nmethodology that departs from asymptotic approximations and restricted\nparameter locations, while practical convenience requires that it is also\nflexible and easy to use in applications with limited data. We submit that this\nmethodology can and should be built on a Bayesian framework, and in this thesis\nwe propose, construct, explore and exploit a new non-asymptotic quantum\nmetrology. First we show the consistency of taking those solutions that are\noptimal in the asymptotic regime of many trials as a guide to calculate a\nBayesian measure of uncertainty. This provides an approximate but useful way of\nstudying the non-asymptotic regime whenever an exact optimisation is\nintractable, and it avoids the non-physical results that can arise when only\nthe asymptotic theory is used. Secondly, we construct a new non-asymptotic\nBayesian bound without relying on the previous approximation by first selecting\na single-shot optimal quantum strategy, and then simulating a sequence of\nrepetitions of this scheme. These methods are then applied to a\nsingle-parameter Mach-Zehnder interferometer, and to multi-parameter qubit and\noptical sensing networks. Our results provide a detailed characterisation of\nhow the interplay between prior information, correlations and a limited amount\nof data affects the performance of quantum metrology protocols, which opens the\ndoor to a vast set of unexplored possibilities to enhance non-asymptotic\nschemes. Finally, we provide practical researchers with a numerical toolbox for\nBayesian metrology, while theoretical workers will benefit from the broader and\nmore fundamental perspective that arises from the unified character of our\nmethodology.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Time bounded reachability is a fundamental problem in model checking\ncontinuous-time Markov chains (CTMCs) and Markov decision processes (CTMDPs)\nfor specifications in continuous stochastic logics. It can be computed by\nnumerically solving a characteristic linear dynamical system but the procedure\nis computationally expensive. We take a control-theoretic approach and propose\na reduction technique that finds another dynamical system of lower dimension\n(number of variables), such that numerically solving the reduced dynamical\nsystem provides an approximation to the solution of the original system with\nguaranteed error bounds. Our technique generalises lumpability (or\nprobabilistic bisimulation) to a quantitative setting. Our main result is a\nLyapunov function characterisation of the difference in the trajectories of the\ntwo dynamics that depends on the initial mismatch and exponentially decreases\nover time. In particular, the Lyapunov function enables us to compute an error\nbound between the two dynamics as well as a convergence rate. Finally, we show\nthat the search for the reduced dynamics can be computed in polynomial time\nusing a Schur decomposition of the transition matrix. This enables us to\nefficiently solve the reduced dynamical system by computing the exponential of\nan upper-triangular matrix characterising the reduced dynamics. For CTMDPs, we\ngeneralise our approach using piecewise quadratic Lyapunov functions for\nswitched affine dynamical systems. We synthesise a policy for the CTMDP via its\nreduced-order switched system that guarantees the time bounded reachability\nprobability lies above a threshold. We provide error bounds that depend on the\nminimum dwell time of the policy. We demonstrate the technique on examples from\nqueueing networks, for which lumpability does not produce any state space\nreduction but our technique synthesises policies using reduced version of the\nmodel.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We provide an in-depth exploration of the mass-transport properties of\nPollard's exact solution for a zonally-propagating surface water-wave in\ninfinite depth. Without resorting to approximations we discuss the Eulerian\nmass transport of this fully nonlinear, Lagrangian solution. We show that it\nhas many commonalities with the linear, Eulerian wave-theory, and also find\nPollard-like solutions in the first and second order Lagrangian theory.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study an optimal M-partition problem for the Yamabe equation on the round\nsphere, in the presence of some particular symmetries. We show that there is a\ncorrespondence between solutions to this problem and least-energy sign-changing\nsymmetric solutions to the Yamabe equation on the sphere with precisely M nodal\ndomains. The existence of an optimal partition is established through the study\nof the limit profiles of least-energy solutions to a weakly coupled competitive\nelliptic system on the sphere.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Parallel to the very rich theory of Kazhdan-Lusztig cells in characteristic\n$0$, we try to build a similar theory in positive characteristic. We study\ncells with respect to the $p$-canonical basis of the Hecke algebra of a\ncrystallographic Coxeter system (see arXiv:1510.01556(2)). Our main technical\ntool are the star-operations introduced by Kazhdan-Lusztig which have\ninteresting numerical consequences for the $p$-canonical basis. As an\napplication, we explicitely describe $p$-cells in finite type $A$ (i.e. for\nsymmetric groups) using the Robinson-Schensted correspondence. Moreover, we\nshow that Kazhdan-Lusztig cells in finite types $B$ and $C$ decompose into\n$p$-cells for $p > 2$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The fermionic Z-portal dark matter model suffers from severe constraints from\ndirect detection experiments. However, a narrow parameter space around the\nZ-funnel region is beyond the reach due to the resonance annihilation. In this\npaper, we provide an intriguing collider prospect for probing the Z-funnel dark\nmatter mass range at the future lepton colliders including the beam\npolarization feature. We have done a comprehensive analysis for mono-photon\nsignal at the colliders for such a dark matter. A realistic estimation for the\n90% C.L. constraints with the systematic beam uncertainties has also been\nprovided.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A $W^{1,p}$-metric on an $n$-dimensional closed Riemannian manifold naturally\ninduces a distance function, provided $p$ is sufficiently close to $n$. If a\nsequence of metrics $g_k$ converges in $W^{1,p}$ to a limit metric $g$, then\nthe corresponding distance functions $d_{g_k}$ subconverge to a limit distance\nfunction $d$, which satisfies $d\\le d_g$.\n  As an application, we show that the above convergence result applies to a\nsequence of conformal metrics with $L^{n/2}$-bounded scalar curvatures, under\ncertain geometric assumptions. In particular, in this special setting, the\nlimit distance function $d$ actually coincides with $d_{g}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Endogeneity and missing data are common issues in empirical research. We\ninvestigate how both jointly affect inference on causal parameters.\nConventional methods to estimate the variance, which treat the imputed data as\nif it was observed in the first place, are not reliable. We derive the\nasymptotic variance and propose a heteroskedasticity robust variance estimator\nfor two-stage least squares which accounts for the imputation. Monte Carlo\nsimulations support our theoretical findings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Future applications demand more performance, but technology advances have\nbeen faltering. A promising approach to further improve computer system\nperformance under energy constraints is to employ hardware accelerators.\nAlready today, mobile systems concurrently employ multiple accelerators in what\nwe call accelerator-level parallelism (ALP). To spread the benefits of ALP more\nbroadly, we charge computer scientists to develop the science needed to best\nachieve the performance and cost goals of ALP hardware and software.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A graph with a semiregular group of automorphisms can be thought of as the\nderived cover arising from a voltage graph. Since its inception, the theory of\nvoltage graphs and their derived covers has been a powerful tool used in the\nstudy of graphs with a significant degree of symmetry. We generalise this\ntheory to graphs with a group of automorphisms that is not necessarily\nsemiregular, and we generalise several well-known results of the classical\ntheory of voltage graphs to this broader setting.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  TThe transition temperature (TC) between normal and superconducting states\nusually exhibits a dramatic increase or decrease with increasing applied\npressure. Here we present, in contrast, a new kind of superconductor that\nexhibits the exotic feature that TC is robust against large volume shrinkages\ninduced by applied pressure (here naming them as \"RSAVS superconductors\").\nExtraordinarily, the TC in these materials stays almost constant over a large\npressure range, e.g. over 136 GPa in the (TaNb)0.67(HfZrTi)0.33 high entropy\nalloy and 141 GPa in the NbTi commercial alloy. We show that the RSAVS behavior\nalso exists in another high entropy alloy (ScZrNbTa)0.6(RhPd)0.4, and in\nsuperconducting elemental Ta and Nb, indicating that this behavior, which has\nnever previously been identified or predicted by theory, occurs universally in\nsome conventional superconductors. Our electronic structure calculations\nindicate that although the electronic density of state (DOS) at the Fermi level\nin the RSAVS state is dominated by the electrons from the degenerate dxy, dxz\nand dyz orbitals, these electrons decrease in influence with increasing\npressure. In contrast, however, the contribution of the degenerate dx2-y2 and\ndz2 orbital electrons remains almost unchanged at the Fermi level, suggesting\nthat these are the electrons that may play a crucial role in stabilizing the TC\nin the RSAVS state.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In neural architecture search (NAS), the space of neural network\narchitectures is automatically explored to maximize predictive accuracy for a\ngiven task. Despite the success of recent approaches, most existing methods\ncannot be directly applied to large scale problems because of their prohibitive\ncomputational complexity or high memory usage. In this work, we propose a\nProbabilistic approach to neural ARchitecture SEarCh (PARSEC) that drastically\nreduces memory requirements while maintaining state-of-the-art computational\ncomplexity, making it possible to directly search over more complex\narchitectures and larger datasets. Our approach only requires as much memory as\nis needed to train a single architecture from our search space. This is due to\na memory-efficient sampling procedure wherein we learn a probability\ndistribution over high-performing neural network architectures. Importantly,\nthis framework enables us to transfer the distribution of architectures learnt\non smaller problems to larger ones, further reducing the computational cost. We\nshowcase the advantages of our approach in applications to CIFAR-10 and\nImageNet, where our approach outperforms methods with double its computational\ncost and matches the performance of methods with costs that are three orders of\nmagnitude larger.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Space Variable Object Monitor (SVOM) is a forthcoming Chinese - French\nastrophysics space mission dedicated to the study of Gamma-ray bursts and\nhigh-energy transients. ECLAIRs, a wide-field hard X-ray coded mask imager, is\nthe leading instrument for the transient detection and their first\nlocalisation. The sensitivity of such instruments is usually limited by the\nbackground, either of instrumental or astrophysical origin. Detailed\nestimations of the background are obtained by simulating the interaction of\nparticles with the matter using, in the present case, the GEANT4 Monte-Carlo\ntoolkit. However, this is a time consuming process, especially when it is\nneeded to carry out all possible geometrical and orbital configurations.\nInstead, we present a much faster method that allows computing the background\nin either a static or dynamic (time dependent) way. The method is based on the\npreliminary calculation of a large particle database using the GEANT4 toolkit\nfollowed by a selection process based on the incoming direction and energy of\nthe particles. This approach is as accurate as direct Monte-Carlo methods,\nwhile it reduces the computation time by a factor of $10^3 - 10^4$ for our\napplication. We apply this method to compute the SVOM/ECLAIRs dynamic\nbackground.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Image retargeting is the task of making images capable of being displayed on\nscreens with different sizes. This work should be done so that high-level\nvisual information and low-level features such as texture remain as intact as\npossible to the human visual system, while the output image may have different\ndimensions. Thus, simple methods such as scaling and cropping are not adequate\nfor this purpose. In recent years, researchers have tried to improve the\nexisting retargeting methods and introduce new ones. However, a specific method\ncannot be utilized to retarget all types of images. In other words, different\nimages require different retargeting methods. Image retargeting has a close\nrelationship to image saliency detection, which is relatively a new image\nprocessing task. Earlier saliency detection methods were based on local and\nglobal but low-level image information. These methods are called bottom-up\nmethods. On the other hand, newer approaches are top-down and mixed methods\nthat consider the high level and semantic information of the image too. In this\npaper, we introduce the proposed methods in both saliency detection and\nretargeting. For the saliency detection, the use of image context and semantic\nsegmentation are examined, and a novel mixed bottom-up, and top-down saliency\ndetection method is introduced. After saliency detection, a modified version of\nan existing retargeting method is utilized for retargeting the images. The\nresults suggest that the proposed image retargeting pipeline has excellent\nperformance compared to other tested methods. Also, the subjective evaluations\non the Pascal dataset can be used as a retargeting quality assessment dataset\nfor further research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article, we study a class of contractive factors of\n$m$-hypercontractions for $m \\in \\mathbb{N}$. We find a characterization of\nsuch factors and this is achieved by finding explicit dilation of these factors\non some weighted Bergman spaces. This is a generalization of the work done in\n[14].\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We announce new existence and $\\varepsilon$-regularity results for minimisers\nof the relaxation of strongly quasiconvex integrals that on smooth maps\n$u\\colon\\Omega\\subset\\mathbb{R}^{n}\\to\\mathbb{R}^{N}$ are defined by $$u\\mapsto\n\\int_{\\Omega}F(\\nabla^{k}u)dx.$$ The results cover the case of integrands $F$\nwith $(1,q)$-growth in the full range of exponents $1<q<\\frac{n}{n-1}$ for\nwhich a measure representation of the relaxed functional is possible and the\nminimizers belong to the space $BV^k$ of maps whose $k$-th order derivatives\nare measures.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The transport behavior of strongly anisotropic systems is significantly\nricher compared to isotropic ones. The most dramatic spatial anisotropy at a\ncritical point occurs at a Lifshitz transition, found in systems with merging\nDirac or Weyl point or near the superconductor-insulator quantum phase\ntransition. Previous work found that in these systems a famous conjecture on\nthe existence of a lower bound for the ratio of a shear viscosity to entropy is\nviolated, and proposed a generalization of this bound for anisotropic systems\nnear charge neutrality involving the electric conductivities. The present study\nuses scaling arguments and the gauge-gravity duality to confirm the previous\nanalysis of universal bounds in anisotropic Dirac systems. We investigate the\nstrongly-coupled phase of quantum Lifshitz systems in a gravitational\nEinstein-Maxwell-dilaton model with a linear massless scalar which breaks\ntranslations in the boundary dual field theory and sources the anisotropy. The\nholographic computation demonstrates that some elements of the viscosity tensor\ncan be related to the ratio of the electric conductivities through a simple\ngeometric ratio of elements of the bulk metric evaluated at the horizon, and\nthus obey a generalized bound, while others violate it. From the IR critical\ngeometry, we express the charge diffusion constants in terms of the square\nbutterfly velocities. The proportionality factor turns out to be\ndirection-independent, linear in the inverse temperature, and related to the\ncritical exponents which parametrize the anisotropic scaling of the dual field\ntheory.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A review of probability representation of quantum states in given for optical\nand photon number tomography approaches. Explicit connection of photon number\ntomogram with measurable by homodyne detector optical tomogram is obtained. New\nintegral relations connecting Hermite polynomials of two variables with\nLaguerre polynomials are found. Examples of generic Gaussian photon states\n(squeezed and correlated states) are studied in detail.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Polar metals are rare because free carriers in metals screen electrostatic\npotential and eliminate internal dipoles. Degenerate doped ferroelectrics may\ncreate an approximate polar metallic phase. We use first-principles\ncalculations to investigate $n$-doped LiNbO$_3$-type oxides (LiNbO$_3$ as the\nprototype) and compare to widely studied perovskite oxides (BaTiO$_3$ as the\nprototype). In the rigid-band approximation, substantial polar displacements in\n$n$-doped LiNbO$_3$ persist even at 0.3 $e$/f.u. ($\\simeq$ 10$^{21}$\ncm$^{-3}$), while polar displacements in $n$-doped BaTiO$_3$ quickly get\nsuppressed and completely vanish at 0.1 $e$/f.u. Furthermore, in $n$-doped\nLiNbO$_3$, Li-O displacements decay more slowly than Nb-O displacements, while\nin $n$-doped BaTiO$_3$, Ba-O and Ti-O displacements decay approximately at the\nsame rate. Supercell calculations that use oxygen vacancies as electron donors\nsupport the main results from the rigid-band approximation and provide more\ndetailed charge distributions. Substantial cation displacements are observed\nthroughout LiNbO$_{3-\\delta}$($\\delta = 4.2\\%$), while cation displacements in\nBaTiO$_{3-\\delta}$($\\delta = 4.2\\%$) are almost completely suppressed. We find\nthat conduction electrons in LiNbO$_{3-\\delta}$ are not as uniformly\ndistributed as in BaTiO$_{3-\\delta}$, implying that the rigid-band\napproximation should be used with caution in simulating electron doped\nLiNbO$_3$-type oxides. Our work shows that polar distortions and conduction can\ncoexist in a wide range of electron concentration in $n$-doped LiNbO$_3$, which\nis a practical approach to generating an approximate polar metallic phase.\nCombining doped ferroelectrics and doped semiconductors may create new\nfunctions for devices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove the rigidity of positive mass theorem for asymptotically hyperbolic\nmanifolds. Namely, if the mass equality holds, then the manifold is isometric\nto hyperbolic space. The result was previously proven for spin manifolds or\nunder special asymptotics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Photon-induced near-field electron microscopy (PINEM) is a currently\ndeveloping spectral approach that characterizes quantum electron-light\ninteractions in electron energy gain/loss spectrum, with symmetrically\ndiscretized gains or losses of light quanta ($\\hbar \\omega$), coupled with a\nlaser induced optical near-field. In this letter, we have demonstrated that\nLinear Particle Accelerator (LPA) and anomalous PINEM (APINEM) can analytically\nemerge from PINEM-kind interaction in a strong coupling regime, because of\nquantum interference of spectral photon sidebands overlap. Furthermore, we also\nfound that the quantum interference in point-particle regime with\npre-interaction drift can produce interesting optical spectral focusing and a\nperiodically bunching of energy/momentum distribution that enable us to improve\nthe spectral resolution of electron microscopy, imaging and spectroscopy. These\nobservation of LPA and APINEM in strong laser physics can be of great interests\nfor both theoretical and experimental communities, such as ultrafast electron\nmicroscopes, attosecond science and laser-driven accelerators.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we introduce the prior knowledge, multi-scale structure, into\nself-attention modules. We propose a Multi-Scale Transformer which uses\nmulti-scale multi-head self-attention to capture features from different\nscales. Based on the linguistic perspective and the analysis of pre-trained\nTransformer (BERT) on a huge corpus, we further design a strategy to control\nthe scale distribution for each layer. Results of three different kinds of\ntasks (21 datasets) show our Multi-Scale Transformer outperforms the standard\nTransformer consistently and significantly on small and moderate size datasets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using $e^+e^-$ annihilation data corresponding to an integrated luminosity of\n3.19\\,fb$^{-1}$ collected at a center-of-mass energy of 4.178~GeV with the\nBESIII detector, we measure the absolute branching fractions\n$\\mathcal{B}_{D_s^+ \\rightarrow \\eta e^{+} \\nu_e }$ = $(2.323\\pm0.063_{\\rm\nstat}\\pm0.063_{\\rm syst})\\%$ and $\\mathcal{B}_{D_s^+ \\rightarrow \\eta^{\\prime}\ne^{+} \\nu_e}$ = $(0.824\\pm0.073_{\\rm stat}\\pm0.027_{\\rm syst})\\%$ via a tagged\nanalysis technique, where one $D_s$ is fully reconstructed in a hadronic mode.\nCombining these measurements with previous BESIII measurements of\n$\\mathcal{B}_{D^+\\to\\eta^{(\\prime)} e^{+} \\nu_e}$, the $\\eta-\\eta^\\prime$\nmixing angle in the quark flavour basis is determined to be $\\phi_{\\rm P} =\n(40.1\\pm2.1_{\\rm stat}\\pm0.7_{\\rm syst})^\\circ$. From the first measurements of\nthe dynamics of $D^+_s\\to \\eta^{(\\prime)}e^+\\nu_e$ decays, the products of the\nhadronic form factors $f_+^{\\eta^{(\\prime)}}(0)$ and the\nCabibbo-Kobayashi-Maskawa matrix element $|V_{cs}|$ are determined with\ndifferent form factor parameterizations. For the two-parameter series\nexpansion, the results are $f^{\\eta}_+(0)|V_{cs}| = 0.4455\\pm0.0053_{\\rm\nstat}\\pm0.0044_{\\rm syst}$ and $f^{\\eta^{\\prime}}_+(0)|V_{cs}| =\n0.477\\pm0.049_{\\rm stat}\\pm0.011_{\\rm syst}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Autonomous explorative robots frequently encounter scenarios where multiple\nfuture trajectories can be pursued. Often these are cases with multiple paths\naround an obstacle or trajectory options towards various frontiers. Humans in\nsuch situations can inherently perceive and reason about the surrounding\nenvironment to identify several possibilities of either manoeuvring around the\nobstacles or moving towards various frontiers. In this work, we propose a 2\nstage Convolutional Neural Network architecture which mimics such an ability to\nmap the perceived surroundings to multiple trajectories that a robot can choose\nto traverse. The first stage is a Trajectory Proposal Network which suggests\ndiverse regions in the environment which can be occupied in the future. The\nsecond stage is a Trajectory Sampling network which provides a finegrained\ntrajectory over the regions proposed by Trajectory Proposal Network. We\nevaluate our framework in diverse and complicated real life settings. For the\noutdoor case, we use the KITTI dataset and our own outdoor driving dataset. In\nthe indoor setting, we use an autonomous drone to navigate various scenarios\nand also a ground robot which can explore the environment using the\ntrajectories proposed by our framework. Our experiments suggest that the\nframework is able to develop a semantic understanding of the obstacles, open\nregions and identify diverse trajectories that a robot can traverse. Our\ncomparisons portray the performance gain of the proposed architecture over a\ndiverse set of methods against which it is compared.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The $k$-means clustering algorithm and its variant, the spherical $k$-means\nclustering, are among the most important and popular methods in unsupervised\nlearning and pattern detection. In this paper, we explore how the spherical\n$k$-means algorithm can be applied in the analysis of only the extremal\nobservations from a data set. By making use of multivariate extreme value\nanalysis we show how it can be adopted to find \"prototypes\" of extremal\ndependence and we derive a consistency result for our suggested estimator. In\nthe special case of max-linear models we show furthermore that our procedure\nprovides an alternative way of statistical inference for this class of models.\nFinally, we provide data examples which show that our method is able to find\nrelevant patterns in extremal observations and allows us to classify extremal\nevents.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For an elliptic curve E over an abelian extension k/K with CM by K of Shimura\ntype, the L-functions of its [k:K] Galois representations are Mellin transforms\nof Hecke theta functions; a modular parametrization (surjective map) from a\nmodular curve to E pulls back the 1-forms on E to give the Hecke theta\nfunctions. This article refines the study of our earlier work and shows that\ncertain class of chiral correlation functions in Type II string theory with\n[E]_C (E as real analytic manifold) as a target space yield the same Hecke\ntheta functions as objects on the modular curve. The Kahler parameter of the\ntarget space [E]_C in string theory plays the role of the index (partially\nordered) set in defining the projective/direct limit of modular curves.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent works have derived non-asymptotic upper bounds for convergence of\nunderdamped Langevin MCMC. We revisit these bound and consider introducing\nscaling terms in the underlying underdamped Langevin equation. In particular,\nwe provide conditions under which an appropriate scaling allows to improve the\nerror bounds in terms of the condition number of the underlying density of\ninterest.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A Milky-Way Type Ia Supernova (SNIa) could be unidentified or even initially\nunnoticed, being dim in radio, X-rays, and neutrinos, and suffering large\noptical/IR extinction in the Galactic plane. But SNIa emit nuclear gamma-ray\nlines from $^{56}{\\rm Ni}\\to ^{56}{\\rm Co}\\to ^{56}{\\rm Fe}$ radioactive\ndecays. These lines fall within the Fermi/GBM energy range, and the $^{56}{\\rm\nNi}$ 158 keV line is detectable by Swift/BAT. Both instruments frequently\nmonitor the Galactic plane, which is transparent to gamma rays. Thus GBM and\nBAT are ideal Galactic SNIa early warning systems. We simulate SNIa MeV light\ncurves and spectra to show that GBM and BAT could confirm a Galactic SNIa\nexplosion, followed by Swift localization and observation in X-rays and UVOIR\nband. The time of detection depends sensitively on the $^{56}{\\rm Ni}$\ndistribution, and can be as early as a few days if $\\gtrsim$ 10% of the\n$^{56}{\\rm Ni}$ is present in the surface as suggested by SN2014J gamma data.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, a single layer Coplanar Waveguide-fed microstrip patch antenna\narray is presented for biomedical applications. The proposed antenna array is\nrealized on a transparent and flexible Polyethylene Terephthalate substrate,\nhas 1x4 radiating elements and measures only 280 x 192 mm2. The antenna array\nresonates at 2.68 GHz and has a peak-simulated gain of 10 dBi. A prototype is\nalso fabricated, and the conductive patterns are drawn using cost-efficient\nadhesive copper foils instead of conventional copper or silver nanoparticle\nink. The corresponding measured results agree well with the simulated results.\nThe proposed low profile and cost-efficient transmit antenna array has the\npotential for wearable born-worn applications, including wireless powering of\nimplantable medical devices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We describe novel topological phases of iso-frequency k-space surfaces in\nbi-anisotropic optical materials - tri- and tetra-hyperbolic materials, which\nare induced by introduction of chirality. This completes the classification of\niso-frequency topologies for bi-anisotropic materials, since as we show all\noptical materials belong to one of the following topological classes: tetra-,\ntri-, bi-, mono- or non-hyperbolic. We show that phase transitions between\nthese classes occur in the k-space directions with zero group velocity at high\nk-vectors. This classification is based on the sets of high-k polaritons\n(HKPs), supported by materials. We obtain the equation describing these sets\nand characterize the longitudinal polarization impedance of HKPs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We analyze approximation rates of deep ReLU neural networks for\nSobolev-regular functions with respect to weaker Sobolev norms. First, we\nconstruct, based on a calculus of ReLU networks, artificial neural networks\nwith ReLU activation functions that achieve certain approximation rates.\nSecond, we establish lower bounds for the approximation by ReLU neural networks\nfor classes of Sobolev-regular functions. Our results extend recent advances in\nthe approximation theory of ReLU networks to the regime that is most relevant\nfor applications in the numerical analysis of partial differential equations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Special curves in the Minkowski space such as Minkowski Pythagorean\nhodographs play an important role in Computer Aided Geometric Design, and their\nusages have been thoroughly studied in the recent years. Also, several papers\nhave been published which describe methods for interpolating Hermite data in\nR2,1 by MPH curves. Bizzarri et al.introduced the class of RE curves and\npresented an interpolation method for G1 Hermite data, where the resulting RE\ncurve yields a rational boundary for the represented domain. We now propose a\nnew application area for RE curves: skinning of a discrete set of input\ncircles. We find the appropriate Hermite data to interpolate so that the\nobtained rational envelope curves touch each circle at previously defined\npoints of contact. This way we overcome the problematic scenarios when the\nlocation of the touching points would not be appropriate for skinning purposes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Modern theories of galaxy formation predict that the Galactic stellar halo\nwas hierarchically assembled from the accretion and disruption of smaller\nsystems. This hierarchical assembly is expected to produce a high degree of\nstructure in the combined phase and chemistry space; this structure should\nprovide a relatively direct probe of the accretion history of our Galaxy.\nRevealing this structure requires precise 3D positions (including distances),\n3D velocities, and chemistry for large samples of stars. The Gaia satellite is\ndelivering proper motions and parallaxes for >1 billion stars to G~20. However,\nradial velocities and metallicities will only be available to G~15, which is\ninsufficient to probe the outer stellar halo (>10 kpc). Moreover, parallaxes\nwill not be precise enough to deliver high-quality distances for stars beyond\n~10 kpc. Identifying accreted systems throughout the stellar halo therefore\nrequires a large ground-based spectroscopic survey to complement Gaia. Here we\nprovide an overview of the H3 Stellar Spectroscopic Survey, which will deliver\nprecise stellar parameters and spectrophotometric distances for 200,000 stars\nto r=18. Spectra are obtained with the Hectochelle instrument at the MMT, which\nis configured for the H3 Survey to deliver resolution R~23,000 spectra covering\nthe wavelength range 5150A-5300A. The survey is optimized for stellar halo\nscience and therefore focuses on high Galactic latitude fields (|b|>30 deg.),\nsparsely sampling 15,000 sq. degrees. Targets are selected on the basis of Gaia\nparallaxes, enabling very efficient selection of bone fide halo stars. The\nsurvey began in the Fall of 2017 and has collected 88,000 spectra to-date. All\nof the data, including the derived stellar parameters, will eventually be made\npublicly available via the survey website: h3survey.rc.fas.harvard.edu.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Here, we show that if $\\{U_n\\}_{n\\ge 0}$ is a Lucas sequence, then the\nlargest $n$ such that $|U_n|=m_1!m_2!\\cdots m_k!$ with $1<m_1\\le m_2\\le\n\\cdots\\le m_k$ satisfies $n<3\\times 10^5$. We also give better bounds in case\nthe roots of the Lucas sequence are real.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A liquid droplet hovering on a hot surface is commonly referred to as a\nLeidenfrost droplet. In this study, we discover that a Leidenfrost droplet\ninvoluntarily performs a series of distinct oscillations as it shrinks during\nthe span of its life. The oscillation first starts out erratically, followed by\na stage with stable frequencies, and finally turns into periodic bouncing with\nsignatures of a parametric oscillation and occasional resonances. The last\nbouncing stage exhibits nearly perfect collisions. We showed experimentally and\ntheoretically the enabling effects of each oscillation mode and how the droplet\nswitches between such modes. We finally show that these self-regulating\noscillation modes and the conditions for transitioning between modes are\nuniversal for all tested combinations of liquids and surfaces.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work we study contributions due to vector and axial-vector meson\nfluctuations to their in-medium spectral functions in an effective low-energy\ntheory inspired by the gauged linear sigma model. In particular, we show how to\ndescribe these fluctuations in the effective theory by massive (axial-)vector\nfields in agreement with the known structure of analogous single-particle or\nresonance contributions to the corresponding conserved currents. The vector and\naxial-vector meson spectral functions are then computed by numerically solving\nthe analytically continued functional renormalization group (aFRG) flow\nequations for their retarded two-point functions at finite temperature and\ndensity in the effective theory. We identify the new contributions that arise\ndue to the (axial-)vector meson fluctuations, and assess their influence on\npossible signatures of a QCD critical endpoint and the restoration of chiral\nsymmetry in thermal dilepton spectra.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We demonstrate control of the magnetism of Pd(100) ultrathin films, which\nshow d-electron quantum-well induced ferromagnetism, via modulation of the\ninterface electronic state using density functional calculation. From an\nanalysis based on the phase model, forming the Au/Pd(100) interface induces\nhybridization of the wave function of d-electron quantum-well states, and\nmodulates the term of the scattering phase shift as a function of the\nreciprocal lattice point. In contrast, forming the Al interface, which has only\ns-electrons at the Fermi energy, cannot modify the scattering phase shift. Our\nfinding indicates the possibility of modifying the phase shift by tailoring the\ninterface electronic states using hybridization of the wave function, and this\nefficiently changes the density of states near the Fermi energy of Pd films,\nand the switching between paramagnetism and ferromagnetism occurs based on the\ncondition for ferromagnetism (Stoner criterion).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Thue-Morse set is the set of those nonnegative integers whose binary\nexpansions have an even number of $1$. We obtain an exact formula for the state\ncomplexity of the multiplication by a constant of the Thue-Morse set\n$\\mathcal{T}$ with respect with any base $b$ which is a power of $2$. Our proof\nis constructive and we are able to explicitly provide the minimal automaton of\nthe language of all $2^p$-expansions of the set of integers $m\\mathcal{T}$ for\nany positive integers $m$ and $p$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Online propaganda is central to the recruitment strategies of extremist\ngroups and in recent years these efforts have increasingly extended to women.\nTo investigate ISIS' approach to targeting women in their online propaganda and\nuncover implications for counterterrorism, we rely on text mining and natural\nlanguage processing (NLP). Specifically, we extract articles published in Dabiq\nand Rumiyah (ISIS's online English language publications) to identify prominent\ntopics. To identify similarities or differences between these texts and those\nproduced by non-violent religious groups, we extend the analysis to articles\nfrom a Catholic forum dedicated to women. We also perform an emotional analysis\nof both of these resources to better understand the emotional components of\npropaganda. We rely on Depechemood (a lexical-base emotion analysis method) to\ndetect emotions most likely to be evoked in readers of these materials. The\nfindings indicate that the emotional appeal of ISIS and Catholic materials are\nsimilar\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Lung tumors, especially those located close to or surrounded by soft tissues\nlike the mediastinum, are difficult to segment due to the low soft tissue\ncontrast on computed tomography images. Magnetic resonance images contain\nsuperior soft-tissue contrast information that can be leveraged if both\nmodalities were available for training. Therefore, we developed a\ncross-modality educed learning approach where MR information that is educed\nfrom CT is used to hallucinate MRI and improve CT segmentation. Our approach,\ncalled cross-modality educed deep learning segmentation (CMEDL) combines CT and\npseudo MR produced from CT by aligning their features to obtain segmentation on\nCT. Features computed in the last two layers of parallelly trained CT and MR\nsegmentation networks are aligned. We implemented this approach on U-net and\ndense fully convolutional networks (dense-FCN). Our networks were trained on\nunrelated cohorts from open-source the Cancer Imaging Archive CT images\n(N=377), an internal archive T2-weighted MR (N=81), and evaluated using\nseparate validation (N=304) and testing (N=333) CT-delineated tumors. Our\napproach using both networks were significantly more accurate (U-net $P\n<0.001$; denseFCN $P <0.001$) than CT-only networks and achieved an accuracy\n(Dice similarity coefficient) of 0.71$\\pm$0.15 (U-net), 0.74$\\pm$0.12\n(denseFCN) on validation and 0.72$\\pm$0.14 (U-net), 0.73$\\pm$0.12 (denseFCN) on\nthe testing sets. Our novel approach demonstrated that educing cross-modality\ninformation through learned priors enhances CT segmentation performance\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The essential task of tensor data analysis focuses on the tensor\ndecomposition and the corresponding notion of rank. In this paper, by\nintroducing the notion of tensor Singular Value Decomposition (t-SVD), we\nestablish a Regularized Tensor Nuclear Norm Minimization (RTNNM) model for\nlow-tubal-rank tensor recovery. As we know that many variants of the Restricted\nIsometry Property (RIP) have proven to be crucial analysis tools for sparse\nrecovery. In the t-SVD framework, we initiatively define a novel tensor\nRestricted Isometry Property (t-RIP). Furthermore, we show that any third-order\ntensor $\\boldsymbol{\\mathcal{X}}$ can stably be recovered from few linear noise\nmeasurements under some certain t-RIP conditions via the RTNNM model. We note\nthat, as far as the authors are aware, such kind of result has not previously\nbeen reported in the literature.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider evidence integration from potentially dependent observation\nprocesses under varying spatio-temporal sampling resolutions and noise levels.\nWe develop a multi-resolution multi-task (MRGP) framework while allowing for\nboth inter-task and intra-task multi-resolution and multi-fidelity. We develop\nshallow Gaussian Process (GP) mixtures that approximate the difficult to\nestimate joint likelihood with a composite one and deep GP constructions that\nnaturally handle biases in the mean. By doing so, we generalize and outperform\nstate of the art GP compositions and offer information-theoretic corrections\nand efficient variational approximations. We demonstrate the competitiveness of\nMRGPs on synthetic settings and on the challenging problem of hyper-local\nestimation of air pollution levels across London from multiple sensing\nmodalities operating at disparate spatio-temporal resolutions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In order to have a comprehensive view of the propagation and evolution of\ncoronal mass ejections (CMEs) from the Sun to deep interplanetary space beyond\n1 au, we carry out a kinematic analysis of 7 CMEs in solar cycle 23. The events\nare required to have coordinated coronagraph observations, interplanetary type\nII radio bursts, and multi-point in-situ measurements at the Earth and Ulysses.\nA graduated cylindrical shell model, an analytical model without free\nparameters and a magnetohydrodynamic model are used to derive CME kinematics\nnear the Sun, to quantify the CME/shock propagation in the Sun-Earth space, and\nto connect in-situ signatures at the Earth and Ulysses, respectively. We find\nthat each of the 7 CME-driven shocks experienced a major deceleration before\nreaching 1 au and thereafter propagated with a gradual deceleration from the\nEarth to larger distances. The resulting CME/shock propagation profile for each\ncase is roughly consistent with all the data, which verifies the usefulness of\nthe simple analytical model for CME/shock propagation in the heliosphere. The\nstatistical analysis of CME kinematics indicates a tendency that the faster the\nCME, the larger the deceleration, and the shorter the deceleration time period\nwithin 1 au. For several of these events, the associated geomagnetic storms\nwere mainly caused by the southward magnetic fields in the sheath region. In\nparticular, the interaction between a CME-driven shock and a preceding ejecta\nsignificantly enhanced the preexisting southward magnetic fields and gave rise\nto a severe complex geomagnetic storm.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we introduce a new function related to the sum of element\norders of finite groups. It is used to give some criteria for a finite group to\nbe cyclic, abelian, nilpotent, supersolvable and solvable, respectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Probing the evaporation of exoplanet atmospheres is key to understanding the\nformation and evolution of exoplanetary systems. The main tracer of evaporation\nin the UV is the Lyman-alpha transition, which can reveal extended exospheres.\nRecently, NIR metastable helium triplet (1.08 microns) revealed extended\nthermospheres in several exoplanets, opening a new window into evaporation. We\naim at spectrally resolving the first helium absorption signature detected in\nWASP-107b with HST/WFC3. We obtained one transit of WASP-107b with the\nhigh-resolution spectrograph CARMENES. We detect an excess helium absorption\nsignature of 5.54+/-0.27 % in the planet rest frame during the transit. The\ndetection is in agreement with the previous detection done with WFC3. The\nsignature shows an excess absorption in the blue part of the lines suggesting\nthat HeI atoms are escaping from the atmosphere of WASP-107b. We interpret the\ntime-series absorption spectra using the 3D EVE code. Our observations can be\nexplained by combining an extended thermosphere filling half the Roche lobe and\na large exospheric tail sustained by an escape rate of metastable helium on the\norder of 10^6 g/s. In this scenario, however, the upper atmosphere needs to be\nsubjected to a reduced photoionisation and radiation pressure from the star for\nthe model to match the observations. The helium feature is detected from space\nand the ground. The ground-based high-resolution signal brings detailed\ninformation about the spatial and dynamical structure of the upper atmosphere,\nand simulations suggest that the HeI signature of WASP-107b probes both its\nthermosphere and exosphere establishing this signature as a robust probe of\nexoplanetary upper atmospheres. Surveys with NIR high-resolution spectrographs\n(e.g. CARMENES, SPIRou or NIRPS) will deliver a statistical understanding of\nexoplanet thermospheres and exospheres via the helium triplet.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose an integrated control architecture to address the gap that\ncurrently exists for efficient real-time implementation of MPC-based control\napproaches for highly nonlinear systems with fast dynamics and a large number\nof control constraints. The proposed architecture contains two types of\ncontrollers: base controllers that are tuned or optimized offline, and parallel\ncontrollers that solve an optimization-based control problem online. The\ncontrol inputs computed by the base controllers provide starting points for the\noptimization problem of the parallel controllers, which operate in parallel\nwithin a limited time budget that does not exceed the control sampling time.\nThe resulting control system is very flexible and its architecture can easily\nbe modified or changed online, e.g., by adding or eliminating controllers, for\nonline improvement of the performance of the controlled system. In a case\nstudy, the proposed control architecture is implemented for highway traffic,\nwhich is characterized by nonlinear, fast dynamics with multiple control\nconstraints, to minimize the overall travel time of the vehicles, while\nincreasing their total traveled distance within the fixed simulation time\nwindow. The results of the simulation show the excellent real-time (i.e.,\nwithin the given time budget) performance of the proposed control architecture,\nwith the least realized value of the overall cost function. Moreover, among the\nonline control approaches considered for the case study, the average cost per\nvehicle for the base-parallel control approach is the closest to the online\nMPC-based controllers, which have excellent performance but may involve\ncomputation times that exceed the given time budget.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Liouville's theorem says that in dimension greater than two, all conformal\nmaps are M\\\"obius transformations. We prove an analogous statement about\nsimplicial complexes, where two simplicial complexes are considered discretely\nconformally equivalent if they are combinatorially equivalent and the lengths\nof corresponding edges are related by scale factors associated with the\nvertices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The recent discovery of higher-order topological insulators (TIs) has opened\nnew possibilities in the search for novel topological materials and\nmetamaterials. Second-order TIs have been implemented in two-dimensional (2D)\nsystems exhibiting topological 'corner states', as well as three-dimensional\n(3D) systems having one-dimensional (1D) topological 'hinge states'.\nThird-order TIs, which have topological states three dimensions lower than the\nbulk (which must thus be 3D or higher), have not yet been reported. Here, we\ndescribe the realization of a third-order TI in an anisotropic diamond-lattice\nacoustic metamaterial. The bulk acoustic bandstructure has nontrivial topology\ncharacterized by quantized Wannier centers. By direct acoustic measurement, we\nobserve corner states at two corners of a rhombohedron-like structure, as\npredicted by the quantized Wannier centers. This work extends topological\ncorner states from 2D to 3D, and may find applications in novel acoustic\ndevices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper considers a wireless communication system with low-resolution\nquantizers, in which transmitted signals are corrupted by fading and additive\nnoise. For such wireless systems, a universal lower bound on the average symbol\nerror probability (SEP), correct for all M-ary modulation schemes, is obtained\nwhen the number of quantization bits is not enough to resolve M signal points.\nIn the special case of M-ary phase shift keying (M-PSK), the optimum maximum\nlikelihood detector for equi-probable signal points is derived. Utilizing the\nstructure of the derived optimum receiver, a general average SEP expression for\nM-PSK modulation with n-bit quantization is obtained when the wireless channel\nis subject to fading with a circularly-symmetric distribution. Adopting this\nresult for Nakagami-m fading channels, easy-to-evaluate expressions for the\naverage SEP for M-PSK modulation are further derived. It is shown that a\ntransceiver architecture with n-bit quantization is asymptotically optimum in\nterms of communication reliability if n is greater than or equal to log_2(M\n+1). That is, the decay exponent for the average SEP is the same and equal to m\nwith infinite-bit and n-bit quantizers for n is greater than or equal to\nlog_2(M+1). On the other hand, it is only equal to half and 0 for n = log_2(M)\nand n < log_2(M), respectively. An extensive simulation study is performed to\nillustrate the derived results and energy efficiency gains obtained by means of\nlow-resolution quantizers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The existence of global-in-time bounded martingale solutions to a general\nclass of cross-diffusion systems with multiplicative Stratonovich noise is\nproved. The equations describe multicomponent systems from physics or biology\nwith volume-filling effects and possess a formal gradient-flow or entropy\nstructure. This structure allows for the derivation of almost surely positive\nlower and upper bounds for the stochastic processes. The existence result holds\nunder some assumptions on the interplay between the entropy density and the\nmultiplicative noise terms. The proof is based on a stochastic Galerkin method,\na Wong--Zakai type approximation of the Wiener process, the\nboundedness-by-entropy method, and the tightness criterion of Brze\\'{z}niak and\ncoworkers. Three-species Maxwell--Stefan systems and $n$-species biofilm models\nare examples that satisfy the general assumptions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper investigates a new class of carrier-sense multiple access (CSMA)\nprotocols that employ deep reinforcement learning (DRL) techniques, referred to\nas carrier-sense deep-reinforcement learning multiple access (CS-DLMA). The\ngoal of CS-DLMA is to enable efficient and equitable spectrum sharing among a\ngroup of co-located heterogeneous wireless networks. Existing CSMA protocols,\nsuch as the medium access control (MAC) of WiFi, are designed for a homogeneous\nnetwork in which all nodes adopt the same protocol. Such protocols suffer from\nsevere performance degradation in a heterogeneous environment where there are\nnodes adopting other MAC protocols. CS-DLMA aims to circumvent this problem by\nmaking use of DRL. In particular, this paper adopts alpha-fairness as the\ngeneral objective of CS-DLMA. With alpha-fairness, CS-DLMA can achieve a range\nof different objectives when coexisting with other MACs by changing the value\nof alpha. A salient feature of CS-DLMA is that it can achieve these objectives\nwithout knowing the coexisting MACs through a learning process based on DRL.\nThe underpinning DRL technique in CS-DLMA is deep Q-network (DQN). However, the\nconventional DQN algorithms are not suitable for CS-DLMA due to their uniform\ntime-step assumption. In CSMA protocols, time steps are non-uniform in that the\ntime duration required for carrier sensing is smaller than the duration of data\ntransmission. This paper introduces a non-uniform time-step formulation of DQN\nto address this issue. Our simulation results show that CS-DLMA can achieve the\ngeneral alpha-fairness objective when coexisting with TDMA, ALOHA, and WiFi\nprotocols by adjusting its own transmission strategy. Interestingly, we also\nfind that CS-DLMA is more Pareto efficient than other CSMA protocols when\ncoexisting with WiFi.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a method for joint state and parameter estimation for natural gas\nnetworks where gas pressures and flows through a network of pipes depend on\ntime-varying injections, withdrawals, and compression controls. The estimation\nis posed as an optimal control problem constrained by coupled partial\ndifferential equations on each pipe that describe space- and time-dependent\ndensity and mass flux. These are discretized and combined with nodal conditions\nto give dynamic constraints for posing the estimation as nonlinear least\nsquares problems. We develop a rapid, scalable computational method for\nperforming the estimation in the presence of measurement and process noise.\nFinally, we evaluate its effectiveness using a data set from a capacity\nplanning model for an actual pipeline system and a month of time-series data\nfrom its supervisory control and data acquisition (SCADA) system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent work has shown that deep generative models can assign higher\nlikelihood to out-of-distribution data sets than to their training data\n(Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is\ncaused by a mismatch between the model's typical set and its areas of high\nprobability density. In-distribution inputs should reside in the former but not\nnecessarily in the latter, as previous work has presumed. To determine whether\nor not inputs reside in the typical set, we propose a statistically principled,\neasy-to-implement test using the empirical distribution of model likelihoods.\nThe test is model agnostic and widely applicable, only requiring that the\nlikelihood can be computed or closely approximated. We report experiments\nshowing that our procedure can successfully detect the out-of-distribution sets\nin several of the challenging cases reported by Nalisnick et al. (2019).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Providing a depth-rich Virtual Reality (VR) experience to users without\ncausing discomfort remains to be a challenge with today's commercially\navailable head-mounted displays (HMDs), which enforce strict measures on\nstereoscopic camera parameters for the sake of keeping visual discomfort to a\nminimum. However, these measures often lead to an unimpressive VR experience\nwith shallow depth feeling. We propose the first method ready to be used with\nexisting consumer HMDs for automated stereoscopic camera control in virtual\nenvironments (VEs). Using radial basis function interpolation and projection\nmatrix manipulations, our method makes it possible to significantly enhance\nuser experience in terms of overall perceived depth while maintaining visual\ndiscomfort on a par with the default arrangement. In our implementation, we\nalso introduce the first immersive interface for authoring a unique 3D\nstereoscopic cinematography for any VE to be experienced with consumer HMDs. We\nconducted a user study that demonstrates the benefits of our approach in terms\nof superior picture quality and perceived depth. We also investigated the\neffects of using depth of field (DoF) in combination with our approach and\nobserved that the addition of our DoF implementation was seen as a degraded\nexperience, if not similar.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Consider a symplectic surface $\\Sigma$ with two properly embedded Hamiltonian\nisotopic curves $L$ and $L'$. Suppose $g \\in Ham (\\Sigma)$ is a Hamiltonian\ndiffeomorphism which sends $L$ to $L'$. Which dynamical properties of $g$ can\nbe detected by the pair $(L, L')$? We discuss two cases where one can deduce\nthat $g$ is `chaotic': non-autonomous or even of positive entropy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A neural network is trained using simulation data from a Runge Kutta\ndiscontinuous Galerkin (RKDG) method and a modal high order limiter. With this\nmethodology, we design one and two-dimensional black-box shock detection\nfunctions. Furthermore, we describe a strategy to adapt the shock detection\nfunction to different numerical schemes without the need of a full training\ncycle and large dataset. We evaluate the performance of the neural network on a\nRKDG scheme for validation. To evaluate the domain adaptation properties of\nthis neural network limiter, our methodology is verified on a residual\ndistribution scheme (RDS), both in one and two-dimensional problems, and on\nCartesian and unstructured meshes. Lastly, we report on the quality of the\nnumerical solutions when using a neural based shock detection method, in\ncomparison to more traditional limiters, as well as on the computational impact\nof using this method in existing codes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We provide a new algorithm to generate images of the generalized fuzzy\nfractal attractors described in Oliveira-2017. We also provide some important\nresults on the approximation of fractal operators to discrete subspaces with\napplication to discrete versions of the deterministic algorithm for fractal\nimage generation in the cases of IFS recovering the classical images from\nBarnsley et al., Fuzzy IFS from Cabrelli-1992 and GIFS's from Jaros-2016.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This short note shows how to solve optimal control problems using second\norder sensitivity analysis\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $\\mathscr{B}=\\{x\\in\\mathbb{R}^d : |x|<R \\}$ ($d\\geq 3$) be a ball. We\nconsider the Dirichlet Laplacian associated with $\\mathscr{B}$ and prove that\nits eigenvalue counting function has an asymptotics \\begin{equation*}\n\\mathscr{N}_\\mathscr{B}(\\mu)=C_d vol(\\mathscr{B})\\mu^d-C'_d vol(\\partial\n\\mathscr{B})\\mu^{d-1}+O\\left(\\mu^{d-2+\\frac{131}{208}}(\\log\n\\mu)^{\\frac{18627}{8320}}\\right) \\end{equation*} as $\\mu\\rightarrow \\infty$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The engineering of topological non-trivial states of matter, using cold\natoms, has made great progress in the last decade. Driven by experimental\nsuccesses, it has become of major interest in the cold atom community. In this\nwork we investigate the time-reversal invariant Hofstadter model with an\nadditional confining potential. By calculating a local spin Chern marker we\nfind that topologically non-trivial phases can be observed in all considered\ntrap geometries. This holds also for spin-orbit coupled fermions, where the\nmodel exhibits a quantum spin Hall regime at half filling. Using dynamical\nmean-field theory, we find that interactions compete against the confining\npotential and induce a topological phase transition depending on the filling of\nthe system. Strong interactions furthermore yield a magnetic edge, which is\nlocalized through the interplay of the density distribution and the underlying\ntopological band structure.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider tubular nanowires with a polygonal cross-section. In this\ngeometry the lowest energy states are separated in two sets, one of corner and\none of side-localized states, respectively. The presence of an external\nmagnetic field transverse to the nanowire imposes an additional localization\nmechanism, the electrons being pushed sideways relatively to the direction of\nthe field. This effect has important implications on the current density, as it\ncreates current loops induced by the Lorentz force. We calculate numerically\nthe electromagnetic field radiated by hexagonal, square, and triangular\nnanowires. We demonstrate that, because of the aforementioned localization\nproperties, the radiated field can have a complex distribution determined by\nthe internal geometry of the nanowire. We suggest that measuring the field in\nthe neighborhood of the nanowire could be the basic idea of a tomography of the\nelectron distribution inside it, if a smaller receiver antenna could be placed\nin that zone.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Henle, Mathias, and Woodin proved that, provided that\n$\\omega\\rightarrow(\\omega)^{\\omega}$ holds in a model $M$ of ZF, then forcing\nwith $([\\omega]^{\\omega},\\subseteq^*)$ over $M$ adds no new sets of ordinals,\nthus earning the name a \"barren\" extension. Moreover, under an additional\nassumption, they proved that this generic extension preserves all strong\npartition cardinals. This forcing thus produces a model $M[\\mathcal{U}]$, where\n$\\mathcal{U}$ is a Ramsey ultrafilter, with many properties of the original\nmodel $M$. This begged the question of how important the Ramseyness of\n$\\mathcal{U}$ is for these results. In this paper, we show that several classes\nof $\\sigma$-closed forcings which generate non-Ramsey ultrafilters have the\nsame properties. Such ultrafilters include Milliken-Taylor ultrafilters, a\nclass of rapid p-points of Laflamme, $k$-arrow p-points of Baumgartner and\nTaylor, and extensions to a class of ultrafilters constructed by Dobrinen,\nMijares and Trujillo. Furthermore, the class of Boolean algebras\n$\\mathcal{P}(\\omega^{\\alpha})/\\mathrm{Fin}^{\\otimes \\alpha}$, $2\\le\n\\alpha<\\omega_1$, forcing non-p-points also produce barren extensions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove laws of large numbers as well as central and non-central limit\ntheorems for the Curie-Weiss model of magnetism. The rather elementary proofs\nare based on the method of moments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The paper proposes an approach to training a convolutional neural network\nusing information on the level of distortion of input data. The learning\nprocess is modified with an additional layer, which is subsequently deleted, so\nthe architecture of the original network does not change. As an example, the\nLeNet5 architecture network with training data based on the MNIST symbols and a\ndistortion model as Gaussian blur with a variable level of distortion is\nconsidered. This approach does not have quality loss of the network and has a\nsignificant error-free zone in responses on the test data which is absent in\nthe traditional approach to training. The responses are statistically dependent\non the level of input image's distortions and there is a presence of a strong\nrelationship between them.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An open challenge in supervised learning is \\emph{conceptual drift}: a data\npoint begins as classified according to one label, but over time the notion of\nthat label changes. Beyond linear autoregressive models, transfer and meta\nlearning address drift, but require data that is representative of disparate\ndomains at the outset of training. To relax this requirement, we propose a\nmemory-efficient \\emph{online} universal function approximator based on\ncompressed kernel methods. Our approach hinges upon viewing non-stationary\nlearning as online convex optimization with dynamic comparators, for which\nperformance is quantified by dynamic regret.\n  Prior works control dynamic regret growth only for linear models. In\ncontrast, we hypothesize actions belong to reproducing kernel Hilbert spaces\n(RKHS). We propose a functional variant of online gradient descent (OGD)\noperating in tandem with greedy subspace projections. Projections are necessary\nto surmount the fact that RKHS functions have complexity proportional to time.\n  For this scheme, we establish sublinear dynamic regret growth in terms of\nboth loss variation and functional path length, and that the memory of the\nfunction sequence remains moderate. Experiments demonstrate the usefulness of\nthe proposed technique for online nonlinear regression and classification\nproblems with non-stationary data.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The relationship between division of labor and individuals' spatial behavior\nin social insect colonies provides a useful context to study how social\ninteractions influence the spreading of agent (which could be information or\nvirus) across distributed agent systems. In social insect colonies, spatial\nheterogeneity associated with variations of individual task roles, affects\nsocial contacts, and thus the way in which agent moves through social contact\nnetworks. We used an Agent Based Model (ABM) to mimic three realistic scenarios\nof agent spreading in social insect colonies. Our model suggests that\nindividuals within a specific task interact more with consequences that agent\ncould potentially spread rapidly within that group, while agent spreads slower\nbetween task groups. Our simulations show a strong linear relationship between\nthe degree of spatial heterogeneity and social contact rates, and that the\nspreading dynamics of agents follow a modified nonlinear logistic growth model\nwith varied transmission rates for different scenarios. Our work provides an\nimportant insights on the dual-functionality of physical contacts. This\ndual-functionality is often driven via variations of individual spatial\nbehavior, and can have both inhibiting and facilitating effects on agent\ntransmission rates depending on environment. The results from our proposed\nmodel not only provide important insights on mechanisms that generate spatial\nheterogeneity, but also deepen our understanding of how social insect colonies\nbalance the benefit and cost of physical contacts on the agents' transmission\nunder varied environmental conditions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Users on Twitter are identified with the help of their profile attributes\nthat consists of username, display name, profile image, to name a few. The\nprofile attributes that users adopt can reflect their interests, belief, or\nthematic inclinations. Literature has proposed the implications and\nsignificance of profile attribute change for a random population of users.\nHowever, the use of profile attribute for endorsements and to start a movement\nhave been under-explored. In this work, we consider #LokSabhaElections2019 as a\nmovement and perform a large-scale study of the profile of users who actively\nmade changes to profile attributes centered around #LokSabhaElections2019. We\ncollect the profile metadata for 49.4M users for a period of 2 months from\nApril 5, 2019 to June 5, 2019 amid #LokSabhaElections2019. We investigate how\nthe profile changes vary for the influential leaders and their followers over\nthe social movement. We further differentiate the organic and inorganic ways to\nshow the political inclination from the prism of profile changes. We report how\nthe addition of election campaign related keywords lead to spread of behavior\ncontagion and further investigate it with respect to \"Chowkidar Movement\" in\ndetail.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose an approach to distinguish between correct and incorrect image\nclassifications. Our approach can detect misclassifications which either occur\n$\\it{unintentionally}$ (\"natural errors\"), or due to\n$\\it{intentional~adversarial~attacks}$ (\"adversarial errors\"), both in a single\n$\\it{unified~framework}$. Our approach is based on the observation that\ncorrectly classified images tend to exhibit robust and consistent\nclassifications under certain image transformations (e.g., horizontal flip,\nsmall image translation, etc.). In contrast, incorrectly classified images\n(whether due to adversarial errors or natural errors) tend to exhibit large\nvariations in classification results under such transformations. Our approach\ndoes not require any modifications or retraining of the classifier, hence can\nbe applied to any pre-trained classifier. We further use state of the art\ntargeted adversarial attacks to demonstrate that even when the adversary has\nfull knowledge of our method, the adversarial distortion needed for bypassing\nour detector is $\\it{no~longer~imperceptible~to~the~human~eye}$. Our approach\nobtains state-of-the-art results compared to previous adversarial detection\nmethods, surpassing them by a large margin.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  I propose a fixed-range interaction multicomponent spin model, to be used as\na physical analog to problems in plane geometry. Specifically, the model is\napplied to the open problem of the chromatic number of the plane. When spin\nvalues are interpreted as colors, the lowest energy configurations of the\nlattice spin system can be interpreted as approximations to plane colorings. In\ngeneral minimum energy configurations of the model give optimal colorings,\ncorresponding to minimum probability of any color realizing distance one.\nApproximate optimal lattice colorings with two to seven colors towards the\ncontinuum limit suggest that a true coloring of the plane cannot be achieved\nwith less than seven colors.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The strong exponential-time hypothesis (SETH) is a commonly used conjecture\nin the field of complexity theory. It states that CNF formulas cannot be\nanalyzed for satisfiability with a speedup over exhaustive search. This\nhypothesis and its variants gave rise to a fruitful field of research,\nfine-grained complexity, obtaining (mostly tight) lower bounds for many\nproblems in P whose unconditional lower bounds are hard to find. In this work,\nwe introduce a framework of Quantum Strong Exponential-Time Hypotheses, as\nquantum analogues to SETH.\n  Using the QSETH framework, we are able to translate quantum query lower\nbounds on black-box problems to conditional quantum time lower bounds for many\nproblems in BQP. As an example, we illustrate the use of the QSETH by providing\na conditional quantum time lower bound of $\\Omega(n^{1.5})$ for the Edit\nDistance problem. We also show that the $n^2$ SETH-based lower bound for a\nrecent scheme for Proofs of Useful Work, based on the Orthogonal Vectors\nproblem holds for quantum computation assuming QSETH, maintaining a quadratic\ngap between verifier and prover.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We measured both the differential cross section ($\\sigma_{p,p^\\prime}$\n$=d^2\\sigma/d\\Omega dE_{x}$) and the $\\gamma$-ray emission probability\n($R_\\gamma(E_x)$ $=\\sigma_{p,p^\\prime\\gamma}$/$\\sigma_{p,p^\\prime}$) from the\ngiant resonances excited by $\\rm^{12}C$(\\textit{p,p}$^\\prime$) reaction at 392\nMeV and 0$^\\circ$, using a magnetic spectrometer and an array of NaI(Tl)\ncounters. The absolute value of $R_\\gamma(E_x)$ was calibrated by using the\nwell-known $\\gamma$-ray emission probability from $\\rm^{12}C^* ( 15.11$ MeV, $\n1^+$, $T=1$) and $\\rm^{16}O^* ( 6.9$ MeV, $2^+$, $T=0$) states within 5\\%\nuncertainty. We found that $R_\\gamma(E_x)$ starts from zero at $E_x=16$ MeV,\nincreases to a maximum of 53.3$\\pm$0.4$\\pm$3.9\\% at $E_x=27$ MeV and then\ndecreases. We also compared the measured values of $R_\\gamma(E_x)$ with\nstatistical model calculation based on the Hauser-Feshbach formalism in the\nenergy region $E_x=$ 16-32 MeV and discussed the features of $\\gamma$-ray\nemission probability quantitatively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we introduce the notion of the containment graph of a family\nof sets and containment classes of graphs and posets. Let $Z$ be a family of\nnonempty sets. We call a (simple, finite) graph G = (V, E) a $Z$-containment\ngraph provided one can assign to each vertex $v_i \\in V $ a set $S_i \\in Z$\nsuch that $v_i v_j \\in E$ if and only if $S_i \\subset S_j$ or $S_j \\subset S_i$\n. Similarly, we call a (strict) partially ordered set $P = (V, <)$ a\n$Z$-containment poset if to each $v_i \\in V $ we can assign a set $S_i \\in Z$\nsuch that $v_i < v_j$ if and only if $S_i \\subset S_j$. Obviously, $G$ is the\ncomparability graph of $P$.\n  We give some basic results on containment graphs and investigate the\ncontainment graphs of iso-oriented boxes in $d$-space. We present a\ncharacterization of those classes of posets and graphs that have containment\nrepresentations by sets of a specific type, and we extend our results to\n``injective'' containment classes. After that we discuss similar\ncharacterizations for intersection, overlap, and disjointedness classes of\ngraphs. Finally, in the last section we discuss the nonexistence of a\ncharacterization theorem for ``strong'' containment classes of graphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Malicious software is detected and classified by either static analysis or\ndynamic analysis. In static analysis, malware samples are reverse engineered\nand analyzed so that signatures of malware can be constructed. These techniques\ncan be easily thwarted through polymorphic, metamorphic malware, obfuscation\nand packing techniques, whereas in dynamic analysis malware samples are\nexecuted in a controlled environment using the sandboxing technique, in order\nto model the behavior of malware. In this paper, we have analyzed Petya,\nSpyeye, VolatileCedar, PAFISH etc. through Agent-based and Agentless dynamic\nsandbox systems in order to investigate and benchmark their efficiency in\nadvanced malware detection.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article, we extend Huisken's theorem that convex surfaces flow to\nround points by mean curvature flow. We construct certain classes of mean\nconvex and non-mean convex hypersurfaces that shrink to round points and use\nthese constructions to create pathological examples of flows. We find a\nsequence of flows that exist on a uniform time interval, have uniformly bounded\ndiameter, and shrink to round points, yet the sequence of initial surfaces has\nno subsequence converging in the Gromov-Hausdorff sense. Moreover, we find a\nsequence of flows which all shrink to round points, yet the initial surfaces\nconverge to a space-filling surface. Also constructed are surfaces of\narbitrarily large area which are close in Hausdorff distance to the round\nsphere yet shrink to round points.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a 1024-element imaging array of superconducting nanowire single\nphoton detectors (SNSPDs) using a 32x32 row-column multiplexing architecture.\nLarge arrays are desirable for applications such as imaging, spectroscopy, or\nparticle detection.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Following the approach of Ding and Frenkel [Comm. Math. Phys. 156 (1993),\n277-300] for type $A$, we showed in our previous work [J. Math. Phys. 61\n(2020), 031701, 41 pages] that the Gauss decomposition of the generator matrix\nin the $R$-matrix presentation of the quantum affine algebra yields the\nDrinfeld generators in all classical types. Complete details for type $C$ were\ngiven therein, while the present paper deals with types $B$ and $D$. The\narguments for all classical types are quite similar so we mostly concentrate on\nnecessary additional details specific to the underlying orthogonal Lie\nalgebras.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We construct blocks of finite groups with arbitrarily large\n$\\mathcal{O}$-Morita Frobenius numbers. There are no known examples of two\nblocks defined over $\\mathcal{O}$ that are not Morita equivalent but the\ncorresponding blocks defined over $k$ are. Therefore, the above strongly\nsuggests that Morita Frobenius numbers are also unbounded, which would answer a\nquestion of Benson and Kessar.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose to view non-rigid surface registration as a probabilistic\ninference problem. Given a target surface, we estimate the posterior\ndistribution of surface registrations. We demonstrate how the posterior\ndistribution can be used to build shape models that generalize better and show\nhow to visualize the uncertainty in the established correspondence.\nFurthermore, in a reconstruction task, we show how to estimate the posterior\ndistribution of missing data without assuming a fixed point-to-point\ncorrespondence.\n  We introduce the closest-point proposal for the Metropolis-Hastings\nalgorithm. Our proposal overcomes the limitation of slow convergence compared\nto a random-walk strategy. As the algorithm decouples inference from modeling\nthe posterior using a propose-and-verify scheme, we show how to choose\ndifferent distance measures for the likelihood model.\n  All presented results are fully reproducible using publicly available data\nand our open-source implementation of the registration framework.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We aim to understand the properties at the locations of supernova (SN)\nexplosion in their host galaxies and compare with the global properties of the\nhost galaxies. We use the integral field spectrograph (IFS) of Mapping Nearby\nGalaxies (MaNGA) at Apache Point Observatory (APO) to get the 2D maps of the\nparameter properties for eleven SN host galaxies. The sample galaxies are\nanalyzed one by one in details on their properties of velocity field, star\nformation rate, oxygen abundance and stellar mass etc. This sample of SN host\ngalaxies have redshifts around $z$ $\\sim$ 0.03, which is higher than those of\nthe previous related works. The higher redshift distribution allows us to\nobtain the properties of more distant SN host galaxies. Metallicity (gas-phase\noxygen abundance) estimated from integrated spectra could represent the local\nmetallicity at SN explosion sites with small bias. All the host galaxies in our\nsample are metal-rich galaxies (12+log(O/H) $>$ 8.5) except for NGC 6387, which\nmeans supernovae (SNe) may be more inclined to explode in rich-metallicity\ngalaxies. There is a positive relation between global gas-phase oxygen\nabundance and the stellar mass of host galaxies. We also try to compare the\ndifferences of the host galaxies between SN Ia and SN II. In our sample, both\nSNe Ia and SNe II could explode in normal galaxies, while SNe II also could\nexplode in an interactive or merger system, which has star formation in the\ngalaxy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report the observation of a parametric instability in the\nout-of-equilibrium steady state of two coupled Kerr microresonators coherently\ndriven by a laser. Using a resonant excitation, we drive the system into an\nunstable regime, where we observe the appearance of intense and well resolved\nsideband modes in the emission spectrum. This feature is a characteristic\nsignature of self-sustained oscillations of the intracavity field. We\ncomprehensively model our findings using semiclassical Langevin equations for\nthe cavity field dynamics combined with a linear stability analysis. The\ninherent scalability of our semiconductor platform, enriched with a strong Kerr\nnonlinearity, is promising for the realization of integrated optical parametric\noscillator networks operating in a few-photon regime.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper evaluates the impact of the power extent on price in the\nelectricity market. The competitiveness extent of the electricity market during\nspecific times in a day is considered to achieve this. Then, the effect of\ncompetitiveness extent on the forecasting precision of the daily power price is\nassessed. A price forecasting model based on multi-layer perception via back\npropagation with the Levenberg-Marquardt mechanism is used. The Residual Supply\nIndex (RSI) and other variables that affect prices are used as inputs to the\nmodel to evaluate the market competitiveness. The results show that using\nmarket power indices as inputs helps to increase forecasting accuracy. Thus,\nthe competitiveness extent of the market power in different daily time periods\nis a notable variable in price formation. Moreover, market players cannot\nignore the explanatory power of market power in price forecasting. In this\nresearch, the real data of the electricity market from 2013 is used and the\nmain source of data is the Grid Management Company in Iran.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The neural seq2seq based question generation (QG) is prone to generating\ngeneric and undiversified questions that are poorly relevant to the given\npassage and target answer. In this paper, we propose two methods to address the\nissue. (1) By a partial copy mechanism, we prioritize words that are\nmorphologically close to words in the input passage when generating questions;\n(2) By a QA-based reranker, from the n-best list of question candidates, we\nselect questions that are preferred by both the QA and QG model. Experiments\nand analyses demonstrate that the proposed two methods substantially improve\nthe relevance of generated questions to passages and answers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we study some properties of propagation of regularity of\nsolutions of the dispersive generalized Benjamin-Ono (BO) equation. This model\ndefines a family of dispersive equations, that can be seen as a dispersive\ninterpolation between Benjamin-Ono equation and Korteweg-de Vries (KdV)\nequation.\n  Recently, it has been showed that solutions of the KdV equation and\nBenjamin-Ono equation, satisfy the following property: if the initial data has\nsome prescribed regularity on the right hand side of the real line, then this\nregularity is propagated with infinite speed by the flow solution.\n  In this case the nonlocal term present in the dispersive generalized\nBenjamin-Ono equation is more challenging that the one in BO equation. To deal\nwith this a new approach is needed. The new ingredient is to combine commutator\nexpansions into the weighted energy estimate. This allow us to obtain the\nproperty of propagation and explicitly the smoothing effect.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An $n$-cross field is a locally-defined orthogonal coordinate system\ninvariant with respect to the cubic symmetry group. Cross fields are finding\nwide-spread use in mesh generation, computer graphics, and materials science\namong many applications. It was recently by other authors that $3$-cross fields\ncan be embedded into the set of symmetric $4$th-order tensors. Another\nconcurrent work further develops a relaxation of this tensor field via a\ncertain set of varieties. In this paper, we consider the problem of generating\nan arbitrary $n$-cross field using a fourth-order $Q$-tensor theory that is\nconstructed out of tensored projection matrices. We establish that by a\nGinzburg-Landau relaxation towards a global projection, one can reliably\ngenerate an $n$-cross field on arbitrary Lipschitz domains. Our work provides a\nrigorous approach that offers several new results including porting the tensor\nframework to arbitrary dimensions, providing a new relaxation method that\nembeds the problem into a global steepest descent, and offering a relaxation\nscheme for aligning the cross field with the boundary. Our approach is designed\nto fit within the classical Ginzburg-Landau PDE theory, offering a concrete\nroad map for the future careful study of singularities of energy minimizers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove that the class of Banach function lattices in which all relatively\nweakly compact sets are equi-integrable sets (i.e. spaces satisfying the\nDunford-Pettis criterion) coincides with the class of 1-disjointly homogeneous\nBanach lattices. A new examples of such spaces are provided. Furthermore, it is\nshown that Dunford-Pettis criterion is equivalent to de la Vallee Poussin\ncriterion in all rearrangement invariant spaces on the interval. Finally, the\nresults are applied to characterize weakly compact pointwise multipliers\nbetween Banach function lattices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Designing controllers for systems affected by model uncertainty can prove to\nbe a challenge, especially when seeking the optimal compromise between the\nconflicting goals of identification and control. This trade-off is explicitly\ntaken into account in the dual control problem, for which the exact solution is\nprovided by stochastic dynamic programming. Due to its computational\nintractability, we propose a sampling-based approximation for systems affected\nby both parametric and structural model uncertainty. The approach proposed in\nthis paper separates the prediction horizon in a dual and an exploitation part.\nThe dual part is formulated as a scenario tree that actively discriminates\namong a set of potential models while learning unknown parameters. In the\nexploitation part, achieved information is fixed for each scenario, and\nopen-loop control sequences are computed for the remainder of the horizon. As a\nresult, we solve one optimization problem over a collection of control\nsequences for the entire horizon, explicitly considering the knowledge gained\nin each scenario, leading to a dual model predictive control formulation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Passivated, hole-selective contacts play important role in reducing surface\nrecombination by lowering the concentration of electrons in the rear side of a\nsolar cell. However, parasitic optical losses in these contacts can still limit\nthe performance of the cell. In this work, the long wavelength optical losses\nof silicon solar cells featuring hole-selective molybdenum oxide (MoOx) rear\ncontacts are investigated using optical simulations. The potential of these\nselective contacts for possible enhancement of photogenerated current density\nwas also investigated for their use with nanostructured dielectric layers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a new correlated topological state which we call a higher-order\ntopological Mott insulator (HOTMI). This state exhibits a striking\nbulk-boundary correspondence due to electron correlations. Namely, the\ntopological properties in the bulk, characterized by the Z3 spin-Berry phase,\nresult in gapless corner modes emerging only in spin excitations (i.e., the\nsingle-particle excitations remain gapped around the corner). We demonstrate\nthe emergence of the HOTMI in a Hubbard model on the kagome lattice, and\nelucidate how strong correlations change gapless corner modes at the\nnoninteracting case.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Nonlocality, as demonstrated by the violation of Bell inequalities, enables\ndevice-independent cryptographic tasks that do not require users to trust their\napparatus. In this article, we consider devices whose inputs are spatiotemporal\ndegrees of freedom, e.g. orientations or time durations. Without assuming the\nvalidity of quantum theory, we prove that the devices' statistical response\nmust respect their input's symmetries, with profound foundational and\ntechnological implications. We exactly characterize the bipartite binary\nquantum correlations in terms of local symmetries, indicating a fundamental\nrelation between spacetime and quantum theory. For Bell experiments\ncharacterized by two input angles, we show that the correlations are accounted\nfor by a local hidden variable model if they contain enough noise, but\nconversely must be nonlocal if they are pure enough. This allows us to\nconstruct a \"Bell witness\" that certifies nonlocality with fewer measurements\nthan possible without such spatiotemporal symmetries, suggesting a new class of\nsemi-device-independent protocols for quantum technologies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the ramification divisors of projections of a smooth projective\nvariety onto a linear subspace of the same dimension. We prove that the\nramification divisors vary in a maximal dimensional family for a large class of\nvarieties. Going further, we study the map that associates to a linear\nprojection its ramification divisor. We show that this map is dominant for most\n(but not all!) varieties of minimal degree, using (linked) limit linear series\nof higher rank. We find the degree of this map in some cases, extending the\nclassical appearance of Catalan numbers in the geometry of rational normal\ncurves, and give a geometric explanation of its fibers in terms of torsion\npoints of naturally occurring elliptic curves in the case of the Veronese\nsurface and the quartic rational surface scroll.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Superconducting fault current limiters (SFCLs) are new high-precision and\nfast-response devices which help to reduce the fault current within the\nbreaking capacity of the protective relays. Nevertheless, the reconfigurable\nstructure of the distribution network can affect their performance negatively\nby changing the supplying path of the electrical loads and thus keeping SFCL in\na useless point which cannot limit the high fault currents. This paper proposes\nan aggregated approach to solve the optimal placement of SFCLs considering the\nreconfiguration of feeders through the pre-located tie and sectionalizing\nswitches. While SFCL placement problem aims to minimize the number of SFCLs and\nlimit the high short circuit currents in the first seconds of the fault, the\nreconfiguration strategy is used to minimize the total grid costs incorporating\nthe cost of power losses and customer interruptions. According to the high\nnon-linearity and complexity of the proposed problem, social spider algorithm\n(SSA) with a two-phase modification method is developed to solve the proposed\nproblem. The feasibility and performance of the proposed method are examined on\nan IEEE test system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Ranked search results and recommendations have become the main mechanism by\nwhich we find content, products, places, and people online. With hiring,\nselecting, purchasing, and dating being increasingly mediated by algorithms,\nrankings may determine career and business opportunities, educational\nplacement, access to benefits, and even social and reproductive success. It is\ntherefore of societal and ethical importance to ask whether search results can\ndemote, marginalize, or exclude individuals of unprivileged groups or promote\nproducts with undesired features. In this paper we present FairSearch, the\nfirst fair open source search API to provide fairness notions in ranked search\nresults. We implement two algorithms from the fair ranking literature, namely\nFA*IR (Zehlike et al., 2017) and DELTR (Zehlike and Castillo, 2018) and provide\nthem as stand-alone libraries in Python and Java. Additionally we implement\ninterfaces to Elasticsearch for both algorithms, that use the aforementioned\nJava libraries and are then provided as Elasticsearch plugins. Elasticsearch is\na well-known search engine API based on Apache Lucene. With our plugins we\nenable search engine developers who wish to ensure fair search results of\ndifferent styles to easily integrate DELTR and FA*IR into their existing\nElasticsearch environment.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have studied electron spin resonance (ESR) absorption spectra for the\nnonmagnetically diluted strong-leg spin ladder magnet\n({C}$_{7}$H$_{10}$N)$_{2}$Cu$_{(1-x)}$Zn$_{x}$Br$_{4}$ (abbreviated as DIMPY)\ndown to 450 mK. Formation of the clusters with non-zero net magnetization is\nconfirmed; the cluster-cluster interaction is evidenced by the concentration\ndependence of ESR absorption. High-temperature spin-relaxation time was found\nto increase with non-magnetic dilution. The ESR linewidth analysis proves that\nthe Dzyaloshinskii-Moriya (DM) interaction remains the dominant spin-relaxation\nchannel in diluted DIMPY. Experimental data indicate that the dilution results\nin the weakening of the effective DM interaction, which can be interpreted as\ntotal suppression of DM interaction in the close vicinity of impurity atom.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The {\\em Schubert derivation} is a distinguished Hasse-Schmidt derivation on\nthe exterior algebra of a free abelian group, encoding the formalism of\nSchubert calculus for all Grassmannians at once. The purpose of this paper is\nto extend the Schubert derivation to the infinite exterior power of a free\n${\\mathbb Z}$-module of infinite rank (fermionic Fock space). Classical vertex\noperators naturally arise from the {\\em integration by parts formula}, that\nalso recovers the generating function occurring in the {\\em bosonic vertex\nrepresentation} of the Lie algebra $gl_\\infty({\\mathbb Z})$, due to Date,\nJimbo, Kashiwara and Miwa (DJKM). In the present framework, the DJKM result\nwill be interpreted as a limit case of the following general observation: the\nsingular cohomology of the complex Grassmannian $G(r,n)$ is an irreducible\nrepresentation of the Lie algebra of $n\\times n$ square matrices.}\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In order to improve the accuracy of face recognition under varying\nillumination conditions, a local texture enhanced illumination normalization\nmethod based on fusion of differential filtering images (FDFI-LTEIN) is\nproposed to weaken the influence caused by illumination changes. Firstly, the\ndynamic range of the face image in dark or shadowed regions is expanded by\nlogarithmic transformation. Then, the global contrast enhanced face image is\nconvolved with difference of Gaussian filters and difference of bilateral\nfilters, and the filtered images are weighted and merged using a coefficient\nselection rule based on the standard deviation (SD) of image, which can enhance\nimage texture information while filtering out most noise. Finally, the local\ncontrast equalization (LCE) is performed on the fused face image to reduce the\ninfluence caused by over or under saturated pixel values in highlight or dark\nregions. Experimental results on the Extended Yale B face database and CMU PIE\nface database demonstrate that the proposed method is more robust to\nillumination changes and achieve higher recognition accuracy when compared with\nother illumination normalization methods and a deep CNNs based illumination\ninvariant face recognition method\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The major obstacle in the design of materials with low lattice thermal\nconductivity is the difficulty in efficiently scattering phonons across the\nentire frequency spectrum. Using first principles calculations, we show that\ndriving PbTe materials to the brink of the ferroelectric phase transition could\nbe a powerful strategy to solve this problem. We illustrate this concept by\napplying tensile [001] strain to PbTe and its alloys with another rock-salt\nIV-VI material, PbSe; and by alloying PbTe with a rhombohedral IV-VI material,\nGeTe. This induces extremely soft optical modes at the zone center, which\nincrease anharmonic acoustic-optical coupling and decrease phonon lifetimes at\nall frequencies. We predict that PbTe, Pb(Se,Te) and (Pb,Ge)Te alloys driven\nclose to the phase transition in the described manner will have considerably\nlower lattice thermal conductivity than that of PbTe (by a factor of 2-3). The\nproposed concept may open new opportunities for the development of more\nefficient thermoelectric materials.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  While many games were designed for steganography and robust watermarking, few\nfocused on reversible watermarking. We present a two-encoder game related to\nthe rate-distortion optimization of content-adaptive reversible watermarking.\nIn the game, Alice first hides a payload into a cover. Then, Bob hides another\npayload into the modified cover. The embedding strategy of Alice affects the\nembedding capacity of Bob. The embedding strategy of Bob may produce\ndata-extraction errors to Alice. Both want to embed as many pure secret bits as\npossible, subjected to an upper-bounded distortion. We investigate\nnon-cooperative game and cooperative game between Alice and Bob. When they\ncooperate with each other, one may consider them as a whole, i.e., an encoder\nuses a cover for data embedding with two times. When they do not cooperate with\neach other, the game corresponds to a separable system, i.e., both want to\nindependently hide a payload within the cover, but recovering the cover may\nneed cooperation. We find equilibrium strategies for both players under\nconstraints.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A new approach to distributed syntonization (frequency alignment) for the\ncoordination of nodes in open loop coherent distributed antenna arrays to\nenable distributed beamforming is presented. This approach makes use of the\nconcept of consensus optimization among nodes without requiring a centralized\ncontrol. Decentralized frequency consensus can be achieved through iterative\nfrequency exchange among nodes. We derive a model of the signal received from a\ncoherent distributed array and analyze the effects on beamforming of phase\nerrors induced by oscillator frequency drift. We introduce and discuss the\naverage consensus protocol for frequency transfer in undirected networks where\neach node transmits and receives frequency information from other nodes. We\nanalyze the following cases: 1) undirected networks with a static topology; 2)\nundirected networks with dynamic topology, where connections between nodes are\nmade and lost dynamically; and 3) undirected networks with oscillator frequency\ndrift. We show that all the nodes in a given network achieve average consensus\nand the number of iterations needed to achieve consensus can be minimized for a\ngiven cluster of nodes. Numerical simulations demonstrate that the consensus\nalgorithm enables tolerable errors to obtain high coherent gain of greater that\n90\\% of the ideal gain in an error-free distributed phased array.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a new approach to constructing globally strictly convex objective\nfunctional in a 1-D inverse medium scattering problem using multi-frequency\nbackscattering data. The global convexity of the proposed objective functional\nis proved using a Carleman estimate. Due to its convexity, no good first guess\nis required in minimizing this objective functional. We also prove the global\nconvergence of the gradient projection algorithm and derive an error estimate\nfor the reconstructed coefficient. Numerical results show reasonable\nreconstruction accuracy for simulated data.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Like termination, confluence is a central property of rewrite systems. Unlike\nfor termination, however, there exists no known complexity hierarchy for\nconfluence. In this paper we investigate whether the decreasing diagrams\ntechnique can be used to obtain such a hierarchy. The decreasing diagrams\ntechnique is one of the strongest and most versatile methods for proving\nconfluence of abstract rewrite systems. It is complete for countable systems,\nand it has many well-known confluence criteria as corollaries.\n  So what makes decreasing diagrams so powerful? In contrast to other\nconfluence techniques, decreasing diagrams employ a labelling of the steps with\nlabels from a well-founded order in order to conclude confluence of the\nunderlying unlabelled relation. Hence it is natural to ask how the size of the\nlabel set influences the strength of the technique. In particular, what class\nof abstract rewrite systems can be proven confluent using decreasing diagrams\nrestricted to 1 label, 2 labels, 3 labels, and so on? Surprisingly, we find\nthat two labels suffice for proving confluence for every abstract rewrite\nsystem having the cofinality property, thus in particular for every confluent,\ncountable system.\n  Secondly, we show that this result stands in sharp contrast to the situation\nfor commutation of rewrite relations, where the hierarchy does not collapse.\n  Thirdly, investigating the possibility of a confluence hierarchy, we\ndetermine the first-order (non-)definability of the notion of confluence and\nrelated properties, using techniques from finite model theory. We find that in\nparticular Hanf's theorem is fruitful for elegant proofs of undefinability of\nproperties of abstract rewrite systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper deals with the design of the secure network in an Enhanced\nInternet of Vehicles by using the Blockchain Governance Game (BGG). The BGG is\na system model of a stochastic game to find best strategies towards preparation\nof preventing a network malfunction by an attacker and the paper applies this\ngame model into the connected vehicle security. Analytically tractable results\nfor decision-making parameters enable to predict the moment for safety\noperations and to deliver the optimal combination of the number of reserved\nnodes with the acceptance probability of backup nodes to protect a connected\ncar. This research helps for whom considers the enhanced secure IoV\narchitecture with the BGG within a decentralized network.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This work presents a statistically principled method for estimating the\nrequired number of instances in the experimental comparison of multiple\nalgorithms on a given problem class of interest. This approach generalises\nearlier results by allowing researchers to design experiments based on the\ndesired best, worst, mean or median-case statistical power to detect\ndifferences between algorithms larger than a certain threshold. Holm's\nstep-down procedure is used to maintain the overall significance level\ncontrolled at desired levels, without resulting in overly conservative\nexperiments. This paper also presents an approach for sampling each algorithm\non each instance, based on optimal sample size ratios that minimise the total\nrequired number of runs subject to a desired accuracy in the estimation of\npaired differences. A case study investigating the effect of 21 variants of a\ncustom-tailored Simulated Annealing for a class of scheduling problems is used\nto illustrate the application of the proposed methods for sample size\ncalculations in the experimental comparison of algorithms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Bayesian inference provides an attractive online-learning framework to\nanalyze sequential data, and offers generalization guarantees which hold even\nwith model mismatch and adversaries. Unfortunately, exact Bayesian inference is\nrarely feasible in practice and approximation methods are usually employed, but\ndo such methods preserve the generalization properties of Bayesian inference ?\nIn this paper, we show that this is indeed the case for some variational\ninference (VI) algorithms. We consider a few existing online, tempered VI\nalgorithms, as well as a new algorithm, and derive their generalization bounds.\nOur theoretical result relies on the convexity of the variational objective,\nbut we argue that the result should hold more generally and present empirical\nevidence in support of this. Our work in this paper presents theoretical\njustifications in favor of online algorithms relying on approximate Bayesian\nmethods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cancer cells can acquire a spectrum of stable hybrid epithelial/mesenchymal\n(E/M) states during epithelial-mesenchymal transition (EMT). Cells in these\nhybrid E/M phenotypes often combine epithelial and mesenchymal features and\ntend to migrate collectively commonly as small clusters. Such collectively\nmigrating cancer cells play a pivotal role in seeding metastases and their\npresence in cancer patients indicates an adverse prognostic factor. Moreover,\ncancer cells in hybrid E/M phenotypes tend to be more associated with stemness\nwhich endows them with tumor-initiation ability and therapy resistance. Most\nrecently, cells undergoing EMT have been shown to promote immune suppression\nfor better survival. A systematic understanding of the emergence of hybrid E/M\nphenotypes and the connection of EMT with stemness and immune suppression would\ncontribute to more effective therapeutic strategies. In this review, we first\ndiscuss recent efforts combining theoretical and experimental approaches to\nelucidate mechanisms underlying EMT multi-stability (i.e. the existence of\nmultiple stable phenotypes during EMT) and the properties of hybrid E/M\nphenotypes. Following we discuss non-cell-autonomous regulation of EMT by cell\ncooperation and extracellular matrix. Afterwards, we discuss various metrics\nthat can be used to quantify EMT spectrum. We further describe possible\nmechanisms underlying the formation of clusters of circulating tumor cells.\nLast but not least, we summarize recent systems biology analysis of the role of\nEMT in the acquisition of stemness and immune suppression.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the generalised Kre\\u{\\i}n-Feller operator $\\Delta_{\\nu, \\mu} $\nwith respect to compactly supported Borel probability measures $\\mu$ and $\\nu$\nwith the natural restrictions that $\\mu$ is atomless, the\n$\\mathrm{supp}(\\nu)\\subseteq\\mathrm{supp}(\\mu)$ and the atoms of $\\nu $ are\nembedded in the $\\mathrm{supp}(\\mu)$. We show that the solutions of the\neigenvalue problem for $\\Delta_{\\nu, \\mu} $ can be transferred to the\ncorresponding problem for the classical Kre\\u{\\i}n-Feller operator $\\Delta_{\\nu\n\\circ F_{\\mu}^{-1}, \\Lambda}$ with respect to the Lebesgue measure $\\Lambda$\nvia an isometric isomorphism determined by the distribution function of $\\mu$.\nIn this way, we obtain a new characterisation of the upper spectral dimension\nand consolidate many known results on the spectral asymptotics of\nKre\\u{\\i}n-Feller operators. We also recover known properties of and\nconnections to generalised gap diffusions associated to theses operators.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents Stanford Doggo, a quasi-direct-drive quadruped capable of\ndynamic locomotion. This robot matches or exceeds common performance metrics of\nstate-of-the-art legged robots. In terms of vertical jumping agility, a measure\nof average vertical speed, Stanford Doggo matches the best performing animal\nand surpasses the previous best robot by 22%. An overall design architecture is\npresented with focus on our quasi-direct-drive design methodology. The hardware\nand software to replicate this robot is open-source, requires only hand tools\nfor manufacturing and assembly, and costs less than $3000.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A beam of holes formed in graphene by a collimating contact is imaged using a\nliquid-He cooled scanning probe microscope (SPM). The mean free path of holes\nis greater than the device dimensions. A zigzag shaped pattern on both sides of\nthe collimating contact absorb holes that enter at large angles. The image\ncharge beneath the SPM tip defects holes, and the pattern of flow is imaged by\ndisplaying the change in conductance between contacts on opposite sides, as the\ntip is raster scanned across the sample. Collimation is confirmed by bending\nhole trajectories away from the receiving contact with an applied magnetic\nfield. The SPM images agree well with ray-tracing simulations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The intensive computation and memory requirements of generative adversarial\nneural networks (GANs) hinder its real-world deployment on edge devices such as\nsmartphones. Despite the success in model reduction of CNNs, neural network\nquantization methods have not yet been studied on GANs, which are mainly faced\nwith the issues of both the effectiveness of quantization algorithms and the\ninstability of training GAN models. In this paper, we start with an extensive\nstudy on applying existing successful methods to quantize GANs. Our observation\nreveals that none of them generates samples with reasonable quality because of\nthe underrepresentation of quantized values in model weights, and the generator\nand discriminator networks show different sensitivities upon quantization\nmethods. Motivated by these observations, we develop a novel quantization\nmethod for GANs based on EM algorithms, named as QGAN. We also propose a\nmulti-precision algorithm to help find the optimal number of bits of quantized\nGAN models in conjunction with corresponding result qualities. Experiments on\nCIFAR-10 and CelebA show that QGAN can quantize GANs to even 1-bit or 2-bit\nrepresentations with results of quality comparable to original models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Proposing new materials by atom substitution based on periodic table\nsimilarity is a conventional strategy of searching for materials with desired\nproperty. We introduce a machine learning frame work that promotes this\nparadigm to be property-specific and quantitative. It is of peculiar usefulness\nin situations where abundance data is accessible for learning general knowledge\nbut samples for the problem of interest are relatively scarce. We showcase its\nusage and viability in the problem of separating high entropy alloys with\ndifferent structural phases, for which a very simple data-driven criterion\nachieves differentiating ability comparable with widely used empirical\ncriteria. Its flexibility and generability make it a promising tool in other\nmaterial discovery tasks and far beyond.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Data scarcity is a long-standing and crucial challenge that hinders quick\ndevelopment of task-oriented dialogue systems across multiple domains:\ntask-oriented dialogue models are expected to learn grammar, syntax, dialogue\nreasoning, decision making, and language generation from absurdly small amounts\nof task-specific data. In this paper, we demonstrate that recent progress in\nlanguage modeling pre-training and transfer learning shows promise to overcome\nthis problem. We propose a task-oriented dialogue model that operates solely on\ntext input: it effectively bypasses explicit policy and language generation\nmodules. Building on top of the TransferTransfo framework (Wolf et al., 2019)\nand generative model pre-training (Radford et al., 2019), we validate the\napproach on complex multi-domain task-oriented dialogues from the MultiWOZ\ndataset. Our automatic and human evaluations show that the proposed model is on\npar with a strong task-specific neural baseline. In the long run, our approach\nholds promise to mitigate the data scarcity problem, and to support the\nconstruction of more engaging and more eloquent task-oriented conversational\nagents.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The recent rising popularity of ultra-fast delivery services on retail\nplatforms fuels the increasing use of urban warehouses, whose proximity to\ncustomers makes fast deliveries viable. The space limit in urban warehouses\nposes a problem for the online retailers: the number of products (SKUs) they\ncarry is no longer \"the more, the better\", yet it can still be significantly\nlarge, reaching hundreds or thousands in a product category. In this paper, we\nstudy algorithms for dynamically identifying a large number of products (i.e.,\nSKUs) with top customer purchase probabilities on the fly, from an ocean of\npotential products to offer on retailers' ultra-fast delivery platforms.\n  We distill the product selection problem into a semi-bandit model with linear\ngeneralization. There are in total $N$ different arms, each with a feature\nvector of dimension $d$. The player pulls $K$ arms in each period and observes\nthe bandit feedback from each of the pulled arms. We focus on the setting where\n$K$ is much greater than the number of total time periods $T$ or the dimension\nof product features $d$. We first analyze a standard UCB algorithm and show its\nregret bound can be expressed as the sum of a $T$-independent part $\\tilde O(K\nd^{3/2})$ and a $T$-dependent part $\\tilde O(d\\sqrt{KT})$, which we refer to as\n\"fixed cost\" and \"variable cost\" respectively. To reduce the fixed cost for\nlarge $K$ values, we propose a novel online learning algorithm, which\niteratively shrinks the upper confidence bounds within each period, and show\nits fixed cost is reduced by a factor of $d$ to $\\tilde O(K \\sqrt{d})$.\nMoreover, we test the algorithms on an industrial dataset from Alibaba Group.\nExperimental results show that our new algorithm reduces the total regret of\nthe standard UCB algorithm by at least 10%.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This report lists the link diagrams in S^3 for all principal congruence link\ncomplements for which such a link diagram is known. Several unpublished link\ndiagrams are included. Related to this, we also include one link diagram for an\narithmetic regular tessellation link complement.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Many popular methods for building confidence intervals on causal effects\nunder high-dimensional confounding require strong \"ultra-sparsity\" assumptions\nthat may be difficult to validate in practice. To alleviate this difficulty, we\nhere study a new method for average treatment effect estimation that yields\nasymptotically exact confidence intervals assuming that either the conditional\nresponse surface or the conditional probability of treatment allows for an\nultra-sparse representation (but not necessarily both). This guarantee allows\nus to provide valid inference for average treatment effect in high dimensions\nunder considerably more generality than available baselines. In addition, we\nshowcase that our results are semi-parametrically efficient.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  How long does it take for an initially advantageous mutant to establish\nitself in a resident population, and what does the population composition look\nlike then? We approach these questions in the framework of the so called Bare\nBones evolution model Klebaner et al (2011) that provides a simplified approach\nto the adaptive population dynamics of binary splitting cells. As the mutant\npopulation grows, cell division becomes less probable, and it may in fact turn\nless likely than that of residents. Our analysis rests on the assumption of the\nprocess starting from resident population, with sizes proportional to a large\ncarrying capacity $K$. Actually, we assume carrying capacities to be $a_1K$ and\n$a_2K$ for the resident and the mutant populations, respectively, and study the\ndynamics for $K\\to\\infty$. We find conditions for the mutant to be successful\nin establishing itself alongside the resident. The time it takes turns out to\nbe proportional to $\\log K$. We introduce the time of establishment through the\nasymptotic behavior of the stochastic nonlinear dynamics describing the\nevolution, and show that it is indeed $\\log K/\\log \\rho$, where $\\rho>1$ is\ntwice the probability of successful division of the mutant at its appearance.\nLooking at the composition of the population, at times $\\log K/\\log \\rho +n, n\n\\in \\mathbb{Z}_+$, we find that the densities (i.e. sizes relative to carrying\ncapacities) of both populations follow closely the corresponding two\ndimensional nonlinear deterministic dynamics that starts at {\\it a random\npoint}. We characterise this random initial condition in terms of the scaling\nlimit of the corresponding dynamics, and the limit of the properly scaled\ninitial binary splitting process of the mutant. The deterministic approximation\nwith random initial condition is in fact valid asymptotically at all times\n$\\log K/\\log \\rho +n$ with $n\\in \\mathbb{Z}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $p ,r $ and $n $ be positive integers. Then the O-Fibonacci $(p,r)$-cube\n$O\\Gamma^{(p,r)}_{n}$ is the subgraph of $Q_{n}$ induced on the binary words in\nwhich there is at least $p-1$ zeros between any two $1$s and there is at most\n$r$ consecutive $10^{p-1}$. These cubes include a wide range of cubes as their\nspecial cases, such as hypercubes, Fibonacci cubes, and postal networks. In\nthis note it is proved that $O\\Gamma^{(p,r)}_{n}$ is a non-trivial Cartesian\nproduct if and only if $p=1$ and $r\\geq n\\geq2$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present an analysis of HI Lyman-alpha emission in deep VLT/MUSE\nobservations of two highly magnified and extended galaxies at z=3.5 and 4.03,\nincluding a newly discovered, almost complete Einstein ring. While these\nLyman-alpha haloes are intrinsically similar to the ones typically seen in\nother MUSE deep fields, the benefits of gravitational lensing allows us to\nconstruct exceptionally detailed maps of Lyman-alpha line properties at sub-kpc\nscales. By combining all multiple images, we are able to observe complex\nstructures in the Lyman-alpha emission and uncover small (~ 120 km/s in\nLyman-alpha peak shift), but significant at > 4 sigma, systematic variations in\nthe shape of the Lyman-alpha line profile within each halo. Indeed, we observe\na global trend for the line peak shift to become redder at large radii,\ntogether with a strong correlation between the peak wavelength and line width.\nThis systematic intrahalo variation is markedly similar to the object-to-object\nvariations obtained from the integrated properties of recent large samples.\nRegions of high surface brightness correspond to relatively small line shifts,\nwhich could indicate that Lyman-alpha emission escapes preferentially from\nregions where the line profile has been less severely affected by scattering of\nLyman-alpha photons.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The paper describes the practical work for students visually clarifying the\nmechanism of the Monte Carlo method applying to approximating the value of Pi.\nConsidering a traditional quadrant (circular sector) inscribed in a square,\nhere we demonstrate the original algorithm for generating random points on the\npaper: you should arbitrarily tear up a paper blank to small pieces (the first\nexperiment). By the similar way the second experiment (with a preliminary\nstaining procedure by bright colors) can be used to prove the quadratic\ndependence of the area of a circle on its radius. Manipulations with tearing up\na paper as a random sampling algorithm can be applied for solving other\nteaching problems in physics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Transfer learning is a very important tool in deep learning as it allows\npropagating information from one \"source dataset\" to another \"target dataset\",\nespecially in the case of a small number of training examples in the latter.\nYet, discrepancies between the underlying distributions of the source and\ntarget data are commonplace and are known to have a substantial impact on\nalgorithm performance. In this work we suggest a novel information theoretic\napproach for the analysis of the performance of deep neural networks in the\ncontext of transfer learning. We focus on the task of semi-supervised transfer\nlearning, in which unlabeled samples from the target dataset are available\nduring the network training on the source dataset. Our theory suggests that one\nmay improve the transferability of a deep neural network by imposing a Lautum\ninformation based regularization that relates the network weights to the target\ndata. We demonstrate the effectiveness of the proposed approach in various\ntransfer learning experiments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The two-dimensional electron liquid which forms between the band insulators\nLaAlO3 (LAO) and SrTiO3 (STO) is a promising component for oxide electronics,\nbut the requirement of using single crystal SrTiO3 substrates for the growth\nlimits its applications in terms of device fabrication. It is therefore\nimportant to find ways to deposit these materials on other substrates,\npreferably Si, or Si-based, in order to facilitate integration with existing\ntechnology. Interesting candidates are micron-sized nanosheets of Ca2Nb3O10\nwhich can be used as seed layers for perovskite materials on any substrate. We\nhave used low-energy electron microscopy (LEEM) with in-situ pulsed laser\ndeposition to study the subsequent growth of STO and LAO on such flakes which\nwere deposited on Si. We can follow the morphology and crystallinity of the\nlayers during growth, as well as fingerprint their electronic properties with\nangle resolved reflected electron spectroscopy. We find that STO layers,\ndeposited on the nanosheets, can be made crystalline and flat; that LAO can be\ngrown in a layer-by-layer fashion; and that the full heterostructure shows the\nsignature of the formation of a conducting interface.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We generalize, and then use, a recently introduced formalism to study thermal\nfluctuations of atomic displacements in several two and three dimensional\ncrystals. We study both close packed as well as open crystals with multi atom\nbases. Atomic displacement fluctuations in a solid, once coarse-grained over\nsome neighborhood may be decomposed into two mutually orthogonal components. In\nany dimension $d$ there are always $d^2$ {\\em affine} displacements\nrepresenting local strains and rotations of the ideal reference configuration.\nIn addition, there exists a number of {\\em non-affine} localized displacement\nmodes that cannot be represented as strains or rotations. The number of these\nmodes depends on $d$ and the size of the coarse graining region. All\nthermodynamic averages and correlation functions concerning the affine and\nnon-affine displacements may be computed within a harmonic theory. We show that\nfor compact crystals, such as the square and triangular in $d=2$ and the\nsimple, body-centered and face-centered cubic crystals in $d=3$, a single set\nof $d-$fold degenerate modes always dominate the non-affine sub-space and are\nseparated from the rest by a large gap. These modes may be identified with\nspecific precursor configurations that lead to lattice defects. Deformation\nmechanisms such as lattice slips and stacking faults in close packed crystals\ncan also be understood within this framework. The qualitative features of these\nconclusions are expected to be independent of the details of the atomic\ninteractions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Well-motivated electroweak dark matter is often hosted by an extended\nelectroweak sector which also contains new lepton pairs with masses near the\nweak scale. In this paper, we explore such electroweak dark matter via\ncombining dark matter direct detections and high-luminosity LHC probes of new\nlepton pairs. Using $Z$- and $W$-associated electroweak processes with two or\nthree lepton final states, we show that dependent on the overall coupling\nconstant, dark matter mass up to $170-210$ GeV can be excluded at $2\\sigma$\nlevel and up to $175-205$ GeV can be discovered at $5\\sigma$ level at the 14\nTeV LHC with integrated luminosities 300 fb$^{-1}$ and 3000 fb$^{-1}$,\nrespectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For a foreseeable future, autonomous vehicles (AVs) will operate in traffic\ntogether with human-driven vehicles. Their planning and control systems need\nextensive testing, including early-stage testing in simulations where the\ninteractions among autonomous/human-driven vehicles are represented. Motivated\nby the need for such simulation tools, we propose a game-theoretic approach to\nmodeling vehicle interactions, in particular, for urban traffic environments\nwith unsignalized intersections. We develop traffic models with heterogeneous\n(in terms of their driving styles) and interactive vehicles based on our\nproposed approach, and use them for virtual testing, evaluation, and\ncalibration of AV control systems. For illustration, we consider two AV control\napproaches, analyze their characteristics and performance based on the\nsimulation results with our developed traffic models, and optimize the\nparameters of one of them.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we generalize the main results of [1] and [31] to Lorentz\nspaces, using a simple procedure. The main results are the following. Let\n$n\\geq 3$ and let $u$ be a Leray-Hopf solution to the $n$-dimensional\nNavier-Stokes equations with viscosity $\\nu$ and divergence free initial\ncondition $u_0\\in L^2(\\mathbb{R}^n)\\cap L^{k}(\\mathbb{R}^n)$ (where $k=k(s)$ is\nsufficiently large). Then there exists a constant $c>0$ such that if\n\\begin{equation}\n\\|p\\|_{L^{r,\\infty}(0,\\infty;L^{s,\\infty}(\\mathbb{R}^n))}<c\\hspace{10mm}\\frac{n}{s}+\\frac{2}{r}\\leq\n2,\\hspace{5mm}s>\\frac{n}{2} \\end{equation} or \\begin{equation} \\|\\nabla\np\\|_{L^{r,\\infty}(0,\\infty;L^{s,\\infty}(\\mathbb{R}^n))}<c\\hspace{10mm}\\frac{n}{s}+\\frac{2}{r}\\leq\n3,\\hspace{5mm}s>\\frac{n}{3} \\end{equation} then $u$ is smooth on $(0, \\infty)\n\\times \\mathbb{R}^n$. Partial results in the case $n=3$ were obtained in [32],\n[33] and then recently extended to all appropriate pairs of $r,s$ in [14]. Our\nresults present a unified proof which works for all dimensions $n\\geq 3$ and\nthe full range or admissible pairs, $(s,r)$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, using additive characters of finite field, we find a codebook\nwhich is equivalent to the measurement matrix in [20]. The advantage of our\nconstruction is that it can be generalized naturally to construct the other\nfive classes of codebooks using additive and multiplicative characters of\nfinite field. We determine the maximal cross-correlation amplitude of these\ncodebooks by the properties of characters and character sums. We prove that all\nthe codebooks we constructed are asymptotically optimal with respect to the\nWelch bound. The parameters of these codebooks are new.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we verify the $L^p$ coarse Baum-Connes conjecture for spaces\nwith finite asymptotic dimension for $p\\in[1,\\infty)$. We also show that the\n$K$-theory of $L^p$ Roe algebras are independent of $p\\in(1,\\infty)$ for spaces\nwith finite asymptotic dimension.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The THERMOS toolkit has been developed to calculate radiative properties of\nplasmas. This article contains a brief survey of some of its key features used\nby calculation of opacities and emissivities and by analysis of specific\nexperiments. The code has recently been upgraded to account for the effect of\nionization potential lowering in dense plasmas. The functionality of the code\nis illustrated for several cases from the 10th NLTE Code Comparison Workshop,\nin particular, for the experimental spectra of chlorine [1] and for the\nmeasured transmission of a silicon plasma [2].\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the potential utility of classical techniques of spectral\nsparsification of graphs as a preprocessing step for digital quantum\nalgorithms, in particular, for Hamiltonian simulation. Our results indicate\nthat spectral sparsification of a graph with $n$ nodes through a sampling\nmethod, e.g.\\ as in \\cite{Spielman2011resistances} using effective resistances,\ngives, with high probability, a locally computable matrix $\\tilde H$ with row\nsparsity at most $\\mathcal{O}(\\text{poly}\\log n)$. For a symmetric matrix $H$\nof size $n$ with $m$ non-zero entries, a one-time classical runtime overhead of\n$\\mathcal{O}(m||H||t\\log n/\\epsilon)$ expended in spectral sparsification is\nthen found to be useful as a way to obtain a sparse matrix $\\tilde H$ that can\nbe used to approximate time evolution $e^{itH}$ under the Hamiltonian $H$ to\nprecision $\\epsilon$. Once such a sparsifier is obtained, it could be used with\na variety of quantum algorithms in the query model that make crucial use of row\nsparsity. We focus on the case of efficient quantum algorithms for sparse\nHamiltonian simulation, since Hamiltonian simulation underlies, as a key\nsubroutine, several quantum algorithms, including quantum phase estimation and\nrecent ones for linear algebra. Finally, we also give two simple quantum\nalgorithms to estimate the row sparsity of an input matrix, which achieve a\nquery complexity of $\\mathcal{O}(n^{3/2})$ as opposed to $\\mathcal{O}(n^2)$\nthat would be required by any classical algorithm for the task.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider generalized models on coral broadcast spawning phenomena\ninvolving diffusion, advection, chemotaxis, and reactions when egg and sperm\ndensities are different. We prove the global-in-time existence of the regular\nsolutions of the models as well as their temporal decays in two and three\ndimensions. We also show that the total masses of egg and sperm density have\npositive lower bounds as time tends to infinity in three dimensions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let (M, g) be an (n + 1)-dimensional asymptotically locally hyperbolic (ALH)\nmanifold with a conformal compactification whose conformal infinity is\n($\\partial$M, [$\\gamma$]). We will first observe that Ch(M, g) $\\le$ n, where\nCh(M, g) is the Cheeger constant of M. We then prove that, if the Ricci\ncurvature of M is bounded from below by --n and its scalar curvature approaches\n--n(n+1) fast enough at infinity, then Ch(M, g) = n if and only Y($\\partial$M,\n[$\\gamma$]) $\\ge$ 0, where Y($\\partial$M, [$\\gamma$]) denotes the Yamabe\ninvariant of the conformal infinity. This gives an answer to a question raised\nby J. Lee [L].\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We compute generating functions for elliptic genera with values in line\nbundles on Hilbert schemes of points on surfaces. As an application we also\ncompute generating functions for elliptic genera with values in determinant\nline bundles on moduli spaces of sheaves on K3 surfaces.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Quantifying uncertainty using confidence regions is a central goal of\nstatistical inference. Despite this, methodologies for confidence bands in\nFunctional Data Analysis are still underdeveloped compared to estimation and\nhypothesis testing. In this work, we present a new methodology for constructing\nsimultaneous confidence bands for functional parameter estimates. Our bands\npossess a number of positive qualities: (1) they are not based on resampling\nand thus are fast to compute, (2) they are constructed under the fairness\nconstraint of balanced false positive rates across partitions of the bands'\ndomain which facilitates the typical global, but also novel local\ninterpretations, and (3) they do not require an estimate of the full covariance\nfunction and thus can be used in the case of fragmentary functional data.\nSimulations show the excellent finite-sample behavior of our bands in\ncomparison to existing alternatives. The practical use of our bands is\ndemonstrated in two case studies on sports biomechanics and fragmentary growth\ncurves.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Quantum states can acquire a geometric phase called the Berry phase after\nadiabatically traversing a closed loop, which depends on the path not the rate\nof motion. The Berry phase is analogous to the Aharonov-Bohm phase derived from\nthe electromagnetic vector potential, and can be expressed in terms of an\nAbelian gauge potential called the Berry connection. Wilczek and Zee extended\nthis concept to include non-Abelian phases -- characterized by the gauge\nindependent Wilson loop -- resulting from non-Abelian gauge potentials. Using\nan atomic Bose-Einstein condensate, we quantum-engineered a non-Abelian SU(2)\ngauge field, generated by a Yang monopole located at the origin of a\n5-dimensional parameter space. By slowly encircling the monopole, we\ncharacterized the Wilczek-Zee phase in terms of the Wilson loop, that depended\non the solid-angle subtended by the encircling path: a generalization of\nStokes' theorem. This observation marks the observation of the Wilson loop\nresulting from a non-Abelian point source.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the partition functions of BPS vortices and magnetic monopole\noperators, in gauge theories describing $N$ M2-branes. In particular, we\nexplore two closely related methods to study the Cardy limit of the index on\n$S^2\\times\\mathbb{R}$. The first method uses the factorization of this index to\nvortex partition functions, while the second one uses a continuum approximation\nfor the monopole charge sums. Monopole condensation confines most of the $N^2$\ndegrees of freedom except $N^{\\frac{3}{2}}$ of them, even in the high\ntemperature deconfined phase. The resulting large $N$ free energy statistically\naccounts for the Bekenstein-Hawking entropy of large BPS black holes in\n$AdS_4\\times S^7$. Our Cardy free energy also suggests a finite $N$ version of\nthe $N^{\\frac{3}{2}}$ degrees of freedom.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The most abundant stars in the Galaxy, M dwarfs, are very commonly hosts to\ndiverse systems of low-mass planets. Their abundancy implies that the general\noccurrence rate of planets is dominated by their occurrence rate around such M\ndwarfs. In this article, we combine the M dwarf surveys conducted with the\nHIRES/Keck, PFS/Magellan, HARPS/ESO, and UVES/VLT instruments supported with\ndata from several other instruments. We analyse the radial velocities of an\napproximately volume- and brightness-limited sample of 426 nearby M dwarfs in\norder to search for Doppler signals of cadidate planets. In addition, we\nanalyse spectroscopic activity indicators and ASAS photometry to rule out\nradial velocity signals corresponding to stellar activity as Doppler signals of\nplanets. We calculate estimates for the occurrence rate of planets around the\nsample stars and study the properties of this occurrence rate as a function of\nstellar properties. Our analyses reveal a total of 118 candidate planets\norbiting nearby M dwarfs. Based on our results accounting for selection effects\nand sample detection threshold, we estimate that M dwarfs have on average at\nleast 2.39$^{+4.58}_{-1.36}$ planets per star orbiting them. Accounting for the\ndifferent sensitivities of radial velocity surveys and Kepler transit\nphotometry implies that there are at least 3.0 planets per star orbiting M\ndwarfs. We also present evidence for a population of cool mini-Neptunes and\nNeptunes with indications that they are found an order of magnitude more\nfrequently orbiting the least massive M dwarfs in our sample.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  It is quite well known for some time that string inspired axionic terms of\nthe form $\\nu (\\phi)\\tilde{R}R$, known also as Chern-Simons terms, do not\naffect the scalar perturbations and the background evolution for a flat\nFriedman-Robertson-Walker Universe. In this paper we study and quantify the\nimplications of the presence of the above term in the context of vacuum $f(R)$.\nParticularly, we assume that axionic dark matter is present during inflation,\nand we examine in a quantitative way the effects of axionic Chern-Simons terms\non the tensor perturbations. The axion field is quantified in terms of a\ncanonical scalar field, with broken Peccei-Quinn symmetry. The model perfectly\ndescribing axions as potential dark matter candidates is based on the so-called\nmisalignment mechanism, in which case the axion is frozen near its non-zero\nvacuum expectation value during early times in which $H\\gg m_a$. In effect, the\ninflationary era is mainly controlled by the $f(R)$ gravity and the\nChern-Simons term. As we demonstrate, the Chern-Simons term may achieve to make\na non-viable $f(R)$ gravity theory to be phenomenologically viable, due to the\nfact that the tensor-to-scalar ratio is significantly reduced, and the same\napplies to the spectral index of the tensor perturbations $n_T$. Also by\nstudying the Starobinsky model in the presence of the Chern-Simons term, we\ndemonstrate that it is possible to further reduce the amount of primordial\ngravitational radiation. The issues of having parity violating gravitational\nwaves, also the graceful exit from inflation due to axion oscillations and\nfinally the unification of dark energy-inflation and axion dark matter in the\nsame $f(R)$ gravity-axion dark matter model, are also briefly discussed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we report a new method to simulate active Brownian particles\n(ABPs) in molecular dynamics (MD) simulations. Immersed in a fluid, each ABP\nconsists of a head particle and a spherical phantom region of fluid where the\nflagellum of a microswimmer takes effect. The orientation of the active\nparticle is governed by a stochastic dynamics, with the orientational\npersistence time determined by the rotational diffusivity. To hydrodynamically\ndrive the active particle as a pusher, a pair of active forces are exerted on\nthe head particle and the phantom fluid region respectively. The active\nvelocity measured along the particle orientation is proportional to the\nmagnitude of the active force. The effective diffusion coefficient of the\nactive particle is first measured in free space, showing semi-quantitative\nagreement with the analytical result predicted by a minimal model for ABPs. We\nthen turn to the probability distribution of the active particle in confinement\npotential. We find that the stationary particle distribution undergoes an\nevolution from the Boltzmann-type to non-Boltzmann distribution as the\norientational persistence time is increased relative to the relaxation time in\nthe potential well. From the stationary distribution in confinement potential,\nthe active part of the diffusion coefficient is measured and compared to that\nobtained in free space, showing a good semi-quantitative agreement while the\norientational persistence time varies greatly relative to the relaxation time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Stable Marriage Problem (SMP) has been extremely discussed in the\nliterature and it is useful to a number of real-world applications. We propose\na generalized version of the SMP in which numbers of the matching groups are\ndifferent as in [9]. However, we go further to make a percentage of each group\nbehave as active message senders. As such, the special case in which all Males\nare active messengers (beta = 1) and all Females are not active (alpha = 0)\nreplicates the results in [9]. Moreover, we use numerical simulation to present\nthree cases (and their extremes) in which we vary the percentage of active\nmessengers in each group. Whereas we are able to replicate previous work, our\nnumerical simulations also suggest that socially optimal comes only when the\ngroups are homogeneous. More real-world like results are presented when members\nfrom both groups are active message senders.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  I give a simple proof for the fact that positive entropy subshifts contain\ninfinite binary trees where branching happens synchronously in each branch, and\nthat the branching times form a set with positive lower asymptotic density.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  To maintain the desired quality of a product or service it is necessary to\nmonitor the process that results in the product or service. This monitoring\nmethod is called Statistical Process Management, or Statistical Process\nControl. It is in widespread usage in industry. Extensive statistical\nmethodology has been developed to make it possible to detect when a process\ngoes out of control while allowing for natural variability that occurs when the\nprocess is in control. This paper introduces nonparametric methods for\nmonitoring data, whether it is univariate or multivariate, and whether the\ninterest is in detecting a change of location or scale or both. These methods,\nbased on sequential normal scores, are much simpler than the most popular\nnonparametric methods currently in use and have good power for detecting out of\ncontrol observations. Sixteen new statistical tests are introduced for the\nfirst time in this paper, with 17 examples, 33 tables, and 48 figures to\ncomplete the instructions for their application.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Background: The role of neonatal pain on the developing nervous system is not\ncompletely understood, but evidence suggests that sensory pathways are\ninfluenced by an infants pain experience. Research has shown that an infants\nprevious pain experiences lead to an increased, and likely abnormal, response\nto subsequent painful stimuli. We are working to improve neonatal pain\ndetection through automated devices that continuously monitor an infant. The\ncurrent study outlines some of the initial steps we have taken to evaluate Near\nInfrared Spectroscopy (NIRS) as a technology to detect neonatal pain. Our\nfindings may provide neonatal intensive care unit (NICU) practitioners with the\ndata necessary to monitor and perhaps better manage an abnormal pain response.\nMethods: A prospective pilot study was conducted to evaluate nociceptive evoked\ncortical activity in preterm infants. NIRS data were recorded for approximately\n10 minutes prior to an acute painful procedure and for approximately 10 minutes\nafter the procedure. Individual data collection events were performed at a\nweekly maximum frequency. Eligible infants included those admitted to the Tampa\nGeneral Hospital (TGH) NICU with a birth gestational age of less than 37 weeks.\nResults: A total of 15 infants were enrolled and 25 individual studies were\ncompleted. Analysis demonstrated a statistically significant difference between\nthe median of the pre- and post-painful procedure data sets in each infants\nfirst NIRS collection (p value = 0.01). Conclusions: Initial analysis shows\nNIRS may be useful in detecting acute pain. An acute painful procedure is\ntypically followed by a negative deflection in NIRS readings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the locus of $r$-tuples of homogeneous forms of some fixed degree\nwhose common vanishing locus in $\\mathbb{P}^r$ is positive dimensional. We show\nthat any component of maximal dimension of that locus either consists of\nhomogeneous forms all vanishing on some line or homogeneous forms where a\nproper subset fail to intersect properly.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce a notion of Morse shellings (and tilings) on finite simplicial\ncomplexes which extends the classical one and its relation to discrete Morse\ntheory.Skeletons and barycentric subdivisions of Morse shellable (or tileable)\nsimplicial complexes are Morse shellable (or tileable). Moreover, every\ntriangulated closed surface is Morse shellable while every closed\nthree-manifold carries Morse shellable triangulations. Finally, any shelling\nencodes a class of discrete Morse functions whose critical points are in\none-to-one correspondence, preserving the index, with the critical tiles of the\nshelling.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  N. Hindman and I. Leader introduced the set of ultrafilters 0+ on (0,1) and\ncharacterize smallest ideal of (0+,+) and proved the Central Set Theorem near\nzero. Recently Polynomial Central Set Theorem has been proved by V. Bergelson,\nJ. H. Johnson Jr. and J. Moreira. In this article, we will prove Polynomial\nCentral Set Theorem near zero.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep metric learning seeks to define an embedding where semantically similar\nimages are embedded to nearby locations, and semantically dissimilar images are\nembedded to distant locations. Substantial work has focused on loss functions\nand strategies to learn these embeddings by pushing images from the same class\nas close together in the embedding space as possible. In this paper, we propose\nan alternative, loosened embedding strategy that requires the embedding\nfunction only map each training image to the most similar examples from the\nsame class, an approach we call \"Easy Positive\" mining. We provide a collection\nof experiments and visualizations that highlight that this Easy Positive mining\nleads to embeddings that are more flexible and generalize better to new unseen\ndata. This simple mining strategy yields recall performance that exceeds state\nof the art approaches (including those with complicated loss functions and\nensemble methods) on image retrieval datasets including CUB, Stanford Online\nProducts, In-Shop Clothes and Hotels-50K.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Approximate analytical solutions of the modified Langevin equation are\nobtained. These solutions are relatively simple and enough accurate. They are\nillustrated by considering a mean-field model of a system with interacting\nsuperparamagnetic particles. Within the framework of this model system we\nderived analytical approximate formulas for the temperature dependencies of the\nsaturation and remnant magnetization, coercive force, initial magnetic\nsusceptibility as well as for the law of approach to saturation. We obtained\nalso some exact analytical relationships for the coercive force. We found\nremarkable similarity between the approximate cubic equation, which is resulted\nfrom the modified Langevin equation, and the exact equation resulting from the\ndivergence condition of a solution derivative. The analytical formulas obtained\nin this work can be used in various models (not only magnetic ones), where the\nmodified Langevin equation is applied.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The chemical composition of protostellar jets and its origin are still badly\nunderstood. More observational constraints are needed to make progress. With\nthat objective, we have carried out a systematic search for molecular species\nin the jet of Cep E-mm, a template for intermediate-mass Class 0 protostars,\nassociated with a luminous, high-velocity outflow. We made use of an unbiased\nspectral line survey in the range 72-350 GHz obtained with the IRAM 30m\ntelescope, complementary observations of the CO $J$=3-2 transition with the\nJCMT, and observations at 1\" angular resolution of the CO $J$=2-1 transition\nwith the IRAM Plateau de Bure interferometer. In addition to CO, we have\ndetected rotational transitions from SiO, SO, H$_2$CO, CS, HCO$^{+}$ and HCN. A\nstrong chemical differentiation is observed in the southern and northern lobes\nof the jet. Radiative transfer analysis in the Large Velocity Gradient\napproximation yields typical molecular abundances of the order of $10^{-8}$ for\nall molecular species other than CO. Overall, the jets exhibit an unusual\nchemical composition, as CS, SO and H$_2$CO are found to be the most abundant\nspecies, with a typical abundance of (3-4)$\\times 10^{-8}$. The transverse size\nof the CO jet emission estimated from interferometric observations is about\n1000 au, suggesting that we are detecting emission from a turbulent layer of\ngas entrained by the jet in its propagation and not the jet itself. We propose\nthat some molecular species could be the signatures of the specific\nphotochemistry driven by the UV radiation field generated in the turbulent\nenvelope.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Most state-of-the-art action localization systems process each action\nproposal individually, without explicitly exploiting their relations during\nlearning. However, the relations between proposals actually play an important\nrole in action localization, since a meaningful action always consists of\nmultiple proposals in a video. In this paper, we propose to exploit the\nproposal-proposal relations using Graph Convolutional Networks (GCNs). First,\nwe construct an action proposal graph, where each proposal is represented as a\nnode and their relations between two proposals as an edge. Here, we use two\ntypes of relations, one for capturing the context information for each proposal\nand the other one for characterizing the correlations between distinct actions.\nThen we apply the GCNs over the graph to model the relations among different\nproposals and learn powerful representations for the action classification and\nlocalization. Experimental results show that our approach significantly\noutperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover,\naugmentation experiments on ActivityNet also verify the efficacy of modeling\naction proposal relationships. Codes are available at\nhttps://github.com/Alvin-Zeng/PGCN.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We find an intervalley wave collective mode in two- and three-dimensional\nDirac semimetals in the presence of a valley population imbalance. The\ndispersion relation of this mode is gapless, proportional to the square of the\nwave vector at small frequencies, and inversely proportional to the\nelectron-electron exchange interaction energy. The valley wave serves as an\nenergy gain source for the external field, that generates the intervalley\ntransitions. The spin wave analog is discussed for the case of a semimetal with\nnonequilibrium spin orientation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The circumstellar habitable zone and its various refinements serves as a\nuseful entry point for discussing the potential for a planet to generate and\nsustain life. But little attention is paid to the quality of available energy\nin the form of stellar photons for phototrophic (e.g. photosynthetic) life.\nThis short paper discusses the application of the concept of exergy to\nexoplanetary environments and the evaluation of the maximum efficiency of\nenergy use, or maximum work obtainable from electromagnetic radiation. Hotter\nstars provide temperate planets with higher maximum obtainable work with higher\nefficiency than cool stars, and cool planets provide higher efficiency of\nradiation conversion from the same stellar photons than do hot planets. These\nstatements are independent of the details of any photochemical and biochemical\nmechanisms and could produce systematic differences in planetary habitability,\nespecially at the extremes of maximal or minimal biospheres, or at critical\necological tipping points. Photoautotrophic biospheres on habitable planets\naround M-dwarf stars may be doubly disadvantaged by lower fluxes of\nphotosynthetically active photons, and lower exergy with lower energy\nconversion efficiency.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We considered the Asynchronous SIR (susceptible-infected-removed) model on\nPenrose and Ammann-Beenker quasiperiodic lattices, and obtained its critical\nbehavior by using Newman-Ziff algorithm to track cluster propagation by making\na tree structure of clusters grown at the dynamics, allowing to simulate SIR\nmodel on non-periodic lattices and measure any observable related to\npercolation. We numerically calculated the order parameter, defined in a\ngeographical fashion by distinguish between an epidemic state, characterized by\na spanning cluster formed by the removed nodes and the endemic state, where\nthere is no spanning cluster. We obtained the averaged mean cluster size which\nplays the role of a susceptibility, and a cumulant ratio defined for\npercolation to estimate the epidemic threshold. Our numerical results suggest\nthat the system falls into two-dimensional dynamic percolation universality\nclass and the quasiperiodic order is irrelevant, in according to results for\nclassical percolation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Seismic data obtained with the space photometric CoRoT and Kepler instruments\nhave led to a unprecendently precise characterization -- in terms of masses and\nages -- of a large sample of post main sequence stars (low mass subgiant and\nred giants). The high quality of the collected seismic data and the subsequent\ntheoretical work for interpreting them brought up a series of issues which\nrevealed that our knowledge of the internal properties of red giant stars\nremains quite limited. Two such important issues are discussed here, namely\nmixing beyond the convective core of helium burning red giant stars and\nevolution of internal angular momentum for post main sequence stars. This\nincludes how they were diagnosed and what are the resulting improvements in our\nunderstanding regarding these issues (or rather how far we are from a proper\nunderstanding and realistic modelling of the structure and evolution of post\nmain sequence stars).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the globular cluster NGC2808, a quasi-standard initial lithium abundance\nis derived for a red giant belonging to the `extreme' population, characterized\nby a large helium overabundance, and by abundances of proton capture elements\ntypical of nuclear processing in gas at very high temperatures, where the\ninitial lithium has been fully destroyed. The observations of lithium in such\nextreme cluster stars are important to test different models for the formation\nof multiple populations in old Globular Clusters. In the asymptotic giant\nbranch (AGB) scenario, fresh lithium is synthetized during the initial phases\nof hot bottom burning which, afterwards, synthetize the other p-capture\nelements. We model the abundance of lithium in the ejecta of superAGB models,\nfinding values consistent or larger than observed in the `extreme' giant; these\nsame models describe correctly the magnesium depletion and silicon enrichment\nof the extreme population of NGC 2808, so the overall agreement provides\nfurther support to the AGB scenario. In the models involving massive or\nsupermassive stars, the Lithium observed requires a mixture of the lithium-free\nejecta of the polluting population with more than 40% of standard-lithium\npristine gas. The extended chemical anomalies of NGC 2808 stars are then to be\nall explained within at most 60% of the possible dilution range, the initial\nhelium mass fraction in the ejecta should be Y >= 0.5, to account for the Ye\n0.38-0.40 of the extreme population, and further observations of p-process\nelements are needed to check the model.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Plasmon-assisted hot carrier processes in metal nanoparticles can be\ndescribed either classically or using the full strength of quantum mechanics.\nWe reconfirm that from the practical applications point of view, when it comes\nto description of the decay of plasmons in nanoparticles, classical description\nis sufficiently adequate for all but the smallest of the nanoparticles. At the\nsame time, the electron temperature rise in nanoparticles is discrete\n(quantized) and neglecting this fact can lead to significant underestimating of\nhot carrier assisted effects, such as photo-catalysis.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Atom interferometers are powerful tools for both measurements in fundamental\nphysics and inertial sensing applications. Their performance, however, has been\nlimited by the available interrogation time of freely falling atoms in a\ngravitational field. We realize an unprecedented interrogation time of 20\nseconds by suspending the spatially-separated atomic wavepackets in a lattice\nformed by the mode of an optical cavity. Unlike traditional atom\ninterferometers, this approach allows potentials to be measured by holding,\nrather than dropping, atoms. After seconds of hold time, gravitational\npotential energy differences from as little as microns of vertical separation\ngenerate megaradians of interferometer phase. This trapped geometry suppresses\nthe phase sensitivity to vibrations by 3-4 orders of magnitude, overcoming the\ndominant noise source in atom-interferometric gravimeters. Finally, we study\nthe wavefunction dynamics driven by gravitational potential gradients across\nneighboring lattice sites.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Optical wireless communication (OWC) is a promising technology that can\nprovide high data rates while supporting multiple users. The Optical Wireless\n(OW) physical layer has been researched extensively, however less work was\ndevoted to multiple access and how the OW front end is connected to the\nnetwork. In this paper, an OWC system which employs a wavelength division\nmultiple access (WDMA) scheme is studied, for the purpose of supporting\nmultiple users. In addition, a cloud/fog architecture is proposed for the first\ntime for OWC to provide processing capabilities. The cloud/fog-integrated\narchitecture uses visible indoor light to create high data rate connections\nwith potential mobile nodes. These optical wireless nodes are further clustered\nand used as fog mini servers to provide processing services through the optical\nwireless channel for other users. Additional fog processing units are located\nin the room, the building, the campus and at the metro level. Further\nprocessing capabilities are provided by remote cloud sites. A mixed-integer\nlinear programming (MILP) model was developed and utilised to optimise resource\nallocation in the indoor OWC system. A second MILP model was developed to\noptimise the placement of processing tasks in the different fog and cloud nodes\navailable. The optimisation of tasks placement in the cloud-/fog-integrated\narchitecture was analysed using the MILP models. Multiple scenarios were\nconsidered where the mobile node locations were varied in the room and the\namount of processing and data rate requested by each optical wireless node is\nvaried. The results help identify the optimum colour and access point to use\nfor communication for a given mobile node location and OWC system\nconfiguration, the optimum location to place processing and the impact of the\nnetwork architecture. Areas for future work are identified.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we study the solvability problem for one kind of fully coupled\nforward-backward stochastic difference equations (FBS{\\Delta}Es). With the help\nof the necessary and sufficient condition for the solvability of the linear\nFBS{\\Delta}Es, under the monotone assumption, we obtain the existence and\nuniqueness theorem for the general nonlinear ones.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We generalise the concept of duality to systems of ordinary difference\nequations (or maps). We propose a procedure to construct a chain of systems of\nequations which are dual, with respect to an integral $H$, to the given system,\nby exploiting the integral relation, defined by the upshifted version and the\noriginal version of $H$. When the numerator of the integral relation is\nbiquadratic or multi-linear, we point out conditions where a dual fails to\nexists. The procedure is applied to several two-component systems obtained as\nperiodic reductions of 2D lattice equations, including the nonlinear\nSchr\\\"{o}dinger system, the two-component potential Korteweg-De Vries equation,\nthe scalar modified Korteweg-De Vries equation, and a modified Boussinesq\nsystem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study some properties of quadratic forms with values in a field whose\nunderlying vector spaces are endowed with the structure of right vector spaces\nover a division ring extension of that field. Some generalized notions of\nisotropy, metabolicity and isometry are introduced and used to find a Witt\ndecomposition for these forms. We then associate to every (skew) hermitian form\nover a division algebra with involution of the first kind a quadratic form\ndefined on its underlying vector space. It is shown that this quadratic form,\nwith its generalized notions of isotropy and isometry, can be used to determine\nthe isotropy behaviour and the isometry class of (skew) hermitian forms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Whether listening to overlapping conversations in a crowded room or recording\nthe simultaneous electrical activity of millions of neurons, the natural world\nabounds with sparse measurements of complex overlapping signals that arise from\ndynamical processes. While tools that separate mixed signals into linear\nsources have proven necessary and useful, the underlying equational forms of\nmost natural signals are unknown and nonlinear. Hence, there is a need for a\nframework that is general enough to extract sources without knowledge of their\ngenerating equations, and flexible enough to accommodate nonlinear, even\nchaotic, sources. Here we provide such a framework, where the sources are\nchaotic trajectories from independently evolving dynamical systems. We consider\nthe mixture signal as the sum of two chaotic trajectories, and propose a\nsupervised learning scheme that extracts the chaotic trajectories from their\nmixture. Specifically, we recruit a complex dynamical system as an intermediate\nprocessor that is constantly driven by the mixture. We then obtain the\nseparated chaotic trajectories based on this intermediate system by training\nthe proper output functions. To demonstrate the generalizability of this\nframework \\emph{in silico}, we employ a tank of water as the intermediate\nsystem, and show its success in separating two-part mixtures of various chaotic\ntrajectories. Finally, we relate the underlying mechanism of this method to the\nstate-observer problem. This relation provides a quantitative theory that\nexplains the performance of our method, such as why separation is difficult\nwhen two source signals are trajectories from the same chaotic system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report on observations and modeling of interspecies magnetic Feshbach\nresonances in dilute ultracold mixtures of open-shell alkali-metal $^6$Li and\nclosed-shell $^{173}$Yb atoms with temperatures just above quantum degeneracy\nfor both fermionic species. Resonances are located by detecting\nmagnetic-field-dependent atom loss due to three-body recombination. We resolve\nclosely-located resonances that originate from a weak separation-dependent\nhyperfine coupling between the electronic spin of $^6$Li and the nuclear spin\nof $^{173}$Yb, and confirm their magnetic field spacing by ab initio\nelectronic-structure calculations. Through quantitative comparisons of\ntheoretical atom-loss profiles and experimental data at various temperatures\nbetween 1 $\\mu$K and 20 $\\mu$K, we show that three-body recombination in\nfermionic mixtures has a $p$-wave Wigner threshold behavior leading to\ncharacteristic asymmetric loss profiles. Such resonances can be applied towards\nthe formation of ultracold doublet ground-state molecules and quantum\nsimulation of superfluid $p$-wave pairing.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We perform a theoretical and numerical investigation of the time-average of\nenergy exchange among modes of Reduced Order Models (ROMs) of fluid flows. We\nare interested in the statistical equilibrium problem, and especially in the\npossible forward and backward average transfer of energy among ROM basis\nfunctions (modes). We consider two types of ROM modes: eigenfunctions of the\nStokes operator and Proper Orthogonal Decomposition (POD) modes. We prove\nanalytical results for both types of ROM modes and we highlight the differences\nbetween them. We also investigate numerically whether the time-average energy\nexchange between POD modes is positive. To this end, we utilize the\none-dimensional Burgers equation as a simplified mathematical model, which is\ncommonly used in ROM tests. The main conclusion of our numerical study is that,\nfor long enough time intervals, the time-average energy exchange from low index\nPOD modes to high index POD modes is positive, as predicted by our theoretical\nresults.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Some ultra-compact dwarf galaxies have large dynamical mass to light (M/L)\nratios and also appear to contain an overabundance of LMXB sources, and some\nMilky Way globular clusters have a low concentration and appear to have a\ndeficit of low-mass stars. These observations can be explained if the stellar\nIMF becomes increasingly top-heavy with decreasing metallicity and increasing\ngas density of the forming object. The thus constrained stellar IMF then\naccounts for the observed trend of metallicity and M/L ratio found amongst M31\nglobular star clusters. It also accounts for the overall shift of the\nobservationally deduced galaxy-wide IMF from top-light to top-heavy with\nincreasing star formation rate amongst galaxies. If the IMF varies similarly to\ndeduced here, then extremely young very massive star-burst clusters observed at\na high redshift would appear quasar-like (Jerabkova et al. 2017) .\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The early acceleration of protons and electrons in the nonrelativistic\ncollisionless shocks with three obliquities are investigated through 1D\nparticle-in-cell simulations. In the simulations, the charged particles\npossessing a velocity of $0.2\\, c$ flow towards a reflecting boundary, and the\nshocks with a sonic Mach number of $13.4$ and a Alf\\'{v}en Mach number of\n$16.5$ in the downstream shock frame are generated. In these quasi-parallel\nshocks with the obliquity angles $\\theta = 15^\\circ$, $30^\\circ$, and\n$45^\\circ$, some of the protons and the electrons can be injected into the\nacceleration processes, and their downstream spectra in the momentum space show\na power law tail at a time of $1.89\\times10^5 \\omega_{\\rm pe}^{-1}$, where\n$\\omega_{\\rm pe}$ is the electron plasma frequency. Moreover, the charged\nparticles reflected at the shock excite magnetic waves upstream of the shock.\nThe shock drift acceleration is more prominent with a larger obliquity angle\nfor the shocks, but the accelerated particles diffuse parallel to the shock\npropagation direction more easily to participate in the diffusive shock\nacceleration. At the time still in the early acceleration stage, more energetic\nprotons and electrons appear in the downstream of the shock for $\\theta =\n15^\\circ$ compared with the other two obliquities; moreover, in the upstream\nregion, the spectrum of the accelerated electrons is the hardest for\n$\\theta_{\\rm nB} = 45^\\circ$ among the three obliquities, whereas the proton\nspectra for $\\theta_{\\rm nB} = 15^\\circ$ and $45^\\circ$ are similar as a result\nof the competition of the effectiveness of the shock drift acceleration and the\ndiffusive shock acceleration.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the question of the largest possible combinatorial diameter among\n$(d-1)$-dimensional simplicial complexes on $n$ vertices, denoted $H_s(n, d)$.\nUsing a probabilistic construction we give a new lower bound on $H_s(n, d)$\nthat is within an $O(d^2)$ factor of the upper bound. This improves on the\npreviously best-known lower bound which was within a factor of $e^{\\Theta(d)}$\nof the upper bound. We also make a similar improvement in the case of\npseudomanifolds.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The vertex coloring problem is a well-known NP-hard problem and has many\napplications in operations research and in scheduling. A conventional approach\nto the problem solves the k-colorability problem iteratively, decreasing k one\nby one. Whether a heuristic algorithm finds a legal k-coloring quickly or not\nis largely affected by an initial solution. We highlight a simple initial\nsolution generator, which we call the recycle method, which makes use of the\nlegal (k+1)-coloring that has been found. An initial solution generated by the\nmethod is expected to guide a heuristic algorithm to find a legal k-coloring\nmore quickly than conventional methods, as demonstrated by experimental\nstudies. The results suggest that the recycle method should be used as the\nstandard initial solution generator for both local search algorithms and modern\nhybrid methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Bergman fan of a matroid is the intersection of tropical hyperplanes\ndefined by the circuits. A tropical basis is a subset of the circuits set that\ndefines the Bergman fan. Yu and Yuster posed a question whether every simple\nregular matroid has a unique minimal tropical basis of its Bergman fan, and\nverified it for graphic, cographic matroids and $R_{10}$. We show every simple\nbinary matroid has a unique minimal tropical basis. Since the regular matroid\nis binary, we positively answered the question.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An interacting system of Langevin dynamics driven particles has been proposed\nfor sampling from a given posterior density by Garbuno-Inigo, Hoffmann, Li and\nStuart in Interacting Langevin Diffusions: Gradient Structure and Ensemble\nKalman Sampler (arXiv:1903:08866v2). The proposed formulation is primarily\nstudied from a formal mean-field limit perspective, while the theoretical\nbehaviour under a finite particle size is left as an open problem. In this note\nwe demonstrate that the particle-based covariance interaction term requires a\nnon-trivial correction. We also show that the corrected dynamics samples\nexactly from the desired posterior provided that the empirical covariance\nmatrix of the particle system remains non-singular and the posterior\nlog-density satisfies the standard Bakry-Emery criterion.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Raman spectroscopy is an appealing technique that probes molecular vibrations\nin a wide variety of materials with virtually no sample preparation. However,\naccurate and reliable Raman measurements are still a challenge and require more\nrobust and practical calibration methods. We demonstrate the implementation of\na simple low-cost continuous-wave stimulated Raman spectroscopy scheme for\naccurate and high-resolution spectroscopy. We perform shot noise limited\ncontinuous-wave stimulated Raman scattering (cwSRS) as well as continuous-wave\ncoherent anti-Stokes Raman scattering (cwCARS) on polystyrene samples. Our\nmethod enables accurate determination of Raman shifts with an uncertainty below\n0.1 cm$^{-1}$. The setup is used for the characterization of reference\nmaterials required for the calibration of Raman spectrometers. Compared with\nexisting standards, we provide an order of magnitude improvement of the\nuncertainty of Raman energy shifts in a polystyrene reference material.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Eccentric nuclear disks (ENDs) are a type of star cluster in which the stars\nlie on eccentric, apsidally-aligned orbits in a disk around a central\nsupermassive black hole (SMBH). These disks can produce a high rate of tidal\ndisruption events (TDEs) via secular gravitational torques. Previous studies of\nENDs have included stars with only one mass. Here, we present the first study\nof an eccentric nuclear disk with two stellar species. We show that ENDs show\nradial mass segregation consistent with previous results from other cluster\ntypes. Additionally, ENDs show vertical mass segregation by which the heavy\nstars sink to lower inclinations than light stars. These two effects cause\nheavy stars to be more susceptible to tidal disruption, which can be seen in\nthe higher fraction of heavy stars that are disrupted compared to light stars.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  There have been vigorous research attempts to test various modified gravity\ntheories by usingphysics of the cosmic microwave background (CMB). Meanwhile,\nsymmetry breaking such as Higgsmechanism is one of the most important phenomena\nin physics but there have been not so muchresearches to make them contact with\ncosmological observations. In this article, with the CMBpower spectra we try to\ndistinguish two different scenarios of spontaneous symmetry breaking\ninprimordial era of the universe. The first model is based on a broken\nsymmetric theory of gravity,which was suggested by A. Zee in 1979. The second\nmodel is an application of Palatini formalismto the first model. Perturbation\nequations are computed and they show differences originated fromthe property of\nsymmetry. Furthermore, it turns out that two models have different features\nofCMB power spectra with the same potential scale. This fact enables us to\nverify distinct kinds ofprimordial symmetry breaking with CMB physics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The goal of threshold group testing is to identify up to $d$ defective items\namong a population of $n$ items, where $d$ is usually much smaller than $n$. A\ntest is positive if it has at least $u$ defective items and negative otherwise.\nOur objective is to identify defective items in sublinear time the number of\nitems, e.g., $\\mathrm{poly}(d, \\ln{n}),$ by using the number of tests as low as\npossible. In this paper, we reduce the number of tests to $O \\left( h \\times\n\\frac{d^2 \\ln^2{n}}{\\mathsf{W}^2(d \\ln{n})} \\right)$ and the decoding time to\n$O \\left( \\mathrm{dec}_0 \\times h \\right),$ where $\\\\mathrm{dec}_0 = O \\left(\n\\frac{d^{3.57} \\ln^{6.26}{n}}{\\mathsf{W}^{6.26}(d \\ln{n})} \\right) + O \\left(\n\\frac{d^6 \\ln^4{n}}{\\mathsf{W}^4(d \\ln{n})} \\right)$, $h = O\\left( \\frac{d_0^2\n\\ln{\\frac{n}{d_0}}}{(1-p)^2} \\right)$ , $d_0 = \\max\\{u, d - u \\}$, $p \\in [0,\n1),$ and $\\mathsf{W}(x) = \\Theta \\left( \\ln{x} - \\ln{\\ln{x}} \\right).$ If the\nnumber of tests is increased to $O\\left( h \\times\n\\frac{d^2\\ln^3{n}}{\\mathsf{W}^2(d \\ln{n})} \\right),$ the decoding complexity is\nreduced to $O \\left(\\mathrm{dec}_1 \\times h \\right),$ where $\\mathrm{dec}_1 =\n\\max \\left\\{ \\frac{d^2 \\ln^3{n}}{\\mathsf{W}^2(d \\ln{n})}, \\frac{ud\n\\ln^4{n}}{\\mathsf{W}^3(d \\ln{n})} \\right\\}.$ Moreover, our proposed scheme is\ncapable of handling errors in test outcomes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We develop a randomized Newton method capable of solving learning problems\nwith huge dimensional feature spaces, which is a common setting in applications\nsuch as medical imaging, genomics and seismology. Our method leverages\nrandomized sketching in a new way, by finding the Newton direction constrained\nto the space spanned by a random sketch. We develop a simple global linear\nconvergence theory that holds for practically all sketching techniques, which\ngives the practitioners the freedom to design custom sketching approaches\nsuitable for particular applications. We perform numerical experiments which\ndemonstrate the efficiency of our method as compared to accelerated gradient\ndescent and the full Newton method. Our method can be seen as a refinement and\nrandomized extension of the results of Karimireddy, Stich, and Jaggi (2019).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Han-Kobayashi (HK) scheme achieves the best known achievable rate region\nfor the K user interference channel (IC). Simple HK schemes are HK schemes with\nGaussian signaling, no time sharing, and no private-common power splitting. The\nclass of simple HK schemes includes the treating interference as noise (TIN)\nscheme and schemes that involve various levels of interference decoding and\ncancellation at each receiver. We derive conditions under which simple HK\nschemes achieve sum capacity for general K user Gaussian ICs. These results\ngeneralize existing sum capacity results for the TIN scheme to the class of\nsimple HK schemes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Several recent works have considered the \\emph{trace reconstruction problem},\nin which an unknown source string $x\\in\\{0,1\\}^n$ is transmitted through a\nprobabilistic channel which may randomly delete coordinates or insert random\nbits, resulting in a \\emph{trace} of $x$. The goal is to reconstruct the\noriginal string~$x$ from independent traces of $x$. While the best algorithms\nknown for worst-case strings use $\\exp(O(n^{1/3}))$ traces\n\\cite{DOS17,NazarovPeres17}, highly efficient algorithms are known\n\\cite{PZ17,HPP18} for the \\emph{average-case} version, in which $x$ is\nuniformly random. We consider a generalization of this average-case trace\nreconstruction problem, which we call \\emph{average-case population recovery in\nthe presence of insertions and deletions}. In this problem, there is an unknown\ndistribution $\\cal{D}$ over $s$ unknown source strings $x^1,\\dots,x^s \\in\n\\{0,1\\}^n$, and each sample is independently generated by drawing some $x^i$\nfrom $\\cal{D}$ and returning an independent trace of $x^i$.\n  Building on \\cite{PZ17} and \\cite{HPP18}, we give an efficient algorithm for\nthis problem. For any support size $s \\leq \\smash{\\exp(\\Theta(n^{1/3}))}$, for\na $1-o(1)$ fraction of all $s$-element support sets $\\{x^1,\\dots,x^s\\} \\subset\n\\{0,1\\}^n$, for every distribution $\\cal{D}$ supported on $\\{x^1,\\dots,x^s\\}$,\nour algorithm efficiently recovers ${\\cal D}$ up to total variation distance\n$\\epsilon$ with high probability, given access to independent traces of\nindependent draws from $\\cal{D}$. The algorithm runs in time\npoly$(n,s,1/\\epsilon)$ and its sample complexity is\npoly$(s,1/\\epsilon,\\exp(\\log^{1/3}n)).$ This polynomial dependence on the\nsupport size $s$ is in sharp contrast with the \\emph{worst-case} version (when\n$x^1,\\dots,x^s$ may be any strings in $\\{0,1\\}^n$), in which the sample\ncomplexity of the most efficient known algorithm \\cite{BCFSS19} is doubly\nexponential in $s$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a full 3D benchmark problem for brittle fracture based on\nexperiments as well as a validation in the context of phase-field models. The\nexample consists of a series of four-point bending tests on graphite specimens\nwith sharp V-notches at different inclination angles. This simple setup leads\nto a mixed mode (I + II + III) loading which results in complex yet stably\nreproducible crack surfaces. The proposed problem is well suited for\nbenchmarking numerical methods for brittle fracture and allows for a\nquantitative comparison of failure loads and propagation paths as well as\ninitiation angles and the fracture surface. For evaluation of the crack\nsurfaces image-based 3D models of the fractured specimen are provided along\nwith experimental and numerical results. In addition, measured failure loads\nand computed load-displacement curves are given. To demonstrate the\napplicability of the benchmark problem, we show that for a phase-field model\nbased on the Finite Cell Method and multi-level hp-refinement the complex crack\nsurface as well as the failure loads can be well reproduced.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Explainable Artificial Intelligence (XAI) methods are typically deployed to\nexplain and debug black-box machine learning models. However, most proposed XAI\nmethods are black-boxes themselves and designed for images. Thus, they rely on\nvisual interpretability to evaluate and prove explanations. In this work, we\napply XAI methods previously used in the image and text-domain on time series.\nWe present a methodology to test and evaluate various XAI methods on time\nseries by introducing new verification techniques to incorporate the temporal\ndimension. We further conduct preliminary experiments to assess the quality of\nselected XAI method explanations with various verification methods on a range\nof datasets and inspecting quality metrics on it. We demonstrate that in our\ninitial experiments, SHAP works robust for all models, but others like\nDeepLIFT, LRP, and Saliency Maps work better with specific architectures.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we construct local and global solutions to the K\\\"ahler-Ricci\nflow from a non-collapsed K\\\"ahler manifold with curvature bounded from below.\nCombines with the mollification technique of McLeod-Simon-Topping, we show that\nthe Gromov-Hausdorff limit of sequence of complete noncompact non-collapsed\nK\\\"ahler manifolds with orthogonal bisectional curvature and Ricci curvature\nbounded from below is homeomorphic to a complex manifold. We also use it to\nstudy the complex structure of complete K\\\"ahler manifolds with nonnegative\northogonal bisectional curvature, nonnegative Ricci curvature and maximal\nvolume growth.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We theoretically predict and classify the localized modes of a skyrmion in a\ncollinear uniaxial antiferromagnet and discuss how they can be excited. As a\ncentral result, we find two branches of skyrmion eigenmodes with distinct\nphysical properties characterized by being low or high energy excitations. The\nfrequency dependence of the low-energy modes scales as $R_0^{-2}$ for skyrmions\nwith large radius $R_0$. Furthermore, we predict localized high-energy\neigenmodes, which have no direct ferromagnetic counterpart. Except for the\nbreathing mode, we find that all localized antiferromagnet skyrmion modes, both\nin the low and high-energy branch, are doubly degenerated in the absence of a\nmagnetic field and split otherwise. We explain our numerical results for the\nlow-energy modes within a string model representing the skyrmion boundary.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cold gas experiments can be tuned to achieve strongly-interacting regimes\nsuch as that of low-density neutron matter found in neutron-stars' crusts. We\nreport $T$=0 diffusion Monte Carlo results (i) for the ground state of both\nspin-1/2 fermions with short-range interactions and low-density neutron matter\nin a cylindrical container, and (ii) properties of these systems with a vortex\nline excitation. We calculate the equation of state for cold atoms and\nlow-density neutron matter in the bulk systems, and we contrast it to our\nresults in the cylindrical container. We compute the vortex line excitation\nenergy for different interaction strengths, and we find agreement between cold\ngases and neutron matter for very low densities. We also calculate density\nprofiles, which allow us to determine the density depletion at the vortex core,\nwhich depends strongly on the short-ranged interaction in cold atomic gases,\nbut it is of $\\approx$ 25% for neutron matter in the density regimes studied in\nthis work. Our results can be used to constrain neutron matter properties by\nusing measurements from cold Fermi gases experiments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, a novel modulation scheme called set partition modulation\n(SPM) is proposed. In this scheme, set partitioning and ordered subsets in the\nset partitions are used to form codewords. We define different SPM variants and\ndepict a practical model for using SPM with orthogonal frequency division\nmultiplexing (OFDM). For the OFDM-SPM schemes, different constellations are\nused to distinguish between different subsets in a set partition. To achieve\ngood distance properties as well as better error performance for the OFDM-SPM\ncodewords, we define a codebook selection problem and formulate such a problem\nas a clique problem in graph theory. In this regard, we propose a fast and\nefficient codebook selection algorithm. We analyze error and achievable rate\nperformance of the proposed schemes and provide asymptotic results for the\nperformance. It is shown that the proposed SPM variants are general schemes,\nwhich encompass multi-mode OFDM with index modulation (MM-OFDM-IM) and\ndual-mode OFDM with index modulation (DM-OFDM-IM) as special cases. It is also\nshown that OFDM-SPM schemes are capable of exhibiting better error performance\nand improved achievable rate than conventional OFDM, OFDM-IM, DM-OFDM-IM, and\nMM-OFDM-IM.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Backus (1962) developed his technique for homogenization of a layered\nstructure solely within the context of linear elastic theory. In this paper we\npropose an extended use of Backus average for finitely deformed materials of a\nlayered structure. We attempt to use two different approaches to account for\nlarge deformations. The first approach utilizes the connections between linear\nand nonlinear transverse elasticity. For the second approach we use a\nformulation based on prestress in the material. We conclude that the first\napproach, although with some limitations, can be used successfully.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In directed last passage site percolation with i.i.d.~random weights with\nfinite support over a $n\\times\\lfloor n^{\\alpha}\\rfloor$ grid, we prove that\nfor $n$ large enough, the order of the $r$-th central moment, $1\\le r<+\\infty$,\nof the last passage time is lower bounded by $n^{r(1-\\alpha)/2}$,\n$0<\\alpha<1/3$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We design and test a cone finding algorithm to robustly address nonlinear\nsystem analysis through differential positivity. The approach provides a\nnumerical tool to study multi-stable systems, beyond Lyapunov analysis. The\ntheory is illustrated on two examples: a consensus problem with some repulsive\ninteractions and second order agent dynamics, and a controlled duffing\noscillator.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Despite remarkable recent progress on both unconditional and conditional\nimage synthesis, it remains a long-standing problem to learn generative models\nthat are capable of synthesizing realistic and sharp images from reconfigurable\nspatial layout (i.e., bounding boxes + class labels in an image lattice) and\nstyle (i.e., structural and appearance variations encoded by latent vectors),\nespecially at high resolution. By reconfigurable, it means that a model can\npreserve the intrinsic one-to-many mapping from a given layout to multiple\nplausible images with different styles, and is adaptive with respect to\nperturbations of a layout and style latent code. In this paper, we present a\nlayout- and style-based architecture for generative adversarial networks\n(termed LostGANs) that can be trained end-to-end to generate images from\nreconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed\nLostGAN consists of two new components: (i) learning fine-grained mask maps in\na weakly-supervised manner to bridge the gap between layouts and images, and\n(ii) learning object instance-specific layout-aware feature normalization\n(ISLA-Norm) in the generator to realize multi-object style generation. In\nexperiments, the proposed method is tested on the COCO-Stuff dataset and the\nVisual Genome dataset with state-of-the-art performance obtained. The code and\npretrained models are available at \\url{https://github.com/iVMCL/LostGANs}.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Hypothesis testing is one of the most common types of data analysis and forms\nthe backbone of scientific research in many disciplines. Analysis of variance\n(ANOVA) in particular is used to detect dependence between a categorical and a\nnumerical variable. Here we show how one can carry out this hypothesis test\nunder the restrictions of differential privacy. We show that the $F$-statistic,\nthe optimal test statistic in the public setting, is no longer optimal in the\nprivate setting, and we develop a new test statistic $F_1$ with much higher\nstatistical power. We show how to rigorously compute a reference distribution\nfor the $F_1$ statistic and give an algorithm that outputs accurate $p$-values.\nWe implement our test and experimentally optimize several parameters. We then\ncompare our test to the only previous work on private ANOVA testing, using the\nsame effect size as that work. We see an order of magnitude improvement, with\nour test requiring only 7% as much data to detect the effect.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Many studies have been done to improve the performance of centrally\ncontrolled business processes and enhance the integration between different\nparties of these collaborations. However, the most serious issues of\ncollaborative business processes remained unsolved in these studies, lack of\ntrust and divided data on various confidential ledgers. Blockchain technology\nhas enormous potential to become a new substantial integration method for\nuntrusted collaborative businesses. Using the governing consensus mechanism,\nblockchain eliminates the necessity of the trusted third party. It provides a\ndistributed shared ledger which facilitates the job of the process monitoring\nfor the parties. The smart contract, as a crucial tool, is used to define the\nguaranteed autonomous programs. In addition, the privacy of the data can be\nensured by using a permissioned blockchain that handles the access control\nbecause in this way, only verifiable participants can have access to the state\nof the business process and its related information. In this study, the\napplicability of execution of a real-word untrusted business process on the\npermissioned blockchain is investigated. Moreover, we determine the advantages\nof using the permissioned access-controller blockchain as the infrastructure\nfor the collaborative business processes, through implementing the process of\nOrder Processing on the Hyperledger Fabric blockchain platform.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Building accurate language models that capture meaningful long-term\ndependencies is a core challenge in natural language processing. Towards this\nend, we present a calibration-based approach to measure long-term discrepancies\nbetween a generative sequence model and the true distribution, and use these\ndiscrepancies to improve the model. Empirically, we show that state-of-the-art\nlanguage models, including LSTMs and Transformers, are \\emph{miscalibrated}:\nthe entropy rates of their generations drift dramatically upward over time. We\nthen provide provable methods to mitigate this phenomenon. Furthermore, we show\nhow this calibration-based approach can also be used to measure the amount of\nmemory that language models use for prediction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Black hole mass measurements outside the local universe are critically\nimportant to derive the growth of supermassive black holes over cosmic time,\nand to study the interplay between black hole growth and galaxy evolution. In\nthis paper we present two measurements of supermassive black hole masses from\nreverberation mapping (RM) of the broad CIV emission line. These measurements\nare based on multi-year photometry and spectroscopy from the Dark Energy Survey\nSupernova Program (DES-SN) and the Australian Dark Energy Survey (OzDES), which\ntogether constitute the OzDES RM Program. The observed reverberation lag\nbetween the DES continuum photometry and the OzDES emission-line fluxes is\nmeasured to be $358^{+126}_{-123}$ and $343^{+58}_{-84}$ days for two quasars\nat redshifts of $1.905$ and $2.593$ respectively. The corresponding masses of\nthe two supermassive black holes are $4.4 \\times 10^{9}$ and $3.3 \\times\n10^{9}$ M$_\\odot$, which are among the highest-redshift and highest-mass black\nholes measured to date with RM studies. We use these new measurements to better\ndetermine the CIV radius$-$luminosity relationship for high-luminosity quasars,\nwhich is fundamental to many quasar black hole mass estimates and demographic\nstudies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a laser scanning reflection-matrix microscopy combining the\nscanning of laser focus and the wide-field mapping of the electric field of the\nbackscattered waves for eliminating higher-order aberrations even in the\npresence of strong multiple light scattering noise. Unlike conventional\nconfocal laser scanning microscopy, we record the amplitude and phase maps of\nreflected waves from the sample not only at the confocal pinhole, but also at\nother non-confocal points. These additional measurements lead us to\nconstructing a time-resolved reflection matrix, with which the sample-induced\naberrations for the illumination and detection pathways are separately\nidentified and corrected. We realized in vivo reflectance imaging of myelinated\naxons through an intact skull of a living mouse with the spatial resolution\nclose to the ideal diffraction limit. Furthermore, we demonstrated\nnear-diffraction-limited multiphoton imaging through an intact skull by\nphysically correcting the aberrations identified from the reflection matrix.\nThe proposed method is expected to extend the range of applications, where the\nknowledge of the detailed microscopic information deep within biological\ntissues is critical.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study model-free learning methods for the output-feedback Linear Quadratic\n(LQ) control problem in finite-horizon subject to subspace constraints on the\ncontrol policy. Subspace constraints naturally arise in the field of\ndistributed control and present a significant challenge in the sense that\nstandard model-based optimization and learning leads to intractable numerical\nprograms in general. Building upon recent results in zeroth-order optimization,\nwe establish model-free sample-complexity bounds for the class of distributed\nLQ problems where a local gradient dominance constant exists on any sublevel\nset of the cost function. %which admit a local gradient dominance constant\nvalid on the sublevel set of the cost function. We prove that a fundamental\nclass of distributed control problems - commonly referred to as Quadratically\nInvariant (QI) problems - as well as others possess this property. To the best\nof our knowledge, our result is the first sample-complexity bound guarantee on\nlearning globally optimal distributed output-feedback control policies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  (Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can\ninherit gender bias from the data they were trained on. We investigate how this\nbias affects downstream classification tasks, using the case study of\noccupation classification (De-Arteaga et al.,2019). We show that traditional\ntechniques for debiasing embeddings can actually worsen the bias of the\ndownstream classifier by providing a less noisy channel for communicating\ngender information. With a relatively minor adjustment, however, we show how\nthese same techniques can be used to simultaneously reduce bias and maintain\nhigh classification accuracy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we introduce the MOldavian and ROmanian Dialectal COrpus\n(MOROCO), which is freely available for download at\nhttps://github.com/butnaruandrei/MOROCO. The corpus contains 33564 samples of\ntext (with over 10 million tokens) collected from the news domain. The samples\nbelong to one of the following six topics: culture, finance, politics, science,\nsports and tech. The data set is divided into 21719 samples for training, 5921\nsamples for validation and another 5924 samples for testing. For each sample,\nwe provide corresponding dialectal and category labels. This allows us to\nperform empirical studies on several classification tasks such as (i) binary\ndiscrimination of Moldavian versus Romanian text samples, (ii) intra-dialect\nmulti-class categorization by topic and (iii) cross-dialect multi-class\ncategorization by topic. We perform experiments using a shallow approach based\non string kernels, as well as a novel deep approach based on character-level\nconvolutional neural networks containing Squeeze-and-Excitation blocks. We also\npresent and analyze the most discriminative features of our best performing\nmodel, before and after named entity removal.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Different investment strategies are adopted in short-term and long-term\ndepending on the time scales, even though time scales are adhoc in nature.\nEmpirical mode decomposition based Hurst exponent analysis and variance\ntechnique have been applied to identify the time scales for short-term and\nlong-term investment from the decomposed intrinsic mode functions(IMF). Hurst\nexponent ($H$) is around 0.5 for the IMFs with time scales from few days to 3\nmonths, and $H\\geq0.75$ for the IMFs with the time scales $\\geq5$ months. Short\nterm time series [$X_{ST}(t)$] with time scales from few days to 3 months and\n$H~0.5$ and long term time series [$X_{LT}(t)$] with time scales $\\geq5$ and\n$H\\geq0.75$, which represent the dynamics of the market, are constructed from\nthe IMFs. The $X_{ST}(t)$ and $X_{LT}(t)$ show that the market is random in\nshort-term and correlated in long term. The study also show that the\n$X_{LT}(t)$ is correlated with fundamentals of the company. The analysis will\nbe useful for investors to design the investment and trading strategy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  By interpreting a traffic scene as a graph of interacting vehicles, we gain a\nflexible abstract representation which allows us to apply Graph Neural Network\n(GNN) models for traffic prediction. These naturally take interaction between\ntraffic participants into account while being computationally efficient and\nproviding large model capacity. We evaluate two state-of-the art GNN\narchitectures and introduce several adaptations for our specific scenario. We\nshow that prediction error in scenarios with much interaction decreases by 30%\ncompared to a model that does not take interactions into account. This suggests\nthat interaction is important, and shows that we can model it using graphs.\nThis makes GNNs a worthwhile addition to traffic prediction systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper comparatively investigates the performance of extended-range\nelectric powertrains composed by integrating dual-motor inputs, multi-speed\ntransmission, and engine in either series or parallel connection. Two\nconfigurations, namely dual-motor series powertrain (DMSP) and dual-motor\nparallel powertrain (DMPP), feature the same electric drivetrain of two\ndownsized motors and a four-speed transmission, but their range extenders are\nfundamentally different. While the DMSP consists of a range extender formed by\nan engine-generator unit, the DMPP connects the engine through a frictional\nclutch. This study starts with the parameter selection that guarantees the\nequivalent dynamics ability of all configurations. Mathematic models are second\nestablished in detail. A model predictive control-based energy management\nstrategy is third presented. Since the recommended configurations aim for bus\napplication, the driving cycle CBDC and ECE15x5 are chosen for simulation.\nPerformance indexes used for comparison include electric consumption, fuel\nconsumption, and emissions (hydrocarbon, carbon monoxide, nitrogen oxides, and\nparticulate matter). Compared to the conventional single-motor series\npowertrain, the DMSP improves all the indexes significantly, while the DMPP\ndecreases fuel consumption further but increases most noxious exhaust\nemissions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Spontaneous emergence of self-organized patterns and their bifurcations\ntowards a regime of complex dynamics in non-equilibrium dissipative systems is\na paradigm of phase transition. Indeed, the behavior of these patterns in the\nhighly nonlinear regime remains less explored, even in recent\nhigh-quality-factor resonators such as Kerr-nonlinear optical ones. Here, we\ninvestigate theoretically and experimentally the alteration of the resulting\nKerr frequency combs from the weakly to the highly nonlinear regime, in the\nframeworks of spatiotemporal chaos, and dissipative phase transitions. We\nreveal the existence of a striking and easily accessible scenario of\nspatiotemporal chaos, free of cavity solitons, in a monostable operating\nregime, wherein a transition to amplitude turbulence via spatiotemporal\nintermittency is evidenced. Moreover, statistics of the light bursts in the\nresulting turbulent regime unveils the existence of rogue waves as extreme\nevents characterized by long-tail statistics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model\nthrough discarding the autoregressive mechanism and generating target words\nindependently, which fails to exploit the target sequential information.\nOver-translation and under-translation errors often occur for the above reason,\nespecially in the long sentence translation scenario. In this paper, we propose\ntwo approaches to retrieve the target sequential information for NAT to enhance\nits translation ability while preserving the fast-decoding property. Firstly,\nwe propose a sequence-level training method based on a novel reinforcement\nalgorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the\ntraining procedure. Secondly, we propose an innovative Transformer decoder\nnamed FS-decoder to fuse the target sequential information into the top layer\nof the decoder. Experimental results on three translation tasks show that the\nReinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU\nwithout decelerating the decoding speed and the FS-decoder achieves comparable\ntranslation performance to the autoregressive Transformer with considerable\nspeedup.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The modeling and simulation of heat source trajectories through phase-change\nmaterials is a relevant problem both for space exploration and for terrestrial\nclimate research, among other fields. In space, the DLR and NASA are both\ninterested in exploring beneath the surfaces of icy moons, primarily Enceladus\nand Europa, where conditions may alloy for extraterrestrial life. On Earth,\nunique sub-glacial aquatic ecosystems offer potential for geo-biological\ndiscoveries. Unfortunately, existing ice-drilling technology is dirty and\ncumbersome.\n  Melting probes are a clean and compact alternative technology which use\nheaters to melt through the ice. A melting probe's trajectory can be controlled\nwith differential heating. Successful trajectory control requires advancements\nnot only in the modeling and simulation of the ambient dynamics, but also of\nthe probe's coupled rigid body dynamics. Fundamentally, the rigid body dynamics\ncan be modeled by the equations of motion; but this approach is prohibitively\ncomplex.\n  This work proposes an approach which exploits that the motion of the probe is\ndriven by contact with the evolving liquid-solid interface. From this\nperspective, an energy minimization problem is formulated. The general\nmathematical problem is formulated as two split operators, respectively for the\nrigid body dynamics and the ambient dynamics. These operators are coupled with\nfeasibility constraints which ensure that the probe does not penetrate the\nsolid. Concrete examples are shown both for the energy minimization problem and\nfor the unsteady ambient dynamics. Finally, an algorithm is presented for the\ntemporal coupling of the split operators, which is implemented using Python and\nC++. Example trajectories are shown, including the dynamic response of the\nprobe velocity to a rapid change in the heat flux.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Mode-locked lasers exhibit complex nonlinear dynamics. Precise observation of\nthese dynamics will aid in understanding of the underlying physics and provide\nnew insights for laser design and applications. The starting dynamics, from\ninitial noise fluctuations to the mode-locking regime, have previously been\nobserved directly by time-stretched transform-based real-time spectroscopy.\nHowever, the regime transition dynamics, which are essential processes in\nmode-locked lasers, have not yet been resolved because regime transition\nprocess tracking is very challenging. Here we demonstrate the first insight\ninto the regime transition dynamics enabled by our design of a real-time\nprogrammable mode-locked fibre laser, in which different operating regimes can\nbe achieved and switched automatically. The regime transition dynamics among\ninitial noise fluctuations, Q-switching, fundamental mode-locking and harmonic\nmode-locking regimes have been observed and thoroughly analysed by both\ntemporal and spectral means. These findings will enrich our understanding of\nthe complex dynamics inside mode-locked lasers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study observational constraints on a specific dark energy model in the\nframework of Gleyzes-Langlois-Piazza-Vernizzi theories, which extends the\nGalileon ghost condensate (GGC) to the domain of beyond Horndeski theories. In\nthis model, we show that the Planck cosmic microwave background (CMB) data,\ncombined with datasets of baryon acoustic oscillations, supernovae type Ia, and\nredshift-space distortions, give the tight upper bound $|\\alpha_{\\rm H}^{(0)}|\n\\le {\\cal O}(10^{-6})$ on today's beyond-Horndeski (BH) parameter $\\alpha_{\\rm\nH}$. This is mostly attributed to the shift of CMB acoustic peaks induced by\nthe early-time changes of cosmological background and perturbations arising\nfrom the dominance of $\\alpha_{\\rm H}$ in the dark energy density. In\ncomparison to the $\\Lambda$-cold-dark-matter ($\\Lambda$CDM) model, our BH model\nsuppresses the large-scale integrated-Sachs-Wolfe (ISW) tail of CMB temperature\nanisotropies due to the existence of cubic Galileons, and it modifies the\nsmall-scale CMB power spectrum because of the different background evolution.\nWe find that the BH model considered fits the data better than $\\Lambda$CDM\naccording to the $\\chi^2$ statistics, yet the deviance information criterion\n(DIC) slightly favors the latter. Given the fact that our BH model with\n$\\alpha_{\\rm H}=0$ (i.e., the GGC model) is favored over $\\Lambda$CDM even by\nthe DIC, there are no particular signatures for the departure from Horndeski\ntheories in current observations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article Dirac operators $A_{\\eta, \\tau}$ coupled with combinations of\nelectrostatic and Lorentz scalar $\\delta$-shell interactions of constant\nstrength $\\eta$ and $\\tau$, respectively, supported on compact surfaces $\\Sigma\n\\subset \\mathbb{R}^3$ are studied. In the rigorous definition of these\noperators the $\\delta$-potentials are modelled by coupling conditions at\n$\\Sigma$. In the proof of the self-adjointness of $A_{\\eta, \\tau}$ a Krein-type\nresolvent formula and a Birman-Schwinger principle are obtained. With their\nhelp a detailed study of the qualitative spectral properties of $A_{\\eta,\n\\tau}$ is possible. In particular, the essential spectrum of $A_{\\eta, \\tau}$\nis determined, it is shown that at most finitely many discrete eigenvalues can\nappear, and several symmetry relations in the point spectrum are obtained.\nMoreover, the nonrelativistic limit of $A_{\\eta, \\tau}$ is computed and it is\ndiscussed that for some special interaction strengths $A_{\\eta, \\tau}$ is\ndecoupled to two operators acting in the domains with the common boundary\n$\\Sigma$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present analytic results that describe fully-differential NNLO QCD\ncorrections to deep-inelastic scattering processes within the nested\nsoft-collinear subtraction scheme. This is the last building block required for\nthe application of this scheme to computations of NNLO QCD corrections to\narbitrary processes at hadron colliders.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the electrodynamic impedance of percolating conductors with a\npre-defined network topology using a scanning microwave impedance microscope\n(sMIM) at GHz frequencies. For a given percolation number we observe strong\nspatial variations across a sample which correlate with the connected regions\n(clusters) in the network when the resistivity is low such as in Aluminum. For\nthe more resistive material NbTiN the impedance becomes dominated by the local\nstructure of the percolating network (connectivity). The results can\nqualitatively be understood and reproduced with a network current spreading\nmodel based on the pseudo-inverse Laplacian of the underlying network graph.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The quality of the incoming experimental data has a significant importance\nfor both analysis and running the experiment. The main point of the Baikal-GVD\nDQM system is to monitor the status of the detector and obtained data on the\nrun-by-run based analysis. It should be fast enough to be able to provide\nanalysis results to detector shifter and for participation in the global\nmulti-messaging system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In recent years, a plethora of deployment technologies evolved, many\nfollowing a declarative approach to automate the delivery of software\ncomponents. Even if such technologies share the same purpose, they differ in\nfeatures and supported mechanisms. Thus, it is difficult to compare and select\ndeployment automation technologies as well as to migrate from one technology to\nanother. Hence, we present a systematic review of declarative deployment\ntechnologies and introduce the Essential Deployment Metamodel (EDMM) by\nextracting the essential parts that are supported by all these technologies.\nThereby, the EDMM enables a common understanding of declarative deployment\nmodels by facilitating the comparison, selection, and migration of\ntechnologies. Moreover, it provides a technology-independent baseline for\nfurther deployment automation research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report the result for a search for the leptonic decay of $B^+ \\to \\mu^+ \\,\n\\nu_{\\mu}$ using the full Belle data set of 711 fb${}^{-1}$ of integrated\nluminosity at the $\\Upsilon(4S)$ resonance. In the Standard Model leptonic\n$B$-meson decays are helicity and CKM suppressed. To maximize sensitivity an\ninclusive tagging approach is used to reconstruct the second $B$ meson produced\nin the collision. The directional information from this second $B$ meson is\nused to boost the observed $\\mu$ into the signal $B$ meson rest-frame, in which\nthe $\\mu$ has a monochromatic momentum spectrum. Though its momentum is smeared\nby the experimental resolution, this technique improves the analysis\nsensitivity considerably. Analyzing the $\\mu$ momentum spectrum in this frame\nwe find $\\mathcal{B}(B^+ \\to \\mu^+ \\, \\nu_\\mu) = \\left( 5.3 \\pm 2.0 \\pm 0.9\n\\right) \\times 10^{-7}$ with a one-sided significance of 2.8 standard\ndeviations over the background-only hypothesis. This translates to a\nfrequentist upper limit of $\\mathcal{B}(B^+ \\to \\mu^+ \\, \\nu_{\\mu}) < 8.6\n\\times 10^{-7}$ at 90% CL. The experimental spectrum is then used to search for\na massive sterile neutrino, $B^+ \\to \\mu^+ \\, N$, but no evidence is observed\nfor a sterile neutrino with a mass in a range of 0 - 1.5 GeV. The determined\n$B^+ \\to \\mu^+ \\, \\nu_{\\mu}$ branching fraction limit is further used to\nconstrain the mass and coupling space of the type II and type III\ntwo-Higgs-doublet models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In preoperative imaging, the demarcation of rectal cancer with magnetic\nresonance images provides an important basis for cancer staging and treatment\nplanning. Recently, deep learning has greatly improved the state-of-the-art\nmethod in automatic segmentation. However, limitations in data availability in\nthe medical field can cause large variance and consequent overfitting to\nmedical image segmentation networks. In this study, we propose methods to\nreduce the model variance of a rectal cancer segmentation network by adding a\nrectum segmentation task and performing data augmentation; the geometric\ncorrelation between the rectum and rectal cancer motivated the former approach.\nMoreover, we propose a method to perform a bias-variance analysis within an\narbitrary region-of-interest (ROI) of a segmentation network, which we applied\nto assess the efficacy of our approaches in reducing model variance. As a\nresult, adding a rectum segmentation task reduced the model variance of the\nrectal cancer segmentation network within tumor regions by a factor of 0.90;\ndata augmentation further reduced the variance by a factor of 0.89. These\napproaches also reduced the training duration by a factor of 0.96 and a further\nfactor of 0.78, respectively. Our approaches will improve the quality of rectal\ncancer staging by increasing the accuracy of its automatic demarcation and by\nproviding rectum boundary information since rectal cancer staging requires the\ndemarcation of both rectum and rectal cancer. Besides such clinical benefits,\nour method also enables segmentation networks to be assessed with bias-variance\nanalysis within an arbitrary ROI, such as a cancerous region.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently Raayoni et al. announced various conjectures on continued fractions\nof fundamental constants automatically generated with machine learning\ntechniques. In this paper we prove some of their stated conjectures for Euler\nnumber $e$ and show the equivalence of some of the listed conjectures.\nMoreover, we propose a simple method that can be used to generate other\ncontinued fractions using their series representations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this note we propose a trilinear bracket formulation for the Hamiltonian\nextended Magnetohydrodynamics (XMHD) model with homogeneous mass density. The\ncorresponding two-dimensional representation is derived by performing spatial\nreduction on the three-dimensional bracket, upon introducing a symmetric\nrepresentation for the field variables. Subsequently, the trilinear bracket of\nthe resulting two-dimensional, four-field model is discretized using a finite\ndifference scheme, which results in semi-discrete dynamics that involve the\nArakawa Jacobian. Simulations of planar dynamics show that this scheme respects\nthe desired conservation properties to high precision.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Motivated by the Dikin walk, we develop aspects of an interior-point theory\nfor sampling in high dimension. Specifically, we introduce a symmetric\nparameter and the notion of strong self-concordance. These properties imply\nthat the corresponding Dikin walk mixes in $\\tilde{O}(n\\bar{\\nu})$ steps from a\nwarm start in a convex body in $\\mathbb{R}^{n}$ using a strongly\nself-concordant barrier with symmetric self-concordance parameter $\\bar{\\nu}$.\nFor many natural barriers, $\\bar{\\nu}$ is roughly bounded by $\\nu$, the\nstandard self-concordance parameter. We show that this property and strong\nself-concordance hold for the Lee-Sidford barrier. As a consequence, we obtain\nthe first walk to mix in $\\tilde{O}(n^{2})$ steps for an arbitrary polytope in\n$\\mathbb{R}^{n}$. Strong self-concordance for other barriers leads to an\ninteresting (and unexpected) connection -- for the universal and entropic\nbarriers, it is implied by the KLS conjecture.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we describe NeurIPS 2019 Learning to Move - Walk Around\nchallenge physics-based environment and present our solution to this\ncompetition which scored 1303.727 mean reward points and took 3rd place. Our\nmethod combines recent advances from both continuous- and discrete-action space\nreinforcement learning, such as Soft Actor-Critic and Recurrent Experience\nReplay in Distributed Reinforcement Learning. We trained our agent in two\nstages: to move somewhere at the first stage and to follow the target velocity\nfield at the second stage. We also introduce novel Q-function split technique,\nwhich we believe facilitates the task of training an agent, allows critic\npretraining and reusing it for solving harder problems, and mitigate reward\nshaping design efforts.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  LIME is a popular approach for explaining a black-box prediction through an\ninterpretable model that is trained on instances in the vicinity of the\npredicted instance. To generate these instances, LIME randomly selects a subset\nof the non-zero features of the predicted instance. After that, the perturbed\ninstances are fed into the black-box model to obtain labels for these, which\nare then used for training the interpretable model. In this study, we present a\nsystematic evaluation of the interpretable models that are output by LIME on\nthe two use-cases that were considered in the original paper introducing the\napproach; text classification and object detection. The investigation shows\nthat the perturbation and labeling phases result in both data and label shift.\nIn addition, we study the correlation between the shift and the fidelity of the\ninterpretable model and show that in certain cases the shift negatively\ncorrelates with the fidelity. Based on these findings, it is argued that there\nis a need for a new sampling approach that mitigates the shift in the LIME's\nframework.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the matrix product state which appears as the boundary state of the\nAdS/dCFT set-up where a probe D7 brane wraps two two-spheres stabilized by\nfluxes. The matrix product state plays a dual role, on one hand acting as a\ntool for computing one-point functions in a domain wall version of N=4 SYM and\non the other hand acting as the initial state in the study of quantum quenches\nof the Heisenberg spin chain. We derive a number of selection rules for the\noverlaps between the matrix product state and the eigenstates of the Heisenberg\nspin chain and in particular demonstrate that the matrix product state does not\nfulfill a recently proposed integrability criterion. Accordingly, we find that\nthe overlaps can not be expressed in the usual factorized determinant form.\nNevertheless, we derive some exact results for one-point functions of simple\noperators and present a closed formula for one-point functions of more general\noperators in the limit of large spin-chain length.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  R. Nandakumar asked whether there is a tiling of the plane by pairwise\nincongruent triangles of equal area and equal perimeter. Recently a negative\nanswer was given by Kupavskii, Pach and Tardos. Still one may ask for weaker\nversions of the problem, or for the analogue of this problem for quadrangles,\npentagons, or hexagons. Several answers were given by the first author in a\nprevious paper. Here we solve three further cases. In particular, our main\nresult shows that there are vertex-to-vertex tilings by pairwise incongruent\ntriangles of unit area and bounded perimeter.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A well--known fact in Spectral Graph Theory is the existence of pairs of\nisospectral nonisomorphic graphs (known as PINGS). The work of A.J. Schwenk (in\n1973) and of C. Godsil and B. McKay (in 1982) shed some light on the\nexplanation of the presence of isospectral graphs, and they gave routines to\nconstruct PINGS. Here, we consider the Godsil-McKay--type routines developed\nfor graphs, whose adjacency matrices are $(0,1)$-matrices, to the level of\nsigned graphs, whose adjacency matrices allow the presence of $-1$'s. We show\nthat, with suitable adaption, such routines can be successfully ported to\nsigned graphs, and we can build pairs of cospectral switching nonisomorphic\nsigned graphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Privacy recently emerges as a severe concern in deep learning, that is,\nsensitive data must be prohibited from being shared with the third party during\ndeep neural network development. In this paper, we propose Morphed Learning\n(MoLe), an efficient and secure scheme to deliver deep learning data. MoLe has\ntwo main components: data morphing and Augmented Convolutional (Aug-Conv)\nlayer. Data morphing allows data providers to send morphed data without privacy\ninformation, while Aug-Conv layer helps deep learning developers to apply their\nnetworks on the morphed data without performance penalty. MoLe provides\nstronger security while introducing lower overhead compared to GAZELLE (USENIX\nSecurity 2018), which is another method with no performance penalty on the\nneural network. When using MoLe for VGG-16 network on CIFAR dataset, the\ncomputational overhead is only 9% and the data transmission overhead is 5.12%.\nAs a comparison, GAZELLE has computational overhead of 10,000 times and data\ntransmission overhead of 421,000 times. In this setting, the attack success\nrate of adversary is 7.9 x 10^{-90} for MoLe and 2.9 x 10^{-30} for GAZELLE,\nrespectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Pre-training models have been proved effective for a wide range of natural\nlanguage processing tasks. Inspired by this, we propose a novel dialogue\ngeneration pre-training framework to support various kinds of conversations,\nincluding chit-chat, knowledge grounded dialogues, and conversational question\nanswering. In this framework, we adopt flexible attention mechanisms to fully\nleverage the bi-directional context and the uni-directional characteristic of\nlanguage generation. We also introduce discrete latent variables to tackle the\ninherent one-to-many mapping problem in response generation. Two reciprocal\ntasks of response generation and latent act recognition are designed and\ncarried out simultaneously within a shared network. Comprehensive experiments\non three publicly available datasets verify the effectiveness and superiority\nof the proposed framework.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Mechanisms for non-invasive target drug delivery using microbubbles and\nultrasound have attracted growing interest. Microbubbles can be loaded with a\ntherapeutic payload and tracked via ultrasound imaging to selectively release\ntheir payload at ultrasound-targeted locations. In this study, an ultrasonic\ntrapping method is proposed for simultaneously imaging and controlling the\nlocation of microbubbles in flow by using acoustic radiation force. Targeted\ndrug delivery methods are expected to benefit from the use of the ultrasonic\ntrap, since trapping will increase the MB concentration at a desired location\nin human body.\n  The ultrasonic trap was generated by using an ultrasound research system UARP\nII and a linear array transducer. The trap was designed asymmetrically to\nproduces a weaker radiation force at the inlet of the trap to further\nfacilitate microbubble entrance. A pulse sequence was generated that can switch\nbetween a long duration trapping waveform and short duration imaging waveform.\nHigh frame rate plane wave imaging was chosen for monitoring trapped\nmicrobubbles at 1 kHz. The working principle of the ultrasonic trap was\nexplained and demonstrated in an ultrasound phantom by injecting SonoVue\nmicrobubbles flowing at 80 mL/min flow rate in a 3.5 mm diameter vessel.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose an attosecond XUV-pump IR-probe photoionization protocol that\nemploys pairs of counter-rotating consecutive harmonics and angularly resolved\nphotoelectron detection, thereby providing direct measurement of ionization\nphases. The present method, which we call circular holographic ionization-phase\nmeter (CHIP), gives also access to the phase of photoemission amplitudes of\neven-parity continuum states from a single time-delay measurement, since the\nrelative phase of one- and two-photon ionization pathways is imprinted in the\nphotoemission anisotropy. The method is illustrated with \\emph{ab initio}\nsimulations of photoionization via autoionizing resonances in helium. The rapid\nphase excursion in the transition amplitude to both the dipole-allowed\n$(2s2p)^1$P$^{\\rm o}$ and the dipole-forbidden $(2p^2)^1$D$^{\\rm e}$ states are\nfaithfully reproduced.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We give a sufficient condition using the Ozsv\\'ath-Stipsicz-Szab\\'o\nconcordance invariant Upsilon for the monodromy of the open book decomposition\nof a fibered knot to be right-veering. As an application, we generalize a\nresult of Baker on ribbon concordances between fibered knots. Following Baker,\nwe conclude that either fibered knots $K$ in $S^{3}$ satisfying that\n$\\Upsilon'(t) = -g(K)$ for some $t \\in [0,1)$ are unique in their smooth\nconcordance classes or there exists a counterexample to the Slice-Ribbon\nConjecture.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the spatial structure of dense square-shoulder fluids. To this\nend we derive analytical perturbative solutions of the Ornstein-Zernike\nequation in the low- and high-temperature limits as expansions around the known\nhard sphere solutions. We then discuss the suitability of perturbative\napproaches in relation to the Ornstein-Zernike equation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We give a closed-form, prescriptive representation of all-multiplicity\ntwo-loop MHV amplitude integrands in fully-color-dressed (non-planar) maximally\nsupersymmetric Yang-Mills theory.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  X-ray security screening is in widespread use to maintain transportation\nsecurity against a wide range of potential threat profiles. Of particular\ninterest is the recent focus on the use of automated screening approaches,\nincluding the potential anomaly detection as a methodology for concealment\ndetection within complex electronic items. Here we address this problem\nconsidering varying segmentation strategies to enable the use of both object\nlevel and sub-component level anomaly detection via the use of secondary\nconvolutional neural network (CNN) architectures. Relative performance is\nevaluated over an extensive dataset of exemplar cluttered X-ray imagery, with a\nfocus on consumer electronics items. We find that sub-component level\nsegmentation produces marginally superior performance in the secondary anomaly\ndetection via classification stage, with true positive of ~98% of anomalies,\nwith a ~3% false positive.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Many dark matter models generically predict invisible and displaced\nsignatures at Belle II, but even striking events may be missed by the currently\nimplemented search programme because of inefficient trigger algorithms. Of\nparticular interest are final states with a single photon accompanied by\nmissing energy and a displaced pair of electrons, muons, or hadrons. We argue\nthat a displaced vertex trigger will be essential to achieve optimal\nsensitivity at Belle II. To illustrate this point, we study a simple but\nwell-motivated model of thermal inelastic dark matter in which this signature\nnaturally occurs and show that otherwise inaccessible regions of parameter\nspace can be tested with such a search. We also evaluate the sensitivity of\nsingle-photon searches at BaBar and Belle II to this model and provide detailed\ncalculations of the relic density target.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  IRS~13E is an enigmatic compact group of massive stars located in projection\nonly 3.6 arcseconds away from Sgr A*. This group has been suggested to be\nbounded by an intermediate-mass black hole (IMBH). We present a\nmulti-wavelength study of the group and its interplay with the environment.\nBased on Chandra observations, we find the X-ray spectrum of IRS~13E can be\nwell characterized by an optically thin thermal plasma. The emission peaks\nbetween two strongly mass-losing Wolf-Rayet stars of the group. These\nproperties can be reasonably well reproduced by simulated colliding winds of\nthese two stars. However, this scenario under-predicts the X-ray intensity in\nouter regions. The residual emission likely results from the ram-pressure\nconfinement of the IRS~13E group wind by the ambient medium and is apparently\nassociated with a shell-like warm gas structure seen in Pa-alpha and in ALMA\nobservations. These latter observations also show strongly peaked thermal\nemission with unusually large velocity spread between the two stars. These\nresults indicate that the group is colliding with the bar of the dense cool gas\nmini-spiral around Sgr A*. The extended X-ray morphology of IRS~13E and its\nassociation with the bar further suggest that the group is physically much\nfarther away than the projected distance from Sgr A*. The presence of an IMBH,\nwhile favorable to keep the stars bound together, is not necessary to explain\nthe observed stellar and gas properties of IRS~13E.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  By viewing non-commutative polynomials, that is, elements in free associative\nalgebras, in terms of linear representations, we generalize Horner's rule to\nthe non-commutative (multivariate) setting. We introduce the concept of Horner\nsystems (which has parallels to that of companion matrices), discuss their\nconstruction and show how they enable the efficient evaluation of\nnon-commutative polynomials by matrices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The vacuum must contain virtual fluctuations of black hole microstates for\neach mass $M$. We observe that the expected suppression for $M\\gg m_p$ is\ncounteracted by the large number $Exp[S_{bek}]$ of such states. From string\ntheory we learn that these microstates are extended objects that are resistant\nto compression. We argue that recognizing this `virtual extended\ncompression-resistant' component of the gravitational vacuum is crucial for\nunderstanding gravitational physics. Remarkably, such virtual excitations have\nno significant effect for observable systems like stars, but they resolve two\nimportant problems: (a) gravitational collapse is halted outside the horizon\nradius, removing the information paradox; (b) spacetime acquires a `stiffness'\nagainst the curving effects of vacuum energy; this ameliorates the cosmological\nconstant problem posed by the existence of a planck scale $\\Lambda$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For evolving datasets with continual reports, the composition rule for\ndifferential privacy (DP) dictates that the scale of DP noise must grow\nlinearly with the number of the queries, or that the privacy budget must be\nsplit equally between all the queries, so that the privacy budget across all\nthe queries remains bounded and consistent with the privacy guarantees. To\navoid this drawback of DP, we consider datasets containing almost periodic time\nseries, composed of periodic components and noisy variations on top that are\nindependent across periods. Our interest in these datasets is motivated by\nthat, for reporting on private periodic time series, we do not need to divide\nthe privacy budget across the entire, possibly infinite, horizon. Instead, for\nperiodic time series, we generate DP reports for the first period and report\nthe same DP reports periodically. In practice, however, exactly periodic time\nseries do not exist as the data always contains small variations due to random\nor uncertain events. For instance, the energy consumption of a household may\nrepeat the same daily pattern with slight variations due to minor changes to\nthe habits of the individuals. The underlying periodic pattern is a function of\nthe private information of the households. It might be desired to protect the\nprivacy of households by not leaking information about the recurring patterns\nwhile the individual daily variations are almost noise-like with little to no\nprivacy concerns (depending on the situation). Motivated by this, we define DP\nfor almost periodic datasets and develop a Laplace mechanism for responding to\nlinear queries. We provide statistical tools for testing the validity of almost\nperiodicity assumption. We use multiple energy datasets containing smart-meter\nmeasurements of households to validate almost periodicity assumption. We\ngenerate DP aggregate reports and investigate their utility.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Motivated by reconstruction results by Rubin, we introduce a new\nreconstruction notion for permutation groups, transformation monoids and\nclones, called automatic action compatibility, which entails automatic\nhomeomorphicity. We further give a characterization of automatic\nhomeomorphicity for transformation monoids on arbitrary carriers with a dense\ngroup of invertibles having automatic homeomorphicity. We then show how to lift\nautomatic action compatibility from groups to monoids and from monoids to\nclones under fairly weak assumptions. We finally employ these theorems to get\nautomatic action compatibility results for monoids and clones over several\nwell-known countable structures, including the strictly ordered rationals, the\ndirected and undirected version of the random graph, the random tournament and\nbipartite graph, the generic strictly ordered set, and the directed and\nundirected versions of the universal homogeneous Henson graphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Modern Internet of Things (IoT) applications generate massive amounts of\ndata, much of it in the form of objects/items of readings, events, and log\nentries. Specifically, most of the objects in these IoT data contain rich\nembedded information (e.g., frequency and uncertainty) and different level of\nimportance (e.g., unit utility of items, interestingness, cost, risk, or\nweight). Many existing approaches in data mining and analytics have limitations\nsuch as only the binary attribute is considered within a transaction, as well\nas all the objects/items having equal weights or importance. To solve these\ndrawbacks, a novel utility-driven data analytics algorithm named HUPNU is\npresented, to extract High-Utility patterns by considering both Positive and\nNegative unit utilities from Uncertain data. The qualified high-utility\npatterns can be effectively discovered for risk prediction, manufacturing\nmanagement, decision-making, among others. By using the developed vertical\nProbability-Utility list with the Positive-and-Negative utilities structure, as\nwell as several effective pruning strategies. Experiments showed that the\ndeveloped HUPNU approach performed great in mining the qualified patterns\nefficiently and effectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the inpainting problem for noisy images. It is very challenge to\nsuppress noise when image inpainting is processed. An image patches based\nnonlocal variational method is proposed to simultaneously inpainting and\ndenoising in this paper. Our approach is developed on an assumption that the\nsmall image patches should be obeyed a distribution which can be described by a\nhigh dimension Gaussian Mixture Model. By a maximum a posteriori (MAP)\nestimation, we formulate a new regularization term according to the\nlog-likelihood function of the mixture model. To optimize this regularization\nterm efficiently, we adopt the idea of the Expectation Maximum (EM) algorithm.\nIn which, the expectation step can give an adaptive weighting function which\ncan be regarded as a nonlocal connections among pixels. Using this fact, we\nbuilt a framework for non-local image inpainting under noise. Moreover, we\nmathematically prove the existence of minimizer for the proposed inpainting\nmodel. By using a spitting algorithm, the proposed model are able to realize\nimage inpainting and denoising simultaneously. Numerical results show that the\nproposed method can produce impressive reconstructed results when the\ninpainting region is rather large.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $M$ be a subset of vector space or projective space. The authors define\nthe \\emph{generalized configuration space} of $M$ which is formed by $n$-tuples\nof elements of $M$ where any $k$ elements of each $n$-tuple are linearly\nindependent. The \\emph{generalized configuration space} gives a generalization\nof the classical configuration space defined by E.Fadell. Denote the\n\\emph{generalized configuration space} of $M$ by $W_{k,n}(M)$. The authors are\nmainly interested in the calculation about the homotopy groups of generalized\nconfiguration space. This article gives the fundamental groups of generalized\nconfiguration spaces of $\\mathbb{R}P^m$ for some special cases, and the\nconnections between the homotopy groups of generalized configuration spaces of\n$S^m$ and the homotopy groups of Stiefel manifolds. It is also proved that the\nhigher homotopy groups of generalized configuration spaces $W_{k,n}(S^m)$ and\n$W_{k,n}(\\mathbb{R}P^m)$ are isomorphic.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Trajectory planning is challenging for autonomous cars since they operate in\nunpredictable environments with limited sensor horizons. To incorporate new\ninformation as it is sensed, planning is done in a loop, with the next plan\nbeing computed as the previous plan is executed. The recent Reachability-based\nTrajectory Design (RTD) is a provably safe, real-time algorithm for trajectory\nplanning. RTD consists of an offline component, where a Forward Reachable Set\n(FRS) is computed for the vehicle tracking parameterized trajectories; and an\nonline part, where the FRS is used to map obstacles to constraints for\ntrajectory optimization in a provably-safe way. In the literature, RTD has only\nbeen applied to small mobile robots. The contribution of this work is applying\nRTD to a passenger vehicle in CarSim, with a full powertrain model, chassis and\ntire dynamics. RTD produces safe trajectory plans with the vehicle traveling up\nto 15 m/s on a two-lane road, with randomly-placed obstacles only known to the\nvehicle when detected within its sensor horizon. RTD is compared with a\nNonlinear Model-Predictive Control (NMPC) and a Rapidly-exploring Random Tree\n(RRT) approach. The experiment demonstrates RTD's ability to plan safe\ntrajectories in real time, in contrast to the existing state-of-the-art\napproaches.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A relation tuple consists of two entities and the relation between them, and\noften such tuples are found in unstructured text. There may be multiple\nrelation tuples present in a text and they may share one or both entities among\nthem. Extracting such relation tuples from a sentence is a difficult task and\nsharing of entities or overlapping entities among the tuples makes it more\nchallenging. Most prior work adopted a pipeline approach where entities were\nidentified first followed by finding the relations among them, thus missing the\ninteraction among the relation tuples in a sentence. In this paper, we propose\ntwo approaches to use encoder-decoder architecture for jointly extracting\nentities and relations. In the first approach, we propose a representation\nscheme for relation tuples which enables the decoder to generate one word at a\ntime like machine translation models and still finds all the tuples present in\na sentence with full entity names of different length and with overlapping\nentities. Next, we propose a pointer network-based decoding approach where an\nentire tuple is generated at every time step. Experiments on the publicly\navailable New York Times corpus show that our proposed approaches outperform\nprevious work and achieve significantly higher F1 scores.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Estimating properties of discrete distributions is a fundamental problem in\nstatistical learning. We design the first unified, linear-time, competitive,\nproperty estimator that for a wide class of properties and for all underlying\ndistributions uses just $2n$ samples to achieve the performance attained by the\nempirical estimator with $n\\sqrt{\\log n}$ samples. This provides off-the-shelf,\ndistribution-independent, \"amplification\" of the amount of data available\nrelative to common-practice estimators.\n  We illustrate the estimator's practical advantages by comparing it to\nexisting estimators for a wide variety of properties and distributions. In most\ncases, its performance with $n$ samples is even as good as that of the\nempirical estimator with $n\\log n$ samples, and for essentially all properties,\nits performance is comparable to that of the best existing estimator designed\nspecifically for that property.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We use holography to develop a physical picture of the real-time evolution of\nthe spinodal instability of a four-dimensional, strongly-coupled gauge theory\nwith a first-order, thermal phase transition. We numerically solve Einstein's\nequations to follow the evolution, in which we identify four generic stages: A\nfirst, linear stage in which the instability grows exponentially; a second,\nnon-linear stage in which peaks and/or phase domains are formed; a third stage\nin which these structures merge; and a fourth stage in which the system finally\nrelaxes to a static, phase-separated configuration. On the gravity side the\nlatter is described by a static, stable, inhomogeneous horizon. We conjecture\nand provide evidence that all static, non-phase separated configurations in\nlarge enough boxes are dynamically unstable. We show that all four stages are\nwell described by the constitutive relations of second-order hydrodynamics that\ninclude all second-order gradients that are purely spatial in the local rest\nframe. In contrast, a M\\\"uller-Israel-Stewart-type formulation of hydrodynamics\nfails to provide a good description for two reasons. First, it misses some\nlarge, purely-spatial gradient corrections. Second, several second-order\ntransport coefficients in this formulation, including the relaxation times\n$\\tau_\\pi$ and $\\tau_\\Pi$, diverge at the points where the speed of sound\nvanishes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently, near-infrared GRAVITY@ESO observations at $2.2\\,\\mu{\\rm m}$ have\nannounced the detection of three bright \"flares\" in the vicinity of the\nGalactic center supermassive black hole (SMBH) that exhibited orbital motion at\na distance of about $6 - 11$ gravitational radii from an $\\sim 4\\times 10^6\\,\nM_{\\odot}$ black hole. There are indications of the presence of a large-scale,\norganized component of the magnetic field at the Galactic center.\nElectromagnetic effects on the flare dynamics were previously not taken into\naccount despite the relativistic motion of a plasma in magnetic field leading\nto the charge separation and nonnegligible net charge density in the plasma.\nApplying various approaches, we find the net charge number density of the flare\ncomponents of the order of $10^{-3} - 10^{-4}$ cm$^{-3}$, while the particles'\ntotal number density is of the order of $10^{6} - 10^{8}$ cm$^{-3}$. However,\neven such a tiny excess of charged particles in the quasi-neutral plasma can\nsignificantly affect the dynamics of flare components, which can then lead to\nthe degeneracy in the measurements of spin of the SMBH. Analyzing the dynamics\nof recent flares in the case of the rapidly rotating black hole, we also\nconstrain the inclination angle between the magnetic field and spin axis to\n$\\alpha < 50^{\\circ}$, as for larger angles, the motion of the hot spot is\nstrongly chaotic.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose and analyze a business model for 5G operators. Each operator is\nentitled to a share of a network operated by an Infrastructure Provider (InP)\nand use network slicing mechanisms to request network resources as needed for\nservice provision. The network operators become Network Slice Tenants (NSTs).\nThe InP performs the resource allocation based on a vector of weights chosen\nstrategically by each NST. The weights distribute the NST's share of resources\nbetween its subscribers in each cell. We propose a strategy profile in which\nthe NST chooses weights equal to the product of its share by the ratio between\nthe total number of subscribers in the cell and the total number of subscribers\nin the network. We characterize the proposed solution in terms of subscription\nratios and fractions of subscribers, for different cell capacities and user\nsensitivities. The proposed solution provides the exact values for the Nash\nequilibrium if the cells are homogeneous in terms of normalized capacity, which\nis a measure of the total amount of resources available in the cell. Otherwise,\nif the cells are heterogeneous, it provides an accurate approximation. We\nquantify the deviation from the equilibrium and conclude that it is highly\naccurate.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The digital camera captured document images may often be warped and distorted\ndue to different camera angles or document surfaces. A robust technique is\nneeded to solve this kind of distortion. The research on dewarping of the\ndocument suffers due to the limited availability of benchmark public dataset.\nIn recent times, deep learning based approaches are used to solve the problems\naccurately. To train most of the deep neural networks a large number of\ndocument images is required and generating such a large volume of document\nimages manually is difficult. In this paper, we propose a technique to generate\na synthetic warped image from a flat-bedded scanned document image. It is done\nby calculating warping factors for each pixel position using two warping\nposition parameters (WPP) and eight warping control parameters (WCP). These\nparameters can be specified as needed depending upon the desired warping. The\nresults are compared with similar real captured images both qualitative and\nquantitative way.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present the prospects of extracting information about the Epoch of\nReionization by identifying the remaining neutral regions, referred to as\nislands, in tomographic observations of the redshifted 21-cm signal. Using\nsimulated data sets we show that at late times the 21-cm power spectrum is\nfairly insensitive to the details of the reionization process but that the\nproperties of the neutral islands can distinguish between different\nreionization scenarios. We compare the properties of these islands with those\nof ionized bubbles. At equivalent volume filling fractions, neutral islands\ntend to be fewer in number but larger compared to the ionized bubbles. In\naddition, the evolution of the size distribution of neutral islands is found to\nbe slower than that of the ionized bubbles and also their percolation behaviour\ndiffers substantially. Even though the neutral islands are relatively rare,\nthey will be easier to identify in observations with the low-frequency\ncomponent of the Square Kilometre Array (SKA-Low) due to their larger size and\nthe lower noise levels at lower redshifts. The size distribution of neutral\nislands at the late stages of reionization is found to depend on the source\nproperties, such as the ionizing efficiency of the sources and their minimum\nmass. We find the longest line of sight through a neutral region to be more\nthan 100 comoving Mpc until very late stages (90-95 per cent reionized), which\nmay have relevance for the long absorption trough at $z=5.6-5.8$ in the\nspectrum of quasar ULAS J0148+0600.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Casimir force between two parallel plates in the G\\\"odel universe is\ncomputed for a scalar field at finite temperature. It is observed that when the\nplates separation is comparable with the scale given by the rotation of the\nspace-time, the force becomes repulsive and then approaches zero. Since it has\nbeen shown previously that the universe may experience a Godel phase for a\nsmall period of time, the induced inhomogeneities from the Casimir force are\nalso studied.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the problem of designing distributed controllers to guarantee\ndissipativity of a networked system comprised of dynamically coupled\nsubsystems. We require that the control synthesis is carried out locally at the\nsubsystem-level, without explicit knowledge of the dynamics of other subsystems\nin the network. We solve this problem in two steps. First, we provide\ndistributed subsystem-level dissipativity analysis conditions whose feasibility\nis sufficient to guarantee dissipativity of the networked system. We then use\nthese conditions to synthesize controllers locally at the subsystem-level,\nusing only the knowledge of the dynamics of that subsystem, and limited\ninformation about the dissipativity of the subsystems to which it is\ndynamically coupled. We show that the subsystem-level controllers synthesized\nin this manner are sufficient to guarantee dissipativity of the networked\ndynamical system. We also provide an approach to make this synthesis\ncompositional, that is, when a new subsystem is added to an existing network,\nonly the dynamics of the new subsystem, and information about the dissipativity\nof the subsystems in the existing network to which it is coupled are used to\ndesign a controller for the new subsystem, while guaranteeing dissipativity of\nthe networked system including the new subsystem. Finally, we demonstrate the\napplication of this synthesis in enabling plug-and-play operations of\ngenerators in a microgrid by extending our results to networked switched\nsystems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Fixed-point quantization and binarization are two reduction methods adopted\nto deploy Convolutional Neural Networks (CNN) on end-nodes powered by low-power\nmicro-controller units (MCUs). While most of the existing works use them as\nstand-alone optimizations, this work aims at demonstrating there is margin for\na joint cooperation that leads to inferential engines with lower latency and\nhigher accuracy. Called CoopNet, the proposed heterogeneous model is conceived,\nimplemented and tested on off-the-shelf MCUs with small on-chip memory and few\ncomputational resources. Experimental results conducted on three different CNNs\nusing as test-bench the low-power RISC core of the Cortex-M family by ARM\nvalidate the CoopNet proposal by showing substantial improvements w.r.t.\ndesigns where quantization and binarization are applied separately.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we use facial landmarks to make the deformation for facial\nimages more authentic. The deformation includes the expansion of eyes and the\nshrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark\ndetector is utilized to provide control points for deformation. Bilinear\ninterpolation is used in the expansion and Moving Least Squares methods (MLS)\nincluding Affine Deformation, Similarity Deformation and Rigid Deformation are\nused in the shrinking. We compare the running time as well as the quality of\ndeformed images using different MLS methods. The experimental results show that\nthe Rigid Deformation which can keep other parts of the images unchanged\nperforms better even if it takes the longest time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  While likelihood-based inference and its variants provide a statistically\nefficient and widely applicable approach to parametric inference, their\napplication to models involving intractable likelihoods poses challenges. In\nthis work, we study a class of minimum distance estimators for intractable\ngenerative models, that is, statistical models for which the likelihood is\nintractable, but simulation is cheap. The distance considered, maximum mean\ndiscrepancy (MMD), is defined through the embedding of probability measures\ninto a reproducing kernel Hilbert space. We study the theoretical properties of\nthese estimators, showing that they are consistent, asymptotically normal and\nrobust to model misspecification. A main advantage of these estimators is the\nflexibility offered by the choice of kernel, which can be used to trade-off\nstatistical efficiency and robustness. On the algorithmic side, we study the\ngeometry induced by MMD on the parameter space and use this to introduce a\nnovel natural gradient descent-like algorithm for efficient implementation of\nthese estimators. We illustrate the relevance of our theoretical results on\nseveral classes of models including a discrete-time latent Markov process and\ntwo multivariate stochastic differential equation models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider Schmidt's game on the space of compact subsets of a given metric\nspace equipped with the Hausdorff metric, and the space of continuous functions\nequipped with the supremum norm. We are interested in determining the generic\nbehaviour of objects in a metric space, mostly in the context of fractal\ndimensions, and the notion of `generic' we adopt is that of being winning for\nSchmidt's game. We find properties whose corresponding sets are winning for\nSchmidt's game that are starkly different from previously established, and\nwell-known, properties which are generic in other contexts, such as being\nresidual or of full measure.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The background field is assumed to play prime role in the erupting structures\nlike prominences. In the flux rope models, the critical decay index ($n_c$) is\na measure of the rate at which background field intensity decreases with height\nover the flux rope or erupting structure. In the real observations, the\ncritical height of the background field is unknown, so a typical value of\n$n_{c}=1.5$ is adopted from the numerical studies. In this study, we determined\nthe $n_c$ of 10 prominence eruptions (PEs). The prominence height in 3D is\nderived from two-perspective observations of \\textit{Solar Dynamics\nObservatory} and \\textit{Solar TErrestrial RElations Observatory}. Synoptic\nmaps of photospheric radial magnetic field are used to construct the background\nfield in the corona. During the eruption, the height-time curve of the sample\nevents exhibits the slow and fast-rise phases and is fitted with the\nlinear-cum-exponential model. From this model, the onset height of fast-rise\nmotion is determined and is considered as the critical height for the onset of\nthe torus-instability because the erupting structure is allowed to expand\nexponentially provided there is no strapping background field. Corresponding to\nthe critical height, the $n_c$ values of our sample events are varied to be in\nthe range of 0.8-1.3. Additionally, the kinematic analysis suggests that the\nacceleration of PEs associated with flares are significantly enhanced compared\nto flare-less PEs. We found that the flare magnetic reconnection is the\ndominant contributor than the torus-instability to the acceleration process\nduring the fast-rise phase of flare-associated PEs in low corona\n($<1.3R_{\\odot}$).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Graphs considered in this paper are finite, undirected and loopless, but we\nallow multiple edges. The point partition number $\\chi_t(G)$ is the least\ninteger $k$ for which $G$ admits a coloring with $k$ colors such that each\ncolor class induces a $(t-1)$-degenerate subgraph of $G$. So $\\chi_1$ is the\nchromatic number and $\\chi_2$ is the point aboricity. The point partition\nnumber $\\chi_t$ with $t\\geq 1$ was introduced by Lick and White. A graph $G$ is\ncalled $\\chi_t$-critical if every proper subgraph $H$ of $G$ satisfies\n$\\chi_t(H)<\\chi_t(G)$. In this paper we prove that if $G$ is a\n$\\chi_t$-critical graph whose order satisfies $|G|\\leq 2\\chi_t(G)-2$, then $G$\ncan be obtained from two non-empty disjoint subgraphs $G_1$ and $G_2$ by adding\n$t$ edges between any pair $u,v$ of vertices with $u\\in V(G_1)$ and $v\\in\nV(G_2)$. Based on this result we establish the minimum number of edges possible\nin a $\\chi_t$-critical graph $G$ of order $n$ and with $\\chi_t(G)=k$, provided\nthat $n\\leq 2k-1$ and $t$ is even. For $t=1$ the corresponding two results were\nobtained in 1963 by Tibor Gallai.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Psi-calculi is a parametric framework for process calculi similar to popular\npi-calculus extensions such as the explicit fusion calculus, the applied\npi-calculus and the spi calculus. Mechanised proofs of standard algebraic and\ncongruence properties of bisimilarity apply to all calculi within the\nframework. A limitation of psi-calculi is that communication channels must be\nsymmetric and transitive. In this paper, we give a new operational semantics to\npsi-calculi that allows us to lift these restrictions and simplify some of the\nproofs. The key technical innovation is to annotate transitions with a\nprovenance -- a description of the scope and channel they originate from. We\ngive mechanised proofs that our extension is conservative, and that the\nstandard algebraic and congruence properties of strong and weak bisimilarity\nare maintained. We show correspondence with a reduction semantics and barbed\nbisimulation. We show how a pi-calculus with preorders that was previously\nbeyond the scope of psi-calculi can be captured, and how to encode mixed choice\nunder very strong quality criteria.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The magnetic Laplacian (also called the line bundle Laplacian) on a connected\nweighted graph is a self-adjoint operator wherein the real-valued adjacency\nweights are replaced by unit complex-valued weights $\\{\\omega_{xy}\\}_{xy\\in\nE}$, satisfying the condition that $\\omega_{xy}=\\overline{\\omega_{yx}}$ for\nevery directed edge $xy$. When properly interpreted, these complex weights give\nrise to magnetic fluxes through cycles in the graph.\n  In this paper we establish the spectrum of the magnetic Laplacian, as a set\nof real numbers with multiplicities, on the Sierpinski gasket graph ($SG$)\nwhere the magnetic fluxes equal $\\alpha$ through the upright triangles, and\n$\\beta$ through the downright triangles. This is achieved upon showing the\nspectral self-similarity of the magnetic Laplacian via a 3-parameter map\n$\\mathcal{U}$ involving non-rational functions, which takes into account\n$\\alpha$, $\\beta$, and the spectral parameter $\\lambda$. In doing so we provide\na quantitative answer to a question of Bellissard [Renormalization Group\nAnalysis and Quasicrystals (1992)] on the relationship between the dynamical\nspectrum and the actual magnetic spectrum.\n  Our main theorems lead to two applications. In the case $\\alpha=\\beta$, we\ndemonstrate the approximation of the magnetic spectrum by the filled Julia set\nof $\\mathcal{U}$, the Sierpinski gasket counterpart to Hofstadter's butterfly.\nMeanwhile, in the case $\\alpha,\\beta\\in \\{0,\\frac{1}{2}\\}$, we can compute the\ndeterminant of the magnetic Laplacian determinant and the corresponding\nasymptotic complexity.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Where machine-learned predictive risk scores inform high-stakes decisions,\nsuch as bail and sentencing in criminal justice, fairness has been a serious\nconcern. Recent work has characterized the disparate impact that such risk\nscores can have when used for a binary classification task. This may not\naccount, however, for the more diverse downstream uses of risk scores and their\nnon-binary nature. To better account for this, in this paper, we investigate\nthe fairness of predictive risk scores from the point of view of a bipartite\nranking task, where one seeks to rank positive examples higher than negative\nones. We introduce the xAUC disparity as a metric to assess the disparate\nimpact of risk scores and define it as the difference in the probabilities of\nranking a random positive example from one protected group above a negative one\nfrom another group and vice versa. We provide a decomposition of bipartite\nranking loss into components that involve the discrepancy and components that\ninvolve pure predictive ability within each group. We use xAUC analysis to\naudit predictive risk scores for recidivism prediction, income prediction, and\ncardiac arrest prediction, where it describes disparities that are not evident\nfrom simply comparing within-group predictive performance.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present $\\textit{Free-MESSAGE}^{p}$, the first zeroth-order algorithm for\n(weakly-)convex mean-semideviation-based risk-aware learning, which is also the\nfirst three-level zeroth-order compositional stochastic optimization algorithm\nwhatsoever. Using a non-trivial extension of Nesterov's classical results on\nGaussian smoothing, we develop the $\\textit{Free-MESSAGE}^{p}$ algorithm from\nfirst principles, and show that it essentially solves a smoothed surrogate to\nthe original problem, the former being a uniform approximation of the latter,\nin a useful, convenient sense. We then present a complete analysis of the\n$\\textit{Free-MESSAGE}^{p}$ algorithm, which establishes convergence in a\nuser-tunable neighborhood of the optimal solutions of the original problem for\nconvex costs, as well as explicit convergence rates for convex, weakly convex,\nand strongly convex costs, and in a unified way. Orderwise, and for fixed\nproblem parameters, our results demonstrate no sacrifice in convergence speed\nas compared to existing first-order methods, while striking a certain balance\namong the condition of the problem, its dimensionality, as well as the accuracy\nof the obtained results, naturally extending previous results in zeroth-order\nrisk-neutral learning.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For text analysis, one often resorts to a lossy representation that either\ncompletely ignores word order or embeds each word as a low-dimensional dense\nfeature vector. In this paper, we propose convolutional Poisson factor analysis\n(CPFA) that directly operates on a lossless representation that processes the\nwords in each document as a sequence of high-dimensional one-hot vectors. To\nboost its performance, we further propose the convolutional Poisson gamma\nbelief network (CPGBN) that couples CPFA with the gamma belief network via a\nnovel probabilistic pooling layer. CPFA forms words into phrases and captures\nvery specific phrase-level topics, and CPGBN further builds a hierarchy of\nincreasingly more general phrase-level topics. For efficient inference, we\ndevelop both a Gibbs sampler and a Weibull distribution based convolutional\nvariational auto-encoder. Experimental results demonstrate that CPGBN can\nextract high-quality text latent representations that capture the word order\ninformation, and hence can be leveraged as a building block to enrich a wide\nvariety of existing latent variable models that ignore word order.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The superradiant instability modes of ultralight massive vector bosons are\nstudied for weakly charged rotating black holes in Einstein--Maxwell gravity\n(the Kerr--Newman solution) and low-energy heterotic string theory (the\nKerr--Sen black hole). We show that in both these cases, the corresponding\nmassive vector (Proca) equations can be fully separated, exploiting the hidden\nsymmetry present in these spacetimes. The resultant ordinary differential\nequations are solved numerically to find the most unstable modes of the Proca\nfield in the two backgrounds and compared to the vacuum (Kerr black hole) case.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A growing body of applied mathematics literature in recent years has focussed\non the application of fractional calculus to problems of anomalous transport.\nIn these analyses, the anomalous transport (of charge, tracers, fluid, etc.) is\npresumed attributable to long-range correlations of material properties within\nan inherently complex, and in some cases self-similar, conducting medium.\nRather than considering an exquisitely discretized (and computationally\nintractable) representation of the medium, the complex and spatially correlated\nheterogeneity is represented through reformulation of the PDE governing the\nrelevant transport physics such that its coefficients are, instead, smooth but\npaired with fractional-order space derivatives. Here we apply these concepts to\nthe scalar Helmholtz equation and its use in electromagnetic interrogation of\nEarth's interior through the magnetotelluric method. We outline a practical\nalgorithm for solving the Helmholtz equation using spectral methods coupled\nwith finite element discretization. Execution of this algorithm for the\nmagnetotelluric problem reveals several interesting features observable in\nfield data: long--range correlation of the predicted electromagnetic fields; a\npower-law relationship between the squared impedance amplitude and squared\nwavenumber whose slope is a function of the fractional exponent within the\ngoverning Helmholtz equation; and, a non-constant apparent resistivity spectrum\nwhose variability arises solely from the fractional exponent. In geologic\nsettings characterized by self--similarity (e.g. fracture systems; thick and\nrichly-textured sedimentary sequences, etc.) we posit that diagnostics are\nuseful for geologic characterization of features far below the typical\nresolution limit of electromagnetic methods in geophysics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A construction described by the current author in 2017 uses two linear\n`prototype' graphs to build a compound graph with Ramsey properties inherited\nfrom the prototypes.\n  This paper describes a generalisation of that construction which has produced\nimproved lower bounds in many cases for multicolour Ramsey numbers. The\nresulting graphs are linear, as before, and under certain specific conditions\nset out here, they can be cyclic.\n  The mechanism of the new construction requires that the first prototype\ncontains a triangle-free `template' in one colour, with defined properties.\nThis paper shows that in the compound graph, clique numbers in the colours of\nthe first prototype may exceed those of the prototype. However, it proves\nnecessary only to test for a limited subset of the possible cliques using these\ncolours, in order to evaluate the relevant clique numbers for the entire graph.\nClique numbers in the colours of the second prototype are equal to those of\nthat prototype. These attributes enable the efficient searching of a new family\nof graphs based on the `template' approach.\n  It has also been found that there are a number of useful cases in which the\nclique numbers of the first prototype are not increased by the compounding\nprocess.\n  As a result of this construction many lower bounds can be improved. The\nimprovements include $R_4(5) \\ge 4073$, $R_5(5) \\ge 38914$, $R_3(6) \\ge 1106$,\n$R_4(6) \\ge 21302$, $R_4(7) \\ge 84623$ and $R_3(9) \\ge 14034$.\n  It is also shown that $R_3(9) \\ge 14081$ using a non-linear construction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Riemann's non-differentiable function is one of the most famous examples of\ncontinuous but nowhere differentiable functions, but it has also been shown to\nbe relevant from a physical point of view. Indeed, it satisfies the\nFrisch-Parisi multifractal formalism, which establishes a relationship with\nturbulence and implies some intermittent nature. It also plays a surprising\nrole as a physical trajectory in the evolution of regular polygonal vortices\nthat follow the binormal flow. With this motivation, we focus on one more\nclassic tool to measure intermittency, namely the fourth-order flatness, and we\nrefine the results that can be deduced from the multifractal analysis to show\nthat it diverges logarithmically. We approach the problem in two ways: with\nstructure functions in the physical space and with high-pass filters in the\nFourier space.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the problem of overbounding and underbounding both the backward\nand forward reachable set for a given polynomial vector field, nonlinear in\nboth state and input, with a given semialgebriac set of initial conditions and\nwith inputs constrained pointwise to lie in a semialgebraic set. Specifically,\nwe represent the forward reachable set using the value function which gives the\noptimal cost to go of an optimal control problems and if smooth satisfies the\nHamilton-Jacobi- Bellman PDE. We then show that there exist polynomial upper\nand lower bounds to this value function and furthermore, these polynomial\nsub-value and super-value functions provide provable upper and lower bounds to\nthe forward reachable set. Finally, by minimizing the distance between these\nsub-value and super-value functions in the L1-norm, we are able to construct\ninner and outer bounds for the reachable set and show numerically on several\nexamples that for relatively small degree, the Hausdorff distance between these\nbounds is negligible.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show theoretically how a correlation of multiple measurements on a qubit\nundergoing pure dephasing can be expressed as environmental noise filtering.\nThe measurement of such correlations can be used for environmental noise\nspectroscopy, and the family of noise filters achievable in such a setting is\nbroader than the one achievable with a standard approach, in which dynamical\ndecoupling sequences are used. We illustrate the advantages of this approach by\nconsidering the case of noise spectrum with sharp features at very low\nfrequencies. We also show how appropriately chosen correlations of a few\nmeasurements can detect the non-Gaussian character of certain environmental\nnoises, particularly the noise affecting the qubit at the so-called optimal\nworking point.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The HDoutliers algorithm is a powerful unsupervised algorithm for detecting\nanomalies in high-dimensional data, with a strong theoretical foundation.\nHowever, it suffers from some limitations that significantly hinder its\nperformance level, under certain circumstances. In this article, we propose an\nalgorithm that addresses these limitations. We define an anomaly as an\nobservation that deviates markedly from the majority with a large distance gap.\nAn approach based on extreme value theory is used for the anomalous threshold\ncalculation. Using various synthetic and real datasets, we demonstrate the wide\napplicability and usefulness of our algorithm, which we call the stray\nalgorithm. We also demonstrate how this algorithm can assist in detecting\nanomalies present in other data structures using feature engineering. We show\nthe situations where the stray algorithm outperforms the HDoutliers algorithm\nboth in accuracy and computational time. This framework is implemented in the\nopen source R package stray.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Due to the heavy reliance of millimeter-wave (mmWave) wireless systems on\ndirectional links, beamforming (BF) with high-dimensional arrays is essential\nfor cellular systems in these frequencies. How to perform the array processing\nin a power efficient manner is a fundamental challenge. Analog and hybrid BF\nrequire fewer analog-to-digital and digital-to-analog converters (ADCs and\nDACs), but can only communicate in a small number of directions at a time,\nlimiting directional search, spatial multiplexing and control signaling.\nDigital BF enables flexible spatial processing, but must be operated at a low\nquantization resolution to stay within reasonable power levels. This decrease\nin quantizer resolution introduces noise in the received signal and degrades\nthe quality of the transmitted signal. To assess the effect of low-resolution\nquantization on cellular system, we present a simple additive white Gaussian\nnoise (AWGN) model for quantization noise. Simulations with this model reveal\nthat at moderate resolutions (3-4 bits per ADC), there is negligible loss in\ndownlink cellular capacity from quantization. In essence, the low-resolution\nADCs limit the high SNR, where cellular systems typically do not operate. For\nthe transmitter, it is shown that DACs with 4 or more bits of resolution do not\nviolate the adjacent carrier leakage limit set by 3-rd Generation Partnership\nProject (3GPP) New Radio (NR) standards for cellular operations. Further, this\nwork studies the effect of low resolution quantization on the error vector\nmagnitude (EVM) of the transmitted signal.In fact, our findings suggests that\nlow-resolution fully digital BF architectures can be a power efficient\nalternative to analog or hybrid beamforming for both transmitters and receivers\nat millimeter wave.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study tropical Grassmanians Tr$(k,n)$ in relation to cluster algebras, and\nassess their applicability to $n$-particle amplitudes for $n=7,8$. In\n$\\mathcal{N}=4$ super Yang-Mills theory, we first show that while the totally\npositive part of Tr$(4,7)$ may encompass the iterated discontinuity structure\nof the seven-point Maximally Helicity Violating (MHV) amplitude, it is too\nsmall for the Next-to-MHV helicity configuration. Then, using Tr$(4,8)$ we\npropose a finite set of 356 cluster $\\mathcal{A}$-coordinates expected to\ncontain the rational symbol letters of the eight-particle MHV amplitude, and\ndiscuss how the remaining square-root letters may be obtained from limits of\ninfinite mutation sequences. Finally, we use a triangulation of the totally\npositive part of Tr$(3,8)$ to obtain the associated generalised biadjoint\nscalar amplitude in a form containing a near-minimal amount of spurious poles.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cooperative relaying is utilized as an efficient method for data\ncommunication in wireless sensor networks and the Internet of Things (IoT).\nHowever, sometimes due to the necessity of multi-hop relaying in such\ncommunication networks, it is challenging to guarantee the secrecy of\ncooperative transmissions when the relays may themselves be eavesdroppers,\ni.e., we may face with the untrusted relaying scenario where the relays are\nboth necessary helpers and potential adversary. To obviate this issue, a new\ncooperative jamming scheme is proposed in this paper, in which the data can be\nconfidentially communicated from the source to the destination through multiple\nuntrusted relays. In our proposed secure transmission scheme, all the\nlegitimate nodes contribute to providing secure communication by intelligently\ninjecting artificial noises to the network in different communication phases.\nFor the sake of analysis, we consider a multi-hop untrusted relaying network\nwith two successive intermediate nodes, i.e, a three-hop communications\nnetwork. Given this system model, a new closed-form expression is presented in\nthe high signal-to-noise ratio (SNR) region for the Ergodic secrecy rate (ESR).\nFurthermore, we evaluate the high SNR slope and power offset of the ESR to gain\nan insightful comparison of the proposed secure transmission scheme and the\nstate-of-arts. Our numerical results highlight that the proposed secure\ntransmission scheme provides better secrecy rate performance compared with the\ntwo-hop untrusted relaying as well as the direct transmission schemes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Energy levels of the $1sns$ and $1snp$ states of ions along the helium\nisoelectronic sequence from carbon to uranium are calculated, with $n=3$-$7$.\nThe computation is performed within the relativistic configuration-interaction\nmethod, including the relativistic nuclear recoil effect, the leading QED\neffects, and the frequency dependence of the Breit interaction. All theoretical\nenergies are supplied with uncertainty estimates.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper is concerned with normal approximation under relaxed moment\nconditions using Stein's method. We obtain the explicit rates of convergence in\nthe central limit theorem for (i) nonlinear statistics with finite absolute\nmoment of order $2+\\delta\\in(2,3];$ (ii) nonlinear statistics with vanishing\nthird moment and finite absolute moment of order $3+\\delta\\in(3,4].$ When\napplied to specific examples, these rates are of the optimal order\n$O(n^{-\\frac{\\delta}{2}})$ and $O(n^{-\\frac{1+\\delta}{2}}).$ Our proof are\nbased on the covariance identify formula and simple observations about the\nsolution of Stein's equation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we are exploring the feasibility of observing non-classical\nfeatures of gravity in a low-energy regime in a quantum optomechanical\nexperiment. If gravity is to have an underlying quantum nature, it should hold\nthe most fundamental quantum characteristics such as the superposition\nprinciple and entanglement. Despite the weakness of gravity, in principle there\nis a chance, to observe such a quantum signature of the gravity by exploiting\nthe quantum optomechanical techniques, without direct observation of graviton.\nWe are investigating a new dynamical scheme called, gravitational quantum\nregime, in which the source of gravity is a quantum particle, and its centre of\nmass is subject to the spatial superposition. In a Gedankenexperiment, a test\nparticle is gravitationally interacting with a quantum nanoparticle in a\ndouble-slit setup. Possible entanglement or superposition of the fields is\ninvestigated. We are looking for the corresponding deviation of the classical\ndescription of gravity despite being far from the Planck scale. Any\nexperimental interrogation which reveals that gravitational field obeys the\nquantum superposition principle would be the first recognition of quantumness\nof gravity. This study will show how feasible it is to search for a\nnon-classical feature of gravity in such a regime of motion. Moreover, this\nproposal would be an attempt to test the objectivity of the quantum\nsuperposition principle and its contribution to the microstructure of\nspace-time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents a multi-view distributed video coding framework for\nindependent camera encoding and centralized decoding. Spatio-temporal-view\nconcealment methods are developed that exploit the interleaved nature of the\nemployed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side\ninformation (SI). We study a number of view concealment methods and develop a\njoint approach that exploits all available correlation for forming the side\ninformation. We apply a diversity technique for fusing multiple such\npredictions thereby achieving more reliable results. We additionally introduce\nsystems enhancements for further improving the rate distortion performance\nthrough selective feedback, inter-view bitplane projection and frame\nsubtraction. Results show a significant improvement in performance relative to\nH.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB\nincrease in PSNR.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce a class of stochastic processes with reinforcement consisting of\na sequence of random partitions $\\{\\mathcal{P}_t\\}_{t \\ge 1}$, where\n$\\mathcal{P}_t$ is a partition of $\\{1,2,\\dots, Rt\\}$. At each time~$t$,~$R$\nnumbers are added to the set being partitioned; of these, a random subset\n(chosen according to a time-dependent probability distribution) joins existing\nblocks, and the others each start new blocks on their own. Those joining\nexisting blocks each choose a block with probability proportional to that\nblock's cardinality, independently. We prove results concerning the asymptotic\ncardinality of a given block and central limit theorems for associated\nfluctuations about this asymptotic cardinality: these are proved both for a\nfixed block and for the maximum among all blocks. We also prove that with\nprobability one, a single block eventually takes and maintains the leadership\nin cardinality. Depending on the way one sees this partition process, one can\ntranslate our results to Balls and Bins processes, Generalized Chinese\nRestaurant Processes, Generalized Urn models and Preferential attachment random\ngraphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we analyze the stability of the traveling wave solution for an\nignition-temperature, first-order reaction model of thermo-diffusive\ncombustion, in the case of high Lewis numbers (${\\rm Le} >1$). The system of\ntwo parabolic PDEs is characterized by a free interface at which ignition\ntemperature $\\Theta_i$ is reached. We turn the model to a fully nonlinear\nproblem in a fixed domain. When the Lewis number is large, we define a\nbifurcation parameter $m=\\Theta_i/(1-\\Theta_i)$ and a perturbation parameter\n$\\varepsilon= 1/{\\rm Le}$. The main result is the existence of a critical value\n$m^c(\\varepsilon)$ close to $m^c=6$ at which Hopf bifurcation holds for\n$\\varepsilon$ small enough. Proofs combine spectral analysis and non-standard\napplication of Hurwitz Theorem with asymptotics as $\\varepsilon\\to 0$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Learning the compatibility between fashion items across categories is a key\ntask in fashion analysis, which can decode the secret of clothing matching. The\nmain idea of this task is to map items into a latent style space where\ncompatible items stay close. Previous works try to build such a transformation\nby minimizing the distances between annotated compatible items, which require\nmassive item-level supervision. However, these annotated data are expensive to\nobtain and hard to cover the numerous items with various styles in real\napplications. In such cases, these supervised methods fail to achieve\nsatisfactory performances. In this work, we propose a semi-supervised method to\nlearn the compatibility across categories. We observe that the distributions of\ndifferent categories have intrinsic similar structures. Accordingly, the better\ndistributions align, the closer compatible items across these categories\nbecome. To achieve the alignment, we minimize the distances between\ndistributions with unsupervised adversarial learning, and also the distances\nbetween some annotated compatible items which play the role of anchor points to\nhelp align. Experimental results on two real-world datasets demonstrate the\neffectiveness of our method.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper our aim is to find the radii of starlikeness and convexity of\nthe generalized Mittag-Leffler function for three different kinds of\nnormalization by using their Hadamard factorization in such a way that the\nresulting functions are analytic in the unit disk of the complex plane. The\ncharacterization of entire functions from Laguerre-P\\'{o}lya class and a result\nof H. Kumar and M.A. Pathan on the reality of the zeros of generalized\nMittag-Leffler functions, which origins goes back to Dzhrbashyan,\nOstrovski\\u{i} and Peresyolkova, play important roles in this paper. Moreover,\nthe interlacing properties of the zeros of Mittag-Leffler function and its\nderivative is also useful in the proof of the main results. By using the\nEuler-Rayleigh inequalities for the real zeros of the generalized\nMittag-Leffler function, we obtain some tight lower and upper bounds for the\nradii of starlikeness and convexity of order zero.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We establish a convergence theorem for the vanishing discount problem for a\nweakly coupled system of Hamilton-Jacobi equations. The crucial step is the\nintroduction of Mather measures and their relatives for the system, which we\ncall respectively viscosity Mather and Green-Poisson measures. This is done by\nthe convex duality and the duality between the space of continuous functions on\na compact set and the space of Borel measures on it. This is part 1 of our\nstudy of the vanishing discount problem for systems, which focuses on the\nlinear coupling, while part 2 will be concerned with nonlinear coupling.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce the Constrained Least-cost Tour (CLT) problem: given an\nundirected graph with weight and cost functions on the edges, minimise the\ntotal cost of a tour rooted at a start vertex such that the total weight lies\nwithin a given range. CLT is related to the family of Travelling Salesman\nProblems with Profits, but differs by defining the weight function on edges\ninstead of vertices, and by requiring the total weight to be within a range\ninstead of being at least some quota. We prove CLT is $\\mathcal{NP}$-hard, even\nin the simple case when the input graph is a path. We derive an informative\nlower bound by relaxing the integrality of edges and propose a heuristic\nmotivated by this relaxation. For the case that requires the tour to be a\nsimple cycle, we develop two heuristics which exploit Suurballe's algorithm to\nfind low-cost, weight-feasible cycles. We demonstrate our algorithms by\naddressing a real-world problem that affects urban populations: finding routes\nthat minimise air pollution exposure for walking, running and cycling in the\ncity of London.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a framework for tame geometry on Henselian valued fields which we\ncall Hensel minimality. In the spirit of o-minimality, which is key to real\ngeometry and several diophantine applications, we develop geometric results and\napplications for Hensel minimal structures that were previously known only\nunder stronger, less axiomatic assumptions. We show existence of\nt-stratifications in Hensel minimal structures and Taylor approximation results\nwhich are key to non-archimedean versions of Pila-Wilkie point counting,\nYomdin's parameterization results and to motivic integration. In this first\npaper we work in equi-characteristic zero; in the sequel paper, we develop the\nmixed characteristic case and a diophantine application.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce an algorithm for model-based hierarchical reinforcement learning\nto acquire self-contained transition and reward models suitable for\nprobabilistic planning at multiple levels of abstraction. We call this\nframework Planning with Abstract Learned Models (PALM). By representing\nsubtasks symbolically using a new formal structure, the lifted abstract Markov\ndecision process (L-AMDP), PALM learns models that are independent and modular.\nThrough our experiments, we show how PALM integrates planning and execution,\nfacilitating a rapid and efficient learning of abstract, hierarchical models.\nWe also demonstrate the increased potential for learned models to be\ntransferred to new and related tasks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we consider an unmanned aerial vehicle (UAV) assisted\ncommunications system, including two cooperative UAVs, a wireless-powered\nground destination node leveraging simultaneous wireless information and power\ntransfer (SWIPT) technique, and a terrestrial passive eavesdropper. One UAV\ndelivers confidential information to destination and the other sends jamming\nsignals to against eavesdropping and assist destination with energy harvesting.\nAssuming UAVs have partial information about eavesdropper's location, we\npropose two transmission schemes: friendly UAV jamming (FUJ) and Gaussian\njamming transmission (GJT) for the cases when jamming signals are known and\nunknown a priori at destination, respectively. Then, we formulate an average\nsecrecy rate maximization problem to jointly optimize the transmission power\nand trajectory of UAVs, and the power splitting ratio of destination. Being\nnon-convex and hence difficult to solve the formulated problem, we propose a\ncomputationally efficient iterative algorithm based on block coordinate descent\nand successive convex approximation to obtain a suboptimal solution. Finally,\nnumerical results are provided to substantiate the effectiveness of our\nproposed multiple-UAV schemes, compared to other existing benchmarks.\nSpecifically, we find that the FUJ demonstrates significant secrecy performance\nimprovement in terms of the optimal instantaneous and average secrecy rate\ncompared to the GJT and the conventional single-UAV counterpart.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  While many algorithms exist for tracing various contours for illustrating a\nmeshed object, few algorithms organize these contours into region-bounding\nclosed loops. Tracing closed-loop boundaries on a mesh can be problematic due\nto switchbacks caused by subtle surface variation, and the organization of\nthese regions into a planar map can lead to many small region components due to\nimprecision and noise. This paper adapts \"snaxels,\" an energy minimizing active\ncontour method designed for robust mesh processing, and repurposes it to\ngenerate visual, shadow and shading contours, and a simplified visual-surface\nplanar map, useful for stylized vector art illustration of the mesh. The snaxel\nactive contours can also track contours as the mesh animates, and\nframe-to-frame correspondences between snaxels lead to a new method to convert\nthe moving contours on a 3-D animated mesh into 2-D SVG curve animations for\nefficient embedding in Flash, PowerPoint and other dynamic vector art\nplatforms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The scalar-isoscalar mode of QCD becomes lighter/nearly massless close to the\nchiral transition/second-order critical point. From nuclear physics we know\nthat this mode is the main responsible for the attractive part of the\nnucleon-nucleon potential at inter-particle distances of 1-2 fm. Therefore one\nexpects that close to the critical point there is a long-range strong\nattraction among nucleons. Using a Walecka-Serot model for the NN potential we\nstudy the effects of the critical point in a finite system of nucleons and\nmesons by solving classical Molecular Dynamics+Langevin equations for the\nfreeze-out conditions of heavy-ion collisions. Going beyond the mean-field\napproximation allows us to account for strong nucleon correlations in the time\nevolution, leading to baryon clustering. We observe that light cluster\nformation, together with an enhancement of higher-order cumulants of the proton\ndistribution can signal the presence of the critical point.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Reinforcement learning algorithms rely on exploration to discover new\nbehaviors, which is typically achieved by following a stochastic policy. In\ncontinuous control tasks, policies with a Gaussian distribution have been\nwidely adopted. Gaussian exploration however does not result in smooth\ntrajectories that generally correspond to safe and rewarding behaviors in\npractical tasks. In addition, Gaussian policies do not result in an effective\nexploration of an environment and become increasingly inefficient as the action\nrate increases. This contributes to a low sample efficiency often observed in\nlearning continuous control tasks. We introduce a family of stationary\nautoregressive (AR) stochastic processes to facilitate exploration in\ncontinuous control domains. We show that proposed processes possess two\ndesirable features: subsequent process observations are temporally coherent\nwith continuously adjustable degree of coherence, and the process stationary\ndistribution is standard normal. We derive an autoregressive policy (ARP) that\nimplements such processes maintaining the standard agent-environment interface.\nWe show how ARPs can be easily used with the existing off-the-shelf learning\nalgorithms. Empirically we demonstrate that using ARPs results in improved\nexploration and sample efficiency in both simulated and real world domains,\nand, furthermore, provides smooth exploration trajectories that enable safe\noperation of robotic hardware.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the diffusive behavior of a Bose polaron immersed in a coherently\ncoupled two-component Bose-Einstein Condensate (BEC). Polaron superdiffuses if\nit couples in the same manner to both components, i.e. either attractively or\nrepulsively to both of them. This is the same behavior as that of an impurity\nimmersed in a single BEC. Conversely, the polaron exhibits a transient\nnontrivial subdiffusive behavior if it couples attractively to one of the\ncomponents and repulsively to the other. The anomalous diffusion exponent and\nthe duration of the subdiffusive interval can be controlled with the Rabi\nfrequency of the coherent coupling between the two components, and with the\ncoupling strength of the impurity to the BEC.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a novel method enabling robots to quickly learn to manipulate\nobjects by leveraging a motion planner to generate \"expert\" training\ntrajectories from a small amount of human-labeled data. In contrast to the\ntraditional sense-plan-act cycle, we propose a deep learning architecture and\ntraining regimen called PtPNet that can estimate effective end-effector\ntrajectories for manipulation directly from a single RGB-D image of an object.\nAdditionally, we present a data collection and augmentation pipeline that\nenables the automatic generation of large numbers (millions) of training image\nand trajectory examples with almost no human labeling effort.\n  We demonstrate our approach in a non-prehensile tool-based manipulation task,\nspecifically picking up shoes with a hook. In hardware experiments, PtPNet\ngenerates motion plans (open-loop trajectories) that reliably (89% success over\n189 trials) pick up four very different shoes from a range of positions and\norientations, and reliably picks up a shoe it has never seen before. Compared\nwith a traditional sense-plan-act paradigm, our system has the advantages of\noperating on sparse information (single RGB-D frame), producing high-quality\ntrajectories much faster than the \"expert\" planner (300ms versus several\nseconds), and generalizing effectively to previously unseen shoes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents evidence supporting the surprising conjecture that in the\ntopological category the slice genus of a satellite knot $P(K)$ is bounded\nabove by the sum of the slice genera of $K$ and $P(U)$. Our main result\nestablishes this conjecture for a variant of the topological slice genus, the\n$\\mathbb{Z}$-slice genus. As an application, we show that the $(n,1)$-cable of\nany 3-genus 1 knot (e.g. the figure 8 or trefoil knot) has topological slice\ngenus at most 1. Further, we show that the lower bounds on the slice genus\ncoming from the Tristram-Levine and Casson-Gordon signatures cannot be used to\ndisprove the conjecture. Notably, the conjectured upper bound does not involve\nthe algebraic winding number of the pattern $P$. This stands in stark contrast\nwith the smooth category, where for example there are many genus 1 knots whose\n$(n,1)$-cables have arbitrarily large smooth 4-genera.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Satellite-based quantum communications enable a bright future for\nglobal-scale information security. However, the spin orbital momentum of light,\ncurrently used in many mainstream quantum communication systems, only allows\nfor quantum encoding in a two-dimensional Hilbert space. The orbital angular\nmomentum (OAM) of light, on the other hand, enables quantum encoding in\nhigher-dimensional Hilbert spaces, opening up new opportunities for\nhigh-capacity quantum communications. Due to its turbulence-induced decoherence\neffects, however, the atmospheric channel may limit the practical usage of OAM.\nIn order to determine whether OAM is useful for satellite-based quantum\ncommunications, we numerically investigate the detection likelihoods for OAM\nstates that traverse satellite-to-ground channels. We show that the use of OAM\nthrough such channels is in fact feasible. We use our new results to then\ninvestigate design specifications that could improve OAM detection -\nparticularly the use of advanced adaptive optics techniques. Finally, we\ndiscuss how our work provides new insights into future implementations of\nspace-based OAM systems within the context of quantum communications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The regularity of systolically extremal surfaces is a notoriously difficult\nproblem already discussed by M. Gromov in 1983, who proposed an argument toward\nthe existence of $L^2$-extremizers exploiting the theory of $r$-regularity\ndeveloped by P. A. White and others by the 1950s. We propose to study the\nproblem of systolically extremal metrics in the context of generalized metrics\nof nonpositive curvature. A natural approach would be to work in the class of\nAlexandrov surfaces of finite total curvature, where one can exploit the tools\nof the completion provided in the context of Radon measures as studied by\nReshetnyak and others. However the generalized metrics in this sense still\ndon't have enough regularity. Instead, we develop a more hands-on approach and\nshow that, for each genus, every systolically extremal nonpositively curved\nsurface is piecewise flat with finitely many conical singularities. This result\nexploits a decomposition of the surface into flat systolic bands and\nnonsystolic polygonal regions, as well as the combinatorial/topological\nestimates of Malestein-Rivin-Theran, Przytycki, Aougab-Biringer-Gaster and\nGreene on the number of curves meeting at most once, combined with a kite\nexcision move. The move merges pairs of conical singularities on a surface of\ngenus $g$ and leads to an asymptotic upper bound $g^{4+\\epsilon}$ on the number\nof singularities.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently, we introduced a Rydberg-atom based mixer capable of detecting and\nmeasuring the phase of a radio-frequency field through the electromagnetically\ninduced transparency (EIT) and Autler-Townes (AT) effect. The ability to\nmeasure phase with this mixer allows for an atom-based receiver to detect\ndigital modulated communication signals. In this paper, we demonstrate\ndetection and reception of digital modulated signals based on various\nphase-shift keying approaches. We demonstrate Rydberg atom-based digital\nreception of binary phase-shift keying (BPSK), quadrature phase-shift keying\n(QPSK), and quadrature amplitude (QAM) modulated signals over a 19.626~GHz\ncarrier to transmit and receive a bit stream in cesium vapor. We present\nmeasured values of Error Vector Magnitude (EVM, a common communication metric\nused to assess how accurate a symbol or bit stream is received) as a function\nof symbol rate for BPSK, QPSK, 16QAM, 32QAM, and 64QAM modulation schemes.\nThese results allow us to discuss the bandwidth of a Rydberg-atom based\nreceiver system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The main aim of this paper is to prove the existence of a new production\nfunction with variable elasticity of factor substitution. This production\nfunction is a more general form which includes the Cobb-Douglas production\nfunction and the CES production function as particular cases. The econometric\nestimates presented in the paper confirm some other results and reinforces the\nconclusion that the sigma is well-below the Cobb-Douglas value of one.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Traffic sign identification using camera images from vehicles plays a\ncritical role in autonomous driving and path planning. However, the front\ncamera images can be distorted due to blurriness, lighting variations and\nvandalism which can lead to degradation of detection performances. As a\nsolution, machine learning models must be trained with data from multiple\ndomains, and collecting and labeling more data in each new domain is time\nconsuming and expensive. In this work, we present an end-to-end framework to\naugment traffic sign training data using optimal reinforcement learning\npolicies and a variety of Generative Adversarial Network (GAN) models, that can\nthen be used to train traffic sign detector modules. Our automated augmenter\nenables learning from transformed nightime, poor lighting, and varying degrees\nof occlusions using the LISA Traffic Sign and BDD-Nexar dataset. The proposed\nmethod enables mapping training data from one domain to another, thereby\nimproving traffic sign detection precision/recall from 0.70/0.66 to 0.83/0.71\nfor nighttime images.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study tropically planar graphs, which are the graphs that appear in smooth\ntropical plane curves. We develop necessary conditions for graphs to be\ntropically planar, and compute the number of tropically planar graphs up to\ngenus $7$. We provide non-trivial upper and lower bounds on the number of\ntropically planar graphs, and prove that asymptotically $0\\%$ of connected\ntrivalent planar graphs are tropically planar.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent studies claimed that planets around the same star have similar sizes\nand masses and regular spacings, and that planet pairs usually show ordered\nsizes such that the outer planet is usually the larger one. Here I show that\nthese patterns can be largely explained by detection biases. The \\emph{Kepler}\nplanet detections are set by the transit signal-to-noise ratio (S/N). For\ndifferent stellar properties and orbital period values, the same S/N\ncorresponds to different planetary sizes. This variation in the detection\nthreshold naturally leads to apparent correlations in planet sizes and the\nobserved size ordering. The apparently correlated spacings, measured in period\nratios, between adjacent planet pairs in systems with at least three detected\nplanets are partially due to the arbitrary upper limit that the earlier study\nimposed on the period ratio, and partially due to the varying stability\nthreshold for different planets. After these detection biases are taken into\naccount, we do not find strong evidence for the so-called \"intra-system\nuniformity\" or the size ordering effect. Instead, the physical properties of\n\\emph{Kepler} planets are largely independent of the properties of their\nsiblings and the parent star. It is likely that the dynamical evolution has\nerased the memory of \\emph{Kepler} planets about their initial formation\nconditions. In other words, it will be difficult to infer the initial\nconditions from the observed properties and the architecture of \\emph{Kepler}\nplanets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  These days deep learning is the fastest-growing area in the field of Machine\nLearning. Convolutional Neural Networks are currently the main tool used for\nimage analysis and classification purposes. Although great achievements and\nperspectives, deep neural networks and accompanying learning algorithms have\nsome relevant challenges to tackle. In this paper, we have focused on the most\nfrequently mentioned problem in the field of machine learning, that is\nrelatively poor generalization abilities. Partial remedies for this are\nregularization techniques e.g. dropout, batch normalization, weight decay,\ntransfer learning, early stopping and data augmentation. In this paper, we have\nfocused on data augmentation. We propose to use a method based on a neural\nstyle transfer, which allows generating new unlabeled images of a high\nperceptual quality that combine the content of a base image with the appearance\nof another one. In a proposed approach, the newly created images are described\nwith pseudo-labels, and then used as a training dataset. Real, labeled images\nare divided into the validation and test set. We validated the proposed method\non a challenging skin lesion classification case study. Four representative\nneural architectures are examined. Obtained results show the strong potential\nof the proposed approach.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper is to study the $\\mathbb{R}^{3}$ case of \\hyperref[Zhao]{[9]}. We\ndetermine all equivalence classes of immersed $3$-manifolds bounded by an\narbitrary immersed surface in $\\mathbb{R}^{3}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The goal of this paper is to construct a multivariate generalisation of the\nGrunwald-Letnikov derivative, a classical fractional derivative operator. To do\nso, we first produce a formalism of fractional derivatives in terms of\ninfinitesimal translations that justifies the \"binomial theorem\" argument for\nthe Grunwald-Letnikov derivative, allowing us to then extend the argument to\nconstruct the multivariate derivative via the more general multi-binomial\ntheorem. We conclude by studying the principal value of the fractional\nderivative of a multivariate power function, obtaining a characteristic\nequation in agreement with recent research in the area.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Assessments are usually thought of as ways for instructors to get information\nfrom students. In this work, we flip this perspective and explore how\nassessments communicate information to students. Specifically, we consider how\nassessments may provide information about what faculty and/or researchers think\nit means to know and do physics, i.e. their epistemologies. Using data from\nstudents completing assessment questions during one-on-one think aloud\ninterviews, we explore how assessment features impact (or did not impact)\nstudent engagement with the assessment problems. We analyze video recordings\nand transcripts to infer the epistemological framings and resources students\nuse while completing introductory-level physics problems. Students' framings\ntended to be fairly stable, but when shifts did occur - they were triggered by\na shift in resource, which can be activated by assessment feature. This work\nextends existing work on epistemological framing into the realm of assessment\nand allows us to consider the effects of assessments on our students'\nunderstanding of physics teaching and learning.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we study the minimum mean square estimator for non-bounded\nrandom variables under sublinear operators. The existence and uniqueness of the\nminimum mean square estimator are obtained. Several properties of the minimum\nmean square estimator for non-bounded random variables are proved under some\nmild assumptions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Genome-wide association studies (GWAS) provide a means of examining the\ncommon genetic variation underlying a range of traits and disorders. In\naddition, it is hoped that GWAS may provide a means of differentiating affected\nfrom unaffected individuals. This has potential applications in the area of\nrisk prediction. Current attempts to address this problem focus on using the\npolygene risk score (PRS) to predict case-control status on the basis of GWAS\ndata. However this approach has so far had limited success for complex traits\nsuch as schizophrenia (SZ). This is essentially a classification problem.\nArtificial neural networks (ANNs) have been shown in recent years to be highly\neffective in such applications. Here we apply an ANN to the problem of\ndistinguishing SZ patients from unaffected controls. We compare the\neffectiveness of the ANN with the PRS in classifying individuals by\ncase-control status based only on genetic data from a GWAS. We use the\nschizophrenia dataset from the Psychiatric Genomics Consortium (PGC) for this\nstudy. Our analysis indicates that the ANN is more sensitive to sample size\nthan the PRS. As larger and larger sample sizes become available, we suggest\nthat ANNs are a promising alternative to the PRS for classification and risk\nprediction for complex genetic disorders.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The majority of stars are part of gravitationally bound stellar systems, such\nas binaries. Observations of protobinary systems constrain the conditions that\nlead to stellar multiplicity and subsequent orbital evolution. We report\nhigh-angular resolution observations of the circumbinary disk around [BHB2007]\n11, a young binary protostar system. The two protostars are embedded in\ncircumstellar disks that have radii of 2 to 3 astronomical units and probably\ncontain a few Jupiter masses. These systems are surrounded by a complex\nstructure of filaments connecting to the larger circumbinary disk. We also\nobserve accretion and radio jets associated with the protobinary system. The\naccretion is preferentially onto the lower-mass protostar, consistent with\ntheoretical predictions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The recently developed precession electron diffraction (PED) technique in\nscanning transmission electron microscopy (STEM) has been used to elucidate the\nlocal strain distribution and crystalline misorientation in CMOS fabricated\nstrained Ge micro disk structure grown on Si substrate. Such structures are\nconsidered to be a compact optical source for the future photonics due to the\nspecific undercut for direct bandgap behaviour under strain. In this study, the\nstrain maps are interpreted and compared with a finite element model (FEM) of\nthe strain in the investigated structure. Results demonstrate that the SiN used\nas a stressor on top of the Ge disk induces an in-plane strain $\\epsilon_{xx}$\nof a maximum value of almost 2 % which is also confirmed by FEM simulations.\nThis tensile strain can reduce the difference between the direct and indirect\nbandgaps leading to direct bandgap radiative transitions, with the potential\nfor applications in strained Ge lasers.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Many infectious diseases can lead to re-infection. We examined the\nrelationship between the prevalence of repeat infection and the basic\nreproductive number (R0). First we solved a generic, deterministic\ncompartmental model of re-infection to derive an analytic solution for the\nrelationship. We then numerically solved a disease specific model of syphilis\ntransmission that explicitly tracked re-infection. We derived a generic\nexpression that reflects a non-linear and monotonically increasing relationship\nbetween proportion re-infection and R0 and which is attenuated by entry/exit\nrates and recovery (i.e. treatment). Numerical simulations from the syphilis\nmodel aligned with the analytic relationship. Re-infection proportions could be\nused to understand how far regions are from epidemic control, and should be\nincluded as a routine indicator in infectious disease surveillance.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  As large scale deployment of smart devices in the power grid continues,\nresearch efforts need to increasingly focus on efficient communication of\ngenerated information. This paper describes a strategy for static routing and\nscheduling of messages in a multi-hop wireless Smart Grid Neighborhood Area\nNetwork (NAN) with multiple source nodes and a common set of destinations or\ngateways. The problem is formulated as a Mixed Integer Linear Program (MILP)\nand solved using commercial optimization solver CPLEX. Feasibility of the\nscheme is demonstrated using different network models, constraints, message\ninjection rates, and initial conditions. It is shown that the proposed approach\ncan be used to generate an optimal link schedule for collecting user-generated\nbids in a transactive energy market in the least possible time. It is also\nshown that the methodology is applicable to multiple destination nodes and that\ntheir location affects message delivery time.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article, we study the $Z_c(3900)$ with the QCD sum rules in details\nby including the two-particle scattering state contributions and nonlocal\neffects between the diquark and antidiquark constituents. The two-particle\nscattering state contributions cannot saturate the QCD sum rules at the hadron\nside, the contribution of the $Z_c(3900)$ plays an un-substitutable role, we\ncan saturate the QCD sum rules with or without the two-particle scattering\nstate contributions. If there exists a barrier or spatial separation between\nthe diquark and antidiquark constituents, the Feynman diagrams can be divided\ninto the factorizable and nonfactorizable diagrams. The factorizable diagrams\nconsist of two colored clusters and lead to a stable tetraquark state. The\nnonfactorizable Feynman diagrams correspond to the tunnelling effects, which\nplay a minor important role in the QCD sum rules, and are consistent with the\nsmall width of the $Z_c(3900)$. It is feasible to apply the QCD sum rules to\nstudy the tetraquark states, which begin to receive contributions at the order\n$\\mathcal{O}(\\alpha_s^0)$, not at the order $\\mathcal{O}(\\alpha_s^2)$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A new nonparametric graphical test of significance of a covariate in\nfunctional GLM is proposed. Our approach is especially interesting due to its\nfunctional graphical interpretation of the results. As such it is able to find\nnot only if the factor of interest is significant but also which functional\ndomain is responsible for the potential rejection. In the case of functional\nmulti-way main effect ANOVA or functional main effect ANCOVA models it is able\nto find which groups differ (and where they differ), in the case of functional\nfactorial ANOVA or functional factorial ANCOVA models it is able to find which\ncombination of levels (which interactions) differ (and where they differ). The\ndescribed tests are extensions of global envelope tests in the GLM models. It\napplies Freedman-Lane algorithm for the permutation of functions and as such it\napproximately achieve the desired significance level.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a numerical study of the interactions between the elongated AGN\noutflows representing an evolved, narrow-angle tail (NAT) radio galaxy and\nplanar, transverse ICM shock fronts characteristic of those induced by galaxy\ncluster mergers (incident Mach numbers 2 - 4). The simulated NAT formation was\nreported previously in \\cite{on19a}. Our simulations utilize a\nthree-dimensional, Eulerian magnetohydrodynamic code along with\nenergy-dependent Eulerian transport of passive cosmic ray electrons. Our\nanalysis of the shock/NAT interaction applies a Riemann problem-based\ntheoretical model to interpret complex shock front behavior during passage\nthrough the highly heterogeneous structures of the simulated NAT tails. In\naddition to shock compression, shock-induced vortical motions are observed\nwithin the tails that contribute to coherent turbulent dynamo processes that\ncontinue to amplify the magnetic fields in the tails well after initial shock\ncompression. We analyze synthetic radio observations spanning the NAT-shock\ninteraction period, and examine the brightness, spectral and polarization\nproperties of our shock-rejuvenated radio tails, as well as the extent to which\nthe pre-shock states of the plasma and particle populations in our tails\ninfluence post-shock observations. Finally, we evaluate our findings in the\npossible context of a physical analogy to our simulated NAT providing the\nprecursor to a cluster ``radio relic'' associated with an impacting ICM shock.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Several detections of wide-orbit planet-mass/sub-stellar companions around\nyoung solar-like stars were reported in the last decade. The origin of those\npossible planets is still unclear but accretion tracers and VLT/SPHERE\nobservations indicate that they are surrounded by circumplanetary material or\neven a circumplanetary disk. We want to investigate if the gas component of\ndisks around wide-orbit companions is detectable with current and future\n(sub)mm telescopes and what constraints such gas observations can provide on\nthe nature of the circumplanetary material and on the mass of the companion. We\napplied the radiation thermo-chemical disk code ProDiMo to model the dust and\ngas component of passive circumplanetary disks and produced realistic synthetic\nobservables. We considered different companion properties, disk parameters and\nradiative environments and compared the resulting synthetic observables to\ntelescope sensitivities and to existing dust observations. The main criterion\nfor a successful detection is the size of the circumplanetary disk. At a\ndistance of about 150 pc, a circumplanetary disk with an outer radius of about\n10 au is detectable with ALMA in about 6 hours in optically thick CO lines.\nOther aspects such as the companion's luminosity, disk inclination and\nbackground radiation fields are also relevant and should be considered to\noptimize the observing strategy for detection experiments. For most of the\nknown wide-orbit planet-mass companions, their maximum theoretical disk size of\none third of the Hill radius would be sufficient to allow detection of CO\nlines. It is therefore feasible to detect their gas disks and constrain the\nmass of the companion through the kinematic signature. Even in the case of\nnon-detections such observations will provide stringent constraints on disk\nsize and gas mass, information crucial for formation theories.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The increasing prevalence of relational data describing interactions among a\ntarget population has motivated a wide literature on statistical network\nanalysis. In many applications, interactions may involve more than two members\nof the population and this data is more appropriately represented by a\nhypergraph. In this paper, we present a model for hypergraph data which extends\nthe well established latent space approach for graphs and, by drawing a\nconnection to constructs from computational topology, we develop a model whose\nlikelihood is inexpensive to compute. A delayed-acceptance MCMC scheme is\nproposed to obtain posterior samples and we rely on Bookstein coordinates to\nremove the identifiability issues associated with the latent representation. We\ntheoretically examine the degree distribution of hypergraphs generated under\nour framework and, through simulation, we investigate the flexibility of our\nmodel and consider estimation of predictive distributions. Finally, we explore\nthe application of our model to two real-world datasets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Random weight change (RWC) algorithm is extremely component and robust for\nthe hardware implementation of neural networks. RWC and Genetic algorithm (GA)\nare well known methodologies used for optimizing and learning the neural\nnetwork (NN). Individually, each of these two algorithms has its strength and\nweakness along with separate objectives. However, recently, researchers combine\nthese two algorithms for better learning and optimization of NN. In this paper,\nwe proposed a methodology by combining the RWC and GA, namely Genetic Random\nWeight Change (GRWC), as well as demonstrate a seminal way to reduce the\ncomplexity of the neural network by removing weak weights of GRWC. In contrast\nto RWC and GA, GRWC contains an effective optimization procedure which is\nworthy at exploring a large and complex space in intellectual strategies\ninfluenced by the GA/RWC synergy. The learning behavior of the proposed\nalgorithm was tested on MNIST dataset and it was able to prove its performance.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article, the operator approach to modeling numeral systems is\nintroduced. This approach can be useful for coding information and providing\ncomputer protection. Certain examples of such numeral systems are considered.\nIn addition, the pseudo-binary representation is investigated. A description of\nfurther investigations of the author of this article is given.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The far side of the Milky Way's disk is one of the most concealed parts of\nthe known Universe due to extremely high interstellar extinction and point\nsource density toward low Galactic latitudes. Large time-domain photometric\nsurveys operating in the near-infrared hold great potential for the exploration\nof these vast uncharted areas of our Galaxy. We conducted a census of distant\nclassical and type II Cepheids along the southern Galactic mid-plane using\nnear-infrared photometry from the VISTA Variables in the V\\'ia L\\'actea survey.\nWe performed a machine-learned classification of the Cepheids based on their\ninfrared light curves using a convolutional neural network. We have discovered\n640 distant classical Cepheids with up to ~40 magnitudes of visual extinction,\nand over 500 type II Cepheids, most of them located in the inner bulge.\nIntrinsic color indices of individual Cepheids were predicted from sparse\nphotometric data using a neural network, allowing their use as accurate\nreddening tracers. They revealed a steep, spatially varying near-infrared\nextinction curve toward the inner bulge. Type II Cepheids in the Galactic bulge\nwere also employed to measure robust mean selective-to-absolute extinction\nratios. They trace a centrally concentrated spatial distribution of the old\nbulge population with a slight elongation, consistent with earlier results from\nRR Lyrae stars. Likewise, the classical Cepheids were utilized to trace the\nGalactic warp and various substructures of the Galactic disk, and to uncover\nsignificant vertical and radial age gradients of the thin disk population at\nthe far side of the Milky Way.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a new period study and light-curve solutions for the A-Type W UMa\nbinary GSC 3208 1986. Contrary to a previous claim by R.G. Samec et al. of a\nrapidly decreasing period, the system's period is increasing moderately on a\ntimescale of two million years. The light curve is variable on the time scale\nof years, which can be understood by changes in how much it overfills its Roche\nlobe.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $A$ be an $n\\times n$ real expanding matrix and $\\mathcal{D}$ be a finite\nsubset of $\\mathbb{R}^n$ with $0\\in\\mathcal{D}$. The family of maps\n$\\{f_d(x)=A^{-1}(x+d)\\}_{d\\in\\mathcal{D}}$ is called a self-affine iterated\nfunction system (self-affine IFS). The self-affine set $K=K(A,\\mathcal{D})$ is\nthe unique compact set determined by $(A, {\\mathcal D})$ satisfying the\nset-valued equation $K=\\displaystyle\\bigcup_{d\\in\\mathcal{D}}f_d(K)$. The\nnumber $s=n\\,\\ln(\\# \\mathcal{D})/\\ln(q)$ with $q=|\\det(A)|$, is the so-called\npseudo similarity dimension of $K$. As shown by He and Lau, one can associate\nwith $A$ and any number $s\\ge 0$ a natural pseudo Hausdorff measure denoted by\n$\\mathcal{H}_w^s.$ In this paper, we show that, if $s$ is chosen to be the\npseudo similarity dimension of $K$, then the condition $\\mathcal{H}_w^s(K)> 0$\nholds if and only if the IFS $\\{f_d\\}_{d\\in\\mathcal{D}}$ satisfies the open set\ncondition (OSC). This extends the well-known result for the self-similar case\nthat the OSC is equivalent to $K$ having positive Hausdorff measure\n$\\mathcal{H}^s$ for a suitable $s$. Furthermore, we relate the exact value of\npseudo Hausdorff measure $\\mathcal{H}_w^s(K)$ to a notion of upper $s$-density\nwith respect to the pseudo norm $w(x)$ associated with $A$ for the measure\n$\\mu=\\lim\\limits_{M\\to\\infty}\\sum\\limits_{d_0,\\dotsc,d_{M-1}\\in\\mathcal{D}}\\delta_{d_0\n+ Ad_1 + \\dotsb + A^{M-1}d_{M-1}}$ in the case that $\\#\\mathcal{D}\\le\\lvert\\det\nA\\rvert$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Molecular dynamics simulations have been extensively used to predict thermal\nproperties, but simulating different phases with similar precision using a\nunified force field is often difficult, due to the lack of accurate and\ntransferrable interatomistic potential fields. As a result, this issue has\nbecome a major barrier to predicting the phase change of materials and their\ntransport properties with atomistic-level modeling techniques. Recently,\nmachine learning based algorithms have emerged as promising tools to develop\naccurate potentials for molecular dynamics simulations. In this work, we\napproach the problem of predicting the thermal conductivity of silicon in\ndifferent phases by performing molecular dynamics simulations with a deep\nneural network potential. This neural network potential is trained with\nab-initio data of silicon in the crystalline, liquid and amorphous phases. The\naccuracy of our potential is first validated through reproducing the atomistic\nstructures during the phase transition, where other empirical potentials\nusually fail. The thermal conductivity of different phases is then calculated,\nshowing a good agreement with the experimental results and ab-initio\ncalculation results. Our work shows that a unified neural network-based\npotential can be a promising tool for studying phase change and thermal\ntransport of materials with high accuracy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we apply Markovian approximation of the fractional Brownian\nmotion (BM), known as the Dobric-Ojeda (DO) process, to the fractional\nstochastic volatility model where the instantaneous variance is modelled by a\nlognormal process with drift and fractional diffusion. Since the DO process is\na semi-martingale, it can be represented as an \\Ito diffusion. It turns out\nthat in this framework the process for the spot price $S_t$ is a geometric BM\nwith stochastic instantaneous volatility $\\sigma_t$, the process for $\\sigma_t$\nis also a geometric BM with stochastic speed of mean reversion and\ntime-dependent colatility of volatility, and the supplementary process\n$\\calV_t$ is the Ornstein-Uhlenbeck process with time-dependent coefficients,\nand is also a function of the Hurst exponent. We also introduce an adjusted DO\nprocess which provides a uniformly good approximation of the fractional BM for\nall Hurst exponents $H \\in [0,1]$ but requires a complex measure. Finally, the\ncharacteristic function (CF) of $\\log S_t$ in our model can be found in closed\nform by using asymptotic expansion. Therefore, pricing options and variance\nswaps (by using a forward CF) can be done via FFT, which is much easier than in\nrough volatility models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep neural networks have been successfully applied in learning the board\ngames Go, chess and shogi without prior knowledge by making use of\nreinforcement learning. Although starting from zero knowledge has been shown to\nyield impressive results, it is associated with high computationally costs\nespecially for complex games. With this paper, we present CrazyAra which is a\nneural network based engine solely trained in supervised manner for the chess\nvariant crazyhouse. Crazyhouse is a game with a higher branching factor than\nchess and there is only limited data of lower quality available compared to\nAlphaGo. Therefore, we focus on improving efficiency in multiple aspects while\nrelying on low computational resources. These improvements include\nmodifications in the neural network design and training configuration, the\nintroduction of a data normalization step and a more sample efficient\nMonte-Carlo tree search which has a lower chance to blunder. After training on\n569,537 human games for 1.5 days we achieve a move prediction accuracy of\n60.4%. During development, versions of CrazyAra played professional human\nplayers. Most notably, CrazyAra achieved a four to one win over 2017 crazyhouse\nworld champion Justin Tan (aka LM Jann Lee) who is more than 400 Elo higher\nrated compared to the average player in our training set. Furthermore, we test\nthe playing strength of CrazyAra on CPU against all participants of the second\nCrazyhouse Computer Championships 2017, winning against twelve of the thirteen\nparticipants. Finally, for CrazyAraFish we continue training our model on\ngenerated engine games. In ten long-time control matches playing Stockfish 10,\nCrazyAraFish wins three games and draws one out of ten matches.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a public, open-source relational database (we name kaepora)\ncontaining a sample of 4975 spectra of 777 Type Ia supernovae (SNe Ia). Since\nwe draw from many sources, we significantly improve the spectra by inspecting\nthese data for quality, removing galactic emission lines and cosmic rays,\ngenerating variance spectra, and correcting for the reddening caused by both MW\nand host-galaxy dust. With our database, we organize this homogenized dataset\nby 56 unique categories of SN-specific and spectrum-specific metadata. With\nkaepora, we produce composite spectra of subpopulations of SNe Ia and examine\nhow spectral features correlate with various SN properties. These composite\nspectra reproduce known correlations with phase, light-curve shape, and\nhost-galaxy morphology. With our large dataset, we are also able to generate\nfine-grained composite spectra simultaneously over both phase and light-curve\nshape. The color evolution of our composite spectra is consistent with other SN\nIa template spectra, and the spectral properties of our composite spectra are\nin rough agreement with these template spectra with some subtle differences. We\ninvestigate the spectral differences of SNe Ia that occur in galaxies with\nvarying morphologies. Controlling for light-curve shape, which is highly\ncorrelated with host-galaxy morphology, we find that SNe Ia residing in\nlate-type and early-type galaxies have similar spectral properties at multiple\nepochs. However for SNe Ia in these different environments, their spectra\nappear to have Ca II near-infrared triplet features that have slightly\ndifferent strengths. Although this is apparent in the composite spectra and\nthere is some difference in the populations as seen by individual spectra, this\ndifference is not large enough to indicate differences in the underlying\npopulations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Digitization techniques for biomedical images yield different visual patterns\nin radiological exams. These differences may hamper the use of data-driven\napproaches for inference over these images, such as Deep Neural Networks.\nAnother noticeable difficulty in this field is the lack of labeled data, even\nthough in many cases there is an abundance of unlabeled data available.\nTherefore an important step in improving the generalization capabilities of\nthese methods is to perform Unsupervised and Semi-Supervised Domain Adaptation\nbetween different datasets of biomedical images. In order to tackle this\nproblem, in this work we propose an Unsupervised and Semi-Supervised Domain\nAdaptation method for segmentation of biomedical images using Generative\nAdversarial Networks for Unsupervised Image Translation. We merge these\nunsupervised networks with supervised deep semantic segmentation architectures\nin order to create a semi-supervised method capable of learning from both\nunlabeled and labeled data, whenever labeling is available. We compare our\nmethod using several domains, datasets, segmentation tasks and traditional\nbaselines, such as unsupervised distance-based methods and reusing pretrained\nmodels both with and without Fine-tuning. We perform both quantitative and\nqualitative analysis of the proposed method and baselines in the distinct\nscenarios considered in our experimental evaluation. The proposed method shows\nconsistently better results than the baselines in scarce labeled data\nscenarios, achieving Jaccard values greater than 0.9 and good segmentation\nquality in most tasks. Unsupervised Domain Adaptation results were observed to\nbe close to the Fully Supervised Domain Adaptation used in the traditional\nprocedure of Fine-tuning pretrained networks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing\nKernel Hilbert Spaces provide an elegant way to extend scalar kernel methods\nwhen the output space is a Hilbert space. Although primarily used in finite\ndimension for problems like multi-task regression, the ability of this\nframework to deal with infinite dimensional output spaces unlocks many more\napplications, such as functional regression, structured output prediction, and\nstructured data representation. However, these sophisticated schemes crucially\nrely on the kernel trick in the output space, so that most of previous works\nhave focused on the square norm loss function, completely neglecting robustness\nissues that may arise in such surrogate problems. To overcome this limitation,\nthis paper develops a duality approach that allows to solve OVK machines for a\nwide range of loss functions. The infinite dimensional Lagrange multipliers are\nhandled through a Double Representer Theorem, and algorithms for\n$\\epsilon$-insensitive losses and the Huber loss are thoroughly detailed.\nRobustness benefits are emphasized by a theoretical stability analysis, as well\nas empirical improvements on structured data applications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Lounesto classification splits spinors in six classes: I, II, III are\nthose for which at least one among scalar and pseudo-scalar bi-linear spinor\nquantities is non-zero, its spinors are called regular, and among them we find\nthe usual Dirac spinor. IV, V, VI are those for which the scalar and\npseudo-scalar bi-linear spinor quantities are identically zero, its spinors are\ncalled singular, and they are split in further sub-classes: IV has no further\nrestrictions, its spinors are called flag-dipole; V is the one for which the\nspin axial-vector vanishes, its spinors are called flagpole, and among them we\nfind the Majorana spinor; VI is the one for which the momentum\nantisymmetric-tensor vanishes, its spinors are called dipole, and among them we\nfind the Weyl spinor. In the quest for exact solutions of fully-coupled systems\nof spinor fields in their own gravity, we have already given examples in the\ncase of Dirac fields and Weyl fields but never in the case of Majorana or more\ngenerally flagpole spinor fields. Flagpole spinor fields in interaction with\ntheir own gravitational field, in the case of axial symmetry, will be\nconsidered. Exact solutions of the field equations will be given.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  European Law now requires AI to be explainable in the context of adverse\ndecisions affecting European Union (EU) citizens. At the same time, it is\nexpected that there will be increasing instances of AI failure as it operates\non imperfect data. This paper puts forward a neurally-inspired framework called\ndecision stacks that can provide for a way forward in research aimed at\ndeveloping explainable AI. Leveraging findings from memory systems in\nbiological brains, the decision stack framework operationalizes the definition\nof explainability and then proposes a test that can potentially reveal how a\ngiven AI decision came to its conclusion.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Blind deconvolution is the problem of recovering a convolutional kernel\n$\\boldsymbol a_0$ and an activation signal $\\boldsymbol x_0$ from their\nconvolution $\\boldsymbol y = \\boldsymbol a_0 \\circledast \\boldsymbol x_0$. This\nproblem is ill-posed without further constraints or priors. This paper studies\nthe situation where the nonzero entries in the activation signal are sparsely\nand randomly populated. We normalize the convolution kernel to have unit\nFrobenius norm and cast the sparse blind deconvolution problem as a nonconvex\noptimization problem over the sphere. With this spherical constraint, every\nspurious local minimum turns out to be close to some signed shift truncation of\nthe ground truth, under certain hypotheses. This benign property motivates an\neffective two stage algorithm that recovers the ground truth from the partial\ninformation offered by a suboptimal local minimum. This geometry-inspired\nalgorithm recovers the ground truth for certain microscopy problems, also\nexhibits promising performance in the more challenging image deblurring\nproblem. Our insights into the global geometry and the two stage algorithm\nextend to the convolutional dictionary learning problem, where a superposition\nof multiple convolution signals is observed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Many popular graph metrics encode average properties of individual network\nelements. Complementing these conventional graph metrics, the eigenvalue\nspectrum of the normalized Laplacian describes a network's structure directly\nat a systems level, without referring to individual nodes or connections. In\nthis paper, we study the spectrum and their applications of normalized\nLaplacian matrices of hype-cubes, a special kind of Cayley graphs. We determine\nexplicitly all the eigenvalues and their corresponding multiplicities by a\nrecursive method. By using the relation between normalized Laplacian spectrum\nand eigentime identity, we derive the explicit formula to the eigentime\nidentity for random walks on the hype-cubes and show that it grows linearly\nwith the network size. Moreover, we compute the number of spanning trees of the\nhype-cubes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Distribution Service (DDS) is a realtime peer-to-peer protocol that serves as\na scalable middleware between distributed networked systems found in many\nIndustrial IoT domains such as automotive, medical, energy, and defense. Since\nthe initial ratification of the standard, specifications have introduced a\nSecurity Model and Service Plugin Interface (SPI) architecture, facilitating\nauthenticated encryption and data centric access control while preserving\ninteroperable data exchange. However, as Secure DDS v1.1, the default plugin\nspecifications presently exchanges digitally signed capability lists of both\nparticipants in the clear during the crypto handshake for permission\nattestation; thus breaching confidentiality of the context of the connection.\nIn this work, we present an attacker model that makes use of network\nreconnaissance afforded by this leaked context in conjunction with formal\nverification and model checking to arbitrarily reason about the underlying\ntopology and reachability of information flow, enabling targeted attacks such\nas selective denial of service, adversarial partitioning of the data bus, or\nvulnerability excavation of vendor implementations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Most companies utilize demographic information to develop their strategy in a\nmarket. However, such information is not available to most retail companies.\nSeveral studies have been conducted to predict the demographic attributes of\nusers from their transaction histories, but they have some limitations. First,\nthey focused on parameter sharing to predict all attributes but capturing\ntask-specific features is also important in multi-task learning. Second, they\nassumed that all transactions are equally important in predicting demographic\nattributes. However, some transactions are more useful than others for\npredicting a certain attribute. Furthermore, decision making process of models\ncannot be interpreted as they work in a black-box manner. To address the\nlimitations, we propose an Embedding Transformation Network with Attention\n(ETNA) model which shares representations at the bottom of the model structure\nand transforms them to task-specific representations using a simple linear\ntransformation method. In addition, we can obtain more informative transactions\nfor predicting certain attributes using the attention mechanism. The\nexperimental results show that our model outperforms the previous models on all\ntasks. In our qualitative analysis, we show the visualization of attention\nweights, which provides business managers with some useful insights.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In 2018, M. Chu and S. Tillmann gave a lower bound for the trisection genus\nof a closed 4-manifold in terms of the Euler characteristic of $M$ and the rank\nof its fundamental group. We show that given a group $G$, there exist a\n4-manifold $M$ with fundamental group $G$ with trisection genus achieving\nChu-Tillmann's lower bound.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The equations of electrodynamics are altered in the presence of a classical\ncoherent axion dark matter background field, changing the dispersion relation\nfor electromagnetic waves. Careful measurements of the frequency stability in\nsensitive atomic clocks could in principle provide evidence for such a\nbackground for $f_a \\ge 10^7$ GeV. Turning on a background magnetic field might\nenhance these effects in a controllable way, and interferometric measurements\nmight also be useful for probing the time-varying photon dispersion relation\nthat results from a coherent cosmic axion background.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Flow based models such as Real NVP are an extremely powerful approach to\ndensity estimation. However, existing flow based models are restricted to\ntransforming continuous densities over a continuous input space into similarly\ncontinuous distributions over continuous latent variables. This makes them\npoorly suited for modeling and representing discrete structures in data\ndistributions, for example class membership or discrete symmetries. To address\nthis difficulty, we present a normalizing flow architecture which relies on\ndomain partitioning using locally invertible functions, and possesses both real\nand discrete valued latent variables. This Real and Discrete (RAD) approach\nretains the desirable normalizing flow properties of exact sampling, exact\ninference, and analytically computable probabilities, while at the same time\nallowing simultaneous modeling of both continuous and discrete structure in a\ndata distribution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A significant number of Be stars show a second Balmer discontinuity (sBD)\nattributed to an extended circumstellar envelope (CE). The fast rotational\nvelocity of Be stars undoubtedly plays a significant role in the formation of\nthe CE. However, Bn stars, which are also B-type rapidly rotating stars, do not\nall present clear evidence of being surrounded by circumstellar material. We\naim to characterize the populations of Be and Bn stars, and discuss the\nappearance of the sBD as a function of the stellar parameters. We expect to\nfind new indices characterizing the properties of CEs in Be stars and\nproperties relating Be and Bn stars. Correlations of the aspect and intensity\nof the sBD and the emission in the H$\\alpha$ line with the stellar parameters\nand the $V\\!\\sin i$ are presented. Some Bn stars exhibit the sBD in absorption,\nwhich may indicate the presence of rather dense CEs. Six Bn stars show emission\nin the H$\\alpha$ line, so they are reclassified as Be stars. The sBD in\nemission appears in Be stars with $V\\!\\sin i \\lesssim 250$ km\\,s$^{-1}$, and in\nabsorption in both Be and Bn stars with \\mbox{$V\\!\\sin i \\gtrsim 50$\nkm\\,s$^{-1}$}. Low-mass Be and Bn stars share the same region in the\nHertzsprung-Russell diagram. The distributions of rotational to critical\nvelocity ratios of Be and Bn stars corresponding to the current stellar\nevolutionary stage are similar, while distributions inferred for the zero-age\nmain sequence have different skewness. We found emission in the H$\\alpha$ line\nand signs of a CE in some Bn stars, which motivated us to think that Bn and Be\nstars probably belong to the same population. It should be noted that some of\nthe most massive Bn stars could display the Be phenomenon at any time. The\nsimilarities found among Be and Bn stars deserve to be more deeply pursued.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A model has two main aims: predicting the behavior of a physical system and\nunderstanding its nature, that is how it works, at some desired level of\nabstraction. A promising recent approach to model building consists in deriving\na Langevin-type stochastic equation from a time series of empirical data. Even\nif the protocol is based upon the introduction of drift and diffusion terms in\nstochastic differential equations, its implementation involves subtle\nconceptual problems and, most importantly, requires some prior theoretical\nknowledge about the system. Here we apply this approach to the data obtained in\na rotational granular diffusion experiment, showing the power of this method\nand the theoretical issues behind its limits. A crucial point emerged in the\ndense liquid regime, where the data reveal a complex multiscale scenario with\nat least one fast and one slow variable. Identifying the latter is a major\nproblem within the Langevin derivation procedure and led us to introduce\ninnovative ideas for its solution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper offers a novel mathematical approach, the modified\nFractional-order Steepest Descent Method (FSDM) for training BackPropagation\nNeural Networks (BPNNs); this differs from the majority of the previous\napproaches and as such. A promising mathematical method, fractional calculus,\nhas the potential to assume a prominent role in the applications of neural\nnetworks and cybernetics because of its inherent strengths such as long-term\nmemory, nonlocality, and weak singularity. Therefore, to improve the\noptimization performance of classic first-order BPNNs, in this paper we study\nwhether it could be possible to modified FSDM and generalize classic\nfirst-order BPNNs to modified FSDM based Fractional-order Backpropagation\nNeural Networks (FBPNNs). Motivated by this inspiration, this paper proposes a\nstate-of-the-art application of fractional calculus to implement a modified\nFSDM based FBPNN whose reverse incremental search is in the negative directions\nof the approximate fractional-order partial derivatives of the square error. At\nfirst, the theoretical concept of a modified FSDM based FBPNN is described\nmathematically. Then, the mathematical proof of the fractional-order global\noptimal convergence, an assumption of the structure, and the fractional-order\nmulti-scale global optimization of a modified FSDM based FBPNN are analysed in\ndetail. Finally, we perform comparative experiments and compare a modified FSDM\nbased FBPNN with a classic first-order BPNN, i.e., an example function\napproximation, fractional-order multi-scale global optimization, and two\ncomparative performances with real data. The more efficient optimal searching\ncapability of the fractional-order multi-scale global optimization of a\nmodified FSDM based FBPNN to determine the global optimal solution is the major\nadvantage being superior to a classic first-order BPNN.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg .\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The building blocks of Hudson-Parthasarathy quantum stochastic calculus start\nwith Weyl operators on a symmetric Fock space. To realize a relativistically\ncovariant version of the calculus we construct representations of Poincare\ngroup in terms of Weyl operators on suitably constructed, Bosonic or Fermionic\nbased on the mass and spin of the fundamental particle, Fock spaces. We proceed\nby describing the orbits of homogeneous Lorentz group on R4 and build fiber\nbundle representations of Poincar\\'e group induced from the stabilizer\nsubgroups (little groups) and build the Boson Fock space of the Hilbert space\nformed from the sections of the bundle. Our Weyl operators are constructed on\nsymmetric Fock space of this space and the corresponding annihilation,\ncreation, and conservation operators are synthesized in the usual fashion in\nrelativistic theories for space-like, time-like, and light-like fields. We\nachieve this by constructing transitive systems of imprimitivity\n(second-quantized SI), which are dynamical systems with trajectories dense in\nthe configuration space, by induced representations. We provide the details of\nthe field operators for the case of massive Bosons as the rest are similar in\nconstruction and indicate the ways to construct adapted processes paving way\nfor building covariant quantum stochastic calculus.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The next Moscow City Duma elections will be held on September 8th with an\noption of Internet voting. Some source code of the voting system is posted\nonline for public testing. Pierrick Gaudry recently showed that due to the\nrelatively small length of the key, the encryption scheme could be easily\nbroken. This issue has been fixed in the current version of the voting system.\nIn this note we show that the new implementation of the ElGamal encryption\nsystem is not semantically secure. We also demonstrate how this newly found\nsecurity vulnerability can be potentially used for counting the number of votes\ncast for a candidate.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent advances in image super-resolution (SR) explored the power of deep\nlearning to achieve a better reconstruction performance. However, the feedback\nmechanism, which commonly exists in human visual system, has not been fully\nexploited in existing deep learning based image SR methods. In this paper, we\npropose an image super-resolution feedback network (SRFBN) to refine low-level\nrepresentations with high-level information. Specifically, we use hidden states\nin an RNN with constraints to achieve such feedback manner. A feedback block is\ndesigned to handle the feedback connections and to generate powerful high-level\nrepresentations. The proposed SRFBN comes with a strong early reconstruction\nability and can create the final high-resolution image step by step. In\naddition, we introduce a curriculum learning strategy to make the network well\nsuitable for more complicated tasks, where the low-resolution images are\ncorrupted by multiple types of degradation. Extensive experimental results\ndemonstrate the superiority of the proposed SRFBN in comparison with the\nstate-of-the-art methods. Code is avaliable at\nhttps://github.com/Paper99/SRFBN_CVPR19.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Parity violation in gravity, if existed, could have important implications,\nand it is meaningful to search and test the possible observational effects.\nChern-Simons modified gravity serves as a natural model for gravitational\nparity-violations. Especially, considering extensions to Einstein-Hilbert\naction up to second order curvature terms, it is known that theories of\ngravitational parity-violation will reduce to the dynamical Chern-Simons\ngravity. In this letter, we outline the theoretical principles of testing the\ndynamical Chern-Simons gravity with orbiting gravity gradiometers, which could\nbe naturally incorporated into future satellite gravity missions. The secular\ngravity gradient signals, due to the Mashhoon-Theiss (anomaly) effect, in\ndynamical Chern-Simons gravity are worked out, which can improve the constraint\nof the corresponding Chern-Simons length scale $\\xi^{\\frac{1}{4}}_{cs}$\nobtained from such measurement scheme. For orbiting superconducting\ngradiometers or gradiometers with optical readout, a bound\n$\\xi^{\\frac{1}{4}}_{cs}\\leq 10^6 \\ km$ (or even better) could in principle be\nobtained, which will be at least 2 orders of magnitude stronger than the\ncurrent one based on the observations from the GP-B mission and the LAGEOS I,\nII satellites.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A grid of 20 millions 3-1100$\\mu$m SED models is presented for synthetic\nyoung clusters embedded in dense clumps. The models depend on four primary\nparameters: the clump mass M$_{clump}$ and dust temperature T$_{dust}$, the\nfraction of mass f$_{core}$ locked in dense cores, and the age of the clump\nt$_{SF}$. We populate the YSO clusters using the IMF from Kroupa(2001) and the\nYSOs SED models grid of Robitaille et al. (2006). We conduct extensive testing\nof SED fitting using a simulated dataset and we find that M$_{clump}$\nessentially depends on the submillimeter portion of the SED, while T$_{dust}$\nis mostly determined from the shape of the SED in the 70-350$\\mu$m range.\nThanks to the large number of models computed we verify that the combined\nanalysis of L/M, [8-24] and [24-70] colours removes much of the SEDs\nf$_{core}$-t$_{SF}$ degeneracy. The L/M values are particularly useful to\ndiagnose f$_{core}$. L/M$\\leq$1 identifies protoclusters with f$_{core}\\leq$0.1\nand t$_{SF} \\leq 10^5$ years, while L/M$\\geq$10 excludes f$_{core}\\leq$0.1. We\ncharacterize lower limits of L/M where ZAMS stars are not found in models, and\nwe also find models with L/M $\\geq$10 and no ZAMS stars, in which\n[8-24]$\\geq0.8\\pm 0.1$ independently from M$_{clump}$, temperature and\nluminosity. This is the first set of synthesis SED models suited to model for\nembedded and unresolved clusters of YSOs. A set of new evolutionary tracks in\nthe L/M diagram is also presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Multimodal learning has been lacking principled ways of combining information\nfrom different modalities and learning a low-dimensional manifold of meaningful\nrepresentations. We study multimodal learning and sensor fusion from a latent\nvariable perspective. We first present a regularized recurrent attention filter\nfor sensor fusion. This algorithm can dynamically combine information from\ndifferent types of sensors in a sequential decision making task. Each sensor is\nbonded with a modular neural network to maximize utility of its own\ninformation. A gating modular neural network dynamically generates a set of\nmixing weights for outputs from sensor networks by balancing utility of all\nsensors' information. We design a co-learning mechanism to encourage\nco-adaption and independent learning of each sensor at the same time, and\npropose a regularization based co-learning method. In the second part, we focus\non recovering the manifold of latent representation. We propose a co-learning\napproach using probabilistic graphical model which imposes a structural prior\non the generative model: multimodal variational RNN (MVRNN) model, and derive a\nvariational lower bound for its objective functions. In the third part, we\nextend the siamese structure to sensor fusion for robust acoustic event\ndetection. We perform experiments to investigate the latent representations\nthat are extracted; works will be done in the following months. Our experiments\nshow that the recurrent attention filter can dynamically combine different\nsensor inputs according to the information carried in the inputs. We consider\nMVRNN can identify latent representations that are useful for many downstream\ntasks such as speech synthesis, activity recognition, and control and planning.\nBoth algorithms are general frameworks which can be applied to other tasks\nwhere different types of sensors are jointly used for decision making.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The transitivity degree of a group $G$ is the supremum of all integers $k$\nsuch that $G$ admits a faithful $k$-transitive action. Few obstructions are\nknown to impose an upper bound on the transitivity degree for infinite groups.\nThe results of this article provide two new classes of groups whose\ntransitivity degree can be computed, as a corollary of a classification of all\n$3$-transitive actions of these groups. More precisely, suppose that $G$ is a\nsubgroup of the homeomorphism group of the circle\n$\\mathsf{Homeo}(\\mathbb{S}^1)$ or the automorphism group of a tree\n$\\mathsf{Aut}(\\mathbb{T})$. Under natural assumptions on the stabilizers of the\naction of $G$ on $\\mathbb{S}^1$ or $\\partial \\mathbb{T}$, we use the dynamics\nof this action to show that every faithful action of $G$ on a set that is at\nleast $3$-transitive must be conjugate to the action of $G$ on one of its\norbits in $\\mathbb{S}^1$ or $\\partial \\mathbb{T}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In large scale systems, approximate nearest neighbour search is a crucial\nalgorithm to enable efficient data retrievals. Recently, deep learning-based\nhashing algorithms have been proposed as a promising paradigm to enable data\ndependent schemes. Often their efficacy is only demonstrated on data sets with\nfixed, limited numbers of classes. In practical scenarios, those labels are not\nalways available or one requires a method that can handle a higher input\nvariability, as well as a higher granularity. To fulfil those requirements, we\nlook at more flexible similarity measures. In this work, we present a novel,\nflexible, end-to-end trainable network for large-scale data hashing. Our method\nworks by transforming the data distribution to behave as a uniform distribution\non a product of spheres. The transformed data is subsequently hashed to a\nbinary form in a way that maximises entropy of the output, (i.e. to fully\nutilise the available bit-rate capacity) while maintaining the correctness\n(i.e. close items hash to the same key in the map). We show that the method\noutperforms baseline approaches such as locality-sensitive hashing and product\nquantisation in the limited capacity regime.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the robust adaptive nonparametric estimation problem for a\nperiodic function observed in the framework of a continuous time regression\nmodel with semimartingale noises.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the evolution of the abundance of the asymmetric thermal Dark\nMatter when its annihilation rate at chemical decoupling is boosted by the\nSommerfeld enhancement. We perform the detailed analysis of the effect of $s$--\nand $p$--wave Sommerfeld enhanced annihilation cross section on the relic\nabundances of asymmetric Dark Matter particle and anti--particle both in\nnumerical and analytical way. The constraints on the coupling and asymmetric\nfactor are given by using the observational data of the relic density of Dark\nMatter.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Object detection is a trendy branch of computer vision, especially on human\nrecognition and pedestrian detection. Recognizing the complete body of a person\nhas always been a difficult problem. Over the years, researchers proposed\nvarious methods, and recently, Mask R-CNN has made a breakthrough for instance\nsegmentation. Based on Faster R-CNN, Mask R-CNN has been able to generate a\nsegmentation mask for each instance. We propose an application to extracts\nmultiple persons from images and videos for pleasant life scenes to grouping\nhappy moments of people such as family or friends and a community for QOL\n(Quality Of Life). We likewise propose a methodology to put extracted images of\npersons into the new background. This enables a user to make a pleasant\ncollection of happy facial expressions and actions of his/her family and\nfriends in his/her life. Mask R-CNN detects all types of object masks from\nimages. Then our algorithm considers only the target person and extracts a\nperson only without obstacles, such as dogs in front of the person, and the\nuser also can select multiple persons as their expectations. Our algorithm is\neffective for both an image and a video irrespective of the length of it. Our\nalgorithm does not add any overhead to Mask R-CNN, running at 5 fps. We show\nexamples of yoga-person in an image and a dancer in a dance-video frame. We\nhope our simple and effective approach would serve as a baseline for replacing\nthe image background and help ease future research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The component-wise or Schur product $C*C'$ of two linear error correcting\ncodes $C$ and $C'$ over certain finite field is the linear code spanned by all\ncomponent-wise products of a codeword in $C$ with a codeword in $C'$. When\n$C=C'$, we call the product the square of $C$ and denote it $C^{*2}$. Motivated\nby several applications of squares of linear codes in the area of cryptography,\nin this paper we study squares of so-called matrix-product codes, a general\nconstruction that allows to obtain new longer codes from several \"constituent\"\ncodes. We show that in many cases we can relate the square of a matrix-product\ncode to the squares and products of their constituent codes, which allow us to\ngive bounds or even determine its minimum distance. We consider the well-known\n$(u,u+v)$-construction, or Plotkin sum (which is a special case of a\nmatrix-product code) and determine which parameters we can obtain when the\nconstituent codes are certain cyclic codes. In addition, we use the same\ntechniques to study the squares of other matrix-product codes, for example when\nthe defining matrix is Vandermonde (where the minimum distance is in a certain\nmaximal with respect to matrix-product codes).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Very-high-energy (VHE) BL Lac spectra extending above $10 \\, \\rm TeV$ provide\na unique opportunity for testing physics beyond the standard model of\nelementary particle and alternative blazar emission models. We consider the\nhadron beam, the photon to axion-like particle (ALP) conversion, and the\nLorentz invariance violation (LIV) scenarios by analyzing their consequences\nand induced modifications to BL Lac spectra. In particular, we consider how\ndifferent processes can provide similar spectral features (e.g. hard tails) and\nwe discuss the ways they can be disentangled. We use HEGRA data of a high state\nof Markarian 501 and the HESS spectrum of the extreme BL Lac (EHBL) 1ES\n0229+200. In addition, we consider two hypothetical EHBLs similar to 1ES\n0229+200 located at redshifts $z=0.3$ and $z=0.5$. We observe that both the\nhadron beam and the photon-ALP oscillations predict a hard tail extending to\nenergies larger than those possible in the standard scenario. Photon-ALP\ninteraction predicts a peak in the spectra of distant BL Lacs at about $20-30\n\\, \\rm TeV$, while LIV produces a strong peak in all BL Lac spectra around\n$\\sim 100 \\, \\rm TeV$. The peculiar feature of the photon-ALP conversion model\nis the production of oscillations in the spectral energy distribution, so that\nits detection/absence can be exploited to distinguish among the considered\nmodels. The above mentioned features coming from the three models may be\ndetected by the upcoming Cherenkov Telescope Array (CTA). Thus, future\nobservations of BL Lac spectra could eventually shed light about new physics\nand alternative blazar emission models, driving fundamental research towards a\nspecific direction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Aesthetic image captioning (AIC) refers to the multi-modal task of generating\ncritical textual feedbacks for photographs. While in natural image captioning\n(NIC), deep models are trained in an end-to-end manner using large curated\ndatasets such as MS-COCO, no such large-scale, clean dataset exists for AIC.\nTowards this goal, we propose an automatic cleaning strategy to create a\nbenchmarking AIC dataset, by exploiting the images and noisy comments easily\navailable from photography websites. We propose a probabilistic\ncaption-filtering method for cleaning the noisy web-data, and compile a\nlarge-scale, clean dataset \"AVA-Captions\", (230, 000 images with 5 captions per\nimage). Additionally, by exploiting the latent associations between aesthetic\nattributes, we propose a strategy for training the convolutional neural network\n(CNN) based visual feature extractor, the first component of the AIC framework.\nThe strategy is weakly supervised and can be effectively used to learn rich\naesthetic representations, without requiring expensive ground-truth\nannotations. We finally show-case a thorough analysis of the proposed\ncontributions using automatic metrics and subjective evaluations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent demonstrations of electrical detection and manipulation of\nantiferromagnets (AFMs) have opened new opportunities towards robust and\nultrafast spintronics devices. However, it is difficult to establish the\nconnection between the spin-transport behavior and the microscopic AFM domain\nstates due to the lack of the real-time AFM domain imaging technique under the\nelectric field. Here we report a significant Voigt rotation up to 60 mdeg in\nthin NiO(001) films at room temperature. Such large Voigt rotation allows us to\ndirectly observe AFM domains in thin-film NiO by utilizing a wide-field optical\nmicroscope. Further complementary XMLD-PEEM measurement confirms that the Voigt\ncontrast originates from the NiO AFM order. We examine the domain pattern\nevolution at a wide range of temperature and with the application of external\nmagnetic field. Comparing to large-scale-facility techniques such as the X-ray\nphotoemission electron microscopy, the use with a wide-field, tabletop optical\nimaging method enables straightforward access to domain configurations of\nsingle-layer AFMs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Owing to its wide (3.4 eV) and direct-tunable band gap, gallium nitride (GaN)\nis an excellent material platform for UV photodetectors. GaN is also stable in\nradiation-rich and high-temperature environments, which makes photodetectors\nfabricated using this material useful for in-situ flame detection and\ncombustion monitoring. In this paper, we use a GaN photodetector to measure\nultraviolet (UV) emissions from a hybrid rocket motor igniter plume. The\nnormalized photocurrent-to-dark current ratio (NPDR) is a performance metric\nwhich simultaneously captures the two desired characteristics of high\nresponsivity and low dark current. The NPDR of our device is record-high with a\nvalue of 6 x 10$^{14}$ W$^{-1}$ and the UV-to-visible rejection ratio is 4 x\n10$^6$. The photodetector shows operation at high temperatures (up to\n250{\\deg}C), with the NPDR still remaining above 10$^9$ W$^{-1}$ and the peak\nwavelength shifting from 362 nm to 375 nm. The photodetector was placed at\nthree radial distances (3\", 5.5\", and 7\") from the base of the igniter plume\nand the oxidizer-to-fuel ratio (O2/CH4) was varied. The data demonstrates a\nclear trend of increasing current (and thus intensity of plume emission) with\nincreasing fuel concentration and decreasing separation between the\nphotodetector and the plume. By treating the plume as a black body, and\ncalculating a radiative configuration factor corresponding to the geometry of\nthe plume and the detector, we calculated average plume temperatures at each of\nthe three oxidizer-to-fuel ratios. The estimated plume temperatures were\nbetween 850 and 950 K for all three combustion conditions. The temperature is\nroughly invariant for a fixed fuel concentration for the three tested\ndistances. These data demonstrate the functionality of GaN as a material\nplatform for use in harsh environment flame monitoring.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Removing rain effects from an image is of importance for various applications\nsuch as autonomous driving, drone piloting, and photo editing. Conventional\nmethods rely on some heuristics to handcraft various priors to remove or\nseparate the rain effects from an image. Recent deep learning models are\nproposed to learn end-to-end methods to complete this task. However, they often\nfail to obtain satisfactory results in many realistic scenarios, especially\nwhen the observed images suffer from heavy rain. Heavy rain brings not only\nrain streaks but also haze-like effect caused by the accumulation of tiny\nraindrops. Different from the existing deep learning deraining methods that\nmainly focus on handling the rain streaks, we design a deep neural network by\nincorporating a physical raining image model. Specifically, in the proposed\nmodel, two branches are designed to handle both the rain streaks and haze-like\neffects. An additional submodule is jointly trained to finally refine the\nresults, which give the model flexibility to control the strength of removing\nthe mist. Extensive experiments on several datasets show that our method\noutperforms the state-of-the-art in both objective assessments and visual\nquality.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Humor is an essential human trait. Efforts to understand humor have called\nout links between humor and the foundations of cognition, as well as the\nimportance of humor in social engagement. As such, it is a promising and\nimportant subject of study, with relevance for artificial intelligence and\nhuman-computer interaction. Previous computational work on humor has mostly\noperated at a coarse level of granularity, e.g., predicting whether an entire\nsentence, paragraph, document, etc., is humorous. As a step toward deep\nunderstanding of humor, we seek fine-grained models of attributes that make a\ngiven text humorous. Starting from the observation that satirical news\nheadlines tend to resemble serious news headlines, we build and analyze a\ncorpus of satirical headlines paired with nearly identical but serious\nheadlines. The corpus is constructed via Unfun.me, an online game that\nincentivizes players to make minimal edits to satirical headlines with the goal\nof making other players believe the results are serious headlines. The edit\noperations used to successfully remove humor pinpoint the words and concepts\nthat play a key role in making the original, satirical headline funny. Our\nanalysis reveals that the humor tends to reside toward the end of headlines,\nand primarily in noun phrases, and that most satirical headlines follow a\ncertain logical pattern, which we term false analogy. Overall, this paper\ndeepens our understanding of the syntactic and semantic structure of satirical\nnews headlines and provides insights for building humor-producing systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Based on the symbolic computation approach, multiple rogue wave solutions of\nthe generalized (2+1)-dimensional Camassa-Holm-Kadomtsev-Petviashvili equation\nare studied. As an example, we present the 1-rogue wave solutions, 3-rogue wave\nsolutions and 6-rogue wave solutions. Furthermore, some dynamics features of\nthe obtained multiple rogue wave solutions are shown by 3D, contour and density\ngraphics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Causal reasoning has been an indispensable capability for humans and other\nintelligent animals to interact with the physical world. In this work, we\npropose to endow an artificial agent with the capability of causal reasoning\nfor completing goal-directed tasks. We develop learning-based approaches to\ninducing causal knowledge in the form of directed acyclic graphs, which can be\nused to contextualize a learned goal-conditional policy to perform tasks in\nnovel environments with latent causal structures. We leverage attention\nmechanisms in our causal induction model and goal-conditional policy, enabling\nus to incrementally generate the causal graph from the agent's visual\nobservations and to selectively use the induced graph for determining actions.\nOur experiments show that our method effectively generalizes towards completing\nnew tasks in novel environments with previously unseen causal structures.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Inversion codes for the polarized radiative transfer equation can be used to\ninfer the temperature $T$, line-of-sight velocity $v_{\\rm los}$, and magnetic\nfield $\\rm{\\bf B}$ as a function of the continuum optical-depth $\\tau_{\\rm c}$.\nHowever, they do not directly provide the gas pressure $P_{\\rm g}$ or density\n$\\rho$. In order to obtain these latter parameters, inversion codes rely\ninstead on the assumption of hydrostatic equilibrium (HE) in addition to the\nequation of state (EOS). Unfortunately, the assumption of HE is rather\nunrealistic across magnetic field lines. This is because the role of the\nLorentz force, among other factors, is neglected. This translates into an\ninaccurate conversion from optical depth $\\tau_{\\rm c}$ to geometrical height\n$z$. We aim at improving this conversion via the application of\nmagneto-hydrostatic (MHS) equilibrium instead of HE. We develop a method to\nsolve the momentum equation under MHS equilibrium (i.e., taking the Lorentz\nforce into account) in three dimensions. The method is based on the solution of\na Poisson-like equation. Considering the gas pressure $P_{\\rm g}$ and density\n$\\rho$ from three-dimensional magneto-hydrodynamic (MHD) simulations of\nsunspots as a benchmark, we compare the results from the application of HE and\nMHS equilibrium. We find that HE retrieves the gas pressure and density within\nan order of magnitude of the MHD values in only about 47 \\% of the domain. This\ntranslates into an error of about $160-200$ km in the determination of the\n$z-\\tau_{\\rm c}$ conversion. On the other hand, the application of MHS\nequilibrium allows determination of $P_{\\rm g}$ and $\\rho$ within an order of\nmagnitude in 84 \\% of the domain. In this latter case, the $z-\\tau_{\\rm c}$\nconversion is obtained with an accuracy of $30-70$ km.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The growing interest in the Internet of Things (IoT) applications is\nassociated with an augmented volume of security threats. In this vein, the\nIntrusion detection systems (IDS) have emerged as a viable solution for the\ndetection and prevention of malicious activities. Unlike the signature-based\ndetection approaches, machine learning-based solutions are a promising means\nfor detecting unknown attacks. However, the machine learning models need to be\naccurate enough to reduce the number of false alarms. More importantly, they\nneed to be trained and evaluated on realistic datasets such that their efficacy\ncan be validated on real-time deployments. Many solutions proposed in the\nliterature are reported to have high accuracy but are ineffective in real\napplications due to the non-representativity of the dataset used for training\nand evaluation of the underlying models. On the other hand, some of the\nexisting solutions overcome these challenges but yield low accuracy which\nhampers their implementation for commercial tools. These solutions are majorly\nbased on single learners and are therefore directly affected by the intrinsic\nlimitations of each learning algorithm. The novelty of this paper is to use the\nmost realistic dataset available for intrusion detection called NSL-KDD, and\ncombine multiple learners to build ensemble learners that increase the accuracy\nof the detection. Furthermore, a deployment architecture in a fog-to-things\nenvironment that employs two levels of classifications is proposed. In such\narchitecture, the first level performs an anomaly detection which reduces the\nlatency of the classification substantially, while the second level, executes\nattack classifications, enabling precise prevention measures. Finally, the\nexperimental results demonstrate the effectiveness of the proposed IDS in\ncomparison with the other state-of-the-arts on the NSL-KDD dataset.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  New images of young stars are revolutionizing our understanding of planet\nformation. ALMA detects large grains in planet-forming disks with few AU scale\nresolution and scattered light imaging with extreme adaptive optics systems\nreveal small grains suspended on the disks' flared surfaces. Tantalizing\nevidence for young exoplanets is emerging from line observations of CO and\nH-alpha. In this white paper, we explore how even higher angular resolution can\nextend our understanding of the key stages of planet formation, to resolve\naccreting circumplanetary disks themselves, and to watch planets forming in\nsitu for the nearest star-forming regions. We focus on infrared observations\nwhich are sensitive to thermal emission in the terrestrial planet formation\nzone and allow access to molecular tracers in warm ro-vibrational states.\nSuccessful planet formation theories will not only be able to explain the\ndiverse features seen in disks, but will also be consistent with the rich\nexoplanet demographics from RV and transit surveys. While we are far from\nexhausting ground-based techniques, the ultimate combination of high angular\nresolution and high infrared sensitivity can only be achieved through\nmid-infrared space interferometry.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The shift operation was recently introduced as an alternative to spatial\nconvolutions. The operation moves subsets of activations horizontally and/or\nvertically. Spatial convolutions are then replaced with shift operations\nfollowed by point-wise convolutions, significantly reducing computational\ncosts. In this work, we investigate how shifts should best be applied to high\naccuracy CNNs. We apply shifts of two different neighbourhood groups to ResNet\non ImageNet: the originally introduced 8-connected (8C) neighbourhood shift and\nthe less well studied 4-connected (4C) neighbourhood shift. We find that when\nreplacing ResNet's spatial convolutions with shifts, both shift neighbourhoods\ngive equal ImageNet accuracy, showing the sufficiency of small neighbourhoods\nfor large images. Interestingly, when incorporating shifts to all point-wise\nconvolutions in residual networks, 4-connected shifts outperform 8-connected\nshifts. Such a 4-connected shift setup gives the same accuracy as full residual\nnetworks while reducing the number of parameters and FLOPs by over 40%. We then\nhighlight that without spatial convolutions, ResNet's downsampling/upsampling\nbottleneck channel structure is no longer needed. We show a new, 4C shift-based\nresidual network, much shorter than the original ResNet yet with a higher\naccuracy for the same computational cost. This network is the highest accuracy\nshift-based network yet shown, demonstrating the potential of shifting in deep\nneural networks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Selecting the optimal Markowitz porfolio depends on estimating the covariance\nmatrix of the returns of $N$ assets from $T$ periods of historical data.\nProblematically, $N$ is typically of the same order as $T$, which makes the\nsample covariance matrix estimator perform poorly, both empirically and\ntheoretically. While various other general purpose covariance matrix estimators\nhave been introduced in the financial economics and statistics literature for\ndealing with the high dimensionality of this problem, we here propose an\nestimator that exploits the fact that assets are typically positively\ndependent. This is achieved by imposing that the joint distribution of returns\nbe multivariate totally positive of order 2 ($\\text{MTP}_2$). This constraint\non the covariance matrix not only enforces positive dependence among the\nassets, but also regularizes the covariance matrix, leading to desirable\nstatistical properties such as sparsity. Based on stock-market data spanning\nover thirty years, we show that estimating the covariance matrix under\n$\\text{MTP}_2$ outperforms previous state-of-the-art methods including\nshrinkage estimators and factor models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Photorealistic stylization aims to transfer the style of a reference photo\nonto a content photo in a natural fashion, such that the stylized image looks\nlike a real photo taken by a camera. State-of-the-art methods stylize the image\nlocally within each matched semantic region and are prone to global color\ninconsistency across semantic objects/parts, making the stylized image less\nphotorealistic. To tackle the challenging issues, we propose a non-local\nrepresentation scheme, constrained with a mutual affine-transfer network\n(NL-MAT). Through a dictionary-based decomposition, NL-MAT is able to\nsuccessfully decouple matched non-local representations and color information\nof the image pair, such that the context correspondence between the image pair\nis incorporated naturally, which largely facilitates local style transfer in a\nglobal-consistent fashion. To the best of our knowledge, this is the first\nattempt to address the photorealistic stylization problem with a non-local\nrepresentation scheme, such that no additional models or steps for semantic\nmatching are required during stylization. Experimental results demonstrate that\nthe proposed method is able to generate photorealistic results with local style\ntransfer while preserving both the spatial structure and global color\nconsistency of the content image.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Motivated by the growing popularity of variants of the Wasserstein distance\nin statistics and machine learning, we study statistical inference for the\nSliced Wasserstein distance--an easily computable variant of the Wasserstein\ndistance. Specifically, we construct confidence intervals for the Sliced\nWasserstein distance which have finite-sample validity under no assumptions or\nunder mild moment assumptions. These intervals are adaptive in length to the\nregularity of the underlying distributions. We also bound the minimax risk of\nestimating the Sliced Wasserstein distance, and as a consequence establish that\nthe lengths of our proposed confidence intervals are minimax optimal over\nappropriate distribution classes. To motivate the choice of these classes, we\nalso study minimax rates of estimating a distribution under the Sliced\nWasserstein distance. These theoretical findings are complemented with a\nsimulation study demonstrating the deficiencies of the classical bootstrap, and\nthe advantages of our proposed methods. We also show strong correspondences\nbetween our theoretical predictions and the adaptivity of our confidence\ninterval lengths in simulations. We conclude by demonstrating the use of our\nconfidence intervals in the setting of simulator-based likelihood-free\ninference. In this setting, contrasting popular approximate Bayesian\ncomputation methods, we develop uncertainty quantification methods with\nrigorous frequentist coverage guarantees.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Geodesic shooting has been successfully applied to diffeo-morphic\nregistration of point sets. Exact computation of the geodesicshooting between\npoint sets, however, requiresO(N2) calculations each time step on the number of\npoints in the point set. We proposean approximation approach based on the\nBarnes-Hut algorithm to speedup point set geodesic shooting. This approximation\ncan reduce the al-gorithm complexity toO(N b+N logN). The evaluation of the\nproposedmethod in both simulated images and the medial temporal lobe thick-ness\nanalysis demonstrates a comparable accuracy to the exact point set geodesic\nshooting while offering up to 3-fold speed up. This improvementopens up a range\nof clinical research studies and practical problems towhich the method can be\neffectively applied.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Most research on intelligent agents centers on the agent and not on the user.\nWe look at the origins of agent-centric research for slot-filling, gaming and\nchatbot agents. We then argue that it is important to concentrate more on the\nuser. After reviewing relevant literature, some approaches for creating and\nassessing user-centric systems are proposed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The rank three tensor model with tetrahedral interaction was shown by\nCarrozza and Tanasa to admit a $1/N$ expansion, dominated by melonic diagrams,\nand double tadpoles decorated with melons at next-to-leading order. This model\nhas generated a renewed interest in tensor models because it has the same large\n$N$ limit as the SYK model. In contrast with matrix models, there is no method\nwhich would be able to prove the existence of $1/N$ expansions in arbitrary\ntensor models. The method used by Carrozza and Tanasa proves the existence of\nthe $1/N$ expansion using two-dimensional topology, before identifying the\nleading order and next-to-leading graphs. However, another method was required\nfor complex, rank three tensor models with planar interactions, which is based\non flips. The latter are moves which cut two propagators of Feynman graphs and\nreglue them differently. They allow transforming graphs while tracking their\norders in the $1/N$ expansion. Here we use this method to re-prove the results\nof Carrozza and Tanasa, thereby proving the existence of the $1/N$ expansion,\nthe melonic dominance at leading order and the melon-decorated double tadpoles\nat next-to-leading order, all in one go.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present the first estimation of the mass and spin magnitude of Kerr black\nholes resulting from the coalescence of binary black holes using a deep neural\nnetwork. The network is trained on a dataset containing 80\\% of the full\npublicly available catalog of numerical simulations of gravitational waves\nemission by binary black hole systems, including full precession effects for\nspinning binaries. The network predicts the remnant black holes mass and spin\nwith an error less than 0.04\\% and 0.3\\% respectively for 90\\% of the values in\nthe non-precessing test dataset, it is 0.1\\% and 0.3\\% respectively in the\nprecessing test dataset. When compared to existing fits in the LIGO algorithm\nsoftware library, the network enables to reduce the remnant mass root mean\nsquare error to one half in the non-precessing case. In the precessing case,\nboth remnant mass and spin mean square errors are decreased to one half, and\nthe network corrects the bias observed in available fits.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The high fidelity generation of strongly entangled states of many particles,\nsuch as cat states, is a particularly demanding challenge. One approach is to\ndrive the system, within a certain final time, as adiabatically as possible, in\norder to avoid the generation of unwanted excitations. However, excitations can\nbe generated also by the presence of dissipative effects such as dephasing.\nHere we compare the effectiveness of Local Adiabatic and the FAst QUasi\nADiabatic protocols in achieving a high fidelity for a target superposition\nstate both with and without dephasing. In particular we consider trapped ions\nset-ups in which each spin interacts with all the others with the uniform\ncoupling strength or with a power-law coupling. In order to mitigate the\neffects of dephasing, we complement the adiabatic protocols with dynamical\ndecoupling and we test its effectiveness. The protocols we study could be\nreadily implemented with state-of-the-art techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We describe an ultra-wide-bandwidth, low-frequency receiver (\"UWL\") recently\ninstalled on the Parkes radio telescope. The receiver system provides\ncontinuous frequency coverage from 704 to 4032 MHz. For much of the band (~60%)\nthe system temperature is approximately 22K and the receiver system remains in\na linear regime even in the presence of strong mobile phone transmissions. We\ndiscuss the scientific and technical aspects of the new receiver including its\nastronomical objectives, as well as the feed, receiver, digitiser and\nsignal-processor design. We describe the pipeline routines that form the\narchive-ready data products and how those data files can be accessed from the\narchives. The system performance is quantified including the system noise and\nlinearity, beam shape, antenna efficiency, polarisation calibration and timing\nstability.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A graph $G$ arrows a graph $H$ if in every $2$-edge-coloring of $G$ there\nexists a monochromatic copy of $H$. Schelp had the idea that if the complete\ngraph $K_n$ arrows a small graph $H$, then every \"dense\" subgraph of $K_n$ also\narrows $H$, and he outlined some problems in this direction. Our main result is\nin this spirit. We prove that for every sufficiently large $n$, if $n = 3t+r$\nwhere $r \\in \\{0,1,2\\}$ and $G$ is an $n$-vertex graph with $\\delta(G) \\ge\n(3n-1)/4$, then for every $2$-edge-coloring of $G$, either there are cycles of\nevery length $\\{3, 4, 5, \\dots, 2t+r\\}$ of the same color, or there are cycles\nof every even length $\\{4, 6, 8, \\dots, 2t+2\\}$ of the same color.\n  Our result is tight in the sense that no longer cycles (of length $>2t+r$)\ncan be guaranteed and the minimum degree condition cannot be reduced. It also\nimplies the conjecture of Schelp that for every sufficiently large $n$, every\n$(3t-1)$-vertex graph $G$ with minimum degree larger than $3|V(G)|/4$ arrows\nthe path $P_{2n}$ with $2n$ vertices. Moreover, it implies for sufficiently\nlarge $n$ the conjecture by Benevides, {\\L}uczak, Scott, Skokan and White that\nfor $n=3t+r$ where $r \\in \\{0,1,2\\}$ and every $n$-vertex graph $G$ with\n$\\delta(G) \\ge 3n/4$, in each $2$-edge-coloring of $G$ there exists a\nmonochromatic cycle of length at least $2t+r$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A theoretical model for describing the emission spectra of microsphere\ncavities is presented, and its prediction of detailed lineshapes of emission\nspectra associated with whispering gallery modes (WGMs) of various orders in\nZnO microspheres (MSs) are verified experimentally by photoluminescence (PL)\nspectroscopy. The interplay of Purcell effect, quality factor, and leaky modes\nin spontaneous and stimulated emission spectra related to WGMs of all orders is\nrevealed. The key success of the theory is based on the expansion of the full\nGreen function of the MS in terms of all possible resonance modes in complex\nfrequency space, which allows incorporation of contributions from leaky modes,\nstimulated emission processes, and Purcell effect. We show that the spontaneous\nemission spectrum calculated according to Mie theory (without Purcell effect)\nis dominated by the contribution of leaky modes, while the spontaneous and\nstimulated emission enhanced by Purcell effect are responsible for the main WGM\nresonance peaks observed experimentally. It is found that the stimulated\nemission peaks are doubly enhanced by their respective mode quality factor Q:\none factor from the Purcell effect and the other factor from the photon number\nderived from the rate equation. After combining all these effects the theory\ncan provide a quantitative description of fine features of both TE and TM modes\n(including higher-order modes) observed in the PL spectra of ZnO MSs.\nSurprisingly, it is found that for ZnO MS with diameter larger than 5 $\\mu m$,\nthe PL emission spectrum is dominated by higher-order modes. The quantitative\nunderstanding of the interplay of these emission mechanisms should prove useful\nfor optimizing the performance of light-emitting devices based on micro\nresonators.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The present work describes the asymptotic local shape of a graph drawn\nuniformly at random from all connected simple planar graphs with n labelled\nvertices. We establish a novel uniform infinite planar graph (UIPG) as quenched\nlimit in the local topology as n tends to infinity. We also establish such\nlimits for random 2-connected planar graphs and maps as their number of edges\ntends to infinity. Our approach encompasses a new probabilistic view on the\nTutte decomposition. This allows us to follow the path along the decomposition\nof connectivity from planar maps to planar graphs in a uniformed way, basing\neach step on condensation phenomena for random walks under subexponentiality\nand Gibbs partitions. Using large deviation results, we recover the asymptotic\nformula by Gim\\'enez and Noy (2009) for the number of planar graphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the analogy between propagation of light rays in a stationary curved\nspacetime and in a toroidal (meta-)material. After introducing a novel\ngravitational analog of the index of refraction of a magneto-electric medium,\nit is argued that light rays not only feel a Lorentz-like force in a\nmagneto-electric medium due to the non-vanishing curl of the toroidal moment,\nbut also there exists an optical analog of Aharonov-Bohm effect for the rays\ntraveling in a region with a curl-free toroidal moment. Experimental\nrealization of this effect could utilize either a multiferroic material or a\ntoroidal metamaterial.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a volumetric mesh-based algorithm for flattening the placenta to a\ncanonical template to enable effective visualization of local anatomy and\nfunction. Monitoring placental function in vivo promises to support pregnancy\nassessment and to improve care outcomes. We aim to alleviate visualization and\ninterpretation challenges presented by the shape of the placenta when it is\nattached to the curved uterine wall. To do so, we flatten the volumetric mesh\nthat captures placental shape to resemble the well-studied ex vivo shape. We\nformulate our method as a map from the in vivo shape to a flattened template\nthat minimizes the symmetric Dirichlet energy to control distortion throughout\nthe volume. Local injectivity is enforced via constrained line search during\ngradient descent. We evaluate the proposed method on 28 placenta shapes\nextracted from MRI images in a clinical study of placental function. We achieve\nsub-voxel accuracy in mapping the boundary of the placenta to the template\nwhile successfully controlling distortion throughout the volume. We illustrate\nhow the resulting mapping of the placenta enhances visualization of placental\nanatomy and function. Our code is freely available at\nhttps://github.com/mabulnaga/placenta-flattening .\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In Weyl semimetals, there is an intriguing possibility of realizing a\npseudo-magnetic field in presence of small strain due to certain special cases\nof static deformations. This pseudo-magnetic field can be large enough to form\nquantized Landau levels and thus become observable in Weyl semimetals. In this\npaper, we experimentally show the emergence of a pseudo-magnetic field (~ 3\nTesla) by Scanning Tunneling Spectroscopy (STS) on the doped Weyl semimetal\nRe-MoTe2, where distinct Landau level oscillations in the tunneling conductance\nare clearly resolved. The crystal lattice is intrinsically strained where large\narea STM imaging of the surface reveals differently strained domains where\natomic scale deformations exist forming topographic ripples with varying\nperiodicity in the real space. The effect of pseudo-magnetic field is clearly\nresolved in areas under maximum strain.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  General relativity (GR) extensions based on renormalization group (RG) flows\nmay lead to scale-dependent couplings with nontrivial effects at large distance\nscales. Here we develop further the approach in which RG effects at large\ndistance scales are fully encoded in an effective action and we apply it to\ncosmology. In order to evaluate the cosmological consequences, our main\nassumption is the use of a RG scale such that the (infrared) RG effects only\nappear at perturbative order (not at the background level). The emphasis here\nis on analytical results and qualitative understanding of the implied\ncosmology. We employ commonly used parametrizations for describing modified\ngravity in cosmology (as the slip parameter). From them, we describe the\ndynamics of the first order perturbations and estimate bounds on the single\ndimensionless parameter ($\\nu$) introduced by this framework. Possible impacts\non dark matter and dark energy are discussed. It is also shown here that the\n$\\nu$ parameter effects to $f\\sigma_8$ are stronger at low redshifts ($z<1.5$),\nwhile different values for $\\nu$ do not appreciably change $f\\sigma_8$ at\nhigher redshifts, thus opening a window to alleviate an issue that is currently\nfaced by $\\Lambda$CDM.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We address the question of global in time existence of solutions to a\nmagnetoviscoelastic system with general initial data. We show that the notion\nof dissipative solutions allows to prove such an existence in two and three\ndimensions. This extends an earlier result for the viscoelastic subsystem to\nthe setting which includes the magnetization vector and its evolution in terms\nof a Landau-Lifshitz-Gilbert equation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Ionization, hydrocarbon breakdown, and other exotic processes can harm\ndiode-pumped alkali laser (DPAL) performance and components. We develop a\nphysical picture of these processes, including those that drive a\nnon-Maxwell-Boltzmann distribution of electrons, and describe an efficient\napproach to solve these kinetics while resolving trace species, and enforcing\nconservation laws. Comparing the model to time-dependent experiments suggests\nthat recombination and supporting processes are weaker than na\\\"{i}vely\nexpected under relevant conditions, while methane seems to improve performance\nin the lab more than it does in the model. Overall, this work highlights the\nimportance of tracking the true electron energy distribution, and how incisive\nexperiments with time-dependent driving are. We also use the model to emphasize\nhow ionization may pose more immediate heat loading problems in devices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Utilizing orthoses and exoskeleton technology in various applications and\nmedical industries, particularly to help elderly and ordinary people in their\ndaily activities is a new growing field for research institutes. In this paper,\nafter introducing an assistive lower limb exoskeleton (RoboWalk), the dynamics\nmodels of both multi-body kinematic tree structure human and robot is derived\nseparately, using Newton's method. The obtained models are then verified by\ncomparing the results with those of the Recursive Newton-Euler Algorithms\n(RNEA). These models are then augmented to investigate the RoboWalk joint\ntorques, and those of the human body, and also the floor reaction force of the\ncomplete system. Since RoboWalk is an under-actuated robot, despite the\nassistive force, an undesirable disturbing force exerts to the human. So,\noptimization strategies are proposed to find an optimal design to maximize the\nassistive behavior of RoboWalk and reduce joint torques of the human body as a\nresult. To this end, a human-in-the-loop optimization algorithm will be used.\nThe solution of this optimization problem is carried out by Particle Swarm\nOptimization (PSO) method. The designed analysis and the optimization results\ndemonstrate the effectiveness of the proposed approaches, leading to the\nelimination of disturbing forces, lower torque demand for RoboWalk motors and\nlower weights.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Quark orbital angular momentum (OAM) in the proton can be calculated directly\ngiven a Wigner function encoding the simultaneous distribution of quark\ntransverse positions and momenta. This distribution can be accessed via proton\nmatrix elements of a quark bilocal operator (the separation in which is Fourier\nconjugate to the quark momentum) featuring a momentum transfer (which is\nFourier conjugate to the quark position). To generate the weighting by quark\ntransverse position needed to calculate OAM, a derivative with respect to\nmomentum transfer is consequently required. This derivative is evaluated using\na direct derivative method, i.e., a method in which the momentum derivative of\na correlator is directly sampled in the lattice calculation, as opposed to\nextracting it a posteriori from the numerical correlator data. The method\nremoves the bias stemming from estimating the derivative a posteriori that was\nseen to afflict a previous exploratory calculation. Data for Ji OAM generated\non a clover ensemble at pion mass $m_{\\pi } = 317\\, \\mbox{MeV} $ are seen to\nagree with the result obtained via the traditional Ji sum rule method. By\nvarying the gauge connection in the quark bilocal operator, also Jaffe-Manohar\nOAM is extracted, and seen to be enhanced significantly compared to Ji OAM.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, the thermodynamical properties and the phase transitions of\nthe charged accelerating anti-de Sitter (AdS) black holes are investigated in\nthe framework of the $f(R)$ gravity. By studying the conditions for the phase\ntransitions, it has been shown that the $P-V$ criticality and the van der Waals\nlike phase transitions can be achieved for $ T \\approx T_{c} $. The\nJoule-Thomson expansion effects are also examined for the charged accelerating\nAdS black holes of the $f(R)$ gravity. Here, we derive the inversion\ntemperatures as well as the inversion curves. Then, we determine the position\nof the reverse point for different values of mass $M$ and parameter $b$ for the\ncorresponding black hole. At this point, the Joule-Thompson coefficient is\nzero. So, in such case, we can say that such point is very important for the\nfinding of cooling - heating regions. Finally, we calculate the ratio of\nminimum inversion temperature and critical temperature for such black hole.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An important parameter used to characterize large-area field electron\nemitters (LAFEs) is the characteristic apex field enhancement factor\n{\\gamma}_C. This parameter is normally extracted from the slope of a\nFowler-Nordheim (FN) plot. Several years ago, the development of an \"orthodoxy\ntest\" allowed a sample of 19 published FN plots relating to LAFEs to be tested,\nand it was found that about 40% of the related papers were reporting spuriously\nhigh values for {\\gamma}_C. In technological papers relating to LAFE\ncharacterization, common practice is to pre-convert the measured voltage into\nan (apparent) value of macroscopic field before making and analyzing a FN plot.\nThis paper suggests that the cause of the \"spurious-FEF-value\" problem is the\nwidespread use of a pre-conversion equation that is defective (for example, not\ncompatible with ordinary electrical circuit theory) when it is applied to\nso-called \"non-ideal\" field emission devices/systems. Many real devices/\nsystems are non-ideal. The author argues that FN plots should be made using raw\nexperimental current-voltage data, and that an orthodoxy test should be applied\nto the resulting FN plot before any more-detailed analysis, and that (in view\nof growing concerns over the reliability of published \"scientific\" results)\nreviewers should scrutinize field emission materials-characterization papers\nwith enhanced care.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  {A} Higher Spin Gravity in five dimensions is constructed. It was shown\nrecently that constructing formally consistent classical equations of motion of\nhigher spin gravities is equivalent to finding a certain deformation of a given\nhigher spin algebra. A strong homotopy algebra encoding the interaction\nvertices then follows. We propose two different and novel realizations of the\ndeformed higher spin algebra in the case of five dimensions: one in terms of\nthe universal enveloping algebra of $su(2,2)$ and the other by means of\noscillator variables. Both the new realizations admit supersymmetric extensions\nand the $\\mathcal{N}=8$ case underlies the massless sector of tensionless\nstrings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report here methods and techniques for creating and improving a model that\nreproduces the scintillation and ionization response of a dual-phase liquid and\ngaseous xenon time-projection chamber. Starting with the recent release of the\nNoble Element Simulation Technique (NEST v2.0), electronic recoil data from the\n$\\beta$ decays of ${}^3$H and ${}^{14}$C in the Large Underground Xenon (LUX)\ndetector were used to tune the model, in addition to external data sets that\nallow for extrapolation beyond the LUX data-taking conditions. This paper also\npresents techniques used for modeling complicated temporal and spatial detector\npathologies that can adversely affect data using a simplified model framework.\nThe methods outlined in this report show an example of the robust applications\npossible with NEST v2.0, while also providing the final electronic recoil model\nand detector parameters that will used in the new analysis package, the LUX\nLegacy Analysis Monte Carlo Application (LLAMA), for accurate reproduction of\nthe LUX data. As accurate background reproduction is crucial for the success of\nrare-event searches, such as dark matter direct detection experiments, the\ntechniques outlined here can be used in other single-phase and dual-phase xenon\ndetectors to assist with accurate ER background reproduction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Vehicular Ad-hoc Networks (VANET) enable efficient communication between\nvehicles with the aim of improving road safety. However, the growing number of\nvehicles in dense regions and obstacle shadowing regions like Manhattan and\nother downtown areas leads to frequent disconnection problems resulting in\ndisrupted radio wave propagation between vehicles. To address this issue and to\ntransmit critical messages between vehicles and drones deployed from service\nvehicles to overcome road incidents and obstacles, we proposed a hybrid\ntechnique based on fog computing called Hybrid-Vehfog to disseminate messages\nin obstacle shadowing regions, and multi-hop technique to disseminate messages\nin non-obstacle shadowing regions. Our proposed algorithm dynamically adapts to\nchanges in an environment and benefits in efficiency with robust drone\ndeployment capability as needed. Performance of Hybrid-Vehfog is carried out in\nNetwork Simulator (NS-2) and Simulation of Urban Mobility (SUMO) simulators.\nThe results showed that Hybrid-Vehfog outperformed Cloud-assisted Message\nDownlink Dissemination Scheme (CMDS), Cross-Layer Broadcast Protocol (CLBP),\nPEer-to-Peer protocol for Allocated REsource (PrEPARE), Fog-Named Data\nNetworking (NDN) with mobility, and flooding schemes at all vehicle densities\nand simulation times.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider Toda field theories in a classical Euclidean $AdS_2$ background.\nWe compute the four-point functions of boundary operators in the $a_1$, $a_2$\nand $b_2$ Toda field theories. They take the same form as the four-point\nfunctions of generators in the corresponding $\\mathcal{W}$-algebras. Therefore\nwe conjecture that the boundary operators are in one-to-one correspondence with\nthe generators in the $\\mathcal{W}$-algebras.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we present a new framework for time-series modeling that\ncombines the best of traditional statistical models and neural networks. We\nfocus on time-series with long-range dependencies, needed for monitoring fine\ngranularity data (e.g. minutes, seconds, milliseconds), prevalent in\noperational use-cases.\n  Traditional models, such as auto-regression fitted with least squares\n(Classic-AR) can model time-series with a concise and interpretable model. When\ndealing with long-range dependencies, Classic-AR models can become intractably\nslow to fit for large data. Recently, sequence-to-sequence models, such as\nRecurrent Neural Networks, which were originally intended for natural language\nprocessing, have become popular for time-series. However, they can be overly\ncomplex for typical time-series data and lack interpretability.\n  A scalable and interpretable model is needed to bridge the statistical and\ndeep learning-based approaches. As a first step towards this goal, we propose\nmodelling AR-process dynamics using a feed-forward neural network approach,\ntermed AR-Net. We show that AR-Net is as interpretable as Classic-AR but also\nscales to long-range dependencies.\n  Our results lead to three major conclusions: First, AR-Net learns identical\nAR-coefficients as Classic-AR, thus being equally interpretable. Second, the\ncomputational complexity with respect to the order of the AR process, is linear\nfor AR-Net as compared to a quadratic for Classic-AR. This makes it possible to\nmodel long-range dependencies within fine granularity data. Third, by\nintroducing regularization, AR-Net automatically selects and learns sparse\nAR-coefficients. This eliminates the need to know the exact order of the\nAR-process and allows to learn sparse weights for a model with long-range\ndependencies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Ferroelectric Rashba semiconductors (FERSC) are a novel class of\nmultifunctional materials showing a giant Rashba spin splitting which can be\nreversed by switching the electric polarization. Although they are excellent\ncandidates as channels in spin field effect transistors, the experimental\nresearch has been limited so far to semiconducting GeTe, in which ferroelectric\nswitching is often prevented by heavy doping and/or large leakage currents.\nHere, we report that CsBiNb2O7, a layered perovskite of Dion-Jacobson type, is\na robust ferroelectric with sufficiently strong spin-orbit coupling and spin\ntexture reversible by electric field. Moreover, we reveal that its topmost\nvalence band's spin texture is quasi-independent from the momentum, as a result\nof the low symmetry of its ferroelectric phase. The peculiar spin polarization\npattern in the momentum space may yield the so-called \"persistent spin helix\",\na specific spin-wave mode which protects the spin from decoherence in diffusive\ntransport regime, potentially ensuring a very long spin lifetime in this\nmaterial.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This document aims to provide an assessment of the potential of future\ncolliding beam facilities to perform Higgs boson studies. The analysis builds\non the submissions made by the proponents of future colliders to the European\nStrategy Update process, and takes as its point of departure the results\nexpected at the completion of the HL-LHC program. This report presents\nquantitative results on many aspects of Higgs physics for future collider\nprojects of sufficient maturity using uniform methodologies. A first version of\nthis report was prepared for the purposes of discussion at the Open Symposium\nin Granada (13-16/05/2019). Comments and feedback received led to the\nconsideration of additional run scenarios as well as a refined analysis of the\nimpact of electroweak measurements on the Higgs coupling extraction.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Galbrun's equation, which is a second order partial differential equation\ndescribing the evolution of a so-called Lagrangian displacement vector field,\ncan be used to study acoustics in background flows as well as perturbations of\nastrophysical flows. Our starting point for deriving Galbrun's equation is\nlinearized Euler's equations, which is a first order system of partial\ndifferential equations that describe the evolution of the so-called Eulerian\nflow perturbations. Given a solution to linearized Euler's equations, we\nintroduce the Lagrangian displacement as the solution to a linear first order\npartial differential equation, driven by the Eulerian perturbation of the fluid\nvelocity. Our Lagrangian displacement solves Galbrun's equation, provided it is\nregular enough and that the so-called \"no resonance\" assumption holds. In the\ncase that the background flow is steady and tangential to the domain boundary,\nwe prove existence, uniqueness, and continuous dependence on data of solutions\nto an initial--boundary value problem for linearized Euler's equations. For\nsuch background flows, we demonstrate that the Lagrangian displacement is\nwell-defined, that the initial datum of the Lagrangian displacement can be\nchosen in order to fulfill the \"no resonance\" assumption, and derive a\nclassical energy estimate for (sufficiently regular solutions to) Galbrun's\nequation. Due to the presence of zeroth order terms of indefinite signs in the\nequations, the energy estimate allows solutions that grow exponentially with\ntime.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Isogeometric Analysis is a spline-based discretization method to partial\ndifferential equations which shows the approximation power of a high-order\nmethod. The number of degrees of freedom, however, is as small as the number of\ndegrees of freedom of a low-order method. This does not come for free as the\noriginal formulation of Isogeometric Analysis requires a global geometry\nfunction. Since this is too restrictive for many kinds of applications, the\ndomain is usually decomposed into patches, where each patch is parameterized\nwith its own geometry function. In simpler cases, the patches can be combined\nin a conforming way. However, for non-matching discretizations or for varying\ncoefficients, a non-conforming discretization is desired. An symmetric interior\npenalty discontinuous Galerkin (SIPG) method for Isogeometric Analysis has been\npreviously introduced. In the present paper, we give error estimates that only\ndepend poly-logarithmically on the spline degree. This opens the door towards\nthe construction and the analysis of fast linear solvers, particularly\nmultigrid solvers for non-conforming multipatch Isogeometric Analysis.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present an iterative method to efficiently solve the optimal\ntransportation problem for a class of strictly convex costs which includes\nquadratic and p-power costs. Given two probability measures supported on a\ndiscrete grid with n points, we compute the optimal map using O(n) storage\nspace and O(n log(n)) operations per iteration, with an approximately\nexponential convergence rate. Our approach allows us to solve optimal\ntransportation problems on spatial grids as large as 4096x4096 and 384x384x384\nin a matter of minutes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We can define a neural network that can learn to recognize objects in less\nthan 100 lines of code. However, after training, it is characterized by\nmillions of weights that contain the knowledge about many object types across\nvisual scenes. Such networks are thus dramatically easier to understand in\nterms of the code that makes them than the resulting properties, such as tuning\nor connections. In analogy, we conjecture that rules for development and\nlearning in brains may be far easier to understand than their resulting\nproperties. The analogy suggests that neuroscience would benefit from a focus\non learning and development.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Transition to turbulence dramatically alters the properties of fluid flows.\nIn most canonical shear flows, the laminar flow is linearly stable and a\nfinite-amplitude perturbation is necessary to trigger transition. Controlling\ntransition to turbulence is achieved via the broadening or narrowing of the\nbasin of attraction of the laminar flow. In this paper, a novel methodology to\nassess the robustness of the laminar flow and the efficiency of control\nstrategies is introduced. It relies on the statistical sampling of the phase\nspace neighborhood around the laminar flow in order to assess the transition\nprobability of perturbations as a function of their energy. This approach is\napplied to a canonical flow (plane Couette flow) and provides invaluable\ninsight: in the presence of the chosen control, transition is significantly\nsuppressed whereas plausible scalar indicators of the nonlinear stability of\nthe flow, such as the edge state energy, do not provide conclusive predictions.\nThe methodology presented here in the context of transition to turbulence is\napplicable to any nonlinear system displaying finite-amplitude instability.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For 3 $\\leq$ n $\\leq$ 7, we prove that a bumpy closed Riemannian n-manifold\ncontains a sequence of connected embedded closed minimal surfaces with\nunbounded area.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The geometrical features of the (non-convex) loss landscape of neural network\nmodels are crucial in ensuring successful optimization and, most importantly,\nthe capability to generalize well. While minimizers' flatness consistently\ncorrelates with good generalization, there has been little rigorous work in\nexploring the condition of existence of such minimizers, even in toy models.\nHere we consider a simple neural network model, the symmetric perceptron, with\nbinary weights. Phrasing the learning problem as a constraint satisfaction\nproblem, the analogous of a flat minimizer becomes a large and dense cluster of\nsolutions, while the narrowest minimizers are isolated solutions. We perform\nthe first steps toward the rigorous proof of the existence of a dense cluster\nin certain regimes of the parameters, by computing the first and second moment\nupper bounds for the existence of pairs of arbitrarily close solutions.\nMoreover, we present a non rigorous derivation of the same bounds for sets of\n$y$ solutions at fixed pairwise distances.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A hotspot is an axis-aligned square of fixed side length $s$, the duration of\nthe presence of an entity moving in the plane in which is maximised. An exact\nhotspot of a polygonal trajectory with $n$ edges can be found in $O(n^2)$.\nDefining a $c$-approximate hotspot as an axis-aligned square of side length\n$cs$, in which the duration of the entity's presence is no less than that of an\nexact hotspot, in this paper we present an algorithm to find a $(1 +\n\\epsilon)$-approximate hotspot of a polygonal trajectory with the time\ncomplexity $O({n\\phi \\over \\epsilon} \\log {n\\phi \\over \\epsilon})$, where\n$\\phi$ is the ratio of average trajectory edge length to $s$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Estimating the predictive uncertainty of a Bayesian learning model is\ncritical in various decision-making problems, e.g., reinforcement learning,\ndetecting adversarial attack, self-driving car. As the model posterior is\nalmost always intractable, most efforts were made on finding an accurate\napproximation the true posterior. Even though a decent estimation of the model\nposterior is obtained, another approximation is required to compute the\npredictive distribution over the desired output. A common accurate solution is\nto use Monte Carlo (MC) integration. However, it needs to maintain a large\nnumber of samples, evaluate the model repeatedly and average multiple model\noutputs. In many real-world cases, this is computationally prohibitive. In this\nwork, assuming that the exact posterior or a decent approximation is obtained,\nwe propose a generic framework to approximate the output probability\ndistribution induced by model posterior with a parameterized model and in an\namortized fashion. The aim is to approximate the true uncertainty of a specific\nBayesian model, meanwhile alleviating the heavy workload of MC integration at\ntesting time. The proposed method is universally applicable to Bayesian\nclassification models that allow for posterior sampling. Theoretically, we show\nthat the idea of amortization incurs no additional costs on approximation\nperformance. Empirical results validate the strong practical performance of our\napproach.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Edge states protected by bulk topology of photonic crystals show robustness\nagainst short-range disorder, making robust information transfer possible.\nHere, topological photonic crystals under long-range deformations are\ninvestigated. Vertices of each regular hexagon in a honeycomb crystalline\nstructure are shifted randomly to establish a deformed system. By increasing\nthe degree of random deformations, a transition from an ordered system to an\namorphous system are investigated, where the close of topological bandgap is\nclearly shown. We further present comprehensive investigations into excitation\nmethods of the proposed deformed system. Due to the lack of strict periodicity,\nexcitation of topological edge modes becomes difficult. Chiral and linearly\npolarized sources as two different methods are investigated respectively. It is\nfound that chiral sources are sensitive and rely on the ordered lattice. Even a\nweak long-range deformation can bring fluctuations to transmission. We further\ndesigned and fabricated metal-dielectric-metal sandwich-like samples working in\nthe microwave band. Using linearly polarized source, we detected the existence\nof topological transport in the deformed system. This work investigates\nexcitation and robustness of bulk topology against long-range deformations and\nmay open the way for exploiting topological properties of materials with a\ndeformed lattice.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Four different numerical approaches are compared for the rise of liquid\nbetween two parallel plates. These are an Arbitrary Lagrangian-Eulerian method\n(OpenFOAM solver interTrackFoam), a geometric volume of fluid code (FS3D), an\nalgebraic volume of fluid method (OpenFOAM solver interFoam), and a level set\napproach (BoSSS). The first three approaches discretize the bulk equation using\na finite volume method while the last one employs an extended discontinuous\nGalerkin discretization. The results are compared to ODE models which are the\nclassical rise model and an extended model that incorporates a Navier slip\nboundary condition on the capillary walls and levels at a corrected stationary\nrise height. All physical parameters are based on common requirements for the\ninitial conditions, short simulation time, and a non-dimensional parameter\nstudy. The comparison shows excellent agreement between the different\nimplementations with minor quantitative deviations for the adapted interFoam\nimplementation. While the qualitative agreement between the full solutions of\nthe continuum mechanical approach and the reference model is good, the\nquantitative comparison is only reasonable, especially for cases with\nincreasing oscillations. Furthermore, reducing the slip length changes the\nsolution qualitatively as oscillations are completely damped in contrast to the\nsolution of the ODE models. To provide reference data for a full continuum\nsimulation of the capillary rise problem, all results are made available\nonline.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Orders and types of entire and meromorphic functions have been actively\ninvestigated by many authors. In the present paper, we aim at investigating\nsome basic properties in connection with sum and product of relative\n$(p,q)$-$\\varphi$ order, relative $(p,q)$-$\\varphi$ type, and relative\n$(p,q)$-$\\varphi$ weak type of meromorphic functions with respect to entire\nfunctions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Insiders usually cause significant losses to organizations and are hard to\ndetect. Currently, various approaches have been proposed to achieve insider\nthreat detection based on analyzing the audit data that record information of\nthe employee's activity type and time. However, the existing approaches usually\nfocus on modeling the users' activity types but do not consider the activity\ntime information. In this paper, we propose a hierarchical neural temporal\npoint process model by combining the temporal point processes and recurrent\nneural networks for insider threat detection. Our model is capable of capturing\na general nonlinear dependency over the history of all activities by the\ntwo-level structure that effectively models activity times, activity types,\nsession durations, and session intervals information. Experimental results on\ntwo datasets demonstrate that our model outperforms the models that only\nconsider information of the activity types or time alone.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Breaking Lorentz reciprocity was believed to be a prerequisite for\nnonreciprocal transmissions of light fields, so the possibility of\nnonreciprocity by linear optical systems was mostly ignored. We put forward a\nstructure of three mutually coupled microcavities or optical fiber rings to\nrealize optical nonreciprocity. Although its couplings with the fields from two\ndifferent input ports are constantly equal, such system transmits them\nnonreciprocally either under the saturation of an optical gain in one of the\ncavities or with the asymmetric couplings of the circulating fields in\ndifferent cavities. The structure made up of optical fiber rings can perform\nnonreciprocal transmissions as a time-independent linear system without\nbreaking Lorentz reciprocity. Optical isolation for inputs simultaneously from\ntwo different ports and even approximate optical isolator operations are\nimplementable with the structure.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  AMS-02 on the International Space Station has been releasing data of\nunprecedented accuracy. This poses new challenges for their interpretation. We\nrefine the methodology to get a statistically sound determination of the\ncosmic-ray propagation parameters. We inspect the numerical precision of the\nmodel calculation, nuclear cross-section uncertainties, and energy correlations\nin data systematic errors. We used the 1D diffusion model in USINE. Our\n$\\chi^2$ analysis includes a covariance matrix of errors for AMS-02 systematics\nand nuisance parameters to account for cross-section uncertainties. Mock data\nwere used to validate some of our choices. We show that any mis-modelling of\nnuclear cross-section values or the energy correlation length of the covariance\nmatrix of errors biases the analysis. It also makes good models ($\\chi^2_{\\rm\nmin}/{\\rm dof}\\approx1$) appear as excluded ($\\chi^2_{\\rm min}/{\\rm dof}\\gg1$).\nWe provide a framework to mitigate these effects (AMS-02 data are interpreted\nin a companion paper). New production cross-section data and the publication by\nthe AMS-02 collaboration of a covariance matrix of errors for each data set\nwould be an important step towards an unbiased view of cosmic-ray propagation\nin the Galaxy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $\\Omega\\subset \\mathbb{R}^N$, $N=1,2,3$, be an open bounded and connected\nset with continuous piecewise $\\mathrm{C}^{\\infty}$ boundary. Here we deal with\nalmost periodic distributions of the form $u(t,x)=\\sum_{n=0}^{+\\infty} c_n\nS_n(x) \\mathrm{e}^{i \\lambda_n t}$ where $(c_n)_{n\\in \\mathbb{N}}\\subset\n\\mathbb{C}$ belong to the space of slowing growing sequences $s^\\prime$, and\n$(\\lambda_n^2)_{n\\in\\mathbb{N}}\\subset \\mathbb{R}$ and\n$(S_n)_{n\\in\\mathbb{N}}\\subset \\mathrm{H}_0^{1}(\\Omega)$ are respectively the\neigenvalues and eigenvectors of the Laplacian. Given $\\omega\\subset\\Omega$, we\nprove that there exists $T_{max}(\\Omega,\\omega)>0$ depending only on $\\Omega$\nand $\\omega$ such that if $T>T_{max}(\\Omega,\\omega)$ and $u|_{\\omega\\times\n]-T,T[}=0$, then $u\\equiv 0$. Using this result we prove a unique continuation\nproperty for the wave equation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We shall show that there is no odd perfect number of the form $2^n+1$ or\n$n^n+1$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Training code-switched language models is difficult due to lack of data and\ncomplexity in the grammatical structure. Linguistic constraint theories have\nbeen used for decades to generate artificial code-switching sentences to cope\nwith this issue. However, this require external word alignments or constituency\nparsers that create erroneous results on distant languages. We propose a\nsequence-to-sequence model using a copy mechanism to generate code-switching\ndata by leveraging parallel monolingual translations from a limited source of\ncode-switching data. The model learns how to combine words from parallel\nsentences and identifies when to switch one language to the other. Moreover, it\ncaptures code-switching constraints by attending and aligning the words in\ninputs, without requiring any external knowledge. Based on experimental\nresults, the language model trained with the generated sentences achieves\nstate-of-the-art performance and improves end-to-end automatic speech\nrecognition.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Given an arbitrary fixed continuously differentiable vector field on\n$\\mathbb{R}^n$, we prove that this vector field is coercive if and only if its\nconservative part is coercive. We apply this result in order to provide\nsufficient conditions to guarantee the co-existence of equilibrium states of a\ncontinuously differentiable vector field and its conservative part.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Birkhoff polytope, defined to be the convex hull of $n\\times n$\npermutation matrices, is a well studied polytope in the context of the Ehrhart\ntheory. This polytope is known to have many desirable properties, such as the\nGorenstein property and existence of regular, unimodular triangulations. In\nthis paper, we study analogues of the Birkhoff polytope for finite irreducible\nCoxeter groups of other types. We focus on a type-$B$ Birkhoff polytope\n$\\mathcal{BB}(n)$ arising from signed permutation matrices and prove that it\nand its dual polytope are reflexive, and hence Gorenstein, and also possess\nregular, unimodular triangulations. Noting that our triangulation proofs do not\nrely on the combinatorial structure of $\\mathcal{BB}(n)$, we define the notion\nof an orthant-lattice property polytope and use this to prove more general\nresults for the existence of regular, unimodular triangulations and unimodular\ncovers for a significant family of reflexive polytopes. We conclude by\nremarking on some connections to Gale-duality, Birkhoff polytopes of other\ntypes, and possible applications of orthant-lattice property.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show that a Reissner-Nordstr\\\"{o}m (RN) black hole can be formed by\ndropping a charged thin dust shell onto a RN naked singularity. This is in\ncontrast to the fact that a RN naked singularity is prohibited from forming by\ndropping a charged thin dust shell onto a RN black hole. This implies the\nstrong tendency of the RN singularity to be covered by a horizon in favour of\ncosmic censorship. We show that an extreme RN black hole can also be formed\nfrom a RN naked singularity by the same process in a finite advanced time. We\nalso discuss the evolution of the charged thin dust shells and the causal\nstructure of the resultant spacetimes.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report the discovery of a square axisymmetric circumstellar nebula around\nthe emission-line star HD 93795 in archival Spitzer Space Telescope 24 micron\ndata. We classify HD 93795 as an B9 Ia star using optical spectra obtained with\nthe Southern African Large Telescope (SALT). A spectral analysis carried out\nwith the stellar atmosphere code FASTWIND indicates that HD 93795 only recently\nleft the main sequence and is evolving redward for the first time. We discuss\npossible scenarios for the origin of the nebula and suggest that HD 93795 was\noriginally a binary system and that the nebula was formed because of merger of\nthe binary components. We also discuss a discrepancy between distance estimates\nfor HD 93795 based on the Gaia data and the possible membership of this star of\nthe Car OB1 association, and conclude that HD 93795 could be at the same\ndistance as Car OB1.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The posterior variance of Gaussian processes is a valuable measure of the\nlearning error which is exploited in various applications such as safe\nreinforcement learning and control design. However, suitable analysis of the\nposterior variance which captures its behavior for finite and infinite number\nof training data is missing. This paper derives a novel bound for the posterior\nvariance function which requires only local information because it depends only\non the number of training samples in the proximity of a considered test point.\nFurthermore, we prove sufficient conditions which ensure the convergence of the\nposterior variance to zero. Finally, we demonstrate that the extension of our\nbound to an average learning bound outperforms existing approaches.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Unlike Stockmayer fluids, that prove to undergo gas-liquid transition on\ncooling, the system of dipolar hard or soft spheres without any additional\ncentral attraction so far has not been shown to have a critical point. Instead,\nin the latter, one observes diverse self-assembly scenarios. Crosslinking\ndipolar soft spheres into supracolloidal magnetic polymer-like structures\n(SMPs) changes the self-assembly behaviour. Moreover, aggregation in systems of\nSMPs strongly depends on the constituent topology. For Y- and X-shaped SMPs,\nunder the same conditions in which dipolar hard spheres would form chains, the\nformation of very large loose gel-like clusters was observed [Journal of\nMolecular Liquids, 271, 631 (2018)]. In this work, using molecular dynamics\nsimulations, we investigate self-assembly in suspensions of four topologically\ndifferent SMPs -- chains, rings, X and Y -- monomers in which interact via\nStockmayer potential. As expected, compact drop-like clusters are formed by\nSMPs in all cases if the central isotropic attraction is introduced, however,\ntheir shape and internal structure turn out to depend on the SMPs topology.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  When random effects are correlated with sample design variables, the usual\napproach of employing individual survey weights (constructed to be inversely\nproportional to the unit survey inclusion probabilities) to form a\npseudo-likelihood no longer produces asymptotically unbiased inference. We\nconstruct a weight-exponentiated formulation for the random effects\ndistribution that achieves unbiased inference for generating hyperparameters of\nthe random effects. We contrast our approach with frequentist methods that rely\non numerical integration to reveal that only the Bayesian method achieves both\nunbiased estimation with respect to the sampling design distribution and\nconsistency with respect to the population generating distribution. Our\nsimulations and real data example for a survey of business establishments\ndemonstrate the utility of our approach across different modeling formulations\nand sampling designs. This work serves as a capstone for recent developmental\nefforts that combine traditional survey estimation approaches with the Bayesian\nmodeling paradigm and provides a bridge across the two rich but disparate\nsub-fields.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper defines a subregular class of functions called the tier-based\nsynchronized strictly local (TSSL) functions. These functions are similar to\nthe the tier-based input-output strictly local (TIOSL) functions, except that\nthe locality condition is enforced not on the input and output streams, but on\nthe computation history of the minimal subsequential finite-state transducer.\nWe show that TSSL functions naturally describe rhythmic syncope while TIOSL\nfunctions cannot, and we argue that TSSL functions provide a more restricted\ncharacterization of rhythmic syncope than existing treatments within Optimality\nTheory.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Given an ideal $\\mathcal{I}$ on $\\omega$ and a sequence $x$ in a topological\nvector space, we let the $\\mathcal{I}$-core of $x$ be the least closed convex\nset containing $\\{x_n: n \\notin I\\}$ for all $I \\in \\mathcal{I}$. We show two\ncharacterizations of the $\\mathcal{I}$-core. This implies that the\n$\\mathcal{I}$-core of a bounded sequence in $\\mathbf{R}^k$ is simply the convex\nhull of its $\\mathcal{I}$-cluster points. As applications, we simplify and\nextend several results in the context of Pringsheim-convergence and\n$e$-convergence of double sequences.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the signal of long-lived sterile neutrino at the LHC produced\nthrough the decay of the $W$ boson. It decays into charged lepton and jets. The\ncharacteristic signature is a hard prompt lepton and a lepton from the\ndisplaced decay of the sterile neutrino, which leads to a bundle of displaced\ntracks with large transverse impact parameter. Different from other studies, we\nneither reconstruct the displaced vertex nor place requirement on its invariant\nmass to maintain sensitivity for low sterile neutrino masses. Instead, we focus\non the displaced track from the lepton. A difficulty for low mass sterile\nneutrino study is that the displaced lepton is usually \\textit{non-isolated}.\nTherefore, leptons from heavy flavor quark is the major source of background.\nWe closely follow a search for displaced electron plus muon search at CMS and\nstudy their control regions, which is related to our signal regions, in great\ndetail to develop a robust estimation of the background for our signals. After\nfurther optimization on the signal limiting the number of jets, low $H_T$ and\nlarge lepton displacement $d_0$ to suppress SM background, we reach an\nexclusion sensitivity of about $10^{-8}$ ($10^{-5}$) for the mixing angle\nsquare at 10 (2) GeV sterile neutrino mass respectively. The strategy we\npropose can cover the light sterile masses complimentary to beam dump and\nforward detector experiments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The categories of open learners (due to Fong, Spivak and Tuy\\'eras) and open\ngames (due to the present author, Ghani, Winschel and Zahn) bear a very\nstriking and unexpected similarity. The purpose of this short note is to prove\nthat there is a faithful symmetric monoidal functor from the former to the\nlatter, which means that any supervised neural network (without feedback or\nother complicating features) can be seen as an open game in a canonical way.\nRoughly, each parameter is controlled by a different player, and the game's\nbest response relation encodes the dynamics of gradient descent. We suggest\npaths for further work exploiting the link.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The job sequencing and tool switching problem (SSP) has been extensively\nstudied in the field of operations research, due to its practical relevance and\nmethodological interest. Given a machine that can load a limited amount of\ntools simultaneously and a number of jobs that require a subset of the\navailable tools, the SSP seeks a job sequence that minimizes the number of tool\nswitches in the machine. To solve this problem, we propose a simple and\nefficient hybrid genetic search based on a generic solution representation, a\ntailored decoding operator, efficient local searches and diversity management\ntechniques. To guide the search, we introduce a secondary objective designed to\nbreak ties. These techniques allow to explore structurally different solutions\nand escape local optima. As shown in our computational experiments on classical\nbenchmark instances, our algorithm significantly outperforms all previous\napproaches while remaining simple to apprehend and easy to implement. We\nfinally report results on a new set of larger instances to stimulate future\nresearch and comparative analyses.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Fourier extension method, also known as the Fourier continuation method,\nis a method for approximating non-periodic functions on an interval using\ntruncated Fourier series with period larger than the interval on which the\nfunction is defined. When the function being approximated is known at only\nfinitely many points, the approximation is constructed as a projection based on\nthis discrete set of points. In this paper we address the issue of estimating\nthe absolute error in the approximation. The error can be expressed in terms of\na system of discrete orthogonal polynomials on an arc of the unit circle, and\nthese polynomials are then evaluated asymptotically using Riemann--Hilbert\nmethods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the last decade, deep learning (DL) has outperformed model-based and\nstatistical approaches in predicting the remaining useful life (RUL) of\nmachinery in the context of condition-based maintenance. One of the major\ndrawbacks of DL is that it heavily depends on a large amount of labeled data,\nwhich are typically expensive and time-consuming to obtain, especially in\nindustrial applications. Scarce training data lead to uncertain estimates of\nthe model's parameters, which in turn result in poor prognostic performance.\nQuantifying this parameter uncertainty is important in order to determine how\nreliable the prediction is. Traditional DL techniques such as neural networks\nare incapable of capturing the uncertainty in the training data, thus they are\noverconfident about their estimates. On the contrary, Bayesian deep learning\nhas recently emerged as a promising solution to account for uncertainty in the\ntraining process, achieving state-of-the-art performance in many classification\nand regression tasks. In this work Bayesian DL techniques such as Bayesian\ndense neural networks and Bayesian convolutional neural networks are applied to\nRUL estimation and compared to their frequentist counterparts from the\nliterature. The effectiveness of the proposed models is verified on the popular\nC-MAPSS dataset. Furthermore, parameter uncertainty is quantified and used to\ngain additional insight into the data.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the Muskat problem for one fluid or two fluids, with or without\nviscosity jump, with or without rigid boundaries, and in arbitrary space\ndimension $d$ of the interface. The Muskat problem is scaling invariant in the\nSobolev space $H^{s_c}(\\mathbb{R}^d)$ where $s_c=1+\\frac{d}{2}$. Employing a\nparadifferential approach, we prove local well-posedness for large data in any\nsubcritical Sobolev spaces $H^s(\\mathbb{R}^d)$, $s>s_c$. Moreover, the rigid\nboundaries are only required to be Lipschitz and can have arbitrarily large\nvariation. The Rayleigh-Taylor stability condition is assumed for the case of\ntwo fluids with viscosity jump but is proved to be automatically satisfied for\nthe case of one fluid. The starting point of this work is a reformulation\nsolely in terms of the Drichlet-Neumann operator. The key elements of proofs\nare new paralinearization and contraction results for the Drichlet-Neumann\noperator in rough domains.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The single-layer FeSe films grown on SrTiO3 (STO) substrates have attracted\nmuch attention because of its record high superconducting critical temperature\n(Tc). It is usually believed that the composition of the epitaxially grown\nsingle-layer FeSe/STO films is stoichiometric, i.e., the ratio of Fe and Se is\n1:1. Here we report the identification of a large amount of excess Fe in the\nsuperconducting single-layer FeSe/STO films. By depositing Se onto the\nsuperconducting single-layer FeSe/STO films, we find by in situ scanning\ntunneling microscopy (STM) the formation of the second-layer FeSe islands on\nthe top of the first layer during the annealing process at a surprisingly low\ntemperature ($\\sim$150{\\deg}C) which is much lower than the usual growth\ntemperature ($\\sim$490{\\deg}C). This observation is used to detect excess Fe\nand estimate its quantity in the single-layer FeSe/STO films. The amount of\nexcess Fe detected is at least 20% that is surprisingly high for the\nsuperconducting single-layer FeSe/STO films. The discovery of such a large\namount of excess Fe should be taken into account in understanding the high-Tc\nsuperconductivity and points to a likely route to further enhance Tc in the\nsuperconducting single-layer FeSe/STO films.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce the quasi-hyperbolicity constant of a metric space, a rough\nisometry invariant that measures how a metric space deviates from being Gromov\nhyperbolic. This number, for unbounded spaces, lies in the closed interval\n$[1,2]$. The quasi-hyperbolicity constant of an unbounded Gromov hyperbolic\nspace is equal to one. For a CAT$(0)$-space, it is bounded from above by\n$\\sqrt{2}$. The quasi-hyperbolicity constant of a Banach space that is at least\ntwo dimensional is bounded from below by $\\sqrt{2}$, and for a non-trivial\n$L_p$-space it is exactly $\\max\\{2^{1/p},2^{1-1/p}\\}$. If $0 < \\alpha < 1$ then\nthe quasi-hyperbolicity constant of the $\\alpha$-snowflake of any metric space\nis bounded from above by $2^\\alpha$. We give an exact calculation in the case\nof the $\\alpha$-snowflake of the Euclidean real line.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present results for single pseudoscalar meson pole contributions and pion\nbox contributions to the hadronic light-by-light (LBL) correction of the muon's\nanomalous magnetic moment. We follow the recently developed dispersive approach\nto LBL, where these contributions are evaluated with intermediate mesons\non-shell. However, the space-like electromagnetic and transition form factors\nare not determined from analytic continuation of time-like data, but directly\ncalculated within the functional approach to QCD using Dyson-Schwinger and\nBethe-Salpeter equations. This strategy allows for a systematic comparison with\na strictly dispersive treatment and also with recent results from lattice QCD.\nWithin error bars, we obtain excellent agreement for the pion electromagnetic\nand transition form factor and the resulting contributions to LBL. In addition,\nwe present results for the $\\eta$ and $\\eta'$ pole contributions and discuss\nthe dynamical effects in the $\\eta-\\eta'$ mixing due to the strange quarks. Our\nresult for the total pseudoscalar pole contributions is $a_\\mu^{\\text{PS-pole}}\n= 91.6 \\,(1.9) \\times 10^{-11}$ and for the pion-box contribution we obtain\n$a_\\mu^{\\pi-\\text{box}} = -16.3 \\,(2)(4) \\times 10^{-11}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove some new cases of the Grothendieck-Serre conjecture for classical\ngroups. This is based on a new construction of the Gersten-Witt complex for\nWitt groups of Azumaya algebras with involution on regular semilocal rings,\nwith explicit second residue maps; the complex is shown to be exact when the\nring is of dimension $\\le 2$ (or $\\le 4$, with additional hypotheses on the\nalgebra with involution). Note that we do not assume that the ring contains a\nfield.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Imaging biological molecules in the gas-phase requires novel sample delivery\nmethods, which generally have to be characterized and optimized to produce\nhigh-density particle beams. A non-destructive characterization method of the\ntransverse particle beam profile is presented. It enables the characterization\nof the particle beam in parallel to the collection of, for instance,\nx-ray-diffraction patterns. As a rather simple experimental method, it requires\nthe generation of a small laser-light sheet using a cylindrical telescope and a\nmicroscope. The working principle of this technique was demonstrated for the\ncharacterization of the fluid-dynamic-focusing behavior of 220 nm polystyrene\nbeads as prototypical nanoparticles. The particle flux was determined and the\nvelocity distribution was calibrated using Mie-scattering calculations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The cyberspace and development of intelligent systems using Artificial\nIntelligence (AI) creates new challenges to computer professionals, data\nscientists, regulators and policy makers. For example, self-driving cars raise\nnew technical, ethical, legal and public policy issues. This paper proposes a\ncourse named Computers, Ethics, Law, and Public Policy, and suggests a\ncurriculum for such a course. This paper presents ethical, legal, and public\npolicy issues relevant to building and using intelligent systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Thermal DMRG is investigated with emphasis of employability in molecular\nmagnetism studies. To this end magnetic observables at finite temperature are\nevaluated for two one-dimensional quantum spin systems: a Heisenberg chain with\nnearest-neighbor antiferromagnetic interaction and a frustrated sawtooth\n(delta) chain. It is found that thermal DMRG indeed accurately approximates\nmagnetic observables for the chain as well as for the sawtooth chain, but in\nthe latter case only for sufficiently high temperatures. We speculate that the\nreason is due to the peculiar structure of the low-energy spectrum of the\nsawtooth chain induced by frustration.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we investigate the importance of a defense system's learning\nrates to fight against the self-propagating class of malware such as worms and\nbots. To this end, we introduce a new propagation model based on the\ninteractions between an adversary (and its agents) who wishes to construct a\nzombie army of a specific size, and a defender taking advantage of standard\nsecurity tools and technologies such as honeypots (HPs) and intrusion detection\nand prevention systems (IDPSes) in the network environment. As time goes on,\nthe defender can incrementally learn from the collected/observed attack samples\n(e.g., malware payloads), and therefore being able to generate attack\nsignatures. The generated signatures then are used for filtering next attack\ntraffic and thus containing the attacker's progress in its malware propagation\nmission. Using simulation and numerical analysis, we evaluate the efficacy of\nsignature generation algorithms and in general any learning-based scheme in\nbringing an adversary's maneuvering in the environment to a halt as an\nadversarial containment strategy.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Inspired by the Gan-Gross-Prasad conjecture and the descent problem for\nclassical groups, in this paper we study the descents of unipotent cuspidal\nrepresentations of orthogonal and symplectic groups over finite fields.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Antibiotic resistance constitutes a major health threat. Predicting bacterial\ncauses of infections is key to reducing antibiotic misuse, a leading driver of\nantibiotic resistance. We train a machine learning algorithm on administrative\nand microbiological laboratory data from Denmark to predict diagnostic test\noutcomes for urinary tract infections. Based on predictions, we develop\npolicies to improve prescribing in primary care, highlighting the relevance of\nphysician expertise and policy implementation when patient distributions vary\nover time. The proposed policies delay antibiotic prescriptions for some\npatients until test results are known and give them instantly to others. We\nfind that machine learning can reduce antibiotic use by 7.42 percent without\nreducing the number of treated bacterial infections. As Denmark is one of the\nmost conservative countries in terms of antibiotic use, this result is likely\nto be a lower bound of what can be achieved elsewhere.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The paper is devoted to the study of the unconditional extremal problem for a\nfractional linear integral functional defined on a set of probability\ndistributions. In contrast to results proved earlier, the integrands of the\nintegral expressions in the numerator and the denominator in the problem under\nconsideration depend on a real optimization parameter vector. Thus, the\noptimization problem is studied on the Cartesian product of a set of\nprobability distributions and a set of admissible values of a real parameter\nvector. Three statements on the extremum of a fractional linear integral\nfunctional are proved. It is established that, in all the variants, the\nsolution of the original problem is completely determined by the extremal\nproperties of the test function of the linear-fractional integral functional;\nthis function is the ratio of the integrands of the numerator and the\ndenominator. Possible applications of the results obtained to problems of\noptimal control of stochastic systems are described.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have considered the interaction of the subharmonic light modes with a\nthree-level atom. We have found that the effect of this interaction is to\ndecrease the quadrature squeezing and the mean photon number of the two-mode\ncavity light.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Visual memory schema (VMS) maps show which regions of an image cause that\nimage to be remembered or falsely remembered. Previous work has succeeded in\ngenerating low resolution VMS maps using convolutional neural networks. We\ninstead approach this problem as an image-to-image translation task making use\nof a variational autoencoder. This approach allows us to generate higher\nresolution dual channel images that represent visual memory schemas, allowing\nus to evaluate predicted true memorability and false memorability separately.\nWe also evaluate the relationship between VMS maps, predicted VMS maps, ground\ntruth memorability scores, and predicted memorability scores.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Strong anomalous diffusion phenomena are often observed in complex physical\nand biological systems, which are characterized by the nonlinear spectrum of\nexponents $q\\nu(q)$ by measuring the absolute $q$-th moment $\\langle\n|x|^q\\rangle$. This paper investigates the strong anomalous diffusion behavior\nof a two-state process with L\\'{e}vy walk and Brownian motion, which usually\nserves as an intermittent search process. The sojourn times in L\\'{e}vy walk\nand Brownian phases are taken as power law distributions with exponents\n$\\alpha_+$ and $\\alpha_-$, respectively. Detailed scaling analyses are\nperformed for the coexistence of three kinds of scalings in this system.\nDifferent from the pure L\\'{e}vy walk, the phenomenon of strong anomalous\ndiffusion can be observed for this two-state process even when the distribution\nexponent of L\\'{e}vy walk phase satisfies $\\alpha_+<1$, provided that\n$\\alpha_-<\\alpha_+$. When $\\alpha_+<2$, the probability density function (PDF)\nin the central part becomes a combination of stretched L\\'{e}vy distribution\nand Gaussian distribution due to the long sojourn time in Brownian phase, while\nthe PDF in the tail part (in the ballistic scaling) is still dominated by the\ninfinite density of L\\'{e}vy walk.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Collision avoidance is one of the most primary requirement in the\ndecentralized multiagent navigations: while the agents are moving towards their\nown targets, attentions should be paid to avoid the collisions with the others.\nIn this paper, we introduce the concept of local action cell, which provides\nfor each agent a set of velocities that are safe to perform. Based on the\nrealtime updated local action cells, we propose the LAC-Nav approach to\nnavigate the agent with the properly selected velocity; and furthermore, we\ncoupled the local action cell with an adaptive learning framework, in which the\neffect of selections are evaluated and used as the references for making\ndecisions in the following updates. Through the experiments for three commonly\nconsidered scenarios, we demonstrated the efficiency of the proposed\napproaches, with the comparison to several widely studied strategies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a two-agent game wherein a questioner must be able to conjure\ndiscerning questions between sentences, incorporate responses from an answerer,\nand keep track of a hypothesis state. The questioner must be able to understand\nthe information required to make its final guess, while also being able to\nreason over the game's text environment based on the answerer's responses. We\nexperiment with an end-to-end model where both agents can learn simultaneously\nto play the game, showing that simultaneously achieving high game accuracy and\nproducing meaningful questions can be a difficult trade-off.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep generative models have made tremendous advances in image and signal\nrepresentation learning and generation. These models employ the full Euclidean\nspace or a bounded subset as the latent space, whose flat geometry, however, is\noften too simplistic to meaningfully reflect the manifold structure of the\ndata. In this work, we advocate the use of a multi-chart latent space for\nbetter data representation. Inspired by differential geometry, we propose a\n\\textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem\non its representation capability. We show that the training data size and the\nnetwork size scale exponentially in approximation error with an exponent\ndepending on the intrinsic dimension of the data manifold. CAE admits desirable\nmanifold properties that auto-encoders with a flat latent space fail to obey,\npredominantly proximity of data. We conduct extensive experimentation with\nsynthetic and real-life examples to demonstrate that CAE provides\nreconstruction with high fidelity, preserves proximity in the latent space, and\ngenerates new data remaining near the manifold. These experiments show that CAE\nis advantageous over existing auto-encoders and variants by preserving the\ntopology of the data manifold as well as its geometry.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With ENZO simulations run on the J\\\"ulich supercomputers, we have\ninvestigated the evolution of magnetic fields in the largest cosmic structures\n(namely galaxy clusters and filaments connecting them) with unprecedented\ndynamical range. These simulations revealed the full development of the\nsmall-scale dynamo in Eulerian cosmological magneto-hydrodynamical simulations.\nThe turbulent motions developed during the formation of clusters are energetic\nenough to foster the growth of magnetic fields by several orders of magnitude,\nstarting from weak magnetic fields up strengths of $\\sim \\rm \\mu G$ as\nobserved. Furthermore, shock waves are launched during cluster formation and\nthey are able to accelerate cosmic-ray electrons, that emit in the radio\nwavelengths. Radio observations of this emission provide information on the\nlocal magnetic field strength. We have incorporated, for the first time, the\ncooling of cosmic-ray electrons when modelling this emission. In this\ncontribution, we present our advances in modelling these physical processes.\nHere, we mostly focus on the most interesting object in our sample of galaxy\nclusters, which shows the complexity of magnetic fields and the potential of\nexisting and future multi-wavelengths observations in the study of the weakly\ncollisional plasma on $\\sim$ Megaparsecs scales.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a bibliographic analysis of Chandra, Hubble, and Spitzer\npublications. We find (a) archival data are used in >60% of the publication\noutput and (b) archives for these missions enable a much broader set of\ninstitutions and countries to scientifically use data from these missions.\nSpecifically, we find that authors from institutions that have published few\npapers from a given mission publish 2/3 archival publications, while those with\nmany publications typically have 1/3 archival publications. We also show that\ncountries with lower GDP per capita overwhelmingly produce archival\npublications, while countries with higher GDP per capital produce guest\nobserver and archival publications in equal amounts. We argue that robust\narchives are thus not only critical for the scientific productivity of mission\ndata, but also the scientific accessibility of mission data. We argue that the\nastronomical community should support archives to maximize the overall\nscientific societal impact of astronomy, and represent an excellent investment\nin astronomy's future.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  As an efficient model for knowledge organization, the knowledge graph has\nbeen widely adopted in several fields, e.g., biomedicine, sociology, and\neducation. And there is a steady trend of learning embedding representations of\nknowledge graphs to facilitate knowledge graph construction and downstream\ntasks. In general, knowledge graph embedding techniques aim to learn vectorized\nrepresentations which preserve the structural information of the graph. And\nconventional embedding learning models rely on structural relationships among\nentities and relations. However, in educational knowledge graphs, structural\nrelationships are not the focus. Instead, rich literals of the graphs are more\nvaluable. In this paper, we focus on this problem and propose a novel model for\nembedding learning of educational knowledge graphs. Our model considers both\nstructural and literal information and jointly learns embedding\nrepresentations. Three experimental graphs were constructed based on an\neducational knowledge graph which has been applied in real-world teaching. We\nconducted two experiments on the three graphs and other common benchmark\ngraphs. The experimental results proved the effectiveness of our model and its\nsuperiority over other baselines when processing educational knowledge graphs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we consider approximations of principal component projection\n(PCP) without explicitly computing principal components. This problem has been\nstudied in several recent works. The main feature of existing approaches is\nviewing the PCP matrix as a matrix function. This underlying function is the\ncomposition of a step function with a rational function. To find an approximate\nPCP, the step function is approximated by a polynomial while the rational\nfunction is evaluated by a fast ridge regression solver. In this work, we\nfurther improve this process by replacing the rational function with carefully\nconstructed polynomials of low degree. We characterize the properties of\npolynomials that are suitable for approximating PCP, and establish an\noptimization problem to select the optimal one from those polynomials. We show\ntheoretically and confirm numerically that the resulting approximate PCP\napproach with optimal polynomials is indeed effective for approximations of\nprincipal component projection.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  End-to-end task-oriented dialogue is challenging since knowledge bases are\nusually large, dynamic and hard to incorporate into a learning framework. We\npropose the global-to-local memory pointer (GLMP) networks to address this\nissue. In our model, a global memory encoder and a local memory decoder are\nproposed to share external knowledge. The encoder encodes dialogue history,\nmodifies global contextual representation, and generates a global memory\npointer. The decoder first generates a sketch response with unfilled slots.\nNext, it passes the global memory pointer to filter the external knowledge for\nrelevant information, then instantiates the slots via the local memory\npointers. We empirically show that our model can improve copy accuracy and\nmitigate the common out-of-vocabulary problem. As a result, GLMP is able to\nimprove over the previous state-of-the-art models in both simulated bAbI\nDialogue dataset and human-human Stanford Multi-domain Dialogue dataset on\nautomatic and human evaluation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider \"Hopfological\" techniques as in \\cite{Ko} but for infinite\ndimensional Hopf algebras, under the assumption of being co-Frobenius. In\nparticular, $H=k[{\\mathbb Z}]\\#k[x]/x^2$ is the first example, whose\ncorepresentations category is d.g. vector spaces. Motivated by this example we\ndefine the \"Homology functor\" (we prove it is homological) for any co-Frobenius\nalgebra, with coefficients in $H$-comodules, that recover usual homology of a\ncomplex when $H=k[{\\mathbb Z}]\\#k[x]/x^2$. Another easy example of co-Frobenius\nHopf algebra gives the category of \"mixed complexes\" and we see (by computing\nan example) that this homology theory differs from cyclic homology, although\nthere exists a long exact sequence analogous to the SBI-sequence. Finally,\nbecause we have a tensor triangulated category, its $K_0$ is a ring, and we\nprove a \"last part of a localization exact sequence\" for $K_0$ that allows us\nto compute -or describe- $K_0$ of some family of examples, giving light of what\nkind of rings can be categorified using this techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show that the edit distance between two strings of length $n$ can be\ncomputed within a factor of $f(\\epsilon)$ in $n^{1+\\epsilon}$ time as long as\nthe edit distance is at least $n^{1-\\delta}$ for some $\\delta(\\epsilon) > 0$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Mixing in the $\\Sigma^0$-$\\Lambda^0$ system is a direct consequence of broken\nisospin symmetry and is a measure of both isospin-symmetry breaking as well as\ngeneral SU(3)-flavour symmetry breaking. In this work we present a new scheme\nfor calculating the extent of $\\Sigma^0$-$\\Lambda^0$ mixing using simulations\nin lattice QCD+QED and perform several extrapolations that compare well with\nvarious past determinations. Our scheme allows us to easily contrast the\nQCD-only mixing case with the full QCD+QED mixing.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The pentagonal number theorem is extended to the sequence of the number of\ninteger partitions with all parts equal. The new pentagonal number theorem\nimplies that the distribution of the primes is just a specific detail of the\napplication of the partition function to a sequence that we introduce and\neasily compute in this paper.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving\ntechnology that can enable identification of survivors under collapsed\nbuildings in the aftermath of natural disasters such as earthquakes or gas\nexplosions. However, these UAVs have to be able to autonomously navigate in\ndisaster struck environments and land on debris piles in order to accurately\nlocate the survivors. This problem is extremely challenging as pre-existing\nmaps cannot be leveraged for navigation due to structural changes that may have\noccurred. Furthermore, existing landing site detection algorithms are not\nsuitable to identify safe landing regions on debris piles. In this work, we\npresent a computationally efficient system for autonomous UAV navigation and\nlanding that does not require any prior knowledge about the environment. We\npropose a novel landing site detection algorithm that computes costmaps based\non several hazard factors including terrain flatness, steepness, depth\naccuracy, and energy consumption information. We also introduce a\nfirst-of-a-kind synthetic dataset of over 1.2 million images of collapsed\nbuildings with groundtruth depth, surface normals, semantics and camera pose\ninformation. We demonstrate the efficacy of our system using experiments from a\ncity scale hyperrealistic simulation environment and in real-world scenarios\nwith collapsed buildings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We use an underground counting lab with an extremely low background to\nperform an activity measurement for the $^{12}$C+$^{13}$C system with energies\ndown to $E\\rm_{c.m.}$=2.323 MeV, at which the $^{12}$C($^{13}$C,$p$)$^{24}$Na\ncross section is found to be 0.22(7) nb. The $^{12}$C+$^{13}$C fusion cross\nsection is derived with a statistical model calibrated using experimental data.\nOur new result of the $^{12}$C+$^{13}$C fusion cross section is the first\ndecisive evidence in the carbon isotope systems which rules out the existence\nof the astrophysical S-factor maximum predicted by the phenomenological\nhindrance model, while confirming the rising trend of the S-factor towards\nlower energies predicted by other models, such as CC-M3Y+Rep, DC-TDHF, KNS, SPP\nand ESW. After normalizing the model predictions with our data, a more reliable\nupper limit is established for the $^{12}$C+$^{12}$C fusion cross sections at\nstellar energies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Various quantum systems are considered as the working substance for the\nanalysis of quantum heat cycles and quantum refrigerators. The ongoing\ntechnological challenge is how efficiently can a heat engine convert thermal\nenergy to mechanical work. The seminal work of Carnot has proposed a\nfundamental upper limit-the Carnot limit on the efficiency of the heat engine.\nHowever, the heat engines can be operated beyond the fundamental upper limit by\nexploiting non-equilibrium reservoirs. Here, the change in the space structure\nintroduces the non-equilibrium effect. So, a question arises whether a change\nin the space structure can provide any boost for the quantum engines and\nrefrigerators. The efficiency of the heat cycle and the coefficient of\nperformance (COP) of the refrigerator cycles in the non-commutative space are\nanalyzed here. The efficiency of the quantum heat engines gets a boost with the\nchange in the space structure than the traditional quantum heat engine but the\neffectiveness of the non-commutative parameter is less for the efficiency\ncompared to the COP of the refrigerator. There is a steep boost for the\ncoefficient of performance of the refrigerator cycles for the non-commutative\nspace harmonic oscillator compared to the harmonic oscillator.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Theory surrounding the origin of the dust-laden winds from evolved stars\nremains mired in controversy. Characterizing the formation loci and the dust\ndistribution within approximately the first stellar radius above the surface is\ncrucial for understanding the physics that underlie the mass-loss phenomenon.\nBy exploiting interferometric polarimetry, we derive the fundamental parameters\nthat govern the dust structure at the wind base of a red supergiant. We present\nnear-infrared aperture-masking observations of Betelgeuse in polarimetric mode\nobtained with the NACO/SAMPol instrument. We used both parametric models and\nradiative transfer simulations to predict polarimetric differential visibility\ndata and compared them to SPHERE/ZIMPOL measurements. Using a thin dust shell\nmodel, we report the discovery of a dust halo that is located at only 0.5\nR$_{\\star}$ above the photosphere (i.e. an inner radius of the dust halo of 1.5\nR$_{\\star}$). By fitting the data under the assumption of Mie scattering, we\nestimate the grain size and density for various dust species. By extrapolating\nto the visible wavelengths using radiative transfer simulations, we compare our\nmodel with SPHERE/ZIMPOL data and find that models based on dust mixtures that\nare dominated by forsterite are most favored. Such a close dusty atmosphere has\nprofound implications for the dust formation mechanisms around red supergiants.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We give a purely combinatorial proof for a two-fold generalization of van der\nWaerden-Brauer's theorem and Hindman's theorem. We also give tower bounds for a\nfinite version of it.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The most prevalent scope of interest for OCR applications used to be scanned\ndocuments, but it has now shifted towards the natural scene. Despite the change\nof times, the existing evaluation methods are still based on the old criteria\nsuited better for the past interests. In this paper, we propose PopEval, a\nnovel evaluation approach for the recent OCR interests. The new and past\nevaluation algorithms were compared through the results on various datasets and\nOCR models. Compared to the other evaluation methods, the proposed evaluation\nalgorithm was closer to the human's qualitative evaluation than other existing\nmethods. Although the evaluation algorithm was devised as a character-level\napproach, the comparative experiment revealed that PopEval is also compatible\non existing benchmark datasets annotated at word-level. The proposed evaluation\nalgorithm is not only applicable to current end-to-end tasks, but also suggests\na new direction to redesign the evaluation concept for further OCR researches.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Motivated by recent quantum gas microscope experiments for fermions in\noptical lattices, we present proof of principle calculations showing that it is\npossible to obtain the complete information about the quantum state on a small\nsubsystem from equilibrium determinantal quantum Monte Carlo simulations. Both\ndiagonal (in the occupation number basis) and off-diagonal elements of the\nreduced density matrix are calculated for a square plaquette, which is embedded\nin a much larger system of the two-dimensional Hubbard model, both at half\nfilling and in the doped case. The diagonalization of the reduced density\nmatrix is done by exploiting the point group symmetry and particle number\nconservation, which allows to attach symmetry labels to its eigenvalues.\nKnowledge of the probabilities of plaquette occupation number configurations is\nuseful for meticulous benchmarking of quantum gas microscope experiments. As\nthe quantum state on the plaquette is exact and self-consistently embedded in\nan exact, correlated bath, the present approach connects to various cluster\napproximation techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Large-scale mobile edge computing (MEC) systems require scalable solutions to\nallocate communication and computing resources to the users. In this letter we\naddress this challenge by applying dynamic spectrum sharing among the base\nstations (BSs), together with local resource allocation in the cells. We show\nthat the network-wide resource allocation can be transformed into a convex\noptimization problem, and propose a distributed, hierarchical solution with\nlimited information exchange among the BSs. Numerical results demonstrate that\nthe proposed solution is superior to other baseline algorithms, when wireless\nand computing resource allocation is not jointly optimized, or the wireless\nresources allocated to the BSs are fixed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we present and verify the non-linear simulation of an\naspherical adaptive lens based on a piezo-glass sandwich membrane with combined\nbending and buckling actuation. To predict the full non-linear piezoelectric\nbehavior, we measured the non-linear charge coefficient, hysteresis and creep\neffects of the piezo material and inserted them into the FEM model using a\nvirtual electric field. We further included and discussed the fabrication\nparameters -- glue layers and thermal stress -- and their variations. To verify\nour simulations, we fabricated and measured a set of lenses with different\ngeometries, where we found good agreement and show that their qualitative\nbehavior is also well described by a simple analytical model. We finally\ndiscuss the effects of the geometry on the electric response and find, e.g., an\nincreased focal power range from $\\pm 4.5$ to $\\pm 9\\, \\mathrm{m}^{-1}$ when\nchanging the aperture from $14$ to $10\\, \\mathrm{mm}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Lecture Notes for Les Houches 2018 Summer School on Active Matter and\nNonequilibrium Statistical Physics. These notes give an introduction to\nstatistical field theories of active matter. After showing an example of how\nsuch theories can be created bottom-up by explicit coarse graining of active\nparticle models, the main approach taken here is top-down: minimal terms are\nadded to well-studied stochastic field theories of passive systems that break\ntime-reversibility and thereby cause nonzero steady-state entropy production.\nThese theories are, so far, rather poorly understood compared to their passive\ncounterparts.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report systematical studies of a new quasi-one-dimensional (1D) compound\nBa3TiTe5 and the high-pressure induced superconductivity therein. Ba3TiTe5 was\nsynthesized at high pressure and high temperature. It crystallizes into a\nhexagonal structure (P63/mcm), which consists of infinite face-sharing\noctahedral TiTe6 chains and Te chains along the c axis, exhibiting a strong 1D\ncharacteristic structure. The first-principles calculations demonstrate that\nBa3TiTe5 is a well-defined 1D conductor and thus, it can be considered a\nstarting point to explore the exotic physics induced by pressure via enhancing\nthe interchain hopping to move the 1D conductor to a high dimensional metal.\nFor Ba3TiTe5, high-pressure techniques were employed to study the emerging\nphysics dependent on interchain hopping, such as the Umklapp scattering effect,\nspin/charge density wave (SDW/CDW), superconductivity and non-Fermi Liquid\nbehavior. Finally, a complete phase diagram was plotted. The superconductivity\nemerges from 8.8 GPa, near which the Umklapp gap is mostly suppressed. Tc is\nenhanced and reaches the maximum ~6 K at about 36.7 GPa, where the spin/charge\ndensity wave (SDW/CDW) is completely suppressed, and a non-Fermi Liquid\nbehavior appears. Our results suggest that the appearance of superconductivity\nis associated with the fluctuation due to the suppression of Umklapp gap and\nthe enhancement of Tc is related with the fluctuation of the SDW/CDW.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The high spectral efficiency of massive MIMO (Multiple Input Multiple Output)\nis mainly achieved through the exploitation of spatial multiplexing, i.e. by\nusing a high number of MIMO layers that are applied simultaneously to many\nusers. The power consumption of a massive MIMO base station is determined by\nthe hardware driving a high number of antenna ports and elements. This paper\nfocuses on practical deployment situations with varying user load. During hours\nwith low number of users a certain significant part of hardware power\nconsumption would remain with conventional massive MIMO processing, while the\nfull potential of spectral efficiency cannot be exploited due to the low number\nof users, resulting in low power efficiency and cost. We investigate the impact\nof different hybrid array architectures on spectral efficiency, average user\nthroughput and power consumption and show how to design a massive MIMO system\nwith significantly improved energy efficiency for a given target scenario,\nwhile maintaining a targeted service quality.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  By exploiting the Fueter theorem, we give new formulas to compute zonal\nharmonic functions in any dimension. We first give a representation of them as\na result of a suitable ladder operator acting on the constant function equal to\none. Then, inspired by recent work of A. Perotti, using techniques from slice\nregularity, we derive explicit expressions for zonal harmonics starting from\nthe 2 and 3 dimensional cases. It turns out that all zonal harmonics in any\ndimension are related to the real part of powers of the standard Hermitian\nproduct in $\\mathbb{C}$. At the end we compare formulas, obtaining interesting\nequalities involving the real part of positive and negative powers of the\nstandard Hermitian product. In the two appendices we show how our computations\nare optimal compared to direct ones.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a method that quantifies the importance, namely relevance, of\naudio segments for classification in weakly-labelled problems. It works by\ndrawing information from a set of class-wise one-vs-all classifiers. By\nselecting the classifiers used in each specific classification problem, the\nrelevance measure adapts to different user-defined viewpoints without requiring\nadditional neural network training. This characteristic allows the relevance\nmeasure to highlight audio segments that quickly adapt to user-defined\ncriteria. Such functionality can be used for computer-assisted audio analysis.\nAlso, we propose a neural network architecture, namely RELNET, that leverages\nthe relevance measure for weakly-labelled audio classification problems. RELNET\nwas evaluated in the DCASE2018 dataset and achieved competitive classification\nresults when compared to previous attention-based proposals.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Online retailers execute a very large number of price updates when compared\nto brick-and-mortar stores. Even a few mis-priced items can have a significant\nbusiness impact and result in a loss of customer trust. Early detection of\nanomalies in an automated real-time fashion is an important part of such a\npricing system. In this paper, we describe unsupervised and supervised anomaly\ndetection approaches we developed and deployed for a large-scale online pricing\nsystem at Walmart. Our system detects anomalies both in batch and real-time\nstreaming settings, and the items flagged are reviewed and actioned based on\npriority and business impact. We found that having the right architecture\ndesign was critical to facilitate model performance at scale, and business\nimpact and speed were important factors influencing model selection, parameter\nchoice, and prioritization in a production environment for a large-scale\nsystem. We conducted analyses on the performance of various approaches on a\ntest set using real-world retail data and fully deployed our approach into\nproduction. We found that our approach was able to detect the most important\nanomalies with high precision.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Optical spectroscopy of atoms and molecules is a field where one usually\noperates very far from thermal equilibrium conditions. A prominent example is\nspectroscopy of thin vapors, where the pump irradiation leads to a non\nequilibrium distribution within the electronic structure that is well shielded\nfrom the environment. Here we describe experimental work investigating\nabsorption and emission lines of rubidium vapor subject to a noble buffer gas\nenvironment with pressure 100 to 200 bar, a regime interpolating between usual\ngas phase and liquid solid state conditions. Frequent elastic collisions in the\ndense buffer gas sample cause a large coupling to the environment. We give a\ndetailed account of recent observations of the Kennard Stepanov scaling, a\nBoltzmann like thermodynamic frequency scaling between absorption and emission\nprofiles, for both atomic and molecular rubidium species in the gaseous\nenvironment. Our observations are interpreted as due to the thermalization of\nalkali noble gas submanifolds in both ground and electronically excited states\nrespectively. Both pressure broadening and shift of the high pressure buffer\ngas D lines system are determined. We also discuss some prospects, including\npossible advances in collisional laser cooling and optical thermometry.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Primordial black holes (PBHs) could provide the dark matter but a variety of\nconstraints restrict the possible mass windows to $10^{16} - 10^{17}$g,\n$10^{20} - 10^{24}$g and $10 - 10^3M_{\\odot}$. The last possibility is of\nspecial interest in view of the recent detection of black-hole mergers by LIGO.\nPBHs larger than $10^3 M_{\\odot}$ might have important cosmological\nconsequences even if they have only a small fraction of the dark matter\ndensity. In particular, they could generate cosmological structures either\nindividually through the \"seed\" effect or collectively through the \"Poisson\"\neffect, thereby alleviating some problems associated with the standard cold\ndark matter scenario.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We perform a post-Newtonian (PN) solar system analysis for Palatini $f(R)$\ntheories considering finite volume non-spherical planets and with emphasis to\n$f(R)$ functions that are analytical about $R=0$. First we consider the\nWill-Nordtvedt parametrized post-Newtonian (PPN) formalism, from which the\nmetric is shown to depend, in general, on terms not covered by the standard PPN\npotentials. Hence, a full analysis of the PN equations of motion is performed.\nFrom the latter we conclude that, apart from redefinitions on the internal\nenergy and the pressure, which cannot be constrained by solar system tests, the\ncenter-of-mass orbits are the same as in general relativity. We discuss further\nthe physics of these redefinitions and use an argument to extend our analytical\n$f(R)$ results towards some non-analytical functions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Most of the current game-theoretic demand-side management methods focus\nprimarily on the scheduling of home appliances, and the related numerical\nexperiments are analyzed under various scenarios to achieve the corresponding\nNash-equilibrium (NE) and optimal results. However, not much work is conducted\nfor academic or commercial buildings. The methods for optimizing\nacademic-buildings are distinct from the optimal methods for home appliances.\nIn my study, we address a novel methodology to control the operation of\nheating, ventilation, and air conditioning system (HVAC). With the development\nof Artificial Intelligence and computer technologies, reinforcement learning\n(RL) can be implemented in multiple realistic scenarios and help people to\nsolve thousands of real-world problems. Reinforcement Learning, which is\nconsidered as the art of future AI, builds the bridge between agents and\nenvironments through Markov Decision Chain or Neural Network and has seldom\nbeen used in power system. The art of RL is that once the simulator for a\nspecific environment is built, the algorithm can keep learning from the\nenvironment. Therefore, RL is capable of dealing with constantly changing\nsimulator inputs such as power demand, the condition of power system and\noutdoor temperature, etc. Compared with the existing distribution power system\nplanning mechanisms and the related game theoretical methodologies, our\nproposed algorithm can plan and optimize the hourly energy usage, and have the\nability to corporate with even shorter time window if needed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Word embeddings such as ELMo have recently been shown to model word semantics\nwith greater efficacy through contextualized learning on large-scale language\ncorpora, resulting in significant improvement in state of the art across many\nnatural language tasks. In this work we integrate acoustic information into\ncontextualized lexical embeddings through the addition of multimodal inputs to\na pretrained bidirectional language model. The language model is trained on\nspoken language that includes text and audio modalities. The resulting\nrepresentations from this model are multimodal and contain paralinguistic\ninformation which can modify word meanings and provide affective information.\nWe show that these multimodal embeddings can be used to improve over previous\nstate of the art multimodal models in emotion recognition on the CMU-MOSEI\ndataset.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This activity was created within the framework of the \"Space for Education\"\nproject, which aims at experiencing physical principles on the basis of topics\nrelated to space travel. This work enables the students to understand how a\nrocket brings crews into the orbit of the International Space Station. Since\nthe way from the simpler basics to a real representation of a rocket flight is\nquite complex and requires knowledge from several class levels, the current\npaper limits itself to the introduction of basic concepts and simple, idealized\napplications. The rocket equation is of central importance. Further\nderivations, which deal with multi-stage rockets, are dealt with in a separate\nwork. To introduce the principle of recoil, a few examples are briefly\npresented. Additional materials at:\nhttps://www.haus-der-astronomie.de/raum-fuer-bildung\n  -----\n  Diese Aktivit\\\"at wurde im Rahmen des Projekts \"Raum f\\\"ur Bildung\" erstellt,\nwelches physikalische Prinzipien anhand der Raumfahrt erlebbar macht. Diese\nAusarbeitung erm\\\"oglicht den Sch\\\"ulerinnen und Sch\\\"ulern nachzuempfinden,\nwie eine Rakete Besatzungen auf den Orbit der Internationalen Raumstation\nbringt. Da der Weg von den einfacheren Grundlagen hin zu einer realen\nDarstellung eines Raketenflugs recht komplex ist und Kenntnisse aus mehreren\nKlassenstufen ben\\\"otigt, beschr\\\"ankt sich die aktuelle Ausarbeitung auf die\nEinf\\\"uhrung der Grundbegriffe und einfachen, idealisierten Anwendungen.\nZentrale Bedeutung hat dabei die Raketengleichung. Weiterf\\\"uhrende\nAbleitungen, die mehrstufige Raketen thematisieren, werden in einer gesonderten\nAusarbeitung behandelt. Zur Einleitung in das Prinzip des R\\\"ucksto{\\ss}es\nwerden kurz einige Beispiele vorgestellt. Weitere Materialien unter:\nhttps://www.haus-der-astronomie.de/raum-fuer-bildung\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Given a variation of Hodge structures on a quasi-projective base $S$, whose\ngeneric Mumford-Tate group is non-product, we prove that the (countable) union\nof positive components of the Hodge locus is either an algebraic subvariety of\n$S$, or is Zariski-dense in $S$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Mass-spring model simulations are used to investigate past spin states of a\nviscoelastic Phobos and Deimos. From an initially tidally locked state, we find\ncrossing of a spin-orbit resonance with Mars or a mean motion resonance with\neach other does not excite tumbling in Phobos or Deimos. However, once tumbling\nour simulations show that these moons can remain so for an extended period and\nduring this time their orbital eccentricity can be substantially reduced. We\nattribute the tendency for simulations of an initially tumbling viscoelastic\nbody to drop into spin-synchronous state at very low eccentricity to the\ninsensitivity of the tumbling chaotic zone volume to eccentricity. After a\ntumbling body enters the spin synchronous resonance, it can exhibit long lived\nnon-principal axis rotation and this too can prolong the period of time with\nenhanced tidally generated energy dissipation. The low orbital eccentricities\nof Phobos and Deimos could in part be due to spin excitation by nearly\ncatastrophic impacts rather than tidal evolution following orbital resonance\nexcitation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The thickness dependence of spin-orbit torque and magnetoresistance in\nferromagnet/heavy-metal bilayers is studied using the first-principles\nnon-equilibrium Green's function formalism combined with the Anderson disorder\nmodel. A systematic expansion in orthogonal vector spherical harmonics is used\nfor the angular dependence of the torque. The damping-like torque in Co/Pt and\nCo/Au bilayers can be described as a sum of the spin-Hall contribution, which\nincreases with thickness in agreement with the spin-diffusion model, and a\ncomparable interfacial contribution. The magnetoconductance in the plane\nperpendicular to the current in Co/Pt bilayers is of the order of a conductance\nquantum per interfacial atom, exceeding the prediction of the spin-Hall model\nby more than an order of magnitude. This suggests that the \"spin-Hall\nmagnetoresistance,\" similarly to the damping-like torque, has a large\ninterfacial contribution unrelated to the spin-Hall effect.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Active galactic Nuclei (AGNs) with their relativistic jets pointed towards\nthe observer, form a subclass of luminous gamma-ray sources commonly known as\nblazars. The study of blazars is essential to improve our understanding on the\nAGNs emission mechanisms and evolution, as well as to map the extragalactic\nbackground light. To do so, however, one needs to correctly classify and\nmeasure a redshift for a large sample of these sources. The Third {\\it\nFermi}--LAT Catalog of High-Energy Sources (3FHL) contains $\\approx1160$\nblazars reported at energies greater than $10$\\,GeV. However $\\sim$25\\% of\nthese sources are unclassified and $\\sim$50\\% lack of redshift information. To\nincrease the spectral completeness of the 3FHL catalog, we are working on an\noptical spectroscopic follow up campaign using 4--m and 8--m telescopes. In\nthis paper, we present the results of the second part of this campaign, where\nwe observed 23 blazars using the 4\\,$m$ telescope at CTIO in Chile. We report\nall the 23 sources to be classified as BL Lacs, a confirmed redshift\nmeasurement for 3 sources, a redshift lower limit for 2 sources and a tentative\nredshift measurement for 3 sources.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently Mertens and Moore [arXiv:1909.01484v1] showed that site percolation\n\"is odd.\" By this they mean that on an $M\\times N$ square lattice the number of\ndistinct site configurations that allow for vertical percolation is odd. We\nreport here an alternative proof, based on recursive use of geometric symmetry,\nfor both free and periodic boundary conditions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The linear stability of granular gas that reflects the contribution of\nself-gravitational force of mass density perturbations is investigated in order\nto clarify the condition of competition between clustering instability and\nJeans instability. It is found that the condition depends on three parameters:\nthe mass density $\\rho_0$, the collision rate $\\omega_0$, and the rate of\nenergy loss per collision $\\epsilon$. When $\\sqrt{G\\rho_0}\\ll\\epsilon\\omega_0$,\nclustering instability dominates, while when\n$\\epsilon\\omega_0\\ll\\sqrt{G\\rho_0}$, Jeans instability dominates. These\ninstabilities are characterized by the decrease and increase, respectively, of\nthe temperature.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We treat the $\\tau$-tilting finiteness of those minimal\nrepresentation-infinite (min-rep-infinite) algebras which are non-distributive.\nBuilding upon the new results of Bongartz, we fully determine which algebras in\nthis family are $\\tau$-tilting finite and which ones are not. This complements\nour previous work in which we carried out a similar analysis for the\nmin-rep-infinite biserial algebras. Consequently, we obtain nontrivial explicit\nsufficient conditions for $\\tau$-tilting infiniteness of a large family of\nalgebras. This also produces concrete families of \"minimal $\\tau$-tilting\ninfinite algebras\"-- the modern counterpart of min-rep-infinite algebras,\nindependently introduced by the author and Wang.\n  We further use our results on the family of non-distributive algebras to\nestablish a conjectural connection between the $\\tau$-tilting theory and two\ngeometric notions in the study of module varieties introduced by Chindris,\nKinser and Weyman. We verify the conjectures for the algebras studied in this\nnote: For the min-rep-infinite algebras which are non-distributive or biserial,\nwe show that if $\\Lambda$ has the dense orbit property, then it must be\n$\\tau$-tilting finite. Moreover, we prove that such an algebra is\nSchur-representation-finite if and only if it is $\\tau$-tilting finite. The\nlatter result gives a categorical interpretation of\nSchur-representation-finiteness over this family of min-rep-infinite algebras.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Face recognition technology has advanced rapidly and has been widely used in\nvarious applications. Due to the extremely huge amount of data of face images\nand the large computing resources required correspondingly in large-scale face\nrecognition tasks, there is a requirement for a face image compression approach\nthat is highly suitable for face recognition tasks. In this paper, we propose a\ndeep convolutional autoencoder compression network for face recognition tasks.\nIn the compression process, deep features are extracted from the original image\nby the convolutional neural networks to produce a compact representation of the\noriginal image, which is then encoded and saved by existing codec such as PNG.\nThis compact representation is utilized by the reconstruction network to\ngenerate a reconstructed image of the original one. In order to improve the\nface recognition accuracy when the compression framework is used in a face\nrecognition system, we combine this compression framework with a existing face\nrecognition network for joint optimization. We test the proposed scheme and\nfind that after joint training, the Labeled Faces in the Wild (LFW) dataset\ncompressed by our compression framework has higher face verification accuracy\nthan that compressed by JPEG2000, and is much higher than that compressed by\nJPEG.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the intelligent reflecting surface (IRS)-enhanced wireless communication\nsystem, channel state information (CSI) is of paramount importance for\nachieving the passive beamforming gain of IRS, which, however, is a practically\nchallenging task due to its massive number of passive elements without\ntransmitting/receiving capabilities. In this letter, we propose a practical\ntransmission protocol to execute channel estimation and reflection optimization\nsuccessively for an IRS-enhanced orthogonal frequency division multiplexing\n(OFDM) system. Under the unit-modulus constraint, a novel reflection pattern at\nthe IRS is designed to aid the channel estimation at the access point (AP)\nbased on the received pilot signals from the user, for which the channel\nestimation error is derived in closed-form. With the estimated CSI, the\nreflection coefficients are then optimized by a low-complexity algorithm based\non the resolved strongest signal path in the time domain. Simulation results\ncorroborate the effectiveness of the proposed channel estimation and reflection\noptimization methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We experimentally investigate spectral statistics in Anderson localization in\ntwo-dimensional amorphous disordered media. Intensity distributions captured\nover an ultrabroad wavelength range of $\\sim 600$~nm and averaged over numerous\nconfigurations provided the Ioffe-Regel parameter to be $\\sim2.5$ over the\ninvestigated wavelength range. The spectra of the disordered structures\nprovided access to several quasimodes, whose widths and separations allowed to\ndirectly estimate the optical Thouless conductance $g_{Th}$, consistently\nobserved to be below unity. The probability distribution of $g_{Th}$ was\nmeasured to be a log-normal. Despite being in the Anderson localization regime,\nthe spacings of energy levels of the system was seen to follow a near\nWigner-Dyson function. Theoretical calculations based on the tight-binding\nmodel, modified to include coupling to a bath, yielded results that were in\nexcellent agreement with experiments. From the model, the level-spacing\nbehavior was attributed to the degree of localization obtained in the optical\ndisordered system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We analyse the 13 TeV TOTEM data on elastic proton-proton scattering through\na thorough statistical analysis, and obtain that $\\rho=0.096\\pm 0.006$ and\n$\\sigma_{tot}=(107.5\\pm 1.5)$ mb. Theoretical errors could lower the cross\nsection by about 2 mb and increase $\\rho$ by about 0.002. We also show that\nthese results do not imply the existence of an odderon at $t=0$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This thesis introduces a novel quadrotor manipulation system that consists of\n2-link manipulator attached to the bottom of a quadrotor. This new system\npresents a solution for the drawbacks found in the current quadrotor\nmanipulation system which uses a gripper fixed to a quadrotor. Unlike the\ncurrent system, the proposed system has a 6-DOF, and it provides enough\ndistance between the quadrotor and the object. System kinematics and dynamics\nare derived. To study the feasibility of the proposed system, a quadrotor with\nhigh enough payload to add the 2-link manipulator is constructed. Its\nparameters are identified to be used in the simulation and controller design. A\nCAD model is developed to calculate the mass and moments of inertia in an\naccurate way. Direct relationships between Pulse Width Modulation and each of\nthe angular speeds, thrust forces, and drag moments of the rotors are\nidentified. A Direction Cosine Matrix complementary filter is used to estimate\nthe attitude of the quadrotor using the IMU measurements. Attitude\nstabilization controller is designed based on feedback linearization technique\nto test the identified parameters and the attitude estimation. The results of\nthe experiments show satisfactory accuracy of the identified structure\nparameters, the identified rotor assembly parameters, and the attitude\nestimation algorithm. A controller for the proposed system is designed based on\nthree control techniques: feedback linearization based PID control, direct\nfuzzy logic control, and fuzzy model reference learning control. These\ncontrollers are tested to provide system stability and trajectory tracking\nunder the effect of picking and placing a payload and the effect of changing\nthe operating region. Simulation results show that the fuzzy model reference\nlearning control technique has superior performance. The results indicate the\nfeasibility of the proposed system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We construct a quadratic basis of generators of matrix-extended\n$\\mathcal{W}_{1+\\infty}$ using a generalization of the Miura transformation.\nThis makes it possible to conjecture a closed-form formula for the operator\nproduct expansions defining the algebra. We study truncations of the algebra.\nAn explicit calculation at low levels shows that these are parametrized in a\nway consistent with the gluing description of the algebra. It is perhaps\nsurprising that in spite of the fact that the algebras are rather complicated\nand non-linear, the structure of their truncations follows very simple gluing\nrules.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The ability to control strongly interacting light quanta (photons) is of\ncentral importance in quantum science and engineering. Recently it was shown\nthat such strong interactions can be engineered in specially prepared quantum\noptical systems. Here, we demonstrate a method for coherent control of strongly\ninteracting photons, extending quantum nonlinear optics into the domain of\nrepulsive photons. This is achieved by coherently coupling photons to several\natomic states, including strongly interacting Rydberg levels in a cold Rubidium\ngas. Using this approach we demonstrate both repulsive and attractive\ninteractions between individual photons and characterize them by the measured\ntwo- and three-photon correlation functions. For the repulsive case, we\ndemonstrate signatures of interference and self ordering from three-photon\nmeasurements. These observations open a route to study strongly interacting\ndissipative systems and quantum matter composed of light such as a crystal of\nindividual photons.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider extensions of quasiconformal maps and the uniformization theorem\nto the setting of metric spaces $X$ homeomorphic to $\\mathbb R^2$. Given a\nmeasure $\\mu$ on such a space, we introduce $\\mu$-quasiconformal maps $f:X \\to\n\\mathbb R^2$, whose definition involves deforming lengths of curves by $\\mu$.\nWe show that if $\\mu$ is an infinitesimally metric measure, i.e., it satisfies\nan infinitesimal version of the metric doubling measure condition of David and\nSemmes, then such a $\\mu$-quasiconformal map exists. We apply this result to\ngive a characterization of the metric spaces admitting an infinitesimally\nquasisymmetric parametrization.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Although Massive Online Open Courses (MOOCs) have the promise to make\nrigorous higher education accessible to everyone, prior research has shown that\nregistrants tend to come from backgrounds of higher socioeconomic status. In\nthis work, I study geographically granular economic patterns in registration\nfor HarvardX and MITx courses, and in the accuracy of identifying users'\nlocations from their IP addresses. Using ZIP Codes identified by the MaxMind IP\ngeolocation database, I find that per-capita registration rates correlate with\neconomic prosperity and population density. Comparing these ZIP Codes with\nuser-provided mailing addresses, I find evidence of bias in MaxMind\ngeolocation: it makes greater errors, both geographically and economically, for\nusers from more economically distressed areas; it disproportionately geolocates\nusers to prosperous areas; and it underestimates the regressive pattern in MOOC\nregistration. Similar economic biases may affect IP geolocation in other\nacademic, commercial, and legal contexts.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Current instance segmentation methods can be categorized into\nsegmentation-based methods that segment first then do clustering, and\nproposal-based methods that detect first then predict masks for each instance\nproposal using repooling. In this work, we propose a one-stage method, named\nEmbedMask, that unifies both methods by taking advantages of them. Like\nproposal-based methods, EmbedMask builds on top of detection models making it\nstrong in detection capability. Meanwhile, EmbedMask applies extra embedding\nmodules to generate embeddings for pixels and proposals, where pixel embeddings\nare guided by proposal embeddings if they belong to the same instance. Through\nthis embedding coupling process, pixels are assigned to the mask of the\nproposal if their embeddings are similar. The pixel-level clustering enables\nEmbedMask to generate high-resolution masks without missing details from\nrepooling, and the existence of proposal embedding simplifies and strengthens\nthe clustering procedure to achieve high speed with higher performance than\nsegmentation-based methods. Without any bells and whistles, EmbedMask achieves\ncomparable performance as Mask R-CNN, which is the representative two-stage\nmethod, and can produce more detailed masks at a higher speed. Code is\navailable at github.com/yinghdb/EmbedMask.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show that individual (single-epoch) spectra of AGN can constrain some of\nthe geometry and dynamics of the AGN broad line region. Studies of the cosmic\ninfluence of supermassive black holes are limited by the current large\nuncertainties in the determination of black hole masses. One dominant\nlimitation is the unknown geometry, dynamics and line-of-sight inclination of\nthe broad line region, used to probe the central black hole mass. Recent\nprogress has been made to constrain the spatial and kinematic structure of the\nbroad line region using dynamical modelling of AGN monitoring data and an\nunderlying physical model for the broad line region. In this work we test the\nability of a modified version of this dynamical modelling code to constrain the\nbroad line region structure using single-epoch spectra. We test our modelling\ncode on single-epoch spectra of nearby Arp 151 by comparing our results with\nthose obtained with monitoring data of this same object. We find that a\nsignificant fraction of the broad line region parameters can indeed be\nadequately constrained, with uncertainties that are comparable to, or at most a\nfactor of ~ a few higher than those obtained from modelling of monitoring data.\nConsidering the wealth of available single-epoch spectroscopic observations,\nthis method is promising for establishing the overall AGN population trends in\nthe geometry and dynamics of the broad line region. This method can be applied\nto spectra of AGN at low and high redshift making it valuable for studies of\ncosmological black hole and AGN evolution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this contribution, the eikonal approximation developed by Roy Glauber to\ndescribe high-energy quantum collisions is presented. This approximation has\nbeen-and still is-extensively used to analyse reaction measurements performed\nto study the structure of nuclei far from stability. This presentation focuses\nmore particularly on the application of the eikonal approximation to the study\nof halo nuclei in modern nuclear physics. To emphasise Roy Glauber's legacy in\ntoday's nuclear physics, recent extensions of this model are reviewed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Graph analytics are at the heart of a broad range of applications such as\ndrug discovery, page ranking, and recommendation systems. When graph size\nexceeds memory size, out-of-core graph processing is needed. For the widely\nused external memory graph processing systems, accessing storage becomes the\nbottleneck. We make the observation that nearly all graph algorithms have a\ndynamically varying number of active vertices that must be processed in each\niteration. However, existing graph processing frameworks, such as GraphChi,\nload the entire graph in each iteration even if a small fraction of the graph\nis active. This limitation is due to the structure of the data storage used by\nthese systems. In this work, we propose to use a compressed sparse row (CSR)\nbased graph storage that is more amenable for selectively loading only a few\nactive vertices in each iteration. But CSR based systems suffers from random\nupdate propagation to many target vertices. To solve this challenge, we propose\nto use a multi-log update mechanism that logs updates separately, rather than\ndirectly update the active edges in a graph. Our proposed multi-log system\nmaintains a separate log per each vertex interval. This separation enables us\nto efficiently process each vertex interval by just loading the corresponding\nlog. Further, while accessing SSD pages with fewer active vertex data, we\nreduce the read amplification due to the page granular accesses in SSD by\nlogging the active vertex data in the current iteration and efficiently reading\nthe log in the next iteration. Over the current state of the art out-of-core\ngraph processing framework, our PartitionedVC improves performance by up to\n$17.84\\times$, $1.19\\times$, $1.65\\times$, $1.38\\times$, $3.15\\times$, and\n$6.00\\times$ for the widely used bfs, pagerank, community detection, graph\ncoloring, maximal independent set, and random-walk applications, respectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove a definitive theorem on the asymptotic stability of point vortex\nsolutions to the full Euler equation in 2 dimensions. More precisely, we show\nthat a small, Gevrey smooth, and compactly supported perturbation of a point\nvortex leads to a global solution of the Euler equation in 2D, which converges\nweakly as $t\\to\\infty$ to a radial profile with respect to the vortex. The\nposition of the point vortex, which is time dependent, stabilizes rapidly and\nbecomes the center of the final, radial profile. The mechanism that leads to\nstabilization is mixing and inviscid damping.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper considers robust modeling of the survival time for cancer\npatients. Accurate prediction can be helpful for developing therapeutic and\ncare strategies. We propose a unified Expectation-Maximization approach\ncombined with the L1-norm penalty to perform variable selection and obtain\nparameter estimation simultaneously for the accelerated failure time model with\nright-censored survival data. Our approach can be used with general loss\nfunctions, and reduces to the well-known Buckley-James method when the\nsquared-error loss is used without regularization. To mitigate the effects of\noutliers and heavy-tailed noise in the real application, we advocate the use of\nrobust loss functions under our proposed framework. Simulation studies are\nconducted to evaluate the performance of the proposed approach with different\nloss functions, and an application to an ovarian carcinoma study is provided.\nMeanwhile, we extend our approach by incorporating the group structure of\ncovariates.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The bulk of the star-formation rate density peak at cosmic noon was obscured\nby dust. How accurately we can assess the role of dust obscured star-formation\nis affected by inherent biases in our empirical methods -- both those that rely\non direct dust emission and those that rely on the inferred dust attenuation of\nstarlight. We use a library of hydrodynamic simulations with radiative transfer\nto explore these biases. We find that for IR luminous galaxies that are in\nrapidly quenching systems (e.g. post-coalescence) standard luminosity-to-SFR\nrelations can strongly overestimate the true SFRs. We propose using the\n$L_{IR}/L_{1.6}$ color to both help identify such systems and provide more\naccurate SFRs. Conversely, we find that the diagnostic UVJ plot misidentifies a\nsubset of dusty star-forming galaxies. This is due to variability in the\neffective attenuation curves including being much grayer in the\noptical-to-near-IR regime than the Calzetti starburst law. This is in agreement\nwith recent observations of IR-selected galaxies at cosmic noon. Our results\nsupport the view that we need a panchromatic approach from the rest-frame UV\nthrough the IR and SED modeling that includes realistic SFHs and allows for\nvariable attenuation curves if we want to fully account for dust obscured\nstar-formation across the epochs of greatest galaxy build-up.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Statistical system models provide the basis for the examination of various\nsorts of distributions. Classification distributions are a very common and\nversatile form of statistics in e.g. real economic, social, and IT systems. The\nstatistical distributions of classification features can be applied in\ndetermining the a priori probabilities in Bayesian networks. We investigate a\nstatistical model of classification distributions based on finding the critical\npoint of a specialized form of entropy. A distribution function for\nclassification features is derived, with the two parameters $n_0$, minimal\nclass, and $\\bar{N}$, average number of classes. Efficient algorithms for the\ncomputation of the class probabilities and the approximation of real frequency\ndistributions are developed and applied to examples from different domains. The\nmethod is compared to established distributions like Zipf's law. The majority\nof examples can be approximated with a sufficient quality ($3-5\\%$).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We find constraints on the highest scale of symmetry breaking of a model with\ngauge symmetry $SU(3)_C \\otimes SU(3)_L \\otimes U(1)_X$ with heavy neutral\nleptons in the fermion triplets, calculating the anomalous magnetic moment of\nthe muon and using results of the relic abundance of dark matter and\nexperiments searching for its direct detection.\n  In order to do this, we have calculated the one-loop contribution of new\nparticles in the model to $(g-2)_{\\mu}$, finding a favoured region for the\nscale at which $SU(3)_L$ is broken, and we have found lower bounds for this\nscale making a comparison of the predictions for the detection of a fermion\ndark matter candidate in the model in terms of simplified dark matter models,\nidentifying the dominant portal for its interactions with standard model\nparticles, and using constraints for the relic abundance and spin-independent\nscattering cross section of the fermion candidate with protons.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The main goal of this work is to accurately reproduce the structural\nproperties of attractive systems modelled by hard-sphere plus square-well\n(HS+SW) interaction potential. Based on the optimized random phase\napproximation (ORPA), the attractive part of the interaction potential is\ntreated as a perturbation of the hard-sphere term. We are able to obtain an\nanalytical expression for the structure factor $ S \\left( k \\right) $ which\nreproduces the low density limit. The microscopical structure of the fluid\nphase of several SW fluids is computed and compared with Monte Carlo (MC)\nsimulation results showing that the structure factor is well reproduced in a\nwide range of wave vectors, in addition, the contact and discontinuity values\nof the radial distribution function are found to be in good agreement.\nAdditionally, we compute the pressure equation of state and perform a\nquantitative analysis comparing with simulation results found that in a large\nset of densities and temperatures our approach outperform its linear form.\nFurthermore, we show that the theoretical approach developed in this study\nworks very well for many thermodynamic states leading us a versatile and\nconfident tool to systematic compute the structure and thermodynamics of SW\nfluids.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We perform an unmodeled search for persistent, directional gravitational wave\n(GW) sources using data from the first and second observing runs of Advanced\nLIGO. We do not find evidence for any GW signals. We place limits on the\nbroadband GW flux emitted at 25~Hz from point sources with a power law spectrum\nat $F_{\\alpha,\\Theta} <(0.05-25)\\times 10^{-8} ~{\\rm\nerg\\,cm^{-2}\\,s^{-1}\\,Hz^{-1}}$ and the (normalized) energy density spectrum in\nGWs at 25 Hz from extended sources at $\\Omega_{\\alpha}(\\Theta)\n<(0.19-2.89)\\times 10^{-8} ~{\\rm sr^{-1}}$ where $\\alpha$ is the spectral index\nof the energy density spectrum. These represent improvements of $2.5-3\\times$\nover previous limits. We also consider point sources emitting GWs at a single\nfrequency, targeting the directions of Sco X-1, SN 1987A, and the Galactic\nCenter. The best upper limits on the strain amplitude of a potential source in\nthese three directions range from $h_0 < (3.6-4.7)\\times 10^{-25}$, 1.5$\\times$\nbetter than previous limits set with the same analysis method. We also report\non a marginally significant outlier at 36.06~Hz. This outlier is not consistent\nwith a persistent gravitational-wave source as its significance diminishes when\ncombining all of the available data.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper deals with distinct computational methods to enumerate the set\n$\\mathrm{PLR}(r,s,n;m)$ of $r \\times s$ partial Latin rectangles on $n$ symbols\nwith $m$ non-empty cells. For fixed $r$, $s$, and $n$, we prove that the size\nof this set is a symmetric polynomial of degree $3m$, and we determine the\nleading terms (the monomials of degree $3m$ through $3m-9$) using\ninclusion-exclusion. For $m \\leq 13$, exact formulas for these symmetric\npolynomials are determined using a chromatic polynomial method. Adapting Sade's\nmethod for enumerating Latin squares, we compute the exact size of\n$\\mathrm{PLR}(r,s,n;m)$, for all $r \\leq s \\leq n \\leq 7$, and all $r \\leq s\n\\leq 6$ when $n=8$. Using an algebraic geometry method together with Burnside's\nLemma, we enumerate isomorphism, isotopism, and main classes when $r \\leq s\n\\leq n \\leq 6$. Numerical results have been cross-checked where possible.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Some general problems of Jacobian computations in non-full rank matrices are\ndiscussed in this work. In particular, the Jacobian of the Moore-Penrose\ninverse derived via matrix differential calculus is revisited. Then the\nJacobian in the full rank case is derived under the simple and old theory of\nthe exterior product.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the next generation of computing, Mobile ad-hoc network (MANET) will play\na very important role in the Internet of Things (IoT). The MANET is a kind of\nwireless networks that are self-organizing and auto connected in a\ndecentralized system. Every device in MANET can be moved freely from one\nlocation to another in any direction. They can create a network with their\nneighbors smart devices and forward data to another device. The IoT-Cloud-MANET\nframework of smart devices is composed of IoT, cloud computing, and MANET. This\nframework can access and deliver cloud services to the MANET users through\ntheir smart devices in the IoT framework where all computations, data handling,\nand resource management are performed. The smart devices can move from one\nlocation to another within the range of the MANET network. Various MANETs can\nconnect to the same cloud, they can use cloud service in a real time. For\nconnecting the smart device of MANET to cloud needs integration with mobile\napps. My main contribution in this research links a new methodology for\nproviding secure communication on the internet of smart devices using MANET\nConcept in 5G. The research methodology uses the correct and efficient\nsimulation of the desired study and can be implemented in a framework of the\nInternet of Things in 5G.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present new ALMA dust continuum observations of 101 $\\log(\\mathrm{M}_* /\n\\mathrm{M}_\\odot)$ > 9.5 galaxies in the COSMOS field to study the effect of\nenvironment on the interstellar medium at z ~ 0.7. At this redshift, our\ntargets span a wide range of environments allowing for a diverse sample of\ngalaxies with densities, $\\Sigma$ = 0.16-10.5 Mpc$^{-2}$ (per $\\Delta$ z =\n0.024). Using the ALMA observations, we calculate the total ISM mass and look\nfor depletion as a function of galaxy density in order to understand the\nquenching or triggering of star formation in galaxies in different\nenvironments. ISM mass is found to have a small dependence on environment,\nwhile the depletion timescale remains constant (~200 Myrs) across all\nenvironments. We find elevated ISM mass values at intermediate densities and\nlower values at high densities compared to low (field) densities. Our observed\nevolution in gas fraction with density in this single redshift slice is\nequivalent to the observed evolution with cosmic time over 2-3 Gyr. To explain\nthe change in gas mass fraction seen in galaxies in intermediate and high\ndensities, these results suggest environmental processes such as mergers and\nram pressure stripping are likely playing a role in dense filamentary-cluster\nenvironments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The lattice Boltzmann method (LBM) is routinely employed in the simulation of\ncomplex multiphase flows comprising bulk phases separated by non-ideal\ninterfaces. LBM is intrinsically mesoscale with an hydro-dynamic equivalence\npopularly set by the Chapman-Enskog analysis, requiring that fields slowly vary\nin space and time. The latter assumptions become questionable close to\ninterfaces, where the method is also known to be affected by spurious non\nhydrodynamical contributions. This calls for quantitative hydrodynamical\nchecks. In this paper we analyze the hydrodynamic behaviour of LBM\npseudo-potential models for the problem of break-up of a liquid ligament\ntriggered by the Plateau-Rayleigh instability. Simulations are performed at\nfixed interface thickness, while increasing the ligament radius, i.e. in the\n\"sharp interface\" limit. Influence of different LBM collision operators is also\nassessed. We find that different distributions of spurious currents along the\ninterface may change the outcome of the pseudo-potential model simulations\nquite sensibly, which suggests that a proper fine-tuning of pseudo-potential\nmodels in time-dependent problems is needed before the utilization in concrete\napplications. Taken all together, we argue that the results of the proposed\nstudy provide a valuable insight for engineering pseudo-potential model\napplications involving the hydrodynamics of liquid jets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep learning, an area of machine learning, is set to revolutionize patient\ncare. But it is not yet part of standard of care, especially when it comes to\nindividual patient care. In fact, it is unclear to what extent data-driven\ntechniques are being used to support clinical decision making (CDS).\nHeretofore, there has not been a review of ways in which research in machine\nlearning and other types of data-driven techniques can contribute effectively\nto clinical care and the types of support they can bring to clinicians. In this\npaper, we consider ways in which two data driven domains - machine learning and\ndata visualizations - can contribute to the next generation of clinical\ndecision support systems. We review the literature regarding the ways heuristic\nknowledge, machine learning, and visualization are - and can be - applied to\nthree types of CDS. There has been substantial research into the use of\npredictive modeling for alerts, however current CDS systems are not utilizing\nthese methods. Approaches that leverage interactive visualizations and\nmachine-learning inferences to organize and review patient data are gaining\npopularity but are still at the prototype stage and are not yet in use. CDS\nsystems that could benefit from prescriptive machine learning (e.g., treatment\nrecommendations for specific patients) have not yet been developed. We discuss\npotential reasons for the lack of deployment of data-driven methods in CDS and\ndirections for future research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $X$ be a $2$-dimensional subshift of finite type generated by a finite\nset of forbidden blocks (of finite size). We give an algorithm for generating\nthe elements of the shift space using sequence of finite matrices (of\nincreasing size). We prove that the sequence generated yields precisely the\nelements of the shift space $X$ and hence characterizes the elements of the\nshift space $X$. We extend our investigations to a general $d$-dimensional\nshift of finite type. In the process, we prove that that elements of\n$d$-dimensional shift of finite type can be characterized by a sequence of\nfinite matrices (of increasing size).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Differintegral methods, currently exploited in calculus, provide a fairly\nunexhausted source of tools to be applied to a wide class of problems involving\nthe theory of special functions and not only. The use of integral transforms of\nBorel type and the associated formalism will be shown to be an effective means,\nallowing a link between umbral and operational methods. We merge these two\npoints of view to get a new and efficient method to obtain integrals of special\nfunctions and the summation of the associated generating functions as well.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We establish upper bounds of the indices of topological Brauer classes over a\nclosed orientable 8-manifolds. In particular, we verify the Topological\nPeriod-Index Conjecture (TPIC) for topological Brauer classes over closed\norientable 8-manifolds of order not congruent to 2 mod 4. In addition, we\nprovide a counter-example which shows that the TPIC fails in general for closed\norientable 8-manifolds.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cyanogen (NCCN) is the simplest member of the dicyanopolyynes group, and has\nbeen proposed as a major source of the CN radical observed in cometary\natmospheres. Although not detected through its rotational spectrum in the cold\ninterstellar medium, this very stable species is supposed to be very abundant.\nThe chemistry of cyanogen in the cold interstellar medium can be investigated\nthrough its metastable isomer, CNCN (isocyanogen). Its formation may provide a\nclue on the widely abundant CN radical observed in cometary atmospheres. We\nperformed an unbiased spectral survey of the L1544 proto-typical prestellar\ncore, using the IRAM-30m and have analysed, for this paper, the nitrogen\nchemistry that leads to the formation of isocyanogen. We report on the first\ndetection of CNCN, NCCNH+, C3N, CH3CN, C2H3CN, and H2CN in L1544. We built a\ndetailed chemical network for NCCN/CNCN/HC2N2+ involving all the nitrogen\nbearing species detected (CN, HCN, HNC, C3N, CNCN, CH3CN, CH2CN, HCCNC, HC3N,\nHNC3, H2CN, C2H3CN, HCNH+, HC3NH+) and the upper limits on C4N, C2N. The main\ncyanogen production pathways considered in the network are the CN + HNC and N +\nC3N reactions. The comparison between the observations of the nitrogen bearing\nspecies and the predictions from the chemical modelling shows a very good\nagreement, taking into account the new chemical network. The expected cyanogen\nabundance is greater than the isocyanogen abundance by a factor of 100.\nAlthough cyanogen cannot be detected through its rotational spectrum, the\nchemical modelling predicts that it should be abundant in the gas phase and\nhence might be traced through the detection of isocyanogen. It is however\nexpected to have a very low abundance on the grain surfaces compared to HCN.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper we address the issue of identity and access control within\nshared permissioned blockchains. We propose the ChainAchor system that provides\nanonymous but verifiable identities for entities on the blockchain. ChainAchor\nalso provides access control to entities seeking to submit transactions to the\nblockchain to read/verify transactions on the the permissioned blockchain.\nConsensus nodes enforce access control to the shared permissioned blockchain by\na simple look-up to a (read-only) list of anonymous members' public-keys.\nChainAnchor also provides unlinkability of transactions belonging to an entity\non the blockchain. This allows for an entity to optionally disclose their\nidentity when a transaction is called into question (e.g. regulatory or\ncompliance requirements), but without affecting the anonymity and unlinkability\nof their remaining transactions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A conjecture for the dimension and the character of the homogenous components\nof the free Jordan algebras is proposed.\n  As a support of the conjecture, some numerical evidences are generated by a\ncomputer and some new theoretical results are proved. One of them is the\ncyclicity of the Jordan operad.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article we study some statistical aspects of surface diffeomorphisms.\nWe first show that for a $C^1$ generic diffeomorphism, a Dirac invariant\nmeasure whose \\emph{statistical basin of attraction} is dense in some open set\nand has positive Lebesgue measure, must be supported in the orbit of a sink. We\nthen construct an example of a $C^1$-diffeomorphism having a Dirac invariant\nmeasure, supported on a hyperbolic fixed point of saddle type, whose\nstatistical basin of attraction is a nowhere dense set with positive Lebesgue\nmeasure. Our technique can be applied also to construct a $C^1$ diffeomorphism\nwhose set of points with historic behaviour has positive measure and is nowhere\ndense.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a framework wherein the trajectory optimization problem (or a\nproblem involving calculus of variations) is formulated as a search problem in\na discrete space. A distinctive feature of our work is the treatment of\ndiscretization of the optimization problem wherein we discretize not only\nindependent variables (such as time) but also dependent variables. Our\ndiscretization scheme enables a reduction in computational cost through\nselection of coarse-grained states. It further facilitates the solution of the\ntrajectory optimization problem via classical discrete search algorithms\nincluding deterministic and stochastic methods for obtaining a global optimum.\nThis framework also allows us to efficiently use quantum computational\nalgorithms for global trajectory optimization. We demonstrate that the discrete\nsearch problem can be solved by a variety of techniques including a\ndeterministic exhaustive search in the physical space or the coefficient space,\na randomized search algorithm, a quantum search algorithm or by employing a\ncombination of randomized and quantum search algorithms depending on the nature\nof the problem. We illustrate our methods by solving some canonical problems in\ntrajectory optimization. We also present a comparative study of the\nperformances of different methods in solving our example problems. Finally, we\nmake a case for using quantum search algorithms as they offer a quadratic\nspeed-up in comparison to the traditional non-quantum algorithms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we develop a convolutional neural network model to predict the\nmechanical properties of a two-dimensional checkerboard composite\nquantitatively. The checkerboard composite possesses two phases, one phase is\nsoft and ductile while the other is stiff and brittle. The ground-truth data\nused in the training process are obtained from finite element analyses under\nthe assumption of plane stress. Monte Carlo simulations and central limit\ntheorem are used to find the size of the dataset needed. Once the training\nprocess is completed, the developed model is validated using data unseen during\ntraining. The developed neural network model captures the stiffness, strength,\nand toughness of checkerboard composites with high accuracy. Also, we integrate\nthe developed model with a genetic algorithm (GA) optimizer to identify the\noptimal microstructural designs. The genetic algorithm optimizer adopted here\nhas several operators, selection, crossover, mutation, and elitism. The\noptimizer converges to configurations with highly enhanced properties. For the\ncase of the modulus and starting from randomly-initialized generation, the GA\noptimizer converges to the global maximum which involves no soft elements.\nAlso, the GA optimizers, when used to maximize strength and toughness, tend\ntowards having soft elements in the region next to the crack tip.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider a stochastic game of contribution to the common good in which the\nplayers have continuous control over the degree of contribution, and we examine\nthe gradualism arising from the free rider effect. This game belongs to the\nclass of variable concession games which generalize wars of attrition.\nPreviously known examples of variable concession games in the literature yield\nequilibria characterized by singular control strategies without any delay of\nconcession. However, these no-delay equilibria are in contrast to mixed\nstrategy equilibria of canonical wars of attrition in which each player delays\nconcession by a randomized time. We find that a variable contribution game with\na single state variable, which extends the Nerlove-Arrow model, possesses an\nequilibrium characterized by regular control strategies that result in a\ngradual concession. This equilibrium naturally generalizes the mixed strategy\nequilibria from the canonical wars of attrition. Stochasticity of the problem\naccentuates the qualitative difference between a singular control solution and\na regular control equilibrium solution. We also find that asymmetry between the\nplayers can mitigate the inefficiency caused by the gradualism.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Centaurs--icy bodies orbiting beyond Jupiter and interior to Neptune--are\nbelieved to be dynamically related to Jupiter Family Comets (JFCs), which have\naphelia near Jupiter's orbit and perihelia in the inner Solar System. Previous\ndynamical simulations have recreated the Centaur/JFC conversion, but the\nmechanism behind that process remains poorly described. We have performed a\nnumerical simulation of Centaur analogues that recreates this process,\ngenerating a dataset detailing over 2.6 million close planet/planetesimal\ninteractions. We explore scenarios stored within that database and, from those,\ndescribe the mechanism by which Centaur objects are converted into JFCs.\nBecause many JFCs have perihelia in the terrestrial planet region, and since\nCentaurs are constantly resupplied from the Scattered Disk and other\nreservoirs, the JFCs are an ever-present impact threat.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study a CR analogue of the Ahlfors derivative for conformal immersions of\nStowe [23] that generalizes the CR Schwarzian derivative studied earlier by the\nsecond-named author [21]. This notion possesses several important properties\nsimilar to those of the conformal counterpart and provides a new invariant for\nspherically equivalent CR maps from strictly pseudoconvex CR manifolds into a\nsphere. The invariant is computable and distinguishes many well-known sphere\nmaps. In particular, it vanishes precisely when the map is spherically\nequivalent to the linear embedding of spheres.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a new Lagrange Multiplier approach to design unconditional energy\nstable schemes for gradient flows. The new approach leads to unconditionally\nenergy stable schemes that are as accurate and efficient as the recently\nproposed SAV approach \\cite{SAV01}, but enjoys two additional advantages: (i)\nschemes based on the new approach dissipate the original energy, as opposed to\na modified energy in the recently proposed SAV approach \\cite{SAV01}; and (ii)\nthey do not require the nonlinear part of the free energy to be bounded from\nbelow as is required in the SAV approach. The price we pay for these advantages\nis that a nonlinear algebraic equation has to be solved to determine the\nLagrange multiplier. We present ample numerical results to validate the new\napproach, and, as a particular example of applications, we consider a coupled\nCahn-Hilliard model for block copolymers (BCP), and carry out interesting\nsimulations which are consistent with experiment results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Supervised deep learning methods for segmentation require large amounts of\nlabelled training data, without which they are prone to overfitting, not\ngeneralizing well to unseen images. In practice, obtaining a large number of\nannotations from clinical experts is expensive and time-consuming. One way to\naddress scarcity of annotated examples is data augmentation using random\nspatial and intensity transformations. Recently, it has been proposed to use\ngenerative models to synthesize realistic training examples, complementing the\nrandom augmentation. So far, these methods have yielded limited gains over the\nrandom augmentation. However, there is potential to improve the approach by (i)\nexplicitly modeling deformation fields (non-affine spatial transformation) and\nintensity transformations and (ii) leveraging unlabelled data during the\ngenerative process. With this motivation, we propose a novel task-driven data\naugmentation method where to synthesize new training examples, a generative\nnetwork explicitly models and applies deformation fields and additive intensity\nmasks on existing labelled data, modeling shape and intensity variations,\nrespectively. Crucially, the generative model is optimized to be conducive to\nthe task, in this case segmentation, and constrained to match the distribution\nof images observed from labelled and unlabelled samples. Furthermore, explicit\nmodeling of deformation fields allow synthesizing segmentation masks and images\nin exact correspondence by simply applying the generated transformation to an\ninput image and the corresponding annotation. Our experiments on cardiac\nmagnetic resonance images (MRI) showed that, for the task of segmentation in\nsmall training data scenarios, the proposed method substantially outperforms\nconventional augmentation techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The question of (non-)uniqueness of one-dimensional self-similar solutions to\nthe Riemann problem for hyperbolic systems of gas dynamics in sets of\nmulti-dimensional admissible weak solutions was addressed in recent years in\nseveral papers culminating in [17] with the proof that the Riemann problem for\nthe isentropic Euler system with a power law pressure is ill-posed if the\none-dimensional self-similar solution contains a shock. Natural question then\narises whether the same holds also for a more involved system of equations, the\nfull Euler system. After the first step in this direction was made in [1],\nwhere the ill-posedness was proved in the case of two shocks appearing in the\nself-similar solution, we prove in this paper that the presence of just one\nshock in the self-similar solution implies the same outcome, i.e. the existence\nof infinitely many admissible weak solutions to the multi-dimensional problem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Bremermann-Bekenstein bound sets a fundamental upper limit on the rate\nwith which information can be processed. However, the original treatment\nheavily relies on cosmological properties and plausibility arguments. In the\npresent analysis, we derive equivalent statements by relying on only two\nfundamental results in quantum information theory and quantum dynamics --\nFannes inequality and the quantum speed limit. As main results, we obtain\nBremermann-Bekenstein-type bounds for the rate of change of the von Neumann\nentropy in quantum systems undergoing open system dynamics, and for the rate of\nchange of the Shannon information over some logical basis in unitary quantum\nevolution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Speech separation refers to extracting each individual speech source in a\ngiven mixed signal. Recent advancements in speech separation and ongoing\nresearch in this area, have made these approaches as promising techniques for\npre-processing of naturalistic audio streams. After incorporating deep learning\ntechniques into speech separation, performance on these systems is improving\nfaster. The initial solutions introduced for deep learning based speech\nseparation analyzed the speech signals into time-frequency domain with STFT;\nand then encoded mixed signals were fed into a deep neural network based\nseparator. Most recently, new methods are introduced to separate waveform of\nthe mixed signal directly without analyzing them using STFT. Here, we introduce\na unified framework to include both spectrogram and waveform separations into a\nsingle structure, while being only different in the kernel function used to\nencode and decode the data; where, both can achieve competitive performance.\nThis new framework provides flexibility; in addition, depending on the\ncharacteristics of the data, or limitations of the memory and latency can set\nthe hyper-parameters to flow in a pipeline of the framework which fits the task\nproperly. We extend single-channel speech separation into multi-channel\nframework with end-to-end training of the network while optimizing the speech\nseparation criterion (i.e., Si-SNR) directly. We emphasize on how tied kernel\nfunctions for calculating spatial features, encoder, and decoder in\nmulti-channel framework can be effective. We simulate spatialized reverberate\ndata for both WSJ0 and LibriSpeech corpora here, and while these two sets of\ndata are different in the matter of size and duration, the effect of capturing\nshorter and longer dependencies of previous/+future samples are studied in\ndetail. We report SDR, Si-SNR and PESQ to evaluate the performance of developed\nsolutions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Within the random matrix theory approach to quantum scattering, we derive the\ndistribution of the Wigner-Smith time delay matrix $\\mathcal{Q}$ for a chaotic\ncavity with uniform absorption, coupled via $N$ perfect channels. In the\nunitary class $\\beta=2$ we obtain a compact expression for the distribution of\nthe full matrix in terms of a matrix integral. In the other symmetry classes we\nderive the joint distribution of the eigenvalues. We show how the large $N$\nproperties of this distribution can be analysed in terms of two interacting\nCoulomb gases living on two different supports. As an application of our\nresults, we study the statistical properties of the Wigner time delay\n$\\tau_{\\mathrm{W}} = \\mathrm{tr}[\\mathcal{Q}]/N$ in the presence of absorption.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  While Nash equilibrium in extensive-form games is well understood, very\nlittle is known about the properties of extensive-form correlated equilibrium\n(EFCE), both from a behavioral and from a computational point of view. In this\nsetting, the strategic behavior of players is complemented by an external\ndevice that privately recommends moves to agents as the game progresses;\nplayers are free to deviate at any time, but will then not receive future\nrecommendations. Our contributions are threefold. First, we show that an EFCE\ncan be formulated as the solution to a bilinear saddle-point problem. To\nshowcase how this novel formulation can inspire new algorithms to compute\nEFCEs, we propose a simple subgradient descent method which exploits this\nformulation and structural properties of EFCEs. Our method has better\nscalability than the prior approach based on linear programming. Second, we\npropose two benchmark games, which we hope will serve as the basis for future\nevaluation of EFCE solvers. These games were chosen so as to cover two natural\napplication domains for EFCE: conflict resolution via a mediator, and\nbargaining and negotiation. Third, we document the qualitative behavior of EFCE\nin our proposed games. We show that the social-welfare-maximizing equilibria in\nthese games are highly nontrivial and exhibit surprisingly subtle sequential\nbehavior that so far has not received attention in the literature.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With the rapid development of smart terminals and infrastructures, as well as\ndiversified applications (e.g., virtual and augmented reality, remote surgery\nand holographic projection) with colorful requirements, current networks (e.g.,\n4G and upcoming 5G networks) may not be able to completely meet quickly rising\ntraffic demands. Accordingly, efforts from both industry and academia have\nalready been put to the research on 6G networks. Recently, artificial\nintelligence (AI) has been utilized as a new paradigm for the design and\noptimization of 6G networks with a high level of intelligence. Therefore, this\narticle proposes an AI-enabled intelligent architecture for 6G networks to\nrealize knowledge discovery, smart resource management, automatic network\nadjustment and intelligent service provisioning, where the architecture is\ndivided into four layers: intelligent sensing layer, data mining and analytics\nlayer, intelligent control layer and smart application layer. We then review\nand discuss the applications of AI techniques for 6G networks and elaborate how\nto employ the AI techniques to efficiently and effectively optimize the network\nperformance, including AI-empowered mobile edge computing, intelligent mobility\nand handover management, and smart spectrum management. Moreover, we highlight\nimportant future research directions and potential solutions for AI-enabled\nintelligent 6G networks, including computation efficiency, algorithms\nrobustness, hardware development and energy management.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Sparse reward is one of the most challenging problems in reinforcement\nlearning (RL). Hindsight Experience Replay (HER) attempts to address this issue\nby converting a failed experience to a successful one by relabeling the goals.\nDespite its effectiveness, HER has limited applicability because it lacks a\ncompact and universal goal representation. We present Augmenting experienCe via\nTeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that\nextends the HER framework using natural language as the goal representation. We\nfirst analyze the differences among goal representation, and show that ACTRCE\ncan efficiently solve difficult reinforcement learning problems in challenging\n3D navigation tasks, whereas HER with non-language goal representation failed\nto learn. We also show that with language goal representations, the agent can\ngeneralize to unseen instructions, and even generalize to instructions with\nunseen lexicons. We further demonstrate it is crucial to use hindsight advice\nto solve challenging tasks, and even small amount of advice is sufficient for\nthe agent to achieve good performance.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report on searches for neutrinos and antineutrinos from astrophysical\nsources performed with the Borexino detector at the Laboratori Nazionali del\nGran Sasso in Italy. Electron antineutrinos ($\\bar{\\nu}_e$) are detected in an\norganic liquid scintillator through the inverse $\\beta$-decay reaction. In the\npresent work we set model-independent upper limits in the energy range 1.8-16.8\nMeV on neutrino fluxes from unknown sources that improve our previous results,\non average, by a factor 2.5. Using the same data set, we first obtain\nexperimental constraints on the diffuse supernova $\\bar{\\nu}_e$ fluxes in the\npreviously unexplored region below 8 MeV. A search for $\\bar{\\nu}_e$ in the\nsolar neutrino flux is also presented: the presence of $\\bar{\\nu}_e$ would be a\nmanifestation of a non-zero anomalous magnetic moment of the neutrino, making\npossible its conversion to antineutrinos in the strong magnetic field of the\nSun. We obtain a limit for a solar $\\bar{\\nu}_e$ flux of 384 cm$^{-2}$s$^{-1}$\n(90% C.L.), assuming an undistorted solar $^{8}$B neutrinos energy spectrum,\nthat corresponds to a transition probability $p_{ \\nu_e \\rightarrow\n\\bar\\nu_{e}}<$ 7.2$\\times$10$^{-5}$ (90% C.L.) for E$_{\\bar {\\nu}_e}$ $>$ 1.8\nMeV. At lower energies, by investigating the spectral shape of elastic\nscattering events, we obtain a new limit on solar $^{7}$Be-$\\nu_e$ conversion\ninto $\\bar{\\nu}_e$ of $p_{ \\nu_e \\rightarrow \\bar \\nu_{e}}<$ 0.14 (90% C.L.) at\n0.862 keV. Last, we investigate solar flares as possible neutrino sources and\nobtain the strongest up-to-date limits on the fluence of neutrinos of all\nflavor neutrino below 3-7 ,MeV. Assuming the neutrino flux to be proportional\nto the flare's intensity, we exclude an intense solar flare as the cause of the\nobserved excess of events in run 117 of the Cl-Ar Homestake experiment.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We discuss the `hd-compactification' of a semi-simple Lie group to a manifold\nwith corners; it is the real analog of the wonderful compactification of\ndeConcini and Procesi. There is a 1-1 correspondence between the boundary faces\nof the compactification and conjugacy classes of parabolic subgroups with the\nboundary face fibering over two copies of the corresponding flag variety with\nfiber modeled on the (compactification of the) reductive part. On the\nhd-compactification Harish-Chandra's Schwartz space is identified with a space\nof conormal functions of rapid-logarithmic decay relative to square-integrable\nfunctions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we present a quasi-polynomial time classical algorithm that\nestimates the partition function of quantum many-body systems at temperatures\nabove the thermal phase transition point. It is known that in the worst case,\nthe same problem is NP-hard below this point. Together with our work, this\nshows that the transition in the phase of a quantum system is also accompanied\nby a transition in the hardness of approximation. We also show that in a system\nof n particles above the phase transition point, the correlation between two\nobservables whose distance is at least log(n) decays exponentially. We can\nimprove the factor of log(n) to a constant when the Hamiltonian has commuting\nterms or is on a 1D chain. The key to our results is a characterization of the\nphase transition and the critical behavior of the system in terms of the\ncomplex zeros of the partition function. Our work extends a seminal work of\nDobrushin and Shlosman on the equivalence between the decay of correlations and\nthe analyticity of the free energy in classical spin models. On the algorithmic\nside, our result extends the scope of a recent approach due to Barvinok for\nsolving classical counting problems to quantum many-body systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a theoretical study of the dispersion and energy properties of the\neigenwaves (TE- and TM- modes) in a four-layer structure composed of a\nmagneto-optical yttrium iron garnet guiding layer on a dielectric substrate\ncovered by a planar nanocomposite guiding multilayer. The bigyrotropic\nproperties of yttrium-iron garnet are taken into account for obtaining the\ndispersion equation and an original algorithm for the guided modes\nidentification is proposed. We demonstrated the polarization switching of TE-\nand TM-modes dependent on the geometrical parameters of the guiding layers. The\ndispersion diagrams and field profiles are used to illustrate the change of\npropagation properties with variation of the multilayer thickness ratio of the\nnanocomposite layers. The energy flux distributions across the structure are\ncalculated and the conditions of the optimal guiding regime are obtained. The\npower switching ratio in the waveguide layers of about 6 dB for the wavelength\nrange of 100 nm is shown to be achieved.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We demonstrate the strong coupling of a quantum dot and a graphene spherical\nshell coating it. Our simulations are the exact solutions of 3D Maxwell\nequations. Interaction produces sharp hybrid modes, even when the two are\noff-resonant, which are voltage-tunable (continuously) in an 80 meV interval.\nDespite a voltage-tunable quantum dot, the coupling of the light to these \"very\nsharp\" plexcitonic resonances is an order of magnitude larger than its coupling\nto a quantum dot. Hence, our results are very attractive for sensing\napplications and graphene display technologies with sharper colors. Moreover,\non a simple theoretical model, we explain why such sharp, highly tunable,\nresonances emerge.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider a dilute, homogeneous Bose gas at positive temperature. The\nsystem is investigated in the Gross-Pitaevskii (GP) limit, where the scattering\nlength $a$ is so small that the interaction energy is of the same order of\nmagnitude as the spectral gap of the Laplacian, and for temperatures that are\ncomparable to the critical temperature of the ideal gas. We show that the\ndifference between the specific free energy of the interacting system and the\none of the ideal gas is to leading order given by $4 \\pi a \\left( 2 \\varrho^2 -\n\\varrho_0^2 \\right)$. Here $\\varrho$ denotes the density of the system and\n$\\varrho_0$ is the expected condensate density of the ideal gas. Additionally,\nwe show that the one-particle density matrix of any approximate minimizer of\nthe Gibbs free energy functional is to leading order given by the one of the\nideal gas. This in particular proves Bose-Einstein condensation with critical\ntemperature given by the one of the ideal gas to leading order. One key\ningredient of our proof is a novel use of the Gibbs variational principle that\ngoes hand in hand with the c-number substitution.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Social tipping, where minorities trigger larger populations to engage in\ncollective action, has been suggested as one key aspect in addressing\ncontemporary global challenges. Here, we refine Granovetter's widely\nacknowledged theoretical threshold model of collective behavior as a numerical\nmodelling tool for understanding social tipping processes and resolve issues\nthat so far have hindered such applications. Based on real-world observations\nand social movement theory, we group the population into certain or potential\nactors, such that -- in contrast to its original formulation -- the model\npredicts non-trivial final shares of acting individuals. Then, we use a network\ncascade model to explain and analytically derive that previously hypothesized\nbroad threshold distributions emerge if individuals become active via social\ninteraction. Thus, through intuitive parameters and low dimensionality our\nrefined model is adaptable to explain the likelihood of engaging in collective\nbehavior where social tipping like processes emerge as saddle-node bifurcations\nand hysteresis.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present three methods for distributed memory parallel inverse\nfactorization of block-sparse Hermitian positive definite matrices. The three\nmethods are a recursive variant of the AINV inverse Cholesky algorithm,\niterative refinement, and localized inverse factorization, respectively. All\nthree methods are implemented using the Chunks and Tasks programming model,\nbuilding on the distributed sparse quad-tree matrix representation and parallel\nmatrix-matrix multiplication in the publicly available Chunks and Tasks Matrix\nLibrary (CHTML). Although the algorithms are generally applicable, this work\nwas mainly motivated by the need for efficient and scalable inverse\nfactorization of the basis set overlap matrix in large scale electronic\nstructure calculations. We perform various computational tests on overlap\nmatrices for quasi-linear Glutamic Acid-Alanine molecules and three-dimensional\nwater clusters discretized using the standard Gaussian basis set STO-3G with up\nto more than 10 million basis functions. We show that for such matrices the\ncomputational cost increases only linearly with system size for all the three\nmethods. We show both theoretically and in numerical experiments that the\nmethods based on iterative refinement and localized inverse factorization\noutperform previous parallel implementations in weak scaling tests where the\nsystem size is increased in direct proportion to the number of processes. We\nshow also that compared to the method based on pure iterative refinement the\nlocalized inverse factorization requires much less communication.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With the advancement of remote-sensed imaging large volumes of very high\nresolution land cover images can now be obtained. Automation of object\nrecognition in these 2D images, however, is still a key issue. High intra-class\nvariance and low inter-class variance in Very High Resolution (VHR) images\nhamper the accuracy of prediction in object recognition tasks. Most successful\ntechniques in various computer vision tasks recently are based on deep\nsupervised learning. In this work, a deep Convolutional Neural Network (CNN)\nbased on symmetric encoder-decoder architecture with skip connections is\nemployed for the 2D semantic segmentation of most common land cover object\nclasses - impervious surface, buildings, low vegetation, trees and cars. Atrous\nconvolutions are employed to have large receptive field in the proposed CNN\nmodel. Further, the CNN outputs are post-processed using Fully Connected\nConditional Random Field (FCRF) model to refine the CNN pixel label\npredictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5%\non the ISPRS Vaihingen Dataset.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Coupling electronic and vibrational degrees of freedom of Rydberg atoms held\nin optical tweezer arrays offers a flexible mechanism for creating and\ncontrolling atom-atom interactions. We find that the state-dependent coupling\nbetween Rydberg atoms and local oscillator modes gives rise to two- and\nthree-body interactions which are controllable through the strength of the\nlocal confinement. This approach even permits the cancellation of two-body\nterms such that three-body interactions become dominant. We analyze the\nstructure of these interactions on two-dimensional bipartite lattice geometries\nand explore the impact of three-body interactions on system ground state on a\nsquare lattice. Focusing specifically on a system of $ ^{87} $Rb atoms, we show\nthat the effects of the multi-body interactions can be maximized via a tailored\ndressed potential within a trapping frequency range of the order of a few\nhundred kHz and for temperatures corresponding to a $ >90\\% $ occupation of the\natomic vibrational ground state. These parameters, as well as the multi-body\ninduced time scales, are compatible with state-of-the-art arrays of optical\ntweezers. Our work shows a highly versatile handle for engineering multi-body\ninteractions of quantum many-body systems in most recent manifestations on\nRydberg lattice quantum simulators.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  As reinforcement learning (RL) achieves more success in solving complex\ntasks, more care is needed to ensure that RL research is reproducible and that\nalgorithms herein can be compared easily and fairly with minimal bias. RL\nresults are, however, notoriously hard to reproduce due to the algorithms'\nintrinsic variance, the environments' stochasticity, and numerous (potentially\nunreported) hyper-parameters. In this work we investigate the many issues\nleading to irreproducible research and how to manage those. We further show how\nto utilise a rigorous and standardised evaluation approach for easing the\nprocess of documentation, evaluation and fair comparison of different\nalgorithms, where we emphasise the importance of choosing the right measurement\nmetrics and conducting proper statistics on the results, for unbiased reporting\nof the results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We solve an open question in code-based cryptography by introducing two\nprovably secure group signature schemes from code-based assumptions. Our basic\nscheme satisfies the CPA-anonymity and traceability requirements in the random\noracle model, assuming the hardness of the McEliece problem, the Learning\nParity with Noise problem, and a variant of the Syndrome Decoding problem. The\nconstruction produces smaller key and signature sizes than the previous group\nsignature schemes from lattices, as long as the cardinality of the underlying\ngroup does not exceed $2^{24}$, which is roughly comparable to the current\npopulation of the Netherlands. We develop the basic scheme further to achieve\nthe strongest anonymity notion, i.e., CCA-anonymity, with a small overhead in\nterms of efficiency. The feasibility of two proposed schemes is supported by\nimplementation results. Our two schemes are the first in their respective\nclasses of provably secure groups signature schemes. Additionally, the\ntechniques introduced in this work might be of independent interest. These are\na new verifiable encryption protocol for the randomized McEliece encryption and\na novel approach to design formal security reductions from the Syndrome\nDecoding problem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Mention detection is an important preprocessing step for annotation and\ninterpretation in applications such as NER and coreference resolution, but few\nstand-alone neural models have been proposed able to handle the full range of\nmentions. In this work, we propose and compare three neural network-based\napproaches to mention detection. The first approach is based on the mention\ndetection part of a state of the art coreference resolution system; the second\nuses ELMO embeddings together with a bidirectional LSTM and a biaffine\nclassifier; the third approach uses the recently introduced BERT model. Our\nbest model (using a biaffine classifier) achieves gains of up to 1.8 percentage\npoints on mention recall when compared with a strong baseline in a HIGH RECALL\ncoreference annotation setting. The same model achieves improvements of up to\n5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on\nthe CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation\nsetting. We then evaluate our models for coreference resolution by using\nmentions predicted by our best model in start-of-the-art coreference systems.\nThe enhanced model achieved absolute improvements of up to 1.7 and 0.7 p.p.\nwhen compared with our strong baseline systems (pipeline system and end-to-end\nsystem) respectively. For nested NER, the evaluation of our model on the GENIA\ncorpora shows that our model matches or outperforms state-of-the-art models\ndespite not being specifically designed for this task.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We demonstrate several examples of driving and steering of colloids when\ndispersed in nematic liquid crystals. The driving mechanism is based on the\nprinciple of nonlinear electrophoresis which is mediated by the asymmetry in\nthe structure of the defects that the inclusions generate in the host elastic\nmatrix. The steering mechanism originates in the photoactivation of the\nanchoring conditions of the nematic liquid crystal on one of the enclosing\nplates. As experimental realizations we first review a scenario of water\nmicrodroplets being phoretically transported for cargo release and chemical\nreaction. Steering is illustrated in terms of the reconfigurable assembly of\ncolloidal particles, either in the form of asters or rotating-mills, commanded\nby predesigned patterns of illumination.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using the factorization method, a method pioneered by Eskin and Mirzakhani in\ntheir groundbreaking work about measure classification over the moduli space of\ntranslation surfaces, we show that generalized $u$-Gibbs states over\nquantitatively non-integrable Anosov systems are absolutely continuous with\nrespect to the whole unstable manifold.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we establish a deformation theory for Dolbeault cohomology\nclasses valued in holomorphic tensor bundles. We prove the extension equation\nwhich will play the role of Maurer-Cartan equation. Following the classical\ntheory of Kodaira-Spencer-Kuranishi, we construct a canonical complete family\nof deformations by using the power series method. We also prove a simple\nrelation between the existence of deformations and the varying of the\ndimensions of Dolbeault cohomology. The deformations of $(p,q)$-forms is shown\nto be unobstructed under some mild conditions. By analyzing Nakamura's example\nof complex parallelizable manifolds, we will see that the deformation theory\ndeveloped in this work provides precise explanations to the jumping phenomenon\nof Dolbeault cohomology.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Reasoning with knowledge expressed in natural language and Knowledge Bases\n(KBs) is a major challenge for Artificial Intelligence, with applications in\nmachine reading, dialogue, and question answering. General neural architectures\nthat jointly learn representations and transformations of text are very\ndata-inefficient, and it is hard to analyse their reasoning process. These\nissues are addressed by end-to-end differentiable reasoning systems such as\nNeural Theorem Provers (NTPs), although they can only be used with small-scale\nsymbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension\nto NTPs addressing their complexity and scalability limitations, thus making\nthem applicable to real-world datasets. This result is achieved by dynamically\nconstructing the computation graph of NTPs and including only the most\npromising proof paths during inference, thus obtaining orders of magnitude more\nefficient models. Then, we propose a novel approach for jointly reasoning over\nKBs and textual mentions, by embedding logic facts and natural language\nsentences in a shared embedding space. We show that GNTPs perform on par with\nNTPs at a fraction of their cost while achieving competitive link prediction\nresults on large datasets, providing explanations for predictions, and inducing\ninterpretable models. Source code, datasets, and supplementary material are\navailable online at https://github.com/uclnlp/gntp.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Low-resolution devices are promising for systems that demand low energy\nconsumption and low complexity as required in IoT systems. In this study, we\npropose a novel waveform for bandlimited channels with 1-bit quantization and\noversampling at the receivers. The proposed method implies that the information\nis conveyed within the time instances of zero-crossings which is then utilized\nin combination with a Gray-coding scheme. Unlike the existing method, the\nproposed method does not require optimization and transmission of a dynamic\ncodebook. The proposed approach outperforms the state-of-the-art method in\nterms of bit error rate.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a few recent developments in the field of electron backscatter\ndiffraction (EBSD). We highlight how open source algorithms and open data\nformats can be used to rapidly to develop microstructural insight of materials.\nWe include use of AstroEBSD for single pixel based EBSD mapping and\nconventional orientation mapping; followed by an unsupervised machine learning\napproach using principal component analysis and multivariate statistics\ncombined with a refined template matching method to rapidly index orientation\ndata with high precision. Next, we compare a diffraction pattern captured using\ndirect electron detector with a dynamical simulation and project this to create\na high quality experimental \"reference diffraction sphere\". Finally, we\nclassify phases using supervised machine learning with transfer learning and a\nconvolutional neural network.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Revenue sharing contracts between Content Providers (CPs) and Internet\nService Providers (ISPs) can act as leverage for enhancing the infrastructure\nof the Internet. ISPs can be incentivized to make investments in network\ninfrastructure that improve Quality of Service (QoS) for users if attractive\ncontracts are negotiated between them and CPs. The idea here is that part of\nthe net profit gained by CPs are given to ISPs to invest in the network. The\nMoral Hazard economic framework is used to model such an interaction, in which\na principal determines a contract, and an agent reacts by adapting her effort.\nIn our setting, several competitive CPs interact through one common ISP. Two\ncases are studied: (i) the ISP differentiates between the CPs and makes a\n(potentially) different investment to improve the QoS of each CP, and (ii) the\nISP does not differentiate between CPs and makes a common investment for both.\nThe last scenario can be viewed as \\emph{network neutral behavior} on the part\nof the ISP. We analyse the optimal contracts and show that the CP that can\nbetter monetize its demand always prefers the non-neutral regime.\nInterestingly, ISP revenue, as well as social utility, are also found to be\nhigher under the non-neutral regime.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Assuming finiteness of the Tate--Shafarevich group, we prove that the\nBirch--Swinnerton-Dyer conjecture correctly predicts the parity of the rank of\nsemistable principally polarised abelian surfaces (with mild extra local\nconditions at 2-adic places)\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we consider the inverse problem of recovering an isotropic\nelastic tensor from the Neumann-to-Dirichlet map. To this end, we prove a\nLipschitz stability estimate for Lam\\'e parameters with certain regularity\nassumptions. In addition, we assume that the Lam\\'e parameters belong to a\nknown finite subspace with a priori known bounds and that they fulfill a\nmonotonicity property. The proof relies on a monotonicity result combined with\nthe techniques of localized potentials. To numerically solve the inverse\nproblem, we propose a Kohn-Vogelius-type cost functional over a class of\nadmissible parameters subject to two boundary value problems. The reformulation\nof the minimization problem via the Neumann-to-Dirichlet operator allows us to\nobtain the optimality conditions by using the Fr\\'echet differentiability of\nthis operator and its inverse. The reconstruction is then performed by means of\nan iterative algorithm based on a quasi-Newton method. Finally, we give and\ndiscuss several numerical examples.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present new lower bounds that show that a polynomial number of passes are\nnecessary for solving some fundamental graph problems in the streaming model of\ncomputation. For instance, we show that any streaming algorithm that finds a\nweighted minimum $s$-$t$ cut in an $n$-vertex undirected graph requires\n$n^{2-o(1)}$ space unless it makes $n^{\\Omega(1)}$ passes over the stream.\n  To prove our lower bounds, we introduce and analyze a new four-player\ncommunication problem that we refer to as the hidden-pointer chasing problem.\nThis is a problem in spirit of the standard pointer chasing problem with the\nkey difference that the pointers in this problem are hidden to players and\nfinding each one of them requires solving another communication problem, namely\nthe set intersection problem. Our lower bounds for graph problems are then\nobtained by reductions from the hidden-pointer chasing problem.\n  Our hidden-pointer chasing problem appears flexible enough to find other\napplications and is therefore interesting in its own right. To showcase this,\nwe further present an interesting application of this problem beyond streaming\nalgorithms. Using a reduction from hidden-pointer chasing, we prove that any\nalgorithm for submodular function minimization needs to make $n^{2-o(1)}$ value\nqueries to the function unless it has a polynomial degree of adaptivity.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The goal of this paper is to get truly subcubic algorithms for Min-Plus\nproduct for less structured inputs than what was previously known, and to apply\nthem to versions of All-Pairs Shortest Paths (APSP) and other problems. The\nresults are as follows:\n  (1) Our main result is the first truly subcubic algorithm for the Min-Plus\nproduct of two $n\\times n$ matrices $A$ and $B$ with $\\text{polylog}(n)$ bit\ninteger entries, where $B$ has a partitioning into $n^{\\epsilon}\\times\nn^{\\epsilon}$ blocks (for any $\\epsilon>0$) where each block is at most\n$n^\\delta$-far (for $\\delta<3-\\omega$, where $2\\leq \\omega<2.373$) in\n$\\ell_\\infty$ norm from a constant rank integer matrix. This result presents\nthe most general case to date of Min-Plus product that is solvable in truly\nsubcubic time.\n  (2) The first application of our main result is a truly subcubic algorithm\nfor APSP in a new type of geometric graph. Our result extends the result of\nChan'10 in the case of integer edge weights by allowing the weights to differ\nfrom a function of the end-point identities by at most $n^\\delta$ for small\n$\\delta$.\n  (3) In the second application we consider a batch version of the range mode\nproblem in which one is given a length $n$ sequence and $n$ contiguous\nsubsequences, and one is asked to compute the range mode of each subsequence.\nWe give the first $O(n^{1.5-\\epsilon})$ time for $\\epsilon>0$ algorithm for\nthis batch range mode problem.\n  (4) Our final application is to the Maximum Subarray problem: given an\n$n\\times n$ integer matrix, find the contiguous subarray of maximum entry sum.\nWe show that Maximum Subarray can be solved in truly subcubic,\n$O(n^{3-\\epsilon})$ (for $\\epsilon>0$) time, as long as the entries are no\nlarger than $O(n^{0.62})$ in absolute value.\n  We also improve all the known conditional hardness results for the\n$d$-dimensional variant of Maximum Subarray.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In derivative-free and blackbox optimization, the objective function is often\nevaluated through the execution of a computer program seen as a blackbox. It\ncan be noisy, in the sense that its outputs are contaminated by random errors.\nSometimes, the source of these errors is identified and controllable, in the\nsense that it is possible to reduce the standard deviation of the stochastic\nnoise it generates. A common strategy to deal with such a situation is to\nmonotonically diminish this standard deviation, to asymptotically make it\nconverge to zero and ensure convergence of algorithms because the noise is\ndismantled. This work presents MpMads, an algorithm which follows this\napproach. However, in practice a reduction of the standard deviation increases\nthe computation time, and makes the optimization process long. Therefore, a\nsecond algorithm called DpMads is introduced to explore another strategy, which\ndoes not force the standard deviation to monotonically diminish. Although these\nstrategies are proved to be theoretically equivalents, tests on analytical\nproblems and an industrial blackbox are presented to illustrate practical\ndifferences.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This work first attempts to automatically recognize pancreatitis on CT scan\nimages. However, different form the traditional object recognition, such\npancreatitis recognition is challenging due to the fine-grained and non-rigid\nappearance variability of the local diseased regions. To this end, we propose a\ncustomized Region-Manipulated Fusion Networks (RMFN) to capture the key\ncharacteristics of local lesion for pancreatitis recognition. Specifically, to\neffectively highlight the imperceptible lesion regions, a novel\nregion-manipulated scheme in RMFN is proposed to force the lesion regions while\nweaken the non-lesion regions by ceaselessly aggregating the multi-scale local\ninformation onto feature maps. The proposed scheme can be flexibly equipped\ninto the existing neural networks, such as AlexNet and VGG. To evaluate the\nperformance of the propose method, a real CT image database about pancreatitis\nis collected from hospitals \\footnote{The database is available later}. And\nexperimental results on such database well demonstrate the effectiveness of the\nproposed method for pancreatitis recognition.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Exploration in environments with continuous control and sparse rewards\nremains a key challenge in reinforcement learning (RL). Recently, surprise has\nbeen used as an intrinsic reward that encourages systematic and efficient\nexploration. We introduce a new definition of surprise and its RL\nimplementation named Variational Assorted Surprise Exploration (VASE). VASE\nuses a Bayesian neural network as a model of the environment dynamics and is\ntrained using variational inference, alternately updating the accuracy of the\nagent's model and policy. Our experiments show that in continuous control\nsparse reward environments VASE outperforms other surprise-based exploration\ntechniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the outcome of generalised Lotka-Volterra dynamics of\necological communities with random interaction coefficients and non-linear\nfeedback. We show in simulations that the saturation of non-linear feedback\nstabilises the dynamics. This is confirmed in an analytical\ngenerating-functional approach to generalised Lotka-Volterra equations with\npiecewise linear saturating response. For such systems we are able to derive\nself-consistent relations governing the stable fixed-point phase, and to carry\nout a linear stability analysis to predict the onset of unstable behaviour. We\ninvestigate in detail the combined effects of the mean, variance and\nco-variance of the random interaction coefficients, and the saturation value of\nthe non-linear response. We find that stability and diversity increases with\nthe introduction of non-linear feedback, where decreasing the saturation value\nhas a similar effect to decreasing the co-variance. We also find co-operation\nto no longer have a detrimental effect on stability with non-linear feedback,\nand the order parameters mean abundance and diversity to be less dependent on\nthe symmetry of interactions with stronger saturation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose two new benchmark scenarios for Higgs-boson searches in the\nMinimal Supersymmetric Standard Model (MSSM). These scenarios are specifically\ndesigned for the low $\\tan\\beta$ region. A light Higgs-boson mass prediction\ncompatible with the observed value of $125$ GeV is ensured in almost the entire\nparameter space by employing a flexible supersymmetric (SUSY) mass scale,\nreaching values of up to $10^{16}$ GeV. The MSSM Higgs-sector predictions are\nevaluated in an effective field theory (EFT) framework that exhibits a\nTwo-Higgs-Doublet-Model at the low scale. In the first scenario all SUSY\nparticles are relatively heavy, whereas the second scenario features light\nneutralinos and charginos. Both scenarios are largely compatible with the most\nrecent results from Run 2 of the LHC, and we highlight the main\nphenomenological features relevant for future LHC searches. In particular, we\nprovide a detailed discussion of heavy Higgs-boson decays to neutralinos and\ncharginos in the second scenario, and the arising collider signatures, in order\nto facilitate the design of dedicated LHC searches in the near future.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the inverse seesaw scenario, several fermion singlets have a small\nMajorana mass term. We show such Majorana masses can be suppressed by some\nheavy fermion and/or Higgs singlets after a global symmetry is spontaneously\nbroken. These interactions can also accommodate a leptogenesis mechanism to\nexplain the cosmic baryon asymmetry.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have calculated the $W$-loop contribution to the amplitude of the decay $H\n\\to Z \\gamma$ in the unitary gauge through the dispersion method and in the\n$R_\\xi$ gauge using dimensional regularization (DimReg). We show that the\nresults of the calculations with DimReg and the dispersion method, adopting the\nboundary condition at the limit $M_W \\to 0$ defined by the Goldstone boson\nequivalence theorem (GBET), completely coincide. This implies that the\ndispersion method obeying the GBET is compatible with DimReg. The advantage of\nthe applied dispersion method is that we work with finite quantities and no\nregularization is required.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  It remains a puzzle that why deep neural networks (DNNs), with more\nparameters than samples, often generalize well. An attempt of understanding\nthis puzzle is to discover implicit biases underlying the training process of\nDNNs, such as the Frequency Principle (F-Principle), i.e., DNNs often fit\ntarget functions from low to high frequencies. Inspired by the F-Principle, we\npropose an effective model of linear F-Principle (LFP) dynamics which\naccurately predicts the learning results of two-layer ReLU neural networks\n(NNs) of large widths. This LFP dynamics is rationalized by a linearized mean\nfield residual dynamics of NNs. Importantly, the long-time limit solution of\nthis LFP dynamics is equivalent to the solution of a constrained optimization\nproblem explicitly minimizing an FP-norm, in which higher frequencies of\nfeasible solutions are more heavily penalized. Using this optimization\nformulation, an a priori estimate of the generalization error bound is\nprovided, revealing that a higher FP-norm of the target function increases the\ngeneralization error. Overall, by explicitizing the implicit bias of the\nF-Principle as an explicit penalty for two-layer NNs, our work makes a step\ntowards a quantitative understanding of the learning and generalization of\ngeneral DNNs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present electronic and transport properties of a zigzag nanoribbon made of\nalpha-$\\mathcal{T}_3$ lattice. Our particular focus is on the effects of the\ncontinous evolution of the edge modes ( from flat to dispersive) on the\nthermoelectric transport properties. Unlike the case of graphene nanoribbon,\nthe zigzag nanoribbon of $\\alpha-\\mathcal{T}_3$ lattice can host a pair of\ndispersive (chiral) edge modes at the two valleys for specific width of the\nribbon. Moreover, gap opening can also occur at the two valleys depending on\nthe width. The slope of the chiral edge modes and the energy gap strongly\ndepend on the relative strength of two kinds of hoping parameters present in\nthe system. We compute corresponding transport coefficients such as\nconductance, thermopower, thermalconductivity and the thermoelectric figure of\nmerits by using the tight-binding Green function formalism, in order to explore\nthe roles of the dispersive edge modes. It is found that the thermopower and\nthermoelectric figure of merits can be enhanced significantly by suitably\ncontrolling the edge modes. The figure of merits can be enhanced by ten times\nunder suitable parameter regime in comparison to the case of graphene. Finally,\nwe reveal that the presence of line defect, close to the edge, can cause a\nsignificant impact on the edge modes as well as on electrical conductance.\nHowever, thermopower is relatively less sensitive to such defects.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Band-resolved frequency modulation spectroscopy is a common method to measure\nweak signals of radiative ensembles. When the optical depth of the medium is\nlarge, the signal drops exponentially and the technique becomes ineffective. In\nthis situation, we show that a signal can be recovered when a larger modulation\nindex is applied. Noticeably, this signal can be dominated by the natural\nlinewidth of the resonance, regardless of the presence of inhomogeneous line\nbroadening. We implement this technique on a cesium vapor, and then explore its\nmain spectroscopic features. This work opens the road towards measurement of\ncooperative emission effects in bulk atomic ensemble.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The interaction picture in a non-Hermitian realization is discussed in detail\nand considered for its practical use in many-body quantum physics. The\nresulting non-Hermitian interaction-picture (NHIP) description of dynamics, in\nwhich both the wave functions and operators belonging to physical observables\ncease to remain constant in time, is a non-Hermitian generalization of the\ntraditional Dirac picture of standard quantum mechanics, which itself is widely\nused in quantum field theory calculations. Particular attention is paid here to\nthe variational (or, better, bivariational) and dynamical (i.e.,\nnon-stationary) aspects that are characteristic of the coupled cluster method\n(CCM) techniques that nowadays form one of the most versatile and most accurate\nof all available formulations of quantum many-body theory. In so doing we\nexpose and exploit multiple parallels between the NHIP and the CCM in its\ntime-dependent versions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cannabinoid research requires the cooperation of experts from various field\nbiochemistry and chemistry to psychological and social sciences. The data that\nhave to be managed and analysed are highly heterogeneous, especially because\nthey are provided by a very diverse range of sources. A number of approaches\nfocused on data collection and the corresponding analysis, restricting the\nscope to a sub-domain. Our goal is to elaborate a solution that would allow for\nautomated management and analysis of heterogeneous data within the complete\ncannabinoids domain. The corresponding integration of diverse data sources\nwould increase the quality and preciseness of the analysis. In this paper, we\nintroduce the core ideas of the proposed framework as well as present the\nimplemented prototype of a cannabinoids data platform.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In order to operate autonomously, a robot should explore the environment and\nbuild a model of each of the surrounding objects. A common approach is to\ncarefully scan the whole workspace. This is time-consuming. It is also often\nimpossible to reach all the viewpoints required to acquire full knowledge about\nthe environment. Humans can perform shape completion of occluded objects by\nrelying on past experience. Therefore, we propose a method that generates\nimages of an object from various viewpoints using a single input RGB image. A\ndeep neural network is trained to imagine the object appearance from many\nviewpoints. We present the whole pipeline, which takes a single RGB image as\ninput and returns a sequence of RGB and depth images of the object. The method\nutilizes a CNN-based object detector to extract the object from the natural\nscene. Then, the proposed network generates a set of RGB and depth images. We\nshow the results both on a synthetic dataset and on real images.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The motion control of a levitated nanoparticle plays a central role in\noptical levitation for fundamental studies and practical applications. Here, we\npresented a digital parametric feedback cooling based on switching between two\ntrapping laser intensity levels with square wave modulations. The effects of\nmodulation depth and modulation signal phase on the cooling result were\ninvestigated in detail. With such a digital parametric feedback method, the\ncentre-of-mass temperature of all three motional degrees of freedom can be\ncooled to dozens of milli-Kelvin, which paved the way to fully control the\nmotion of the levitated nanoparticle with a programmable digital process for\nwild applications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The classical perceptron is a simple neural network that performs a binary\nclassification by a linear mapping between static inputs and outputs and\napplication of a threshold. For small inputs, neural networks in a stationary\nstate also perform an effectively linear input-output transformation, but of an\nentire time series. Choosing the temporal mean of the time series as the\nfeature for classification, the linear transformation of the network with\nsubsequent thresholding is equivalent to the classical perceptron. Here we show\nthat choosing covariances of time series as the feature for classification maps\nthe neural network to what we call a 'covariance perceptron'; a mapping between\ncovariances that is bilinear in terms of weights. By extending Gardner's theory\nof connections to this bilinear problem, using a replica symmetric mean-field\ntheory, we compute the pattern and information capacities of the covariance\nperceptron in the infinite-size limit. Closed-form expressions reveal superior\npattern capacity in the binary classification task compared to the classical\nperceptron in the case of a high-dimensional input and low-dimensional output.\nFor less convergent networks, the mean perceptron classifies a larger number of\nstimuli. However, since covariances span a much larger input and output space\nthan means, the amount of stored information in the covariance perceptron\nexceeds the classical counterpart. For strongly convergent connectivity it is\nsuperior by a factor equal to the number of input neurons. Theoretical\ncalculations are validated numerically for finite size systems using a\ngradient-based optimization of a soft-margin, as well as numerical solvers for\nthe NP hard quadratically constrained quadratic programming problem, to which\ntraining can be mapped.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Consider a smooth complex surface $X$ which is a double cover of the\nprojective plane $\\mathbb{P}^2$ branched along a smooth curve of degree $2s$.\nIn this article, we study the geometric conditions which are equivalent to the\nexistence of Ulrich line bundles on $X$ with respect to this double covering.\nAlso, for every $s\\geq 1$, we describe the classes of such surfaces which admit\nUlrich line bundles and give examples.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Developing reconfigurable millimeter-wave (mmWave) antennas and devices is an\noutstanding challenge, with switch technologies being a primary impediment.\nRecently, it has been shown that vanadium dioxide (VO2), a thermochromic\nmaterial whose resistance changes with temperature, could provide a path\nforward in developing reconfigurable mmWave devices. As an initial step towards\nthis vision, we investigate the integration of VO2 switches in reconfigurable\ncomponents at 15 GHz. In particular, a frequency reconfigurable antenna and a\nreconfigurable phase shifter are shown. The low loss and minimal parasitics of\nVO2 technology have the potential to enable devices at 15 GHz and beyond.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have used data from the Galaxy Evolution Explorer to study the different\ncomponents of the diffuse ultraviolet background in the region between the\nGalactic latitudes 70-80 degree. We find an offset at zero dust column density\n(E(B - V) = 0) of $240 \\pm 18$ photon units in the FUV (1539A) and $394 \\pm 37$\nphoton units in the NUV (2316A). This is approximately half of the total\nobserved radiation with the remainder divided between an extragalactic\ncomponent of $114 \\pm 18$ photon units in the FUV and $194 \\pm 37$ photon units\nin the NUV and starlight scattered by Galactic dust at high latitudes. The\noptical constants of the dust grains were found to be a=0.4$\\pm$0.1 and\ng=0.8$\\pm$0.1 (FUV) and a=0.4$\\pm$0.1 and g=0.5$\\pm$0.1 (NUV). We cannot\ndifferentiate between a Galactic or extragalactic origin for the zero-offset\nbut can affirm that it is not from any known source.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Knowledge of optical properties, such as the refractive index (RI), of\nbiological tissues is important in optical imaging, as they influence the\ndistribution and propagation of light in tissue. To accurately study the\nresponse of cancerous cells to drugs, optimised imaging protocols are required.\nThis study uses a simple custom-built spectral domain optical coherence\ntomography (OCT) system to conduct RI measurements of multicellular spheroids,\nthree-dimensional \\textit{in-vitro} culture systems, of the cell line HCT116.\nThe spheroid RIs are compared to study the effect of growth time. To improve\nconfocal microscopy imaging protocols, two immersion media (glycerol and\nScaleView-A2) matching the spheroid RIs were trialled, with the aim to reduce\nthe RI mismatch between the spheroid and the immersion medium and thus\nimproving imaging depth with confocal microscopy. ScaleView-A2 (n = 1.380)\naided in achieving greater depths of imaging of the multicellular spheroids\nunder confocal microscopy. This improvement in imaging depth confirmed the\nutility of our RI measurements, proving the promising outlook of OCT as a\ncomplementary tool to microscopy in cancer research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce the arrow product, a systematic generating function technique\nfor directed graph enumeration. It provides short proofs for previous results\nof Gessel on the number of directed acyclic graphs and of Liskovets, Robinson\nand Wright on the number of strongly connected directed graphs. We also recover\nRobinson's enumerative results on directed graphs where all strongly connected\ncomponents belong to a given family.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The discovery of nongravitational interactions between dark matter and the\nStandard Model would be an important step in unraveling the nature of dark\nmatter. If such an interaction exists, it would have profound implications on\nhow dark matter is produced in both the early universe and in collider\nexperiments. In addition, it would also allow dark matter to deposit energy\ninto Standard Model particles in unexpected ways. This thesis details some\nrecent progress made in understanding these implications, including (i) a new\nfreezeout mechanism for thermal dark matter dominated by a 3-to-2 process\nwithin a vector portal dark sector model; (ii) a study of how the existence of\ndark sector bound states can influence collider, direct and indirect searches\nfor dark matter; (iii) a new axion dark matter interferometric search using a\ncavity that is sensitive to the axion-induced rotation of linearly polarized\nlight; (iv) a definitive assessment of the potential contribution of dark\nmatter annihilation and decay to cosmic reionization; (v) new constraints on\ndark matter annihilation rates and decay lifetimes from 21-cm cosmology, and\n(vi) a new numerical code, DarkHistory, which significantly improves the\ncomputation of the ionization and thermal histories of the universe in the\npresence of exotic sources of energy injection. These novel ideas span length\nscales ranging from table-top experiments to the entire cosmos, and represent\njust a few of the myriad of ways in which dark matter may yet surprise us.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a neural program synthesis approach integrating components which\nwrite, execute, and assess code to navigate the search space of possible\nprograms. We equip the search process with an interpreter or a\nread-eval-print-loop (REPL), which immediately executes partially written\nprograms, exposing their semantics. The REPL addresses a basic challenge of\nprogram synthesis: tiny changes in syntax can lead to huge changes in\nsemantics. We train a pair of models, a policy that proposes the new piece of\ncode to write, and a value function that assesses the prospects of the code\nwritten so-far. At test time we can combine these models with a Sequential\nMonte Carlo algorithm. We apply our approach to two domains: synthesizing text\nediting programs and inferring 2D and 3D graphics programs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A real univariate polynomial with all roots real is called hyperbolic. By\nDescartes' rule of signs for hyperbolic polynomials (HPs) with all coefficients\nnonvanishing, a HP with $c$ sign changes and $p$ sign preservations in the\nsequence of its coefficients has exactly $c$ positive and $p$ negative roots.\nFor $c=2$ and for degree $6$ HPs, we discuss the question: When the moduli of\nthe $6$ roots of a HP are arranged in the increasing order on the real\nhalf-line, at which positions can be the moduli of its two positive roots\ndepending on the positions of the two sign changes in the sequence of\ncoefficients?\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This is an integrative review that address the question, \"What makes for a\ngood explanation?\" with reference to AI systems. Pertinent literatures are\nvast. Thus, this review is necessarily selective. That said, most of the key\nconcepts and issues are expressed in this Report. The Report encapsulates the\nhistory of computer science efforts to create systems that explain and instruct\n(intelligent tutoring systems and expert systems). The Report expresses the\nexplainability issues and challenges in modern AI, and presents capsule views\nof the leading psychological theories of explanation. Certain articles stand\nout by virtue of their particular relevance to XAI, and their methods, results,\nand key points are highlighted. It is recommended that AI/XAI researchers be\nencouraged to include in their research reports fuller details on their\nempirical or experimental methods, in the fashion of experimental psychology\nresearch reports: details on Participants, Instructions, Procedures, Tasks,\nDependent Variables (operational definitions of the measures and metrics),\nIndependent Variables (conditions), and Control Conditions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  While analysing time-to-event data, it is possible that a certain fraction of\nsubjects will never experience the event of interest and they are said to be\ncured. When this feature of survival models is taken into account, the models\nare commonly referred to as cure models. In the presence of covariates, the\nconditional survival function of the population can be modelled by using cure\nmodel which depends on the probability of being uncured (incidence) and the\nconditional survival function of the uncured subjects (latency), and a\ncombination of logistic regression and Cox proportional hazards (PH) regression\nis used to model the incidence and latency respectively. In this paper, we have\nshown the asymptotic normality of the profile likelihood estimator via\nasymptotic expansion of the profile likelihood and obtain the explicit form of\nthe variance estimator with an implicit function in the profile likelihood. We\nhave also shown the efficient score function based on projection theory and the\nprofile likelihood score function are equal. Our contribution in this paper is\nthat we have expressed the efficient information matrix as the variance of the\nprofile likelihood score function. A simulation study suggests that the\nestimated standard errors from bootstrap samples (SMCURE package) and the\nprofile likelihood score function (our approach) are providing similar and\ncomparable results. The numerical result of our proposed method is also shown\nby using the melanoma data from SMCURE R-package (Cai et al., 2012) and we\ncompare the results with the output obtained from SMCURE package.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Developing a detailed understanding for the role of core electron excitation\nin liquid water under proton irradiation has become important due to the\ngrowing use of proton beams in radiation oncology. Using a first-principles,\nnon-equilibrium simulation approach based on real-time time-dependent density\nfunctional theory, we determine the electronic stopping power, the\nvelocity-dependent energy transfer rate from irradiating ions to electrons. The\nelectronic stopping power curve agrees quantitatively over the entire velocity\nrange for which experimental data is available. Also notably, we observe\nsignificant differences between our first-principles results and commonly-used\nperturbation theoretic models. Excitations of the water molecules' oxygen core\nelectrons are a crucial factor in determining the electronic stopping power\ncurve beyond its maximum. The core electron contribution is responsible for as\nmuch as one-third of the stopping power at the high proton velocity of 8.0 a.u.\n(1.6 MeV). These K-shell core electron excitations not only provide an\nadditional channel for the energy transfer but they also significantly\ninfluence the valence electron excitation. In the excitation process, generated\nholes remain highly localized within a few angstroms around the irradiating\nproton path whereas electrons are excited away from the path. In spite of its\ngreat contribution to the electronic stopping power, the K-shell electrons play\na rather minor role in terms of the excitation density: Only one percent of the\nhole population comprises K-shell holes, even at the high proton velocity of\n8.0 a.u.. The excitation behavior revealed here is distinctly different from\nthat of X/gamma-ray photon radiation, which is the most commonly used type of\nionizing radiation in radiation oncology.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep neural networks are a promising approach towards multi-task learning\nbecause of their capability to leverage knowledge across domains and learn\ngeneral purpose representations. Nevertheless, they can fail to live up to\nthese promises as tasks often compete for a model's limited resources,\npotentially leading to lower overall performance. In this work we tackle the\nissue of interfering tasks through a comprehensive analysis of their training,\nderived from looking at the interaction between gradients within their shared\nparameters. Our empirical results show that well-performing models have low\nvariance in the angles between task gradients and that popular regularization\nmethods implicitly reduce this measure. Based on this observation, we propose a\nnovel gradient regularization term that minimizes task interference by\nenforcing near orthogonal gradients. Updating the shared parameters using this\nproperty encourages task specific decoders to optimize different parts of the\nfeature extractor, thus reducing competition. We evaluate our method with\nclassification and regression tasks on the multiDigitMNIST, NYUv2 and SUN RGB-D\ndatasets where we obtain competitive results.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Scattering of light dark matter with sub-eV energy deposition can be detected\nwith collective excitations in condensed matter systems. When dark matter has\nspin-independent couplings to atoms or ions, it has been shown to efficiently\nexcite phonons. Here we show that, if dark matter couples to the electron spin,\nmagnon excitations in materials with magnetic dipole order offer a promising\ndetection path. We derive general formulae for single magnon excitation rates\nfrom dark matter scattering, and demonstrate as a proof of principle the\nprojected reach of a yttrium iron garnet target for several dark matter models\nwith spin-dependent interactions. This highlights the complementarity of\nvarious collective excitations in probing different dark matter interactions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Our goal is to study the termination of an AGN jet in the young universe and\nto deduce physical parameters of the jet and the intergalactic medium. We use\nLOFAR to image the long-wavelength radio emission of the high-redshift blazar\nS5 0836+710 on arcsecond scales between 120 MHz and 160 MHz. The LOFAR image\nshows a compact unresolved core and a resolved emission region about 1.5 arcsec\nto the southwest of the radio core. This structure is in general agreement with\nprevious higher-frequency radio observations with MERLIN and the VLA. The\nsouthern component shows a moderately steep spectrum with a spectral index of\nabout $\\gtrsim -1$ while the spectral index of the core is flat to slightly\ninverted. In addition, we detect for the first time a resolved steep-spectrum\nhalo with a spectral index of about $-1$ surrounding the core. The\narcsecond-scale radio structure of S5 0836+710 can be understood as an FR\nII-like radio galaxy observed at a small viewing angle. The southern component\ncan be interpreted as the region of the approaching jet's terminal hotspot and\nthe halo-like diffuse component near the core can be interpreted as the\ncounter-hotspot region. From the differential Doppler boosting of both\nfeatures, we can derive the hotspot advance speed to $(0.01-0.036)$ c. At a\nconstant advance speed, the derived age of the source would exceed the total\nlifetime of such a powerful FR II-like radio galaxy substantially. Thus, the\nhotspot advance speed must have been higher in the past in agreement with a\nscenario in which the originally highly relativistic jet has lost collimation\ndue to the growth of instabilities and has transformed into an only mildly\nrelativistic flow. Our data suggest that the density of the intergalactic\nmedium around this distant ($z=2.22$) AGN could be substantially higher than\nthe values typically found in less distant FR II radio galaxies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have studied front dynamics for the discrete $A+A \\leftrightarrow A$\nreaction-diffusion system, which in the continuum is described by the\n(stochastic) Fisher-Kolmogorov-Petrovsky-Piscunov equation. We have revisited\nthis discrete model in two space dimensions by means of extensive numerical\nsimulations and an improved analysis of the time evolution of the interface\nseparating the stable and unstable phases. In particular, we have measured the\nfull set of critical exponents which characterize the spatio-temporal\nfluctuations of such front for different lattice sizes, focusing mainly in the\nfront width and correlation length. These exponents are in very good agreement\nwith those computed in [E. Moro, Phys. Rev. Lett. 87, 238303 (2001)] and\ncorrespond to those of the Kardar-Parisi-Zhang (KPZ) universality class for\none-dimensional interfaces. Furthermore, we have studied the one-point\nstatistics and the covariance of rescaled front fluctuations, which had\nremained thus far unexplored in the literature and allows for a further\nstringent test of KPZ universality.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Spectral degeneracies of quantum magnets are often described as diabolical\npoints or magnetic Weyl points, which carry topological charge. Here, we study\na simple, yet experimentally relevant quantum magnet: two localized interacting\nelectrons subject to spin-orbit coupling. In this setting, the degeneracies are\nnot necessarily isolated points, but can also form a line or a surface. We\nidentify ten different possible geometrical patterns formed by these degeneracy\npoints, and study their stability under small perturbations of the Hamiltonian.\nStable structures are found to depend on the relative sign of the determinants\nof the two $g$-tensors, $\\cal S$. Both for ${\\cal S}=+1$ and ${\\cal S}=-1$, two\nstable configurations are found, and three out of these four configurations are\nformed by pairs of Weyl points. These stable regions are separated by a surface\nof almost stable configurations, with a structure akin to co-dimension one\nbifurcations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This note deals with the local exact controllability to a particular class of\ntrajectories for the Boussinesq system with nonlinear Navier-slip boundary\nconditions and internal controls having vanishing components. Briefly speaking,\nin two dimensions, the local exact controllability property is obtained using\nonly one control in the heat equation, meanwhile two scalar controls are\nrequired in three dimensions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We extend the recently developed hybrid quark-meson-nucleon model by\naugmenting a six-point scalar interaction and investigate the consequences for\nneutron-star sequences in the mass-radius diagram. The model has the\ncharacteristic feature that, at increasing baryon density, the chiral symmetry\nis restored within the hadronic phase by lifting the mass splitting between\nchiral partner states (parity doubling), before quark deconfinement takes\nplace. At low temperature and finite baryon density, the model predicts a\nfirst-, second-order chiral phase transition, or a crossover, depending on the\nexpectation value of the scalar field, and a first-order deconfinement phase\ntransition. We discuss two sets of free parameters, which result in\ncompact-star mass-radius relations that are at tension with the combined\nconstraints for maximum-mass ($2~M_\\odot$) and the compactness (GW170817). We\nfind that the most preferable mass-radius relations result in isospin-symmetric\nphase diagram with rather low temperature for the critical point of the chiral\nphase transition.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we study the indirect boundary stability and exact\ncontrollability of a one-dimensional Timoshenko system. In the first part of\nthe paper, we consider the Timoshenko system with only one boundary fractional\ndamping. We first show that the system is strongly stable but not uniformly\nstable. Hence, we look for a polynomial decay rate for smooth initial data.\nUsing frequency domain arguments combined with the multiplier method, we prove\nthat the energy decay rate depends on coefficients appearing in the PDE and on\nthe order of the fractional damping. Moreover, under the equal speed\npropagation condition, we obtain the optimal polynomial energy decay rate. In\nthe second part of this paper, we study the indirect boundary exact\ncontrollability of the Timoshenko system with mixed Dirichlet-Neumann boundary\nconditions and boundary control. Using non-harmonic analysis, we first\nestablish a weak observability inequality, which depends on the ratio of the\nwaves propagation speeds. Next, using the HUM method, we prove that the system\nis exactly controllable in appropriate spaces and that the control time can be\nsmall.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The presented algorithms for segmentation and tracking follow a 3-step\napproach where we detect, track and finally segment nuclei. In the\npreprocessing phase, we detect centroids of the cell nuclei using a\nconvolutional neural network (CNN) for the 2D images and a\nLaplacian-of-Gaussian Scale Space Maximum Projection approach for the 3D data\nsets. Tracking was performed in a backwards fashion on the predicted seed\npoints, i.e., starting at the last frame and sequentially connecting\ncorresponding objects until the first frame was reached. Correspondences were\nidentified by propagating detections of a frame t to its preceding frame t-1\nand by combining redundant detections using a hierarchical clustering approach.\nThe tracked centroids were then used as input to variants of the seeded\nwatershed algorithm to obtain the final segmentation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We establish an error term in the Sato-Tate theorem of Birch. That is, for\n$p$ prime, $q=p^r$ we show that $\\#\\{ (a,b) \\in \\mathbb{F}_q^2 :\n\\theta_{a,b}\\in I\\} =\\mu_{ST}(I)q^2 + O_r(q^{7/4})$ for any interval\n$I\\subseteq[0,\\pi]$ where for an elliptic curve $E: y^2= x^3 +ax +b$, the\nquantity $\\theta_{a,b}$ is defined by $2\\sqrt{q}\\cos\\theta_{a,b} =\nq+1-E(\\mathbb{F}_q)$ and $\\mu_{ST}(I)$ denotes the Sato-Tate measure of the\ninterval $I$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We propose a nonparametric model for time series with missing data based on\nlow-rank matrix factorization. The model expresses each instance in a set of\ntime series as a linear combination of a small number of shared basis\nfunctions. Constraining the functions and the corresponding coefficients to be\nnonnegative yields an interpretable low-dimensional representation of the data.\nA time-smoothing regularization term ensures that the model captures meaningful\ntrends in the data, instead of overfitting short-term fluctuations. The\nlow-dimensional representation makes it possible to detect outliers and cluster\nthe time series according to the interpretable features extracted by the model,\nand also to perform forecasting via kernel regression. We apply our methodology\nto a large real-world dataset of infant-sleep data gathered by caregivers with\na mobile-phone app. Our analysis automatically extracts daily-sleep patterns\nconsistent with the existing literature. This allows us to compute\nsleep-development trends for the cohort, which characterize the emergence of\ncircadian sleep and different napping habits. We apply our methodology to\ndetect anomalous individuals, to cluster the cohort into groups with different\nsleeping tendencies, and to obtain improved predictions of future sleep\nbehavior.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The ubiquitous availability of wearable sensors is responsible for driving\nthe Internet-of-Things but is also making an impact on sport sciences and\nprecision medicine. While human activity recognition from smartphone data or\nother types of inertial measurement units (IMU) has evolved to one of the most\nprominent daily life examples of machine learning, the underlying process of\ntime-series feature engineering still seems to be time-consuming. This lengthy\nprocess inhibits the development of IMU-based machine learning applications in\nsport science and precision medicine. This contribution discusses a feature\nengineering workflow, which automates the extraction of time-series feature on\nbased on the FRESH algorithm (FeatuRe Extraction based on Scalable Hypothesis\ntests) to identify statistically significant features from synchronized IMU\nsensors (IMeasureU Ltd, NZ). The feature engineering workflow has five main\nsteps: time-series engineering, automated time-series feature extraction,\noptimized feature extraction, fitting of a specialized classifier, and\ndeployment of optimized machine learning pipeline. The workflow is discussed\nfor the case of a user-specific running-walking classification, and the\ngeneralization to a multi-user multi-activity classification is demonstrated.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The disordered many-body systems can undergo a transition from the extended\nensemble to a localized ensemble, known as many-body localization (MBL), which\nhas been intensively explored in recent years. Nevertheless, the relation\nbetween Anderson localization (AL) and MBL is still elusive. Here we show that\nthe MBL can be regarded as an infinite-dimensional AL with the correlated\ndisorder in a virtual lattice. We demonstrate this idea using the disordered\nXXZ model, in which the excitation of $d$ spins over the fully polarized phase\ncan be regarded as a single-particle model in a $d$ dimensional virtual\nlattice. With the increasing of $d$, the system will quickly approach the MBL\nphase, in which the infinite-range correlated disorder ensures the saturation\nof the critical disorder strength in the thermodynamic limit. From the\ntransition from AL to MBL, the entanglement entropy and the critical exponent\nfrom energy level statics are shown to depend weakly on the dimension,\nindicating that belonging to the same universal class. This work clarifies the\nfundamental concept of MBL and presents a new picture for understanding the MBL\nphase in terms of AL.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Several recently devised machine learning (ML) algorithms have shown improved\naccuracy for various predictive problems. Model searches, which explore to find\nan optimal ML algorithm and hyperparameter values for the target problem, play\na critical role in such improvements. During a model search, data scientists\ntypically use multiple ML implementations to construct several predictive\nmodels; however, it takes significant time and effort to employ multiple ML\nimplementations due to the need to learn how to use them, prepare input data in\nseveral different formats, and compare their outputs. Our proposed framework\naddresses these issues by providing simple and unified coding method. It has\nbeen designed with the following two attractive features: i) new machine\nlearning implementations can be added easily via common interfaces between the\nframework and ML implementations and ii) it can be scaled to handle large model\nconfiguration search spaces via profile-based scheduling. The results of our\nevaluation indicate that, with our framework, implementers need only write\n55-144 lines of code to add a new ML implementation. They also show that ours\nwas the fastest framework for the HIGGS dataset, and the second-fastest for the\nSECOM dataset.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Unsupervised learning of disentangled representations is an open problem in\nmachine learning. The Disentanglement-PyTorch library is developed to\nfacilitate research, implementation, and testing of new variational algorithms.\nIn this modular library, neural architectures, dimensionality of the latent\nspace, and the training algorithms are fully decoupled, allowing for\nindependent and consistent experiments across variational methods. The library\nhandles the training scheduling, logging, and visualizations of reconstructions\nand latent space traversals. It also evaluates the encodings based on various\ndisentanglement metrics. The library, so far, includes implementations of the\nfollowing unsupervised algorithms VAE, Beta-VAE, Factor-VAE, DIP-I-VAE,\nDIP-II-VAE, Info-VAE, and Beta-TCVAE, as well as conditional approaches such as\nCVAE and IFCVAE. The library is compatible with the Disentanglement Challenge\nof NeurIPS 2019, hosted on AICrowd, and achieved the 3rd rank in both the first\nand second stages of the challenge.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Superconductor-Ferromagnet hybrid structures (SF) have attracted much\ninterest in the last decades, due to a variety of interesting phenomena\npredicted and observed in these structures. One of them is the so-called\ninverse proximity effect. It is described by a spin polarization of Cooper\npairs, which occurs not only in the ferromagnet (F), but also in the\nsuperconductor (S) yielding a finite magnetic moment $M_{\\text{S}}$ inside the\nsuperconductor. This effect has been predicted and experimentally studied.\nHowever, interpretation of the experimental data is mostly ambiguous. Here, we\nstudy theoretically the impact of the spin polarized Cooper pairs on the\nJosephson effect in an SFS junction. We show that the induced magnetic moment\n$M_{\\text{S}}$ does depend on the phase difference $\\varphi$ and therefore,\nwill oscillate in time with the Josephson frequency $2eV/\\hbar$ if the current\nexceeds a critical value. Most importantly, the spin polarization in the\nsuperconductor causes a significant change in the Fraunhofer pattern, which can\nbe easily accessed experimentally.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper is about the length $X_{\\rm MAX}$ of the longest path in directed\nacyclic graph (DAG) $G=(V,E)$ with random edge lengths, where $|V|=n$ and\n$|E|=m$. When the edge lengths are mutually independent and uniformly\ndistributed, the problem of computing the distribution function $\\Pr[X_{\\rm\nMAX}\\le x]$ is known to be $\\#$P-hard even in case $G$ is a directed path. In\nthis case, $\\Pr[X_{\\rm MAX}\\le x]$ is equal to the volume of the knapsack\npolytope, an $m$-dimensional unit hypercube truncated by a halfspace. In this\npaper, we show that there is a deterministic fully polynomial time\napproximation scheme (FPTAS) for computing $\\Pr[X_{\\rm MAX}\\le x]$ in case the\ntreewidth of $G$ is at most a constant $k$. The running time of our algorithm\nis $O(k^2 n(\\frac{16(k+1)mn^2}{\\epsilon})^{4k^2+6k+2})$ to achieve a\nmultiplicative approximation ratio $1+\\epsilon$. Before our FPTAS, we present a\nfundamental formula that represents $\\Pr[X_{\\rm MAX}\\le x]$ by at most $n-1$\nrepetitions of definite integrals. Moreover, in case the edge lengths follow\nthe mutually independent standard exponential distribution, we show a\n$((4k+2)mn)^{O(k)}$ time exact algorithm. For random edge lengths satisfying\ncertain conditions, we also show that computing $\\Pr[X_{\\rm MAX}\\le x]$ is\nfixed parameter tractable if we choose treewidth $k$, the additive error\n$\\epsilon'$, and $x$ as the parameters.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently, the BESIII collaboration has reported numerous measurements of\nvarious $D_{(s)}$ meson semileptonic decays with significantly improved\nprecision. Together with similar studies carried out at BABAR, Belle, and CLEO,\nnew windows to a better understanding of weak and strong interactions in the\ncharm sector have been opened. In light of new experimental data, we review the\ntheoretical description and predictions for the semileptonic decays of\n$D_{(s)}$ to a pseudoscalar or a vector meson. This review is essentially an\nextended discussion of our recently published results obtained in the framework\nof the covariant confining quark model.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  These lectures review the formalism of renormalization in quantum field\ntheories with special regard to effective quantum field theories. While\nrenormalization theory is part of every advanced course on quantum field\ntheory, for effective theories some more advanced topics become particularly\nimportant. This includes the renormalization of composite operators, operator\nmixing under scale evolution, and the resummation of large logarithms of scale\nratios. This course thus sets the basis for many of the more specialized\nlecture courses delivered at the 2017 Les Houches Summer School.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $\\psi$ be a sentence in the counting monadic second-order logic of\nmatroids and let $\\mathbb{F}$ be a finite field. Hlin\\v{e}n\\'{y}'s Theorem says\nthat we can test whether $\\mathbb{F}$-representable matroids satisfy $\\psi$\nusing an algorithm that is fixed-parameter tractable with respect to\nbranch-width. In a previous paper we proved there is a similar fixed-parameter\ntractable algorithm that can test the members of any efficiently pigeonhole\nclass. In this sequel we apply results from the first paper and thereby extend\nHlin\\v{e}n\\'{y}'s Theorem to the classes of fundamental transversal matroids,\nlattice path matroids, bicircular matroids, and $H$-gain-graphic matroids, when\n$H$ is a finite group. As a consequence, we can obtain a new proof of\nCourcelle's Theorem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We derive the optimal rate of convergence for the mean squared error at the\nterminal point for anticipating linear stochastic differential equations, where\nthe integral is interpreted in Skorohod sense. Although alternative proof\ntechniques are needed, our results can be seen as generalizations of the\ncorresponding results for It\\=o SDEs. As a key tool we extend optimal\napproximation results for vectors of correlated Wiener integrals to general\nrandom vectors, which contain the solutions of our Skorohod SDEs.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A semantic parser maps natural language commands (NLs) from the users to\nexecutable meaning representations (MRs), which are later executed in certain\nenvironment to obtain user-desired results. The fully-supervised training of\nsuch parser requires NL/MR pairs, annotated by domain experts, which makes them\nexpensive to collect. However, weakly-supervised semantic parsers are learnt\nonly from pairs of NL and expected execution results, leaving the MRs latent.\nWhile weak supervision is cheaper to acquire, learning from this input poses\ndifficulties. It demands that parsers search a large space with a very weak\nlearning signal and it is hard to avoid spurious MRs that achieve the correct\nanswer in the wrong way. These factors lead to a performance gap between\nparsers trained in weakly- and fully-supervised setting. To bridge this gap, we\nexamine the intersection between weak supervision and active learning, which\nallows the learner to actively select examples and query for manual annotations\nas extra supervision to improve the model trained under weak supervision. We\nstudy different active learning heuristics for selecting examples to query, and\nvarious forms of extra supervision for such queries. We evaluate the\neffectiveness of our method on two different datasets. Experiments on the\nWikiSQL show that by annotating only 1.8% of examples, we improve over a\nstate-of-the-art weakly-supervised baseline by 6.4%, achieving an accuracy of\n79.0%, which is only 1.3% away from the model trained with full supervision.\nExperiments on WikiTableQuestions with human annotators show that our method\ncan improve the performance with only 100 active queries, especially for\nweakly-supervised parsers learnt from a cold start.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove that properly rescaled large planar Eulerian triangulations converge\nto the Brownian map. This result requires more than a standard application of\nthe methods that have been used to obtain the convergence of other families of\nplanar maps to the Brownian map, as the natural distance for Eulerian\ntriangulations is a canonical oriented pseudo-distance. To circumvent this\ndifficulty, we adapt the layer decomposition method established by Curien and\nLe Gall, which yields asymptotic proportionality between three natural\ndistances on planar Eulerian triangulations: the usual graph distance, the\ncanonical oriented pseudo-distance, and the Riemannian metric. This notably\ngives the first mathematical proof of a convergence to the Brownian map for\nmaps endowed with their Riemannian metric. Along the way, we also construct new\nmodels of infinite random maps, as local limits of large planar Eulerian\ntriangulations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Earth's magnetic field induces Zeeman splitting of the magnetic dipole\ntransitions of molecular oxygen in the atmosphere, which produces polarized\nemission in the millimeter-wave regime. This polarized emission is primarily\ncircularly polarized and manifests as a foreground with a dipole-shaped sky\npattern for polarization-sensitive ground-based cosmic microwave background\nexperiments, such as the Cosmology Large Angular Scale Surveyor (CLASS), which\nis capable of measuring large angular scale circular polarization. Using\natmospheric emission theory and radiative transfer formalisms, we model the\nexpected amplitude and spatial distribution of this signal and evaluate the\nmodel for the CLASS observing site in the Atacama Desert of northern Chile.\nThen, using two years of observations at 32.3 GHz to 43.7 GHz from the CLASS\nQ-band telescope, we present a detection of this signal and compare the\nobserved signal to that predicted by the model. We recover an angle between\nmagnetic north and true north of $(-5.5 \\pm 0.6)^\\circ$, which is consistent\nwith the expectation of $-5.9^\\circ$ for the CLASS observing site. When\ncomparing dipole sky patterns fit to both simulated and data-derived sky maps,\nthe dipole directions match to within a degree, and the measured amplitudes\nmatch to within ${\\sim}20\\%$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recently, the Belle Collaboration has updated the analysis of the cross\nsections for the processes $e^+ e^- \\to \\Upsilon(nS)\\, \\pi^+ \\pi^-$ ($n = 1,\\,\n2,\\, 3$) in the $e^+ e^-$ center-of-mass energy range from 10.52 to 11.02 GeV.\nA new structure, called here $Y_b (10750)$, with the mass $M (Y_b) = (10752.7\n\\pm 5.9^{+0.7}_{-1.1})$ MeV and the Breit-Wigner width $\\Gamma (Y_b) =\n(35.5^{+17.6 +3.9}_{-11.3 -3.3})$ MeV was observed \\cite{Abdesselam:2019gth}.\nWe interpret $Y_b (10750)$ as a compact $J^{PC} = 1^{--}$ state with a dominant\ntetraquark component. The mass eigenstate $Y_b (10750)$ is treated as a linear\ncombination of the diquark-antidiquark and $b \\bar b$ components due to the\nmixing via gluonic exchanges shown recently to arise in the limit of large\nnumber of quark colors. The mixing angle between $Y_b$ and $\\Upsilon(5S)$ can\nbe estimated from the electronic width, recently determined to be $\\Gamma_{ee}\n(Y_b) = (13.7 \\pm 1.8)$ eV. The mixing provides a plausible mechanism for $Y_b\n(10750)$ production in high energy collisions from its $b \\bar b$ component and\nwe work out the Drell-Yan and prompt production cross sections for $p p \\to Y_b\n(10750) \\to \\Upsilon (nS)\\, \\pi^+ \\pi^-$ at the LHC. The resonant part of the\ndipion invariant mass spectrum in $Y_b (10750) \\to \\Upsilon (1S)\\, \\pi^+ \\pi^-$\nand the corresponding angular distribution of $\\pi^+$-meson in the dipion rest\nframe are presented as an example.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Depth from a monocular video can enable billions of devices and robots with a\nsingle camera to see the world in 3D. In this paper, we present an approach\nwith a differentiable flow-to-depth layer for video depth estimation. The model\nconsists of a flow-to-depth layer, a camera pose refinement module, and a depth\nfusion network. Given optical flow and camera pose, our flow-to-depth layer\ngenerates depth proposals and the corresponding confidence maps by explicitly\nsolving an epipolar geometry optimization problem. Our flow-to-depth layer is\ndifferentiable, and thus we can refine camera poses by maximizing the\naggregated confidence in the camera pose refinement module. Our depth fusion\nnetwork can utilize depth proposals and their confidence maps inferred from\ndifferent adjacent frames to produce the final depth map. Furthermore, the\ndepth fusion network can additionally take the depth proposals generated by\nother methods to improve the results further. The experiments on three public\ndatasets show that our approach outperforms state-of-the-art depth estimation\nmethods, and has reasonable cross dataset generalization capability: our model\ntrained on KITTI still performs well on the unseen Waymo dataset.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  $^{115}$In Nuclear magnetic resonance data are presented for a series of\nCe$_{1-x}$La$_x$CoIn$_5$ crystals with different La dilutions, $x$. Multiple\nIn(1) sites associated with different numbers of nearest-neighbor cerium atoms\nexhibit different Knight shifts and spin lattice relaxation rates. Analysis of\nthe temperature dependence of these sites reveals both an evolution of the\nheavy electron coherence as a function of dilution, as well as spatial\ninhomogeneity associated with a complete suppression of antiferromagnetic\nfluctuations in the vicinity of the La sites. Quantum critical fluctuations\npersist within disconnected Ce clusters with dilution levels up to 75%, despite\nthe fact that specific heat shows Fermi liquid behavior in dilute samples.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Deep Neural Networks (DNNs) are known to be susceptible to adversarial\nexamples. Adversarial examples are maliciously crafted inputs that are designed\nto fool a model, but appear normal to human beings. Recent work has shown that\npixel discretization can be used to make classifiers for MNIST highly robust to\nadversarial examples. However, pixel discretization fails to provide\nsignificant protection on more complex datasets. In this paper, we take the\nfirst step towards reconciling these contrary findings. Focusing on the\nobservation that discrete pixelization in MNIST makes the background completely\nblack and foreground completely white, we hypothesize that the important\nproperty for increasing robustness is the elimination of image background using\nattention masks before classifying an object. To examine this hypothesis, we\ncreate foreground attention masks for two different datasets, GTSRB and\nMS-COCO. Our initial results suggest that using attention mask leads to\nimproved robustness. On the adversarially trained classifiers, we see an\nadversarial robustness increase of over 20% on MS-COCO.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We determine the degree sequence of the generalized Sierpinski graph and its\ngeneral first Zagreb index in terms of the same parameters of the base graph G.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the effect of memory terms on the entropy decay of the\nsolutions to equations with Ornstein-Uhlenbeck operators. Our assumptions on\nthe memory kernels include Caputo-Fabrizio operators and, more generally, the\nstretched exponential functions. We establish a sharp rate decay for the\nentropy. Examples and numerical simulations are also given to illustrate the\nresults.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The cooling process of a protoneutron star is investigated with focus on its\nsensitivity to properties of hot and dense matter. An equation of state, which\nincludes the nucleon effective mass and nuclear symmetry energy at twice the\nsaturation density as control parameters, is constructed for systematic\nstudies. The numerical code utilized in this study follows a quasi-static\nevolution of a protoneutron star solving the general-relativistic stellar\nstructure with neutrino diffusion. The cooling timescale evaluated from the\nneutrino light curve is found to be longer for the models with larger effective\nmasses and smaller symmetry energies at high densities. The present results are\ncompared with those for other equations of state and it is found that they are\nconsistent in terms of their dependences on the effective mass and neutron star\nradius.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper concerns the problem of learning control policies for an unknown\nlinear dynamical system to minimize a quadratic cost function. We present a\nmethod, based on convex optimization, that accomplishes this task robustly:\ni.e., we minimize the worst-case cost, accounting for system uncertainty given\nthe observed data. The method balances exploitation and exploration, exciting\nthe system in such a way so as to reduce uncertainty in the model parameters to\nwhich the worst-case cost is most sensitive. Numerical simulations and\napplication to a hardware-in-the-loop servo-mechanism demonstrate the approach,\nwith appreciable performance and robustness gains over alternative methods\nobserved in both.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Automatic facial behavior analysis has a long history of studies in the\nintersection of computer vision, physiology and psychology. However it is only\nrecently, with the collection of large-scale datasets and powerful machine\nlearning methods such as deep neural networks, that automatic facial behavior\nanalysis started to thrive. Three of its iconic tasks are automatic recognition\nof basic expressions (e.g. happy, sad, surprised), estimation of continuous\nemotions (e.g., valence and arousal), and detection of facial action units\n(activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these\ntasks have been mostly studied independently collecting a dataset for the task.\nWe present the first and the largest study of all facial behaviour tasks\nlearned jointly in a single multi-task, multi-domain and multi-label network,\nwhich we call FaceBehaviorNet. For this we utilize all publicly available\ndatasets in the community (around 5M images) that study facial behaviour tasks\nin-the-wild. We demonstrate that training jointly an end-to-end network for all\ntasks has consistently better performance than training each of the single-task\nnetworks. Furthermore, we propose two simple strategies for coupling the tasks\nduring training, co-annotation and distribution matching, and show the\nadvantages of this approach. Finally we show that FaceBehaviorNet has learned\nfeatures that encapsulate all aspects of facial behaviour, and can be\nsuccessfully applied to perform tasks (compound emotion recognition) beyond the\nones that it has been trained in a zero- and few-shot learning setting.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The loss function of deep networks is known to be non-convex but the precise\nnature of this nonconvexity is still an active area of research. In this work,\nwe study the loss landscape of deep networks through the eigendecompositions of\ntheir Hessian matrix. In particular, we examine how important the negative\neigenvalues are and the benefits one can observe in handling them\nappropriately.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper investigates the optimal transmit beamforming design of\nsimultaneous wireless information and power transfer (SWIPT) in the multiuser\nmultiple-input-single-output (MISO) downlink with specific absorption rate\n(SAR) constraints. We consider the power splitting technique for SWIPT, where\neach receiver divides the received signal into two parts: one for information\ndecoding and the other for energy harvesting with a practical non-linear\nrectification model. The problem of interest is to maximize as much as possible\nthe received signal-to-interference-plus-noise ratio (SINR) and the energy\nharvested for all receivers, while satisfying the transmit power and the SAR\nconstraints by optimizing the transmit beamforming at the transmitter and the\npower splitting ratios at different receivers. The optimal beamforming and\npower splitting solutions are obtained with the aid of semidefinite programming\nand bisection search. Low-complexity fixed beamforming and hybrid beamforming\ntechniques are also studied. Furthermore, we study the effect of imperfect\nchannel information and radiation matrices, and design robust beamforming to\nguarantee the worst-case performance. Simulation results demonstrate that our\nproposed algorithms can effectively deal with the radio exposure constraints\nand significantly outperform the conventional transmission scheme with power\nbackoff.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Prior research has shown that round and planar buoyant jets \"puff\" at a\nfrequency that depends on the balance of momentum and buoyancy fluxes at the\ninlet, as parametrized by the Richardson number. Experiments have revealed the\nexistence of scaling relations between the Strouhal number of the puffing and\nthe inlet Richardson number, but geometry-specific relations are required when\nthe characteristic length is taken to be the diameter (for round inlets) or\nwidth (for planar inlets). In the present study, we show that when the\nhydraulic radius of the inlet is instead used as the characteristic length, a\nsingle Strouhal-Richardson scaling relation is obtained for a variety of inlet\ngeometries. In particular, we use adaptive mesh numerical simulations to\nmeasure puffing Strouhal numbers for circular, rectangular (with three\ndifferent aspect ratios), triangular, and annular high-temperature buoyant jets\nover a range of Richardson numbers. We then combine these results with prior\nexperimental data for round and planar buoyant jets to propose a new scaling\nrelation that accurately describes puffing Strouhal numbers for various inlet\nshapes and for Richardson numbers spanning over four orders of magnitude.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The formation of neutron stars (NSs), both from collapses of massive stars\nand mergers of compact objects, can be usually indicated by bright transients\nemitted from explosively-ejected material. In particular, if the newborn NSs\ncan rotate at a millisecond period and have a sufficiently high magnetic field,\nthen the spin-down of the NSs would provide a remarkable amount of energy to\nthe emitting material. As a result, super-luminous supernovae could be produced\nin the massive stellar collapse cases, while some unusual fast evolving and\nluminous optical transients could arise from the cases of NS mergers and\naccretion-induced collapses of white dwarfs. In all cases, if the dipolar\nmagnetic fields of the newborn NSs can be amplified to be as high as $10^{15}$\nG, a relativistic jet could be launched and then a gamma-ray burst can be\nproduced as the jet successfully breaks out from the surrounding\nnearly-isotropic ejected material.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Effective plant growth and yield prediction is an essential task for\ngreenhouse growers and for agriculture in general. Developing models which can\neffectively model growth and yield can help growers improve the environmental\ncontrol for better production, match supply and market demand and lower costs.\nRecent developments in Machine Learning (ML) and, in particular, Deep Learning\n(DL) can provide powerful new analytical tools. The proposed study utilises ML\nand DL techniques to predict yield and plant growth variation across two\ndifferent scenarios, tomato yield forecasting and Ficus benjamina stem growth,\nin controlled greenhouse environments. We deploy a new deep recurrent neural\nnetwork (RNN), using the Long Short-Term Memory (LSTM) neuron model, in the\nprediction formulations. Both the former yield, growth and stem diameter\nvalues, as well as the microclimate conditions, are used by the RNN\narchitecture to model the targeted growth parameters. A comparative study is\npresented, using ML methods, such as support vector regression and random\nforest regression, utilising the mean square error criterion, in order to\nevaluate the performance achieved by the different methods. Very promising\nresults, based on data that have been obtained from two greenhouses, in Belgium\nand the UK, in the framework of the EU Interreg SMARTGREEN project (2017-2021),\nare presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Purpose: Managing IT with firm performance has always been a debatable topic\nin literature and practice. Prior studies examining the above relationship have\nreported mixed results and have yet ignored the eminent managing IT practices.\nThe purpose of this paper is to empirically investigate the relevance of ValIT\n2.0 practice in managing IT investment, and its mediating role in the firm\nperformance context. Design,methodology,approach:This paper developed on two\nthemes of literature. First managing IT as a firm's IT capability in order to\ngenerate value from IT investment. Second IT as a firm's resource under\nresource-based view offers firm's competence that deploys potentials in\nachieving firm performance. The structural equation modeling with PLS\ntechniques used for analyzing data collected from 176 organization's IT, and\nbusiness executives in China. Findings: The results of this study show\nempirical evidence that Val-IT's components (value governance, portfolio\nmanagement, and investment management) are significantly linked to the\nmanagement of IT, and it found to be a significant mediator between Val-IT\ncomponents and firm performance. Research implications: This research\ncontributes to the literature and practice by way of highlighting the value\ngeneration through managing IT on firm performance. Originality: This study is\nfully based on ValIT 2.0 with the firm performance where the managing IT\nmediate this relationship in a country-specific study in China. This study adds\nto the Chinese information system literature which suffers the lack of\nempirical studies in the context of management of IT research.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Global cosmic strings are generically predicted in particle physics beyond\nthe Standard Model, e.g., a post-inflationary global $U(1)$ symmetry breaking\nwhich may associate with axion-like dark matter. We demonstrate that although\nsubdominant to Goldstone emission, gravitational waves (GWs) radiated from\nglobal strings can be observable with current or future GW detectors. The\nfrequency spectrum of such GWs is also shown to be a powerful tool to probe the\nHubble expansion rate of the Universe at times prior to the Big Bang\nnucleosynthesis where the standard cosmology has yet to be tested.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The optimized assignment of staff is of great significance for improving the\nproduction efficiency of the society. For specific tasks, the key to optimizing\nstaffing is personnel scheduling. The assignment problem is classical in the\npersonnel scheduling. In this paper, we abstract it as an optimal matching\nmodel of a bipartite graph and propose the Ultimate Hungarian Algorithm(UHA).\nBy introducing feasible labels, iteratively searching for the augmenting path\nto get the optimal match(maximum-weight matching). And we compare the algorithm\nwith the traditional brute force method, then conclude that our algorithm has\nlower time complexity and can solve the problems of maximum-weight matching\nmore effectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Graphene is an attractive material for microelectronics applications, given\nsuch favourable electrical characteristics as high mobility, high operating\nfrequency, and good stability. If graphene is to be implemented in electronic\ndevices on a mass scale, then it must be compatible with existing semiconductor\nindustry fabrication processes. Unfortunately, such processing introduces\ndefects and impurities to the graphene, which cause scattering of the charge\ncarriers and changes in doping level. Scattering results in degradation of\nelectrical performance, including lower mobility and Dirac point shifts. In\nthis paper, we review methods by which to mitigate the effects of charged\nimpurities and defects in graphene devices. Using capping layers such as\nfluoropolymers, statistically significant improvement of mobility, on/off\nratio, and Dirac point voltage for graphene FETs have been demonstrated. These\neffects are also reversible and can be attributed to the presence of highly\npolar groups in these capping layers such as carbon-fluoride bonds in the\nfluoropolymer acting to electrostatically screen charged impurities and defects\nin or near the graphene. In other experiments, graphene FETs were exposed to\nvapour-phase, polar, organic molecules in an ambient environment. This resulted\nin significant improvement to electrical characteristics, and the magnitude of\nimprovement to the Dirac point scaled with the dipole moment of the delivered\nmolecule type. The potential profile produced in the plane of the graphene\nsheet by the impurities was calculated to be significantly reduced by the\npresence of polar molecules. We present strong evidence that the polar nature\nof capping layers or polar vapour molecules introduced to the surface of a\ngraphene FET act to mitigate detrimental effects of charged impurities/defects.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper provides an analytical treatment of accelerated and geodesic\nmotion within the framework of the Friedmann -Lemaitre-Robertson-Walker (FLRW)\nspacetime. By employing conformal time transformations we manage to convert\nsecond order differential equations of motion in FLRW spacetime to first order\nequations in the conformally transformed spacetime. This allows us to derive a\ngeneral analytical solution in closed-form for accelerated motion in spatially\ncurved FLRW spacetime. We provide few examples of this general solution for the\nspatially flat cases. The last part of our work focuses on the return journey\nfor a traveler exploring a FLRW universe. We derive certain condition for de\nSitter universe that must be satisfied in order to have an actual return\njourney.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove that every simple graph of order 12 which has minimum degree 6\ncontains a K_6 minor.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Arising from the interplay between charge, spin and orbital of electrons,\nspin-orbit torque (SOT) has attracted immense interest in the past decade.\nDespite vast progress, the existing quantification methods of SOT still have\ntheir respective restrictions on the magnetic anisotropy, the entanglement\nbetween SOT effective fields, and the artifacts from the thermal gradient and\nthe planar Hall effect, etc. Thus, accurately characterizing SOT across diverse\nsamples remains as a critical need. In this work, with the aim of removing the\nafore-mentioned restrictions, thus enabling the universal SOT quantification,\nwe report the characterization of the sign and amplitude of SOT by angular\nmeasurements. We first validate the applicability of our angular\ncharacterization in a perpendicularly magnetized Pt/Co-Ni heterostructure by\nshowing excellent agreements to the results of conventional quantification\nmethods. Remarkably, the thermoelectric effects, i.e., the anomalous Nernst\neffect (ANE) arising from the temperature gradient can be self-consistently\ndisentangled and quantified from the field dependence of the angular\ncharacterization. The superiority of this angular characterization has been\nfurther demonstrated in a Cu/CoTb/Cu sample with large ANE but negligible SOT,\nand in a Pt/Co-Ni sample with weak perpendicular magnetic anisotropy (PMA), for\nwhich the conventional quantification methods are not applicable and even yield\nfatal error. By providing a comprehensive and versatile way to characterize SOT\nand thermoelectric effects in diverse heterostructures, our results pave the\nimportant foundation for the spin-orbitronic study as well as the\ninterdisciplinary research of thermal spintronic.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We realize the $\\mathrm{GL}_n(\\mathbb{C})$-modules $S^k(S^m(\\mathbb{C}^n))$\nand $\\Lambda^k(S^m(\\mathbb{C}^n))$ as spaces of polynomial functions on\n$n\\times k$ matrices. In the case $k=3$, we describe explicitly all the\n$\\mathrm{GL}_n(\\mathbb{C})$-highest weight vectors which occur in\n$S^3(S^m(\\mathbb{C}^n))$ and in $\\Lambda^3(S^m(\\mathbb{C}^n))$ respectively.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  DC-magnetization data measured down to 40 mK speak against conventional\nfreezing and reinstate YbMgGaO$_4$ as a triangular spin-liquid candidate.\nMagnetic susceptibility measured parallel and perpendicular to the $c$-axis\nreaches constant values below 0.1 and 0.2 K, respectively, thus indicating the\npresence of gapless low-energy spin excitations. We elucidate their nature in\nthe triple-axis inelastic neutron scattering experiment that pinpoints the\nlow-energy ($E$ $\\leq$ $J_0$ $\\sim$ 0.2 meV) part of the excitation continuum\npresent at low temperatures ($T$ $<$ $J_0$/$k_B$), but \\emph{completely}\ndisappearing upon warming the system above $T$ $\\gg$ $J_0$/$k_B$. In contrast\nto the high-energy part at $E$ $>$ $J_0$ that is rooted in the breaking of\nnearest-neighbor valence bonds and persists to temperatures well above\n$J_0$/$k_B$, the low-energy one originates from the rearrangement of the\nvalence bonds and thus from the propagation of unpaired spins. We further\nextend this picture to herbertsmithite, the spin-liquid candidate on the kagome\nlattice, and argue that such a hierarchy of magnetic excitations may be a\nuniversal feature of quantum spin liquids.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The full statistical distribution of the superfluid fraction characterizing\none-dimensional Bose gases in random potentials is discussed. Rare\nconfigurations with extreme fluctuations of the disorder potential can fragment\nthe condensate and reduce the superfluid fraction to zero. The resulting\nbimodal probability distribution for the superfluid fraction is calculated\nnumerically in the quasi-1D mean-field regime of ultracold atoms in laser\nspeckle potentials. Using extreme-value statistics, an analytical scaling of\nthe zero-superfluid probability as function of disorder strength, disorder\ncorrelation length and system size is presented. It is argued that similar\nresults can be expected for point-like impurities, and that these findings are\nin reach for present-day experiments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cavitation in solids can be caused by tensile dead-load traction or impulse\ntraction. The two different types of boundary conditions lead to different\nstatic and dynamic solutions. In addition, if the material is stochastic, i.e.,\nthe model parameters are represented by probability distributions, the expected\nbehaviour is more complicated to describe. Here, following the first instalment\nof this work, we examine the static and dynamic cavitation of a stochastic\nmaterial under a uniform tensile impulse traction in different spherical\ngeometries. We find that the critical load at which a cavity forms at the\ncentre of the sphere is the same as for the homogeneous sphere composed\nentirely of the material found at its centre, while the post-cavitation radial\nmotion is non-oscillatory. However, there are some important differences in the\nnonlinear elastic responses. Specifically, subcritical bifurcation, with snap\ncavitation, is obtained in a static sphere of stochastic neo-Hookean material\nand also in a radially inhomogeneous sphere, whereas for composite spheres, a\nsupercritical bifurcation, with stable cavitation, is possible as well. Given\nthe non-deterministic material parameters, the results are characterised by\nprobability distributions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we construct a universal model to study the search and rescue\nwork in lost planes. We establish an evaluation and decision model for traffic\nrescue tools. According to the characteristics of different periods, we utilize\nthe combination of Analytic Hierarchy Process (AHP) and Fuzzy Synthetic\nEvaluation (FSE) to assess the capability of rescue tools in different period.\nThen, combined with the actual situation, determine the selection of rescue\ntools in different periods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show that certain classes of modules have universal models with respect to\npure embeddings.\n  $Theorem.$ Let $R$ be a ring, $T$ a first-order theory with an infinite model\nextending the theory of $R$-modules and $K^T=(Mod(T), \\leq_{pp})$ (where\n$\\leq_{pp}$ stands for pure submodule). Assume $K^T$ has joint embedding and\namalgamation.\n  If $\\lambda^{|T|}=\\lambda$ or $\\forall \\mu < \\lambda( \\mu^{|T|} < \\lambda)$,\nthen $K^T$ has a universal model of cardinality $\\lambda$.\n  As a special case we get a recent result of Shelah [Sh17, 1.2] concerning the\nexistence of universal reduced torsion-free abelian groups with respect to pure\nembeddings.\n  We begin the study of limit models for classes of $R$-modules with joint\nembedding and amalgamation. We show that limit models with chains of long\ncofinality are pure-injective and we characterize limit models with chains of\ncountable cofinality. This can be used to answer Question 4.25 of [Maz].\n  As this paper is aimed at model theorists and algebraists an effort was made\nto provide the background for both.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The screened Coulomb potential plays a crucial role in the binding energies\nof excitons in a thin dielectric slab. The asymptotic behavior of this\npotential is studied when the thickness of the slab is very small as compared\nto the exciton Bohr radius. A regularized expression is given and the exact\neffective 2D potential is derived. These expressions may be useful for the\ncomputation of the exciton binding energy in 2D or quasi-2D materials.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Super-oscillating beams can be used to create light spots whose size is below\nthe diffraction limit with a side ring of high intensity adjacent to them.\nOptical traps made of the super-oscillating part of such beams exhibit superior\nlocalization of submicron beads compared to regular optical traps. Here we\nfocus on the effect of the ratio of particle size to trap size on the\nlocalization and stiffness of optical traps made of super-oscillating beams. We\nfind a non-monotonic dependence of trapping stiffness on the ratio of particle\nsize to beam size. Optimal trapping is achieved when the particle is larger\nthan the beam waist of the superoscillating feature but small enough not to\noverlap with the side ring.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Magnetic light and matter interactions are generally too weak to be detected,\nstudied and applied technologically. However, if one can increase the magnetic\npower density of light by several orders of magnitude, the coupling between\nmagnetic light and matter could become of the same order of magnitude as the\ncoupling with its electric counterpart. For that purpose, photonic nanoantennas\nhave been proposed, and in particular dielectric nanostructures, to engineer\nstrong local magnetic field and therefore increase the probability of magnetic\ninteractions. Unfortunately, dielectric designs suffer from physical\nlimitations that confine the magnetic hot spot in the core of the material\nitself, preventing experimental and technological implementations. Here, we\ndemonstrate that evolutionary algorithms can overcome such limitations by\ndesigning new dielectric photonic nanoantennas, able to increase and extract\nthe optical magnetic field from high refractive index materials. We also\ndemonstrate that the magnetic power density in an evolutionary optimized\ndielectric nanostructure can be increased by a factor 5 compared to state of\nthe art dielectric nanoantennas. In addition, we show that the fine details of\nthe nanostructure are not critical in reaching these aforementioned features,\nas long as the general shape of the motif is maintained. This advocates for the\nfeasibility of nanofabricating the optimized antennas experimentally and their\nsubsequent application. By designing all dielectric magnetic antennas that\nfeature local magnetic hot-spots outside of high refractive index materials,\nthis work highlights the potential of evolutionary methods to fill the gap\nbetween electric and magnetic light-matter interactions, opening up new\npossibilities in many research fields.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Automatic post-disaster damage detection using aerial imagery is crucial for\nquick assessment of damage caused by disaster and development of a recovery\nplan. The main problem preventing us from creating an applicable model in\npractice is that damaged (positive) examples we are trying to detect are much\nharder to obtain than undamaged (negative) examples, especially in short time.\nIn this paper, we revisit the classical bootstrap aggregating approach in the\ncontext of modern transfer learning for data-efficient disaster damage\ndetection. Unlike previous classical ensemble learning articles, our work\npoints out the effectiveness of simple bagging in deep transfer learning that\nhas been underestimated in the context of imbalanced classification. Benchmark\nresults on the AIST Building Change Detection dataset show that our approach\nsignificantly outperforms existing methodologies, including the recently\nproposed disentanglement learning.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A linear multi-factor model is one of the most important tools in equity\nportfolio management. The linear multi-factor models are widely used because\nthey can be easily interpreted. However, financial markets are not linear and\ntheir accuracy is limited. Recently, deep learning methods were proposed to\npredict stock return in terms of the multi-factor model. Although these methods\nperform quite well, they have significant disadvantages such as a lack of\ntransparency and limitations in the interpretability of the prediction. It is\nthus difficult for institutional investors to use black-box-type machine\nlearning techniques in actual investment practice because they should show\naccountability to their customers. Consequently, the solution we propose is\nbased on LSTM with LRP. Specifically, we extend the linear multi-factor model\nto be non-linear and time-varying with LSTM. Then, we approximate and linearize\nthe learned LSTM models by LRP. We call this LSTM+LRP model a deep recurrent\nfactor model. Finally, we perform an empirical analysis of the Japanese stock\nmarket and show that our recurrent model has better predictive capability than\nthe traditional linear model and fully-connected deep learning methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper proposes a new privacy-enhancing, context-aware user\nauthentication system, ConSec, which uses a transformation of general\nlocation-sensitive data, such as GPS location, barometric altitude and noise\nlevels, collected from the user's device, into a representation based on\nlocality-sensitive hashing (LSH). The resulting hashes provide a dimensionality\nreduction of the underlying data, which we leverage to model users' behaviour\nfor authentication using machine learning. We present how ConSec supports\nlearning from categorical and numerical data, while addressing a number of\non-device and network-based threats. ConSec is implemented subsequently for the\nAndroid platform and evaluated using data collected from 35 users, which is\nfollowed by a security and privacy analysis. We demonstrate that LSH presents a\nuseful approach for context authentication from location-sensitive data without\ndirectly utilising plain measurements.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Chow-Robbins game is a classical still partly unsolved stopping problem\nintroduced by Chow and Robbins in 1965. You repeatedly toss a fair coin. After\neach toss, you decide if you take the fraction of heads up to now as a payoff,\notherwise you continue.\n  As a more general stopping problem this reads\n  \\[V(n,x) = \\sup_{\\tau }\\operatorname{E} \\left [ \\frac{x +\nS_\\tau}{n+\\tau}\\right]\\] where $S$ is a random walk.\n  We give a tight upper bound for $V$ when $S$ has subgassian increments. We do\nthis by usinf the analogous time continuous problem with a standard Brownian\nmotion as the driving process. From this we derive an easy proof for the\nexistence of optimal stopping times in the discrete case.\n  For the Chow-Robbins game we as well give a tight lower bound and use these\nto calculate, on the integers, the complete continuation and the stopping set\nof the problem for $n\\leq 10^{5}$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study existence and Lorentz regularity of distributional solutions to\nelliptic equations with either a convection or a drift first order term. The\npresence of such a term makes the problem not coercive. The main tools are\npointwise estimates of the rearrangements of both the solution and its\ngradient.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent modeling of NICER observations of thermal X-ray pulsations from the\nsurface of the isolated millisecond pulsar PSR J0030+0451 suggests that the hot\nemitting regions on the pulsar's surface are far from antipodal, which is at\nodds with the classical assumption that the magnetic field in the pulsar\nmagnetosphere is predominantly that of a centered dipole. Here, we review these\nresults and examine previous attempts to constrain the magnetospheric\nconfiguration of PSR J0030+0451. To the best of our knowledge, there is in fact\nno direct observational evidence that PSR J0030+0451's magnetic field is a\ncentered dipole. Developing models of physically motivated, non-canonical\nmagnetic field configurations and the currents that they can support poses a\nchallenging task. However, such models may have profound implications for many\naspects of pulsar research, including pulsar braking, estimates of birth\nvelocities, and interpretations of multi-wavelength magnetospheric emission.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the problem of maximizing payoff generated over a period of time in\na general class of closed queueing networks with a finite, fixed number of\nsupply units which circulate in the system. Demand arrives stochastically, and\nserving a demand unit (customer) causes a supply unit to relocate from the\n``origin'' to the ``destination'' of the customer. The key challenge is to\nmanage the distribution of supply in the network. We consider general controls\nincluding customer entry control, pricing, and assignment. Motivating\napplications include shared transportation platforms and scrip systems.\n  Inspired by the mirror descent algorithm for optimization and the\nbackpressure policy for network control, we introduce a rich family of\n\\emph{Mirror Backpressure} (MBP) control policies. The MBP policies are simple\nand practical, and crucially do not need any statistical knowledge of the\ndemand (customer) arrival rates (these rates are permitted to vary in time).\nUnder mild conditions, we propose MBP policies that are provably near optimal.\nSpecifically, our policies lose at most $O(\\frac{K}{T}+\\frac{1}{K} + \\sqrt{\\eta\nK})$ payoff per customer relative to the optimal policy that knows the demand\narrival rates, where $K$ is the number of supply units, $T$ is the total number\nof customers over the time horizon, and $\\eta$ is the demand process' average\nrate of change per customer arrival. An adaptation of MBP is found to perform\nwell in numerical experiments based on data from ride-hailing.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Waring Problem over polynomial rings asks how to decompose a homogeneous\npolynomial $p$ of degree $d$ as a finite sum of $d$-{th} powers of linear\nforms. In this work we give an algorithm to obtain a real Waring decomposition\nof any given real binary form $p$ of length at most its degree. In fact, we\nconstruct a semialgebraic family of Waring decompositions for $p$. Some\nexamples are shown to highlight the difference between the real and the complex\ncase.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Understanding how supermassive black holes (SMBHs) pair and merge helps to\ninform predictions of off-center, dual, and binary AGN, and provides key\ninsights into how SMBHs grow and co-evolve with their galaxy hosts. As the\nloudest known gravitational wave source, binary SMBH mergers also hold\ncenterstage for the Laser Interferometer Space Antenna (LISA), a joint ESA/NASA\ngravitational wave observatory set to launch in 2034. Here, we continue our\nwork to characterize SMBH binary formation and evolution through increasingly\nmore realistic high resolution direct $N$-body simulations, focusing on the\neffect of SMBH mass ratio, orientation, and eccentricity within a rotating and\nflattened stellar host. During the dynamical friction phase, we found a\nprolonged orbital decay for retrograde SMBHs and swift pairing timescales for\nprograde SMBHs compared to their counterparts in non-rotating models, an effect\nthat becomes more pronounced for smaller mass ratios $M_{\\rm sec}/M_{\\rm prim}\n= q$. During this pairing phase, the eccentricity dramatically increases for\nretrograde configurations, but as the binary forms, the orbital plane flips so\nthat it is almost perfectly prograde, which stifles the rapid eccentricity\ngrowth. In prograde configurations, SMBH binaries form and remain at\ncomparatively low eccentricities. As in our prior work, we note that the center\nof mass of a prograde SMBH binary itself settles into an orbit about the center\nof the galaxy. Since even the initially retrograde binaries flip their orbital\nplane, we expect few binaries in rotating systems to reside at rest in the\ndynamic center of the host galaxy, though this effect is smaller as $q$\ndecreases.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Social Internet of Things (SIoT), integration of the Internet of Things\nand Social Networks paradigms, has been introduced to build a network of smart\nnodes that are capable of establishing social links. In order to deal with\nmisbehaving service provider nodes, service requestor nodes must evaluate their\ntrustworthiness levels. In this paper, we propose a novel trust management\nmechanism in the SIoT to predict the most reliable service providers for each\nservice requestor, which leads to reduce the risk of being exposed to malicious\nnodes. We model the SIoT with a flexible bipartite graph (containing two sets\nof nodes: service providers and service requestors), then build a social\nnetwork among the service requestor nodes, using the Hellinger distance.\nAfterward, we develop a social trust model using nodes' centrality and\nsimilarity measures to extract trust behaviors among the social network nodes.\nFinally, a matrix factorization technique is designed to extract latent\nfeatures of SIoT nodes, find trustworthy nodes, and mitigate the data sparsity\nand cold start problems. We analyze the effect of parameters in the proposed\ntrust prediction mechanism on prediction accuracy. The results indicate that\nfeedbacks from the neighboring nodes of a specific service requestor with high\nHellinger similarity in our mechanism outperforms the best existing methods. We\nalso show that utilizing the social trust model, which only considers a\nsimilarity measure, significantly improves the accuracy of the prediction\nmechanism. Furthermore, we evaluate the effectiveness of the proposed trust\nmanagement system through a real-world SIoT use case. Our results demonstrate\nthat the proposed mechanism is resilient to different types of network attacks,\nand it can accurately find the most proper and trustworthy service provider.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Most of today's mobile devices are equipped with multiple network interfaces\nand one of the main bandwidth-hungry applications that would benefit from\nmultipath communications is wireless video streaming. However, most of the\ncurrent transport protocols do not match the requirements of video streaming\napplications or are not designed to address relevant issues, such as delay\nconstraints, networks heterogeneity, and head-of-line blocking issues. This\nsurvey provides a holistic literature review of multipath wireless video\nstreaming, shedding light on the different alternatives from an end-to-end\nlayered stack perspective, unveiling trade-offs of each approach, and\npresenting a suitable taxonomy to classify the state-of-the-art. Finally, we\ndiscuss open issues and avenues for future work.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the study of \"holographic complexity\", upper bounds on the rate of growth\nof the (specific) complexity of field theories with holographic duals have\nattracted much attention. Underlying these upper bounds there are inequalities\nrelating the parameters of the dual black hole. We derive such an inequality in\nthe case of the five-dimensional AdS-Kerr black hole, dual to a\nfour-dimensional field theory with a non-zero angular momentum density. We\npropose to test these underlying inequalities \"experimentally\", by using the\nconjectured analogy of the field theory with phenomenological models of the\nQuark-Gluon Plasma. The test consists of comparing data for the parameters of\nthe QGP with the upper bound on the relevant combination of black hole\nparameters. The bound in the non-rotating case passes the test: in this sense,\nit is confirmed \"experimentally\". In the rotating case, the inequality makes\npredictions regarding the entropy density of the vortical plasma, recently\nobserved by the STAR collaboration.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce a new, rigorously-formulated Bayesian meta-learning algorithm\nthat learns a probability distribution of model parameter prior for few-shot\nlearning. The proposed algorithm employs a gradient-based variational inference\nto infer the posterior of model parameters to a new task. Our algorithm can be\napplied to any model architecture and can be implemented in various machine\nlearning paradigms, including regression and classification. We show that the\nmodels trained with our proposed meta-learning algorithm are well calibrated\nand accurate, with state-of-the-art calibration and classification results on\ntwo few-shot classification benchmarks (Omniglot and Mini-ImageNet), and\ncompetitive results in a multi-modal task-distribution regression.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Networks are one of the most powerful structures for modeling problems in the\nreal world. Downstream machine learning tasks defined on networks have the\npotential to solve a variety of problems. With link prediction, for instance,\none can predict whether two persons will become friends on a social network.\nMany machine learning algorithms, however, require that each input example is a\nreal vector. Network embedding encompasses various methods for unsupervised,\nand sometimes supervised, learning of feature representations of nodes and\nlinks in a network. Typically, embedding methods are based on the assumption\nthat the similarity between nodes in the network should be reflected in the\nlearned feature representations. In this paper, we review significant\ncontributions to network embedding in the last decade. In particular, we look\nat four methods: Spectral Clustering, DeepWalk, Large-scale Information Network\nEmbedding (LINE), and node2vec. We describe each method and list its advantages\nand shortcomings. In addition, we give examples of real-world machine learning\nproblems on networks in which the embedding is critical in order to maximize\nthe predictive performance of the machine learning task. Finally, we take a\nlook at research trends and state-of-the art methods in the research on network\nembedding.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A Neural Network (NN) based numerical method is formulated and implemented\nfor solving Boundary Value Problems (BVPs) and numerical results are presented\nto validate this method by solving Laplace equation with Dirichlet boundary\ncondition and Poisson's equation with mixed boundary conditions. The principal\nadvantage of NN based numerical method is the discrete data points where the\nfield is computed, can be unstructured and do not suffer from issues of meshing\nlike traditional numerical methods such as Finite Difference Time Domain or\nFinite Element Method. Numerical investigations are carried out for both\nuniform and non-uniform training grid distributions to understand the efficacy\nand limitations of this method and to provide qualitative understanding of\nvarious parameters involved.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Tumor treating fields (TTFields) is an FDA approved therapy for the treatment\nof Gliobastoma Multiform (GBM) and currently being investigated for additional\ntumor types. TTFields are delivered to the tumor through the placement of\ntransducer arrays (TAs) placed on the patient scalp. The positions of the TAs\nare associated with treatment outcomes via simulations of the electric fields.\nTherefore, we are currently developing a method for recommending optimal\nplacement of TAs. A key step to achieve this goal is to correctly segment the\nhead into tissues of similar electrical properties. Visual inspection of\nsegmentation quality is invaluable but time-consuming. Automatic quality\nassessment can assist in automatic refinement of the segmentation parameters,\nsuggest flaw points to the user and indicate if the segmented method is of\nsufficient accuracy for TTFields simulation. As a first step in this direction,\nwe identified a set of features that are relevant to atlas-based segmentation\nand show that these are significantly correlated (p < 0.05) with a similarity\nmeasure between validated and automatically computed segmentations.\nFurthermore, we incorporated these features in a decision tree regressor to\npredict the similarity of the validated and computed segmentations of 20\nTTFields patients using a leave-one-out approach. The predicted similarity\nmeasures were highly correlated with the actual ones (average abs. difference\n3% (SD = 3%); r = 0.92, p < 0.001). We conclude that quality estimation of\nsegmentations is feasible by incorporating machine learning and\nsegmentation-relevant features.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A broad variety of light sources exhibit photon-count fluctuations that\ndisplay inverse-square spectral behavior at extremely low frequencies. These\nsources include light-emitting diodes, superluminescent diodes, laser diodes,\nincandescent sources, and betaluminescent sources. In a series of experiments\ncarried out over an 18-month period, the photon-count fluctuations for these\nsources were found to exhibit a $1/f^2$ spectral signature over the frequency\nrange $1.0 \\times 10^{-6} \\le f \\le 5.0 \\times 10^{-4}$ Hz, corresponding to\n$33$ min $ \\le T_f \\le 11.6$ d, where $T_f \\equiv 1/f$. The lower time limit is\nestablished by the photodetector noise floor while the upper time limit is\ndetermined by the duration of the individual experiments. Scalograms computed\nfrom our data are consistent with the periodograms. The universal character of\nthis inverse-square baseband spectral behavior stands in sharp contrast to the\noptical spectra of these sources, which differ markedly. Time traces of the\nphoton-count fluctuations are examined to shed light on the origin of this\nenigmatic spectral behavior.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In 2011 a decision was made by Czech Metrology Institute to build a free-air\nionization chamber (FAC) intended to be used as a primary standard of air kerma\nrate for medium-energy X-rays (photon energy from 40 to 300 keV, including\nmammography X-ray qualities) in order to replace currently used secondary\nionization chamber and to decrease the uncertainty of air kerma reference\nvalue. In period of 2013-2017, the FAC has been designed, manufactured and put\ninto operation. Correction factors were measured or calculated by a Monte Carlo\nmethod. FAC performance was preliminary tested using a calibrated secondary\nchamber. Physical characteristics of the FAC are described and a summary of the\ncorrection factors with the uncertainty budget is presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $G$ be a connected reductive group scheme acting on a spherical scheme\n$X$. In the case where $G$ is of type $A_n$, Aizenbud and Avni proved the\nexistence of a number $C$ such that the multiplicity\n$\\dim\\hom(\\rho,\\mathbb{C}[X(F)])$ is bounded by $C$, for any finite field $F$\nand any irreducible representation $\\rho$ of $G(F)$. In this paper, we\ngeneralize this result to the case where $G$ is a connected reductive group\nscheme over $\\mathbb{Z}$, and prove Conjecture A of [1].\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We develop a method based on tensor networks to create localized single\nparticle excitations on top of strongly-correlated quantum spin chains. In\nanalogy to the problem of creating localized Wannier modes, this is achieved by\noptimizing the gauge freedom of momentum excitations on top of matrix product\nstates. The corresponding wavepackets propagate almost dispersionless. The\ntime-dependent variational principle is used to scatter two such wavepackets,\nand we extract the phase shift from the collision data. We also study\nreflection and transmission coefficients of a wavepacket impinging on an\nimpurity.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a novel problem setting in zero-shot learning, zero-shot object\nrecognition and detection in the context. Contrary to the traditional zero-shot\nlearning methods, which simply infers unseen categories by transferring\nknowledge from the objects belonging to semantically similar seen categories,\nwe aim to understand the identity of the novel objects in an image surrounded\nby the known objects using the inter-object relation prior. Specifically, we\nleverage the visual context and the geometric relationships between all pairs\nof objects in a single image, and capture the information useful to infer\nunseen categories. We integrate our context-aware zero-shot learning framework\ninto the traditional zero-shot learning techniques seamlessly using a\nConditional Random Field (CRF). The proposed algorithm is evaluated on both\nzero-shot region classification and zero-shot detection tasks. The results on\nVisual Genome (VG) dataset show that our model significantly boosts performance\nwith the additional visual context compared to traditional methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let ${\\mathcal O}$ be the ring of $S$-integers in a number field $K$. For\n$A\\in\\rm{SL}_{2}(\\mathcal{O})$ and $k\\geq 1$, we define matrix-factorization\nvarieties $V_k(A)$ over ${\\mathcal O}$ which parametrize factoring $A$ into a\nproduct of $k$ elementary matrices; the equations defining $V_k(A)$ are written\nin terms of Euler's continuant polynomials. We show that the $V_k(A)$ are\nrational $(k-3)$-folds with an inductive fibration structure. We combine this\ngeometric structure with arithmetic results to study the Zariski closure of the\n${\\mathcal O}$-points of $V_k(A)$. We prove that for $k\\geq 4$ the ${\\mathcal\nO}$-points on $V_k(A)$ are Zariski dense if $V_{k}(A)({\\mathcal\nO})\\neq\\emptyset$ assuming the group of units ${\\mathcal O}^{\\times}$ is\ninfinite. This shows that if $A$ can be written as a product of $k\\geq 4$\nelementary matrices, then this can be done in infinitely many ways in the\nstrongest sense possible. This can then be combined with results on factoring\ninto elementary matrices for ${\\rm SL}_{2}({\\mathcal O})$. One result is that\nfor $k\\geq 9$ the ${\\mathcal O}$-points on $V_{k}(A)$ are Zariski dense if\n${\\mathcal O}^{\\times}$ is infinite.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The basics of the premetric approach are discussed, including the essential\ndetails of the formalism and some of its beautiful consequences. We demonstrate\nhow the classical electrodynamics can be developed without a metric in a quite\nstraightforward way: Maxwell's equations, together with the general response\nlaw for material media, admit a consistent premetric formulation. Furthermore,\nwe show that in relativistic theories of gravity, the premetric program leads\nto a better understanding of the interdependence between topological, affine,\nand metric concepts.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In a frequency division duplexing (FDD) massive multiple-input\nmultiple-output (MIMO) system, the acquisition of downlink channel state\ninformation (CSI) at base station (BS) is a very challenging task due to the\noverwhelming overheads required for downlink training and uplink feedback. In\nthis paper, we reveal a deterministic uplink-to-downlink mapping function when\nthe position-to-channel mapping is bijective. Motivated by the universal\napproximation theorem, we then propose a sparse complex-valued neural network\n(SCNet) to approximate the uplink-to-downlink mapping function. Different from\ngeneral deep networks that operate in the real domain, the SCNet is constructed\nin the complex domain and is able to learn the complex-valued mapping function\nby off-line training. After training, the SCNet is used to directly predict the\ndownlink CSI based on the estimated uplink CSI without the need of either\ndownlink training or uplink feedback. Numerical results show that the SCNet\nachieves better performance than general deep networks in terms of prediction\naccuracy and exhibits remarkable robustness over complicated wireless channels,\ndemonstrating its great potential for practical deployments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  There has been an increasing interest in the area of emergent communication\nbetween agents which learn to play referential signalling games with realistic\nimages. In this work, we consider the signalling game setting of Havrylov and\nTitov and investigate the effect of the feature extractor's weights and of the\ntask being solved on the visual semantics learned or captured by the models. We\nimpose various augmentation to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our set of experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Control on microscopic scales depends critically on our ability to manipulate\ninteractions with different physical fields. The creation of micro-machines\ntherefore requires us to understand how multiple fields, such as surface\ncapillary or electro-magnetic, can be used to produce predictable behaviour.\nRecently, a spinning micro-raft system was developed that exhibited both static\nand dynamic self-assembly [Wang et al. (2017) Sci. Adv. 3, e1602522]. These\nrafts employed both capillary and magnetic interactions and, at a critical\ndriving frequency, would suddenly change from stable orbital patterns to static\nassembled structures. In this paper, we explain the dynamics of two interacting\nmicro-rafts through a combination of theoretical models and experiments. This\nis first achieved by identifying the governing physics of the orbital patterns,\nthe assembled structures, and the collapse separately. We find that the orbital\npatterns are determined by the short range capillary interactions between the\ndisks, while the explanations of the other two behaviours only require the\ncapillary far field. Finally we combine the three models to explain the\ndynamics of a new micro-raft experiment.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the past few years, several hints of lepton flavour universality (LFU)\nviolation have emerged in the $b \\to c \\tau \\bar\\nu$ and $b \\to s \\ell^+\n\\ell^-$ data. Quite recently, the Belle Collaboration has reported the first\nmeasurement of the $D^*$ longitudinal polarization fraction in the $B \\to D^*\n\\tau \\bar\\nu$ decay. Motivated by this intriguing result, together with the\nrecent measurements of $R_{J/\\psi}$ and $\\tau$ polarization, we study $b \\to c\n\\tau \\bar\\nu$ decays in the Supersymmetry (SUSY) with $R$-parity violation\n(RPV). We consider $B \\to D^{(*)} \\tau \\bar\\nu$, $B_c \\to \\eta_c \\tau \\bar\\nu$,\n$B_c \\to J/\\psi \\tau \\bar\\nu$ and $\\Lambda_b \\to \\Lambda_c \\tau \\bar\\nu$ modes\nand focus on the branching ratios, the LFU ratios, the forward-backward\nasymmetries, polarizations of daughter hadrons and $\\tau$ lepton. It is found\nthat the RPV SUSY can explain the $R_{D^{(*)}}$ anomalies at $2\\sigma$ level,\nafter taking into account various flavour constraints. In the allowed parameter\nspace, the differential branching fractions and LFU ratios are largely enhanced\nby the SUSY effects, especially in the large dilepton invariant mass region. In\naddition, a lower bound $\\mathcal B(B^+ \\to K^+ \\nu \\bar\\nu) > 7.37 \\times\n10^{-6}$ is obtained. These observables could provide testable signatures at\nthe High-Luminosity LHC and SuperKEKB, and correlate with direct searches for\nSUSY.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we investigate the performance of a millimeter waves (mmWaves)\ncellular system with free space optical (FSO) backhauling. MmWave channels are\nsubject to Nakagami-m fading while the optical links experience the Double\nGeneralized Gamma including atmospheric turbulence, path loss and the\nmisalignment between the transmitter and the receiver aperture (also known as\nthe pointing errors). The FSO model also takes into account the receiver\ndetection technique which could be either heterodyne or intensity modulation\nand direct detection (IM/DD). Each user equipment (UE) has to be associated to\none serving base station (BS) based on the received signal strength (RSS) or\nChannel State Information (CSI). We assume partial relay selection (PRS) with\nCSI based on mmWaves channels to select the BS associated with the highest\nreceived CSI. Each serving BS decodes the received signal for denoising,\nconverts it into modulated FSO signal, and then forwards it to the data center.\nThereby, each BS can be viewed as a decode-and-forward (DF) relay. In practice,\nthe relay hardware suffers from nonlinear high power amplification (HPA)\nimpairments which, substantially degrade the system performance. In this work,\nwe will discuss the impacts of three common HPA impairments named respectively,\nsoft envelope limiter (SEL), traveling wave tube amplifier (TWTA), and solid\nstate power amplifier (SSPA). Novel closed-forms and tight upper bounds of the\noutage probability, the probability of error, and the achievable rate are\nderived. Capitalizing on these performance, we derive the high SNR asymptotes\nto get engineering insights into the system gain such as the diversity order.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The regression of multiple inter-connected sequence data is a problem in\nvarious disciplines. Formally, we name the regression problem of multiple\ninter-connected data entities as the \"dynamic network regression\" in this\npaper. Within the problem of stock forecasting or traffic speed prediction, we\nneed to consider both the trends of the entities and the relationships among\nthe entities. A majority of existing approaches can't capture that information\ntogether. Some of the approaches are proposed to deal with the sequence data,\nlike LSTM. The others use the prior knowledge in a network to get a fixed graph\nstructure and do prediction on some unknown entities, like GCN. To overcome the\nlimitations in those methods, we propose a novel graph neural network, namely\nGraph Neural Lasso (GNL), to deal with the dynamic network problem. GNL extends\nthe GDU (gated diffusive unit) as the base neuron to capture the information\nbehind the sequence. Rather than using a fixed graph structure, GNL can learn\nthe dynamic graph structure automatically. By adding the attention mechanism in\nGNL, we can learn the dynamic relations among entities within each network\nsnapshot. Combining these two parts, GNL is able to model the dynamic network\nproblem well. Experimental results provided on two networked sequence datasets,\ni.e., Nasdaq-100 and METR-LA, show that GNL can address the network regression\nproblem very well and is also very competitive among the existing approaches.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Colorado Ultraviolet Transit Experiment (CUTE) is a 6U NASA CubeSat\ncarrying on-board a low-resolution (R~2000--3000), near-ultraviolet (2500--3300\n{\\AA}) spectrograph. It has a rectangular primary Cassegrain telescope to\nmaximize the collecting area. CUTE, which is planned for launch in Spring 2020,\nis designed to monitor transiting extra-solar planets orbiting bright, nearby\nstars aiming at improving our understanding of planet atmospheric escape and\nstar-planet interaction processes. We present here the CUTE data simulator,\nwhich we complemented with a basic data reduction pipeline. This pipeline will\nbe then updated once the final CUTE data reduction pipeline is developed. We\nshow here the application of the simulator to the HD209458 system and a first\nestimate of the precision on the measurement of the transit depth as a function\nof temperature and magnitude of the host star. We also present estimates of the\neffect of spacecraft jitter on the final spectral resolution. The simulator has\nbeen developed considering also scalability and adaptability to other missions\ncarrying on-board a long-slit spectrograph. The data simulator will be used to\ninform the CUTE target selection, choose the spacecraft and instrument settings\nfor each observation, and construct synthetic CUTE wavelength-dependent transit\nlight curves on which to develop the CUTE data reduction pipeline.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Using the Atmospheric Imaging Assembly 304 A images obtained from the Solar\nDynamics Observatory, we study two jets which occurred during the M5.8 class\nflare on 2017 April 3 and the M5.5 class flare on 2016 July 23, respectively.\nDuring the M5.8 class flare, many vortex-like structures occurred in the\nupstream and downstream regimes of the associated jet. While the jet was\nejected upwards to the corona, some dark material at its base flowed through a\nbright structure with a velocity of 110 km/s. The boundary between the material\nand the structure changed from smooth to uneven. Later, the jet material at the\nhigher atmosphere started to fall down with velocities of over 200 km/s, and\nthe left boundary of the jet developed into a sawtooth pattern. The vortex-like\nstructures were formed, and the growth rates of two structures were presented.\nDuring the M5.5 class flare, we also observed many vortex-like structures in\nthe downstream regime of the jet. At the late stage of the jet, some material\nat the south boundary of the jet fell back to the solar surface, and\nvortex-like structures at the boundary grew from ripple-like minim into\nvortices with diameters of 3.4-5.4 Mm. The growth rates of the vortex-like\nstructures were calculated. We suggest that the vortex-like structures in the\nupstream regime are the manifestations of Kelvin-Helmholtz instability, and\nthose in the downstream regime are simultaneously driven by Kelvin-Helmholtz\ninstability and Raleigh-Taylor instability.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present Ordinary Differential Equation Variational Auto-Encoder\n(ODE$^2$VAE), a latent second order ODE model for high-dimensional sequential\ndata. Leveraging the advances in deep generative models, ODE$^2$VAE can\nsimultaneously learn the embedding of high dimensional trajectories and infer\narbitrarily complex continuous-time latent dynamics. Our model explicitly\ndecomposes the latent space into momentum and position components and solves a\nsecond order ODE system, which is in contrast to recurrent neural network (RNN)\nbased time series models and recently proposed black-box ODE techniques. In\norder to account for uncertainty, we propose probabilistic latent ODE dynamics\nparameterized by deep Bayesian neural networks. We demonstrate our approach on\nmotion capture, image rotation and bouncing balls datasets. We achieve\nstate-of-the-art performance in long term motion prediction and imputation\ntasks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Attacks on industrial enterprises are increasing in number as well as in\neffect. Since the introduction of industrial control systems in the 1970's,\nindustrial networks have been the target of malicious actors. More recently,\nthe political and warfare-aspects of attacks on industrial and critical\ninfrastructure are becoming more relevant. In contrast to classic home and\noffice IT systems, industrial IT, so-called OT systems, have an effect on the\nphysical world. Furthermore, industrial devices have long operation times,\nsometimes several decades. Updates and fixes are tedious and often not\npossible. The threats on industry with the legacy requirements of industrial\nenvironments creates the need for efficient intrusion detection that can be\nintegrated into existing systems. In this work, the network data containing\nindustrial operation is analysed with machine learning- and time series- based\nanomaly detection algorithms in order to discover the attacks introduced to the\ndata. Two different data sets are used, one Modbus-based gas pipeline control\ntraffic and one OPC UA-based batch processing traffic. In order to detect\nattacks, two machine learning-based algorithms are used, namely \\textit{SVM}\nand Random Forest. Both perform well, with Random Forest slightly outperforming\nSVM. Furthermore, extracting and selecting features as well as handling missing\ndata is addressed in this work.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Exact numerical primordial primordial power spectra are computed and plotted\nfor the for the best-fit Planck 2018 curved universe parameters. It is found\nthat the spectra have generic cut-offs and oscillations within the observable\nwindow for the level of curvature allowed by current CMB measurements and\nprovide a better fit to current data. Derivations for the Mukhanov-Sasaki\nequation for curved universes are presented and analysed, and theoretical\nimplications for the quantum and classical initial conditions for inflation are\ndiscussed within the curved regime.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, the general problem of the characterization of the topological\nphase of an open quantum system is addressed. In particular, we study the\ntopological properties of Kitaev chains and ladders under the perturbing effect\nof a current flux injected into the system using an external normal lead and\nderived from it via a superconducting electrode. After discussing the\ntopological phase diagram of the isolated systems, using a scattering technique\nwithin the Bogoliubov de Gennes formulation, we analyze the differential\nconductance properties of these topological devices as a function of all\nrelevant model parameters. The relevant problem of implementing local\nspectroscopic measurements to characterize topological systems is also\naddressed by studying the system electrical response as a function of the\nposition and the distance of the normal electrode (tip). The results show how\nthe signatures of topological order affect the electrical response of the\nanalyzed systems, a subset of which being robust also against the effects of a\nmoderate amount of disorder. The analysis of the internal modes of the\nnanodevices demonstrates that topological protection can be lost when quantum\nstates of an initially isolated topological system are hybridized with those of\nthe external reservoirs. The conclusions of this work could be useful in\nunderstanding the topological phases of nanowire-based mesoscopic devices.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper studies a decentralized stochastic gradient tracking (DSGT)\nalgorithm for non-convex empirical risk minimization problems over a\npeer-to-peer network of nodes, which is in sharp contrast to the existing DSGT\nonly for convex problems. To ensure exact convergence and handle the variance\namong decentralized datasets, each node performs a stochastic gradient (SG)\ntracking step by using a mini-batch of samples, where the batch size is\ndesigned to be proportional to the size of the local dataset. We explicitly\nevaluate the convergence rate of DSGT with respect to the number of iterations\nin terms of algebraic connectivity of the network, mini-batch size, gradient\nvariance, etc. Under certain conditions, we further show that DSGT has a\nnetwork independence property in the sense that the network topology only\naffects the convergence rate up to a constant factor. Hence, the convergence\nrate of DSGT can be comparable to the centralized SGD method. Moreover, a\nlinear speedup of DSGT with respect to the number of nodes is achievable for\nsome scenarios. Numerical experiments for neural networks and logistic\nregression problems on CIFAR-10 finally illustrate the advantages of DSGT.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The ability to predict the likelihood of impurity incorporation and their\nelectronic energy levels in semiconductors is crucial for controlling its\nconductivity, and thus the semiconductor's performance in solar cells,\nphotodiodes, and optoelectronics. The difficulty and expense of experimental\nand computational determination of impurity levels makes a data-driven machine\nlearning approach appropriate. In this work, we show that a density functional\ntheory-generated dataset of impurities in Cd-based chalcogenides CdTe, CdSe,\nand CdS can lead to accurate and generalizable predictive models of defect\nproperties. By converting any semiconductor + impurity system into a set of\nnumerical descriptors, regression models are developed for the impurity\nformation enthalpy and charge transition levels. These regression models can\nsubsequently predict impurity properties in mixed anion CdX compounds (where X\nis a combination of Te, Se and S) fairly accurately, proving that although\ntrained only on the end points, they are applicable to intermediate\ncompositions. We make machine-learned predictions of the Fermi-level dependent\nformation energies of hundreds of possible impurities in 5 chalcogenide\ncompounds, and suggest a list of impurities which can shift the equilibrium\nFermi level in the semiconductor as determined by the dominant intrinsic\ndefects. These dominating impurities as predicted by machine learning compare\nwell with DFT predictions, revealing the power of machine-learned models in the\nquick screening of impurities likely to affect the optoelectronic behavior of\nsemiconductors.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this chapter, we illustrate how a trapped ion system can be used for the\nexperimental study of quantum thermodynamics, in particular, quantum\nfluctuation of work. As technology of nano/micro scale develops, it becomes\ncritical to understand thermodynamics at the quantum mechanical level. The\ntrapped ion system is a representative physical platform to experimentally\ndemonstrate quantum phenomena with excellent control and precision. We provide\na basic introduction of the trapped ion system and present the theoretical\nframework for the experimental study of quantum thermodynamics. Then we bring\nout two concrete examples of the experimental demonstrations. Finally, we\ndiscuss the results and the future of the experimental study of quantum\nthermodynamics with trapped ion systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  App classification is useful in a number of applications such as adding apps\nto an app store or building a user model based on the installed apps. Presently\nthere are a number of existing methods to classify apps based on a given\ntaxonomy on the basis of their text metadata. However, text based methods for\napp classification may not work in all cases, such as when the text\ndescriptions are in a different language, or missing, or inadequate to classify\nthe app. One solution in such cases is to utilize the app images to supplement\nthe text description. In this paper, we evaluate a number of approaches in\nwhich app images can be used to classify the apps. In one approach, we use\nOptical character recognition (OCR) to extract text from images, which is then\nused to supplement the text description of the app. In another, we use pic2vec\nto convert the app images into vectors, then train an SVM to classify the\nvectors to the correct app label. In another, we use the captionbot.ai tool to\ngenerate natural language descriptions from the app images. Finally, we use a\nmethod to detect and label objects in the app images and use a voting technique\nto determine the category of the app based on all the images. We compare the\nperformance of our image-based techniques to classify a number of apps in our\ndataset. We use a text based SVM app classifier as our base and obtained an\nimproved classification accuracy of 96% for some classes when app images are\nadded.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We fabricate a microscale electromechanical system, in which a suspended\nsuperconducting membrane, treated as a mechanical oscillator, capacitively\ncouples to a superconducting microwave resonator. As the microwave driving\npower increases, nonmonotonic dependence of the resonance frequency of the\nmechanical oscillator on the driving power has been observed. We also\ndemonstrate the optical switching of the resonance frequency of the mechanical\noscillator. Theoretical models for qualitative understanding of our\nexperimental observations are presented. Our experiment may pave the way for\nthe application of a mechanical oscillator with its resonance frequency\ncontrolled by the electromagnetic and/or optical fields, such as a\nmicrowave-optical interface and a controllable element in a\nsuperqubit-mechanical oscillator hybrid system.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Predator-prey networks originating from different aqueous and terrestrial\nenvironments are compared to assess if the difference in environments of these\nnetworks produce any significant difference in the structure of such\npredator-prey networks. Spectral graph theory is used firstly to discriminate\nbetween the structure of such predator-prey networks originating from aqueous\nand terrestrial environments and secondly to establish that the difference\nobserved in the structure of networks originating from these two environments\nare precisely due to the way edges are oriented in these networks and are not a\nproperty of random networks.We use random projections in $\\mathbb{R^2}$ and\n$\\mathbb{R^3}$ of weighted spectral distribution (WSD) of the networks\nbelonging to the two classes viz. aqueous and terrestrial to differentiate\nbetween the structure of these networks. The spectral theory of graph\nnon-randomness and relative non-randomness is used to establish the deviation\nof structure of these networks from having a topology similar to random\nnetworks.We thus establish the absence of a universal structural pattern across\npredator-prey networks originating from different environments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  With a selected sample of neutron star (NS) equation-of-states (EOSs) that\nare consistent with the current observations and have a range of maximum\nmasses, we investigate the relations between NS gravitational mass $M_g$ and\nbaryonic mass $M_b$, and the relations between the maximum NS mass supported\nthrough uniform rotation ($M_{\\rm max}$) and that of nonrotating NSs ($M_{\\rm\nTOV}$). We find that if one intends to apply an EOS-independent quadratic,\nuniversal transformation formula ($M_b=M_g+A\\times M_{g}^2$) to all EOSs, the\nbest fit $A$ value is 0.080 for non-rotating NSs only and 0.073 when different\nspin periods are considered. The residual error of the transformation is as\nlarge as $\\sim0.1M_{\\odot}$. For different EOSs, we find that the parameter $A$\nfor non-rotating NSs is proportional to $R_{1.4}^{-1}$ (where $R_{1.4}$ is NS\nradius for 1.4$M_\\odot$ in unit of km). For a particular EOS, if one adopts the\nbest-fit parameters for different spin periods, the residual error of the\ntransformation is smaller, which is of the order of 0.01$M_\\odot$ for the\nquadratic form and less than 0.01$M_\\odot$ for the cubic form\n($M_b=M_g+A_1\\times M_{g}^2+A_2\\times M_{g}^3$). We also find a very tight and\ngeneral correlation between the normalized mass gain due to spin $\\Delta\nm\\equiv(M_{\\rm max}-M_{\\rm TOV})/M_{\\rm TOV}$ and the spin period normalized to\nthe Keplerian period ${\\cal P}$, i.e. ${\\rm log_{10}}\\Delta m =\n(-2.74\\pm0.05){\\rm log_{10}}{\\cal P}+{\\rm log_{10}}(0.20\\pm 0.01)$, which is\nindependent of EOS models. Applications of our results to GW170817 is\ndiscussed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present the first sentence simplification model that learns explicit edit\noperations (ADD, DELETE, and KEEP) via a neural programmer-interpreter\napproach. Most current neural sentence simplification systems are variants of\nsequence-to-sequence models adopted from machine translation. These methods\nlearn to simplify sentences as a byproduct of the fact that they are trained on\ncomplex-simple sentence pairs. By contrast, our neural programmer-interpreter\nis directly trained to predict explicit edit operations on targeted parts of\nthe input sentence, resembling the way that humans might perform simplification\nand revision. Our model outperforms previous state-of-the-art neural sentence\nsimplification models (without external knowledge) by large margins on three\nbenchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89\nWikiSmall, +1.41 Newsela), and is judged by humans to produce overall better\nand simpler output sentences.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the current paper, we have developed an analytical apparatus, allowing to\ncalculate the phase error, produced by miscalibration of modulation parameters.\nThe case of harmonic modulation is considered, the analysis is performed for\ncases of two parameters miscalibration: amplitude and start phase. Two\ndemodulation algorithms are considered: conventional 4-point algorithm, based\non ordinary least squares approach and previously developed 4+1 algorithm with\nhigh immunity to phase error, induced by change of target phase on demodulation\ninterval. Predictions, given by developed analytical equations are verified by\nmeans of numeric modeling.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove N.Takahashi's conjecture determining the contribution of each\ncontact point in genus-$0$ maximal contact Gromov-Witten theory of\n$\\mathbb{P}^2$ relative to a smooth cubic $E$. This is a new example of a\nquestion in Gromov-Witten theory which can be fully solved despite the presence\nof contracted components and multiple covers. The proof relies on a tropical\ncomputation of the Gromov-Witten invariants and on the interpretation of the\ntropical picture as describing wall-crossing in the derived category of\ncoherent sheaves on $\\mathbb{P}^2$, giving a translation of the original\nGromov-Witten question into a known statement about Euler characteristics of\nmoduli spaces of one-dimensional Gieseker semistable sheaves on $\\mathbb{P}^2$.\n  The same techniques allow us to prove a new sheaves/Gromov-Witten\ncorrespondence, relating Betti numbers of moduli spaces of one-dimensional\nGieseker semistable sheaves on $\\mathbb{P}^2$, or equivalently refined\ngenus-$0$ Gopakumar-Vafa invariants of local $\\mathbb{P}^2$, with higher-genus\nmaximal contact Gromov-Witten theory of $(\\mathbb{P}^2,E)$. The correspondence\ninvolves the non-trivial change of variables $y=e^{i \\hbar}$, where $y$ is the\nrefined/cohomological variable on the sheaf side, and $\\hbar$ is the genus\nvariable on the Gromov-Witten side. We explain how this correspondence can be\nheuristically motivated by a combination of mirror symmetry and hyperk\\\"ahler\nrotation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a framework for Nesterov's accelerated gradient flows in\nprobability space to design efficient mean-field Markov chain Monte Carlo\n(MCMC) algorithms for Bayesian inverse problems. Here four examples of\ninformation metrics are considered, including Fisher-Rao metric, Wasserstein-2\nmetric, Kalman-Wasserstein metric and Stein metric. For both Fisher-Rao and\nWasserstein-2 metrics, we prove convergence properties of accelerated gradient\nflows. In implementations, we propose a sampling-efficient discrete-time\nalgorithm for Wasserstein-2, Kalman-Wasserstein and Stein accelerated gradient\nflows with a restart technique. We also formulate a kernel bandwidth selection\nmethod, which learns the gradient of logarithm of density from Brownian-motion\nsamples. Numerical experiments, including Bayesian logistic regression and\nBayesian neural network, show the strength of the proposed methods compared\nwith state-of-the-art algorithms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recurrent neural networks (RNN) are powerful tools to explain how attractors\nmay emerge from noisy, high-dimensional dynamics. We study here how to learn\nthe ~N^(2) pairwise interactions in a RNN with N neurons to embed L manifolds\nof dimension D << N. We show that the capacity, i.e. the maximal ratio L/N,\ndecreases as |log(epsilon)|^(-D), where epsilon is the error on the position\nencoded by the neural activity along each manifold. Hence, RNN are flexible\nmemory devices capable of storing a large number of manifolds at high spatial\nresolution. Our results rely on a combination of analytical tools from\nstatistical mechanics and random matrix theory, extending Gardner's classical\ntheory of learning to the case of patterns with strong spatial correlations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Collisionless shock acceleration of carbon ions (C$^{6+}$) is investigated in\nthe ultra-relativistic regime of laser-plasma interaction by accounting for the\nradiation reaction force and the pair production in particle-in-cell\nsimulations. Both radiation reaction force and pair plasma formation tend to\nslow down the shock velocity, reducing the energy of the accelerated ions,\nalbeit extending the time scales of the acceleration process. Slab plasma\ntarget achieves lower energy spread while target with a tailored density\nprofile yields higher ion acceleration energies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study a local to global principle for certain higher zero-cycles over\nglobal fields. We thereby verify a conjecture of Colliot-Th\\'el\\`ene for these\ncycles. Our main tool are the Kato conjectures proved by Jannsen, Kerz and\nSaito. Our approach also allows to reprove the ramified global class field\ntheory of Kato and Saito. Finally, we apply the Kato conjectures to study the\n$p$-adic cycle class map over henselian discrete valuation rings of mixed\ncharacteristic and to deduce finiteness theorems for arithmetic schemes in low\ndegree.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a machine built for experiments with ultracold mixtures of\nstrontium and lithium atoms. The machine includes a science vacuum chamber and\nthe relevant laser systems for cooling and trapping the atoms. With this\nmachine, we realize a D2-line compressed magneto-optical trap (MOT) for\n${^6}$Li and a narrow-linewidth 689-nm MOT for $^{84}$Sr, obtaining ~$10^9$\n${^6}$Li atoms at 700 $\\mu$K and ~$10^7$ $^{84}$Sr atoms at 1.8 $\\mu$K. Such a\ndual-species MOT provides an ideal starting point for realizing double\ndegenerate mixtures of $^6$Li and Sr atoms.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The ever-increasing demand from mobile Machine Learning (ML) applications\ncalls for evermore powerful on-chip computing resources. Mobile devices are\nempowered with heterogeneous multi-processor Systems-on-Chips (SoCs) to process\nML workloads such as Convolutional Neural Network (CNN) inference. Mobile SoCs\nhouse several different types of ML capable components on-die, such as CPU,\nGPU, and accelerators. These different components are capable of independently\nperforming inference but with very different power-performance characteristics.\nIn this article, we provide a quantitative evaluation of the inference\ncapabilities of the different components on mobile SoCs. We also present\ninsights behind their respective power-performance behavior. Finally, we\nexplore the performance limit of the mobile SoCs by synergistically engaging\nall the components concurrently. We observe that a mobile SoC provides up to 2x\nimprovement with parallel inference when all its components are engaged, as\nopposed to engaging only one component.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The concept of detecting arrays was developed to locate and detect\ninteraction faults arising between the factors in a component-based system\nduring software testing. In this paper, we propose a family of consecutive\ndetecting arrays (CDAs) in which the interactions between factors are\nconsidered to be ordered. CDAs can be used to generate test suites for locating\nand detecting interaction faults between neighboring factors. We establish a\ngeneral criterion for measuring the optimality of CDAs in terms of their size.\nBased on this optimality criterion, the equivalence between optimum CDAs and\nconsecutive orthogonal arrays with prescribed properties is explored. Using the\nadvantages of this equivalence, a great number of optimum CDAs are presented.\nIn particular, the existence of optimum CDAs with few factors is almost\ncompletely determined.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The influence of hydrogen on dislocation junctions was analysed by\nincorporating a hydrogen dependent core force into nodal based discrete\ndislocation dynamics. Hydrogen reduces the core energy of dislocations, which\nreduces the magnitude of the dislocation core force. We refer to this as\nhydrogen core force shielding, as it is analogous to hydrogen elastic shielding\nbut occurs at much lower hydrogen concentrations. The dislocation core energy\nchange due to hydrogen was calibrated at the atomic scale accounting for the\nnonlinear inter-atomic interactions at the dislocation core, giving the model a\nsound physical basis. Hydrogen was found to strengthen binary junctions and\npromote the nucleation of dislocations from triple junctions. Simulations of\nmicrocantilever bend tests with hydrogen core force shielding showed an\nincrease in the junction density and subsequent hardening. These simulations\nwere performed at a small hydrogen concentration realistic for bcc iron.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the Hubbard model on a square lattice, using the dynamical vertex\napproximation and the parquet approximation. These methods allow us to describe\nthe mutual interference of spin-fluctuations in the particle-hole channel and\nsuperconducting fluctuations in the cooperon channel in an unbiased way. For\nsmall dopings we find predominant commensurable antiferromagnetic spin- and\nd-wave superconducting fluctuations; for larger doping incommensurate\nantiferromagnetic spin fluctuations are concomitant to triplet s-wave\nsuperconducting fluctuations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Word embedding models such as the skip-gram learn vector representations of\nwords' semantic relationships, and document embedding models learn similar\nrepresentations for documents. On the other hand, topic models provide latent\nrepresentations of the documents' topical themes. To get the benefits of these\nrepresentations simultaneously, we propose a unifying algorithm, called neural\nembedding allocation (NEA), which deconstructs topic models into interpretable\nvector-space embeddings of words, topics, documents, authors, and so on, by\nlearning neural embeddings to mimic the topic models. We showcase NEA's\neffectiveness and generality on LDA, author-topic models and the recently\nproposed mixed membership skip gram topic model and achieve better performance\nwith the embeddings compared to several state-of-the-art models. Furthermore,\nwe demonstrate that using NEA to smooth out the topics improves coherence\nscores over the original topic models when the number of topics is large.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Bacteria can chemotactically migrate up attractant gradients by controlling\nrun-and-tumble motility patterns. In addition to this well-known chemotactic\nbehaviour, several soil and marine bacterial species perform chemokinesis: they\nadjust their swimming speed according to the local concentration of\nchemoeffector, with higher speed at higher concentration. A field of attractant\nthen induces a spatially varying swimming speed, which results in a drift\ntowards lower attractant concentrations -- contrary to the drift created by\nchemotaxis. Here, to explore the biological benefits of chemokinesis and\ninvestigate its impact on the chemotactic response, we extend a\nKeller-Segel-type model to include chemokinesis. We apply the model to predict\nthe dynamics of bacterial populations capable of chemokinesis and chemotaxis in\nchemoeffector fields inspired by microfluidic and agar plate migration assays.\nWe find that chemokinesis combined with chemotaxis not only may enhance the\npopulation response with respect to pure chemotaxis, but also modifies it\nqualitatively. We conclude presenting predictions for bacteria around dynamic\nfinite-size nutrient sources, simulating, e.g., a marine particle or a root. We\nshow that chemokinesis can reduce the measuring bias that is created by a\ndecaying attractant gradient.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Segmentation is a key step in analyzing and processing medical images. Due to\nthe low fault tolerance in medical imaging, manual segmentation remains the de\nfacto standard in this domain. Besides, efforts to automate the segmentation\nprocess often rely on large amounts of manually labeled data. While existing\nsoftware supporting manual segmentation is rich in features and delivers\naccurate results, the necessary time to set it up and get comfortable using it\ncan pose a hurdle for the collection of large datasets. This work introduces a\nclient/server based online environment, referred to as Studierfenster\n(studierfenster.at), that can be used to perform manual segmentations directly\nin a web browser. The aim of providing this functionality in the form of a web\napplication is to ease the collection of ground truth segmentation datasets.\nProviding a tool that is quickly accessible and usable on a broad range of\ndevices, offers the potential to accelerate this process. The manual\nsegmentation workflow of Studierfenster consists of dragging and dropping the\ninput file into the browser window and slice-by-slice outlining the object\nunder consideration. The final segmentation can then be exported as a file\nstoring its contours and as a binary segmentation mask. In order to evaluate\nthe usability of Studierfenster, a user study was performed. The user study\nresulted in a mean of 6.3 out of 7.0 possible points given by users, when asked\nabout their overall impression of the tool. The evaluation also provides\ninsights into the results achievable with the tool in practice, by presenting\ntwo ground truth segmentations performed by physicians.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We address the problem of real-time 3D object detection from point clouds in\nthe context of autonomous driving. Computation speed is critical as detection\nis a necessary component for safety. Existing approaches are, however,\nexpensive in computation due to high dimensionality of point clouds. We utilize\nthe 3D data more efficiently by representing the scene from the Bird's Eye View\n(BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs\noriented 3D object estimates decoded from pixel-wise neural network\npredictions. The input representation, network architecture, and model\noptimization are especially designed to balance high accuracy and real-time\nefficiency. We validate PIXOR on two datasets: the KITTI BEV object detection\nbenchmark, and a large-scale 3D vehicle detection benchmark. In both datasets\nwe show that the proposed detector surpasses other state-of-the-art methods\nnotably in terms of Average Precision (AP), while still runs at >28 FPS.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a study of the relative orientation between the magnetic field\nprojected onto the plane of sky ($B_{\\perp}$) on scales down to 0.4 pc,\ninferred from the polarized thermal emission of Galactic dust observed by\nPlanck at 353 GHz, and the distribution of gas column density ($N_{\\rm H}$)\nstructures on scales down to 0.026 pc, derived from the observations by\nHerschel in submillimeter wavelengths, toward ten nearby ($d$$<$450 pc)\nmolecular clouds. Using the histogram of relative orientation technique in\ncombination with tools from circular statistics, we found that the mean\nrelative orientation between $N_{\\rm H}$ and $B_{\\perp}$ toward these regions\nincreases progressively from 0\\deg, where the $N_{\\rm H}$ structures lie mostly\nparallel to $B_{\\perp}$, with increasing $N_{\\rm H}$, in many cases reaching\n90\\deg, where the $N_{\\rm H}$ structures lie mostly perpendicular to\n$B_{\\perp}$. We also compared the relative orientation between $N_{\\rm H}$ and\n$B_{\\perp}$ and the distribution of $N_{\\rm H}$, which is characterized by the\nslope of the tail of the $N_{\\rm H}$ probability density functions (PDFs). We\nfound that the slopes of the $N_{\\rm H}$ PDF tail are steepest in regions where\n$N_{\\rm H}$ and $B_{\\perp}$ are close to perpendicular. This coupling between\nthe $N_{\\rm H}$ distribution and the magnetic field suggests that the magnetic\nfields play a significant role in structuring the interstellar medium in and\naround molecular clouds. However, we found no evident correlation between the\nstar formation rates, estimated from the counts of young stellar objects, and\nthe relative orientation between $N_{\\rm H}$ and $B_{\\perp}$ in these regions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This short note serves as an introduction to gravitational radiation through\nreviewing the inspiral-plunge transition phase in extreme mass ratio binaries.\nWe study the relativistic motion of a compact object (CO) of mass $m$ around a\nmassive black hole of mass $M\\gg m$. The Kerr-Newman metric, effective\npotential for the general case of elliptical orbits, gravitational radiation,\norbital energy and angular momentum of a coalescing CO in Kerr spacetime and\ngravitational wave frequency and signal to noise ratio are briefly reviewed.\nThe main focus is on the transition from inspiral to plunge for a CO assuming\nthat a test particle approach is plausible in the regime $m\\ll M$ without\nappealing to a perturbative analysis. The effective potential is used to obtain\nthe properties of the Innermost Stable Circular Orbit (ISCO) near which the\nadiabatic inspiral phase ends abruptly and the CO enters the plunge phase. For\nthe transition phase, the effective potential is expanded in terms of\nparameters such as the radial (coordinate) distance from the ISCO and the\ndeviation of particle's angular momentum from its value at the ISCO to obtain\nthe equation of motion. The equations of motion, during the inspiral and\ntransition phases, are joined numerically and the gravitational wave frequency,\nnumber of wave cycles and signal to noise ratio (SN) during the transition is\nobtained for circular/inclined as well as elliptical/inclined orbits. The\nlimitations and inaccuracies of the current methods used to approach this\nproblem is discussed. A short introduction to the fundamental concepts of\nGeneral Relativity, in particular Einstein Field Equations is also provided in\nthe Appendix.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The fireball concept of Rolf Hagedorn, developed in the 1960's, is an\nalternative description of hadronic matter. Using a recently derived mass\nspectrum, we use the transport model GiBUU to calculate the shear viscosity of\na gas of such Hagedorn states, applying the Green-Kubo method to Monte-Carlo\ncalculations. Since the entropy density is rising ad infinitum near $T_H$, this\nleads to a very low shear viscosity to entropy density ratio near $T_H$.\nFurther, by comparing our results with analytic expressions, we find a nice\nextrapolation behavior, indicating that a gas of Hagedorn states comes close or\neven below the boundary $1/4\\pi$ from AdS-CFT.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper extends the idea of Universum learning [1, 2] to single-class\nlearning problems. We propose Single Class Universum-SVM setting that\nincorporates a priori knowledge (in the form of additional data samples) into\nthe single class estimation problem. These additional data samples or Universum\nbelong to the same application domain as (positive) data samples from a single\nclass (of interest), but they follow a different distribution. Proposed\nmethodology for single class U-SVM is based on the known connection between\nbinary classification and single class learning formulations [3]. Several\nempirical comparisons are presented to illustrate the utility of the proposed\napproach.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Coherent ranging, also known as frequency-modulated continuous-wave (FMCW)\nlaser based ranging (LIDAR) is currently developed for long range 3D distance\nand velocimetry in autonomous driving. Its principle is based on mapping\ndistance to frequency, and to simultaneously measure the Doppler shift of\nreflected light using frequency chirped signals, similar to Sonar or Radar.\nYet, despite these advantages, coherent ranging exhibits lower acquisition\nspeed and requires precisely chirped and highly-coherent laser sources,\nhindering their widespread use and impeding Parallelization, compared to modern\ntime-of-flight (TOF) ranging that use arrays of individual lasers. Here we\ndemonstrate a novel massively parallel coherent LIDAR scheme using a photonic\nchip-based microcomb. By fast chirping the pump laser in the soliton existence\nrange of a microcomb with amplitudes up to several GHz and sweep rate up to 10\nMHz, the soliton pulse stream acquires a rapid change in the underlying carrier\nwaveform, while retaining its pulse-to-pulse repetition rate. As a result, the\nchirp from a single narrow-linewidth pump laser is simultaneously transferred\nto all spectral comb teeth of the soliton at once, and allows for true\nparallelism in FMCW LIDAR. We demonstrate this approach by generating 30\ndistinct channels, demonstrating both parallel distance and velocity\nmeasurements at an equivalent rate of 3 Mpixel/s, with potential to improve\nsampling rates beyond 150 Mpixel/s and increase the image refresh rate of FMCW\nLIDAR up to two orders of magnitude without deterioration of eye safety. The\npresent approach, when combined with photonic phase arrays based on\nnanophotonic gratings, provides a technological basis for compact, massively\nparallel and ultra-high frame rate coherent LIDAR systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  For ell a generic n-tuple of positive numbers, N(ell) denotes the space of\nisometry classes of oriented n-gons in R^3 with side lengths specified by ell.\nWe determine the algebra K(N(ell)) and use this to obtain nonimmersions of the\n2(n-3)-manifold N(ell) in Euclidean space for several families of ell. We also\nuse obstruction theory to tell exactly when N(ell) immerses in R^{4n-14} for\ntwo families of ell.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  List-wise based learning to rank methods are generally supposed to have\nbetter performance than point- and pair-wise based. However, in real-world\napplications, state-of-the-art systems are not from list-wise based camp. In\nthis paper, we propose a new non-linear algorithm in the list-wise based\nframework called ListMLE, which uses the Plackett-Luce (PL) loss. Our\nexperiments are conducted on the two largest publicly available real-world\ndatasets, Yahoo challenge 2010 and Microsoft 30K. This is the first time in the\nsingle model level for a list-wise based system to match or overpass\nstate-of-the-art systems in real-world datasets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate the relationship between the variable gamma-ray emission and\njet properties in the blazar 3C 279, by combining the Fermi/LAT data spanning a\nperiod of eight years with concurrent radio measurements made at multiple\nepochs with VLBA at 15 and 43 GHz within the MOJAVE and VLBA-BU monitoring\nprograms. The aim of this paper is to compare the flux variability of the\ndifferent components found in the VLBA observations, to the variability in the\ngamma-rays. This analysis helps to investigate whether any of the jet\ncomponents can be associated with the gamma-ray variability. Through Spearman\nrank correlation we found that the gamma-ray variability is correlated with a\nparticular region (feature B, in the MOJAVE images) downstream from the\nobserved base (core) of the jet. This jet component is, therefore, a likely\nlocation at which an important fraction of the variable gamma-ray emission is\nproduced. We also calculated the average proper motion of the component with\nrespect to the VLBA core and found that it moves at an apparent superluminal\nvelocity of $(3.70 \\pm 0.35)c$, implying that one of the gamma-ray emission\nzones is not stationary. This jet component is also found between 6.86 mas and\n8.68 mas, which translates to a distance from the radio core of at least 42 pc.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We incorporate Tensor-Product Representations within the Transformer in order\nto better support the explicit representation of relation structure. Our\nTensor-Product Transformer (TP-Transformer) sets a new state of the art on the\nrecently-introduced Mathematics Dataset containing 56 categories of free-form\nmath word-problems. The essential component of the model is a novel attention\nmechanism, called TP-Attention, which explicitly encodes the relations between\neach Transformer cell and the other cells from which values have been retrieved\nby attention. TP-Attention goes beyond linear combination of retrieved values,\nstrengthening representation-building and resolving ambiguities introduced by\nmultiple layers of standard attention. The TP-Transformer's attention maps give\nbetter insights into how it is capable of solving the Mathematics Dataset's\nchallenging problems. Pretrained models and code will be made available after\npublication.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the kinetic Fokker-Planck equation with a class of general force.\nWe prove the existence and uniqueness of a positive normalized equilibrium (in\nthe case of a general force) and establish some exponential rate of convergence\nto the equilibrium (and the rate can be explicitly computed). Our results\nimprove results about classical force to general force case. Our result also\nimprove the rate of convergence for the Fitzhugh-Nagumo equation from\nnon-quantitative to quantitative explicit rate.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $\\Omega \\subset \\mathbb{R}^N$, $N \\geq 2$, be a smooth bounded domain. We\nconsider the boundary value problem \\begin{equation}\n\\label{Plambda-Abstract-ch3} \\tag{$P_{\\lambda}$} -\\Delta u = c_{\\lambda}(x) u +\n\\mu |\\nabla u|^2 + h(x)\\,, \\quad u \\in H_0^1(\\Omega) \\cap L^{\\infty}(\\Omega)\\,,\n\\end{equation} where $c_{\\lambda}$ and $h$ belong to $L^q(\\Omega)$ for some $q\n> N/2$, $\\mu$ belongs to $\\mathbb{R} \\setminus \\{0\\}$ and we write\n$c_{\\lambda}$ under the form $c_{\\lambda}:= \\lambda c_{+} - c_{-}$ with $c_{+}\n\\gneqq 0$, $c_{-} \\geq 0$, $c_{+} c_{-} \\equiv 0$ and $\\lambda \\in \\mathbb{R}$.\nHere $c_{\\lambda}$ and $h$ are both allowed to change sign. As a first main\nresult we give a necessary and sufficient condition which guarantees the\nexistence of a unique solution to \\eqref{Plambda-Abstract-ch3} when $\\lambda\n\\leq 0$. Then, assuming that $(P_0)$ has a solution, we prove existence and\nmultiplicity results for $\\lambda > 0$. Our proofs rely on a suitable change of\nvariable of type $v = F(u)$ and the combination of variational methods with\nlower and upper solution techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a new clustering method in the form of a single clustering\nequation that is able to directly discover groupings in the data. The main\nproposition is that the first neighbor of each sample is all one needs to\ndiscover large chains and finding the groups in the data. In contrast to most\nexisting clustering algorithms our method does not require any\nhyper-parameters, distance thresholds and/or the need to specify the number of\nclusters. The proposed algorithm belongs to the family of hierarchical\nagglomerative methods. The technique has a very low computational overhead, is\neasily scalable and applicable to large practical problems. Evaluation on well\nknown datasets from different domains ranging between 1077 and 8.1 million\nsamples shows substantial performance gains when compared to the existing\nclustering techniques.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Layered two-dimensional materials like graphene are highly appealing for\nlithium battery applications owing to their high surface-volume ratios.\nHowever, a critical issue that limits their practical applications is the\nconfined motion of lithium atoms within their van der Waal's gaps, which is the\nleading cause for battery failure due to severe clustering and phase\nseparation. Here we demonstrate that antimonene, an exfoliatable 2D material\nwith a high structural stability, exhibits a highly mobile cross-sheet motion\nowing to its unique structural features. The advent of the vertically permeable\nchannels opens a new pathway of lithium besides the normal motion along the\nbasal plane, rendering a 2+1 dimensional kinetics. Specifically, our\nfirst-principles calculations combined with the discrete geometry analysis\nrevealed that the energy barrier for a lithium atom to diffuse across the\nantimonene sheet is as low as 0.36 eV, which can be further reduced to 0.18 eV\nunder a tensile strain of 4%. These ultralow diffusion barriers across the\nsheet can open a new dimension for controlling the motion of lithium atoms,\nleading to a new paradigm for high-performance lithium batteries or inorganic\nsolid-state lithium-ion conductors.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a simple method that achieves unexpectedly superior performance\nfor Complex Reasoning involved Visual Question Answering. Our solution collects\nstatistical features from high-frequency words of all the questions asked about\nan image and use them as accurate knowledge for answering further questions of\nthe same image. We are fully aware that this setting is not ubiquitously\napplicable, and in a more common setting one should assume the questions are\nasked separately and they cannot be gathered to obtain a knowledge base.\nNonetheless, we use this method as an evidence to demonstrate our observation\nthat the bottleneck effect is more severe on the feature extraction part than\nit is on the knowledge reasoning part. We show significant gaps when using the\nsame reasoning model with 1) ground-truth features; 2) statistical features; 3)\ndetected features from completely learned detectors, and analyze what these\ngaps mean to researches on visual reasoning topics. Our model with the\nstatistical features achieves the 2nd place in the GQA Challenge 2019.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present a fully-differential calculation of the $H\\rightarrow\nb\\overline{b}$ decay at next-to-next- to-next-to-leading order (N$^3$LO)\naccuracy. Our calculation considers diagrams in which the Higgs boson couples\ndirectly to the bottom quarks, i.e. the perturbative order we consider is\n$\\mathcal{O}(\\alpha_s^3y_b^2)$. In order to regulate the infrared divergences\npresent at this order we use the Projection-to-Born technique coupled with\nN-jettiness slicing. After validating our methodology at\nnext-to-next-to-leading order (NNLO) we present exclusive jet rates and\ndifferential distributions for jet observables at N3LO accuracy using the\nDurham jet algorithm in the Higgs rest frame.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider a wave equation with a potential on the half-line as a model\nproblem for wave propagation close to an extremal horizon, or the\nasymptotically flat end of a black hole spacetime. We propose a definition of\nquasinormal frequencies (QNFs) as eigenvalues of the generator of time\ntranslations for a null foliation, acting on an appropriate (Gevrey based)\nHilbert space. We show that this QNF spectrum is discrete in a subset of\n$\\mathbb{C}$ which includes the region $\\{$Re$(s) >-b$, $|$Im $(s)|> K\\}$ for\nany $b>0$ and some $K=K(b) \\gg 1$. As a corollary we establish the\nmeromorphicity of the scattering resolvent in a sector $|$arg$(s)| <\\varphi_0$\nfor some $\\varphi_0 > \\frac{2\\pi}{3}$, and show that the poles occur only at\nquasinormal frequencies according to our definition. This result applies in\nsituations where the method of complex scaling cannot be directly applied, as\nour potentials need not be analytic. Finally, we show that QNFs computed by the\ncontinued fraction method of Leaver are necessarily QNFs according to our new\ndefinition. This paper is a companion to [D. Gajic and C. Warnick, Quasinormal\nmodes in extremal Reissner-Nordstr\\\"om spacetimes, preprint (2019)], which\ndeals with the QNFs of the wave equation on the extremal Reissner-Nordstr\\\"om\nblack hole.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The diffuse-interface model (DIM) is a tool for studying interfacial\ndynamics. In particular, it is used for modeling contact lines, i.e., curves\nwhere a liquid, gas, and solid are in simultaneous contact. As well as all\nother models of contact lines, the DIM implies an additional assumption: that\nthe flow near the liquid/gas interface is isothermal. In this work, this\nassumption is checked for the four fluids for which all common models of\ncontact lines fail. It is shown that, for two of these fluids (including\nwater), the assumption of isothermality does not hold.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Distant starlight passing through the Earth's atmosphere is refracted by an\nangle of just over one degree near the surface. This focuses light onto a focal\nline starting at an inner (and chromatic) boundary out to infinity - offering\nan opportunity for pronounced lensing. It is shown here that the focal line\ncommences at ~85% of the Earth-Moon separation, and thus placing an orbiting\ndetector between here and one Hill radius could exploit this refractive lens.\nAnalytic estimates are derived for a source directly behind the Earth (i.e.\non-axis) showing that starlight is lensed into a thin circular ring of\nthickness $W H_{\\Delta}/R$, yielding an amplification of $8 H_{\\Delta}/W$,\nwhere $H_{\\Delta}$ is the Earth's refractive scale height, $R$ is its\ngeopotential radius and $W$ is the detector diameter. These estimates are\nverified through numerical ray-tracing experiments from optical to 30 micron\nlight with standard atmospheric models. The numerical experiments are extended\nto include extinction from both a clear atmosphere and one with clouds. It is\nfound that a detector at one Hill radius is least affected by extinction since\nlensed rays travel no deeper than 13.7 km, within the stratosphere and above\nmost clouds. Including extinction, a 1 metre Hill radius 'terrascope' is\ncalculated to produce an amplification of ~45,000 for a lensing timescale of\n~20 hours. In practice, the amplification is likely halved in order to avoid\ndaylight scattering i.e. 22,500 ($\\Delta$mag=10.9) for $W=$1 metre, or\nequivalent to a 150 metre optical/infrared telescope.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Online data assimilation in time series models over a large spatial extent is\nan important problem in both geosciences and robotics. Such models are\nintrinsically high-dimensional, rendering traditional particle filter\nalgorithms ineffective. Though methods that begin to address this problem\nexist, they either rely on additional assumptions or lead to error that is\nspatially inhomogeneous. I present a novel particle-based algorithm for online\napproximation of the filtering problem on such models, using the fact that each\nlocus affects only nearby loci at the next time step. The algorithm is based on\na Metropolis-Hastings-like MCMC for creating hybrid particles at each step. I\nshow simulation results that suggest the error of this algorithm is uniform in\nboth space and time, with a lower bias, though higher variance, as compared to\na previously-proposed algorithm.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Supervised deep-embedding methods project inputs of a domain to a\nrepresentational space in which same-class instances lie near one another and\ndifferent-class instances lie far apart. We propose a probabilistic method that\ntreats embeddings as random variables. Extending a state-of-the-art\ndeterministic method, Prototypical Networks (Snell et al., 2017), our approach\nsupposes the existence of a class prototype around which class instances are\nGaussian distributed. The prototype posterior is a product distribution over\nlabeled instances, and query instances are classified by marginalizing relative\nprototype proximity over embedding uncertainty. We describe an efficient\nsampler for approximate inference that allows us to train the model at roughly\nthe same space and time cost as its deterministic sibling. Incorporating\nuncertainty improves performance on few-shot learning and gracefully handles\nlabel noise and out-of-distribution inputs. Compared to the state-of-the-art\nstochastic method, Hedged Instance Embeddings (Oh et al., 2019), we achieve\nsuperior large- and open-set classification accuracy. Our method also aligns\nclass-discriminating features with the axes of the embedding space, yielding an\ninterpretable, disentangled representation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The RISC Algorithm Language (RISCAL) is a language for the formal modeling of\ntheories and algorithms. A RISCAL specification describes an infinite class of\nmodels each of which has finite size; this allows to fully automatically check\nin such a model the validity of all theorems and the correctness of all\nalgorithms. RISCAL thus enables us to quickly verify/falsify the specific truth\nof propositions in sample instances of a model class before attempting to prove\ntheir general truth in the whole class: the first can be achieved in a fully\nautomatic way while the second typically requires our assistance. RISCAL has\nbeen mainly developed for educational purposes. To this end this paper reports\non some new enhancements of the tool: the automatic generation of checkable\nverification conditions from algorithms, the visualization of the execution of\nprocedures and the evaluation of formulas illustrating the computation of their\nresults, and the generation of Web-based student exercises and assignments from\nRISCAL specifications. Furthermore, we report on our first experience with\nRISCAL in the teaching of courses on logic and formal methods and on further\nplans to use this tool to enhance formal education.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The vanishing viscosity limit of the two-dimensional (2D) compressible\nisentropic Navier-Stokes equations is studied in the case that the\ncorresponding 2D inviscid Euler equations admit a planar rarefaction wave\nsolution. It is proved that there exists a family of smooth solutions for the\n2D compressible Navier-Stokes equations converging to the planar rarefaction\nwave solution with arbitrary strength for the 2D Euler equations. A uniform\nconvergence rate is obtained in terms of the viscosity coefficients away from\nthe initial time. In the proof, the hyperbolic wave is crucially introduced to\nrecover the physical viscosities of the inviscid rarefaction wave profile, in\norder to rigorously justify the vanishing viscosity limit.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We report on measurements on the 5S-6P rubidium transition frequencies for\nrubidium isotopes with an absolute uncertainty of better than \\SI{450}{kHz} for\nthe 5S $\\rightarrow$ 6P$_{1/2}$ transition and \\SI{20}{kHz} for the 5S\n$\\rightarrow$ 6P$_{3/2}$ transition, achieved by saturation absorption\nspectroscopy. From the results we derive the hyperfine splitting with an\naccuracy of \\SI{460}{kHz} and \\SI{30}{kHz}, respectively. We also verify the\nliterature values for the isotope shifts as well as magnetic dipole constant\nand the electric quadrupole constant.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Referring expression comprehension aims to localize the object instance\ndescribed by a natural language expression. Current referring expression\nmethods have achieved good performance. However, none of them is able to\nachieve real-time inference without accuracy drop. The reason for the\nrelatively slow inference speed is that these methods artificially split the\nreferring expression comprehension into two sequential stages including\nproposal generation and proposal ranking. It does not exactly conform to the\nhabit of human cognition. To this end, we propose a novel Realtime\nCross-modality Correlation Filtering method (RCCF). RCCF reformulates the\nreferring expression comprehension as a correlation filtering process. The\nexpression is first mapped from the language domain to the visual domain and\nthen treated as a template (kernel) to perform correlation filtering on the\nimage feature map. The peak value in the correlation heatmap indicates the\ncenter points of the target box. In addition, RCCF also regresses a 2-D object\nsize and 2-D offset. The center point coordinates, object size and center point\noffset together to form the target bounding box. Our method runs at 40 FPS\nwhile achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg\nbenchmarks. In the challenging RefClef dataset, our methods almost double the\nstate-of-the-art performance (34.70% increased to 63.79%). We hope this work\ncan arouse more attention and studies to the new cross-modality correlation\nfiltering framework as well as the one-stage framework for referring expression\ncomprehension.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have performed a quantum mechanic calculation (including solving the\ncoupled Gross-Pitaevskii equations to obtain the spatial wave functions, and\ndiagonalizing the spin-dependent Hamiltonian in the spin-space to obtain the\ntotal spin state) together with an analytical analysis based on a classical\nmodel. Then, according to the relative orientations of the spins $S_A$, $S_B$\nand $S_C$ of the three species, the spin-textures of the ground state can be\nclassified into two types. In Type-I the three spins are either parallel or\nanti-parallel to each others, while in Type-II they point to different\ndirections but remain to be coplanar. Moreover, according to the magnitudes of\n$S_A$, $S_B$ and $S_C$ the spin-textures can be further classified into four\nkinds, namely, $p$+$p$+$p$ (all atoms of each species are in singlet-pairs),\none species in $f$ (fully polarized) and two species in $q$ (a mixture of\npolarized atoms and singlet-pairs), two in $f$ and one in $q$, and $f$+$f$+$f$.\nOther combinations are not allowed. The scopes of the parameters that supports\na specific spin-texture have been specified. A number of\nspin-texture-transitions have been found. For Type-I, the critical values at\nwhich a transition takes place are given by simple analytical formulae,\ntherefore these values can be predicted.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Graph states, which include for example Bell states, GHZ states and cluster\nstates, form a well-known class of quantum states with applications ranging\nfrom quantum networks to error-correction. Deciding whether two graph states\nare equivalent up to single-qubit Clifford operations is known to be decidable\nin polynomial time and have been studied both in the context of producing\ncertain required states in a quantum network but also in relation to stabilizer\ncodes. The reason for the latter this is that single-qubit Clifford equivalent\ngraph states exactly corresponds to equivalent stabilizer codes. We here\nconsider the computational complexity of, given a graph state |G>, counting the\nnumber of graph states, single-qubit Clifford equivalent to |G>. We show that\nthis problem is #P-Complete. To prove our main result we make use of the notion\nof isotropic systems in graph theory. We review the definition of isotropic\nsystems and point out their strong relation to graph states. We believe that\nthese isotropic systems can be useful beyond the results presented in this\npaper.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We discuss an exotic phase that adjoint QCD possibly exhibits in the deep\ninfrared (IR). It is a confining phase, with a light spectrum consisting of\nmassless composite fermions. The discrete chiral symmetry is broken, with\nunbroken continuous chiral symmetry. We argue that it may give a description of\nthe IR of adjoint QCD with three massless Weyl flavors and that it passes all\nconsistency checks known to us.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The decay constant of the $\\eta$-meson in the framework of 'resummed' chiral\nperturbation theory is discussed. A theoretical prediction is compared to the\navailable determinations. Compatibility of these determinations with the latest\nfits of the SU(3) low energy coupling constants is investigated. Preliminary\nresults for the obtained constraints on the low energy coupling constants\n$L^r_5$ and $L^r_4$, using Bayesian statistical approach, are presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work the development results of the TRITIUM project is presented. The\nmain objective of the project is the construction of a near real-time monitor\nfor low activity tritium in water, aimed at in-situ surveillance and\nradiological protection of river water in the vicinity of nuclear power plants.\nThe European Council Directive 2013/51/Euratom requires that the maximum level\nof tritium in water for human consumption to be lower than 100 Bq/L. Tritium\nlevels in the cooling water of nuclear power plants in normal operation are\nmuch higher than the levels caused by the natural and cosmogenic components,\nand may easily surmount the limit required by the Directive. The current\nliquid-scintillation measuring systems in environmental radioactivity\nlaboratories are sensitive to such low levels, but they are not suitable for\nreal-time monitoring. Moreover, there is no currently available device with\nenough sensitivity and monitoring capabilities that could be used for\nsurveillance of the cooling water of nuclear power plants. A detector system\nbased on scintillation fibers read out by photomultiplier tubes (PMTs) or\nsilicon photomultiplier (SiPM) arrays is under development for in-water tritium\nmeasurement. This detector will be installed in the vicinity of Almaraz nuclear\npower plant (Spain) in Spring 2019. An overview of the project development and\nthe results of first prototypes are presented.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We show that for real numbers $p,\\,q$ with $q<p$, and the related power means\n$\\mathscr{P}_p$, $\\mathscr{P}_q$, the inequality\n$$\\frac{\\mathscr{P}_p(x)}{\\mathscr{P}_q(x)} \\le \\exp \\bigg( \\frac{p-q}8 \\cdot\n\\bigg(\\ln\\bigg(\\frac{\\max x}{\\min x}\\bigg)\\bigg)^2 \\:\\bigg)$$ holds for every\nvector $x$ of positive reals. Moreover we prove that, for all such pairs\n$(p,q)$, the constant $\\tfrac{p-q}8$ is sharp.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Global localization is an important and widely studied problem for many\nrobotic applications. Place recognition approaches can be exploited to solve\nthis task, e.g., in the autonomous driving field. While most vision-based\napproaches match an image w.r.t. an image database, global visual localization\nwithin LiDAR-maps remains fairly unexplored, even though the path toward high\ndefinition 3D maps, produced mainly from LiDARs, is clear. In this work we\nleverage Deep Neural Network (DNN) approaches to create a shared embedding\nspace between images and LiDAR-maps, allowing for image to 3D-LiDAR place\nrecognition. We trained a 2D and a 3D DNN that create embeddings, respectively\nfrom images and from point clouds, that are close to each other whether they\nrefer to the same place. An extensive experimental activity is presented to\nassess the effectiveness of the approach w.r.t. different learning paradigms,\nnetwork architectures, and loss functions. All the evaluations have been\nperformed using the Oxford Robotcar Dataset, which encompasses a wide range of\nweather and light conditions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Suppose there are $n$ Markov chains and we need to pay a per-step\n\\emph{price} to advance them. The \"destination\" states of the Markov chains\ncontain rewards; however, we can only get rewards for a subset of them that\nsatisfy a combinatorial constraint, e.g., at most $k$ of them, or they are\nacyclic in an underlying graph. What strategy should we choose to advance the\nMarkov chains if our goal is to maximize the total reward \\emph{minus} the\ntotal price that we pay?\n  In this paper we introduce a Markovian price of information model to capture\nsettings such as the above, where the input parameters of a combinatorial\noptimization problem are given via Markov chains. We design\noptimal/approximation algorithms that jointly optimize the value of the\ncombinatorial problem and the total paid price. We also study \\emph{robustness}\nof our algorithms to the distribution parameters and how to handle the\n\\emph{commitment} constraint.\n  Our work brings together two classical lines of investigation: getting\noptimal strategies for Markovian multi-armed bandits, and getting exact and\napproximation algorithms for discrete optimization problems using combinatorial\nas well as linear-programming relaxation ideas.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Let $p(\\cdot):\\mathbb R^n\\rightarrow(0,\\infty)$ be a variable exponent\nfunction satisfying the globally log-H\\\"older continuous condition. In this\npaper, we obtain the boundedness of para-product operators $\\pi_b$ on variable\nHardy spaces $H^{p(\\cdot)}(\\mathbb R^n)$, where $b\\in BMO(\\mathbb R^n)$. As an\napplication, we show that non-convolution type Calder\\'on-Zygmund operators $T$\nare bounded on $H^{p(\\cdot)}(\\mathbb R^n)$ if and only if $T^\\ast1=0$, where\n$\\frac{n}{n+\\epsilon}<\\mbox{essinf}_{x\\in\\mathbb R^n} p\\le\n\\mbox{esssup}_{x\\in\\mathbb R^n} p\\le 1$, $\\epsilon$ is the regular exponent of\nkernel of $T$. Our approach relies on the discrete version of Calder\\'on's\nreproducing formula, discrete Littlewood-Paley-Stein theory and almost\northogonal estimates. These results still hold for variable Hardy space on\nspaces of homogeneous type by using our methods.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents a simple model for predicting electrical conductivity of\nair with varying electrode separation and different moisture content present in\nair. Our system consists of a metallic thin film (Cu) coated sample and a\nneedle tip in a box filled with air. A constant potential difference has been\napplied between the tip and the sample and electrical current is calculated for\ndifferent air-gaps and for different humidity conditions using COMSOL\nMultiphysics software. The electric potential, electric field and current\ndensity in the region between the probe tip and sample have been presented. The\nnumerical results showing the variation of terminal current as a function of\ngap separation and relative humidity are found to be analogous with theoretical\nprediction. This study will be useful for finding the optimal current in case\nof electric field induced scanning probe nanolithography techniques such as\nelectrolithography.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this manuscript we introduce numerical Gaussian process Kalman filtering\n(GPKF). Numerical Gaussian processes have recently been developed to simulate\nspatiotemporal models. The contribution of this paper is to embed numerical\nGaussian processes into the recursive Kalman filter equations. This embedding\nenables us to do Kalman filtering on infinite-dimensional systems using\nGaussian processes. This is possible because i) we are obtaining a linear model\nfrom numerical Gaussian processes, and ii) the states of this model are by\ndefinition Gaussian distributed random variables. Convenient properties of the\nnumerical GPKF are that no spatial discretization of the model is necessary,\nand manual setting up of the Kalman filter, that is fine-tuning the process and\nmeasurement noise levels by hand is not required, as they are learned online\nfrom the data stream. We showcase the capability of the numerical GPKF in a\nsimulation study of the advection equation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Word embeddings predict a word from its neighbours by learning small, dense\nembedding vectors. In practice, this prediction corresponds to a semantic score\ngiven to the predicted word (or term weight). We present a novel model that,\ngiven a target word, redistributes part of that word's weight (that has been\ncomputed with word embeddings) across words occurring in similar contexts as\nthe target word. Thus, our model aims to simulate how semantic meaning is\nshared by words occurring in similar contexts, which is incorporated into\nbag-of-words document representations. Experimental evaluation in an\nunsupervised setting against 8 state of the art baselines shows that our model\nyields the best micro and macro F1 scores across datasets of increasing\ndifficulty.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Computational analysis has become ubiquitous within the heliophysics\ncommunity. However, community standards for peer-review of codes and analysis\nhave lagged behind these developments. This absence has contributed to the\nreproducibility crisis, where inadequate analysis descriptions and loss of\nscientific data have made scientific studies difficult or impossible to\nreplicate. The heliophysics community has responded to this challenge by\nexpressing a desire for a more open, collaborative set of analysis tools. This\narticle summarizes the current state of these efforts and presents an overview\nof many of the existing Python heliophysics tools. It also outlines the\nchallenges facing community members who are working towards the goal of an\nopen, collaborative, Python heliophysics toolkit and presents guidelines that\ncan ease the transition from individualistic data analysis practices to an\naccountable, communalistic environment.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper considers the stress-induced phase transitions of shape memory\nalloy slender cylinder, and analytically studies the phase transition process\nand the associated instability. A three-dimensional (3D) phenomenological model\nwith an internal variable is adopted, which is simplified to a 1D system of two\nstrains in three regions (austenite, martensite and phase transition regions).\nSuitable boundary conditions and interface conditions are proposed.\nTheoretically, it is a free boundary problem, as the position and shape of\nphase interfaces are unknown. We then focus on planar interfaces (which are\nenergetically favored), and a symmetric case when phase transition occurs in\nthe middle. For given applied stress, two-region solutions, three-region\nsolutions and the connecting solutions between them are obtained analytically\nor semi-analytically, including many period-k solutions.\n  Two-region solutions show that phase transition does not take place at one\npoint, but simultaneously in a small region. The width of phase transition\nregion is found analytically, revealing the roles of the material and\ngeometrical parameters. Three-region solutions represent localized\ninhomogeneous deformations in experiments, and capture that the stress stays\nalmost at the Maxwell stress during propagation of transformation front. For\ndisplacement-controlled process, the transition process is demonstrated by the\nstress-strain curve, which captures the stress drop/rise and the transition\nfrom homogeneous deformations to period-1 localized inhomogeneous deformations.\nWhen the radius is smaller than a critical value (given by material constants),\nthe stress drop is very sharp due to transition of solutions in a snap-back\nbifurcation. These features show good agreement with experimental observations\nand shed light on the difficulties of numerical simulations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  IceCube is a cubic-kilometer Cherenkov telescope operating at the South Pole.\nIn 2013 IceCube discovered high-energy astrophysical neutrinos and has more\nrecently found compelling evidence for a flaring blazar being a source of\nhigh-energy neutrinos. However, as blazars can only be responsible for a small\nfraction of the observed neutrino flux, the sources responsible for the\nmajority of the detected neutrinos remain unknown. In this work, we explore the\npossibility that the observed neutrino flux is produced in the cores of Active\nGalactic Nuclei (AGN). Various models have predicted neutrino emission from the\naccretion disks of AGN. According to these models, the neutrino luminosity\nwould not depend strongly on either the orientation or other parameters of the\nrelativistic jet. Both jetted and non-jetted AGN could contribute to the\nneutrino flux. We perform a stacking analysis to test for correlation between\nvarious sub-populations of AGN and high-energy neutrinos using a decade of\nIceCube data. We select AGN based on their radio emission, infrared color\nproperties, and X-ray flux using the NVSS, AllWISE, ROSAT and XMMSL2 catalogs.\nWe use the accretion disk luminosity, estimated from the observed soft X-ray\nflux, to weight the contribution of selected galaxies to the neutrino signal.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce a presentation of the Chow ring of a matroid by a new set of\ngenerators, called \"simplicial generators.\" These generators are analogous to\nnef divisors on projective toric varieties, and admit a combinatorial\ninterpretation via the theory of matroid quotients. Using this combinatorial\ninterpretation, we (i) produce a bijection between a monomial basis of the Chow\nring and a relative generalization of Schubert matroids, (ii) recover the\nPoincar\\'e duality property, (iii) give a formula for the volume polynomial,\nwhich we show is log-concave in the positive orthant, and (iv) recover the\nvalidity of Hodge-Riemann relations in degree 1, which is the part of the Hodge\ntheory of matroids that currently accounts for all combinatorial applications\nof [AHK18]. Our work avoids the use of \"flips,\" the key technical tool employed\nin [AHK18].\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Dark matter direct detection experiments have poor sensitivity to a galactic\npopulation of dark matter with mass below the GeV scale. However, such dark\nmatter can be produced copiously in supernovae. Since this thermally-produced\npopulation is much hotter than the galactic dark matter, it can be observed\nwith direct detection experiments. In this paper, we focus on a dark sector\nwith fermion dark matter and a heavy dark photon as a specific example. We\nfirst extend existing supernova cooling constraints on this model to the regime\nof strong coupling where the dark matter becomes diffusively trapped in the\nsupernova. Then, using the fact that even outside these cooling constraints the\ndiffuse galactic flux of these dark sector particles can still be large, we\nshow that this flux is detectable in direct detection experiments such as\ncurrent and next-generation liquid xenon detectors. As a result, due to\nsupernova production, light dark matter has the potential to be discovered over\nmany orders of magnitude of mass and coupling.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  An elastic-visco-plastic thermomechanical model for the simulation of cold\nforming and subsequent sintering of ceramic powders is introduced and based on\nmicromechanical modelling of the compaction process of granulates.\nMicromechanics leads to an upper-bound estimate of the compaction curve of a\ngranular material, which compares well with other models and finite element\nsimulations. The parameters of the thermomechanical model are determined on the\nbasis of available data and dilatometer experiments. Finally, after computer\nimplementation, validation of the model is performed against experiments\ndeveloped on specially designed ceramic pieces, characterized by zones of\ndifferent density. The mechanical model is found to accurately describe forming\nand sintering of stoneware ceramics and can therefore be used to analyze and\noptimize industrial processes involving compaction of powders and subsequent\nfiring of the greens.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Spin-orbit coupling (SOC) has gained much attention for its rich physical\nphenomena and highly promising applications in spintronic devices. The\nRashba-type SOC in systems with inversion symmetry breaking is particularly\nattractive for spintronics applications since it allows for flexible\nmanipulation of spin current by external electric fields. Here, we report the\ndiscovery of a giant anisotropic Rashba-like spin splitting along three\nmomentum directions (3D Rashba-like spin splitting) with a helical spin\npolarization around the M points in the Brillouin zone of trigonal layered\nPtBi2. Due to its inversion asymmetry and reduced symmetry at the M point,\nRashba-type as well as Dresselhaus-type SOC cooperatively yield a 3D spin\nsplitting with alpha~ 4.36 eVA in PtBi2. The experimental realization of 3D\nRashba-like spin splitting not only has fundamental interests but also paves\nthe way to the future exploration of a new class of material with unprecedented\nfunctionalities for spintronics applications.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In autonomous navigation, a planning system reasons about other agents to\nplan a safe and plausible trajectory. Before planning starts, agents are\ntypically processed with computationally intensive models for recognition,\ntracking, motion estimation and prediction. With limited computational\nresources and a large number of agents to process in real time, it becomes\nimportant to efficiently rank agents according to their impact on the decision\nmaking process. This allows spending more time processing the most important\nagents. We propose a system to rank agents around an autonomous vehicle (AV) in\nreal time. We automatically generate a ranking data set by running the planner\nin simulation on real-world logged data, where we can afford to run more\naccurate and expensive models on all the agents. The causes of various planner\nactions are logged and used for assigning ground truth importance scores. The\ngenerated data set can be used to learn ranking models. In particular, we show\nthe utility of combining learned features, via a convolutional neural network,\nwith engineered features designed to capture domain knowledge. We show the\nbenefits of various design choices experimentally. When tested on real AVs, our\nsystem demonstrates the capability of understanding complex driving situations.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Quadrupole moments of decuplet baryons and the octet-decuplet transition\nquadrupole moments are calculated using Morpurgo's general QCD parameterization\nmethod. Certain relations among the decuplet and the octet to decuplet\ntransition quadrupole moments are derived. These can be used to predict the\n$\\Delta$ quadrupole moments which are difficult to measure.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In our recent papers [Sh1,2], we introduced a {\\it twisted tensor product} of\ndg categories, and provided, in terms of it, {\\it a contractible 2-operad\n$\\mathcal{O}$}, acting on the category of small dg categories, in which the\n\"natural transformations\" are derived. We made use of some homotopy theory\ndeveloped in [To] to prove the contractibility of the 2-operad $\\mathcal{O}$.\nThe contractibility is an important issue, in vein of the theory of Batanin\n[Ba1,2], according to which an action of a contractible $n$-operad on $C$ makes\n$C$ a weak $n$-category.\n  In this short note, we provide a new elementary proof of the contractibility\nof the 2-operad $\\mathcal{O}$. The proof is based on a direct computation, and\nis independent from the homotopy theory of dg categories (in particular, it is\nindependent from [To] and from Theorem 2.4 of [Sh1]).\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We establish rigorously the regularization of the p-adic open string\namplitudes, with Chan-Paton rules and a constant B-field, introduced by Ghoshal\nand Kawano. In this study we use techniques of multivariate local zeta\nfunctions depending on multiplicative characters and a phase factor which\ninvolves an antisymmetric bilinear form. These local zeta functions are new\nmathematical objects. We attach to each amplitude a multivariate local zeta\nfunction depending on the kinematics parameters, the B-field and the Chan-Paton\nfactors. We show that these integrals admit meromorphic continuations in the\nkinematic parameters, this result allows us to regularize the Goshal-Kawano\namplitudes, the regularized amplitudes do not have ultraviolet divergences. Due\nto the need of a certain symmetry, the theory works only for prime numbers\nwhich are congruent to 3 modulo 4. We also discuss the limit p tends to 1 in\nthe noncommutative effective field theory and in the Ghoshal-Kawano amplitudes.\nWe show that in the case of four points, the limit p tends to 1 of the\nregularized Ghoshal-Kawano amplitudes coincides with the Feynman amplitudes\nattached to the limit p tends to 1 of the noncommutative Gerasimov-Shatashvili\nLagrangian.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider a random walk on the hyperoctahedral group $B_n$ generated by the\nsigned permutations of the forms $(i,n)$ and $(-i,n)$ for $1\\leq i\\leq n$. We\ncall this the flip-transpose top with random shuffle on $B_n$. We find the\nspectrum of the transition probability matrix for this shuffle. We prove that\nthe mixing time for this shuffle is of order $n\\log n$. We also show that this\nshuffle exhibits the cutoff phenomenon. In the appendix, we show that a similar\nrandom walk on the demihyperoctahedral group $D_n$ also has a cutoff at\n$\\left(n-\\frac{1}{2}\\right)\\log n$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Despite significant progress on stability analysis of conventional multiagent\nnetworked systems with weakly coupled state-network dynamics, most of the\nexisting results have shortcomings in addressing multiagent systems with highly\ncoupled state-network dynamics. Motivated by numerous applications of such\ndynamics, in our previous work [1], we initiated a new direction for stability\nanalysis of such systems that uses a sequential optimization framework.\nBuilding upon that, in this paper, we extend our results by providing another\nangle on multiagent network dynamics from a duality perspective, which allows\nus to view the network structure as dual variables of a constrained nonlinear\nprogram. Leveraging that idea, we show that the evolution of the coupled\nstate-network multiagent dynamics can be viewed as iterates of a primal-dual\nalgorithm for a static constrained optimization/saddle-point problem. This view\nbridges the Lyapunov stability of state-dependent network dynamics and\nfrequently used optimization techniques such as block coordinated descent,\nmirror descent, the Newton method, and the subgradient method. As a result, we\ndevelop a systematic framework for analyzing the Lyapunov stability of\nstate-dependent network dynamics using techniques from nonlinear optimization.\nFinally, we support our theoretical results through numerical simulations from\nsocial science.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The low efficiency of Raman spectroscopy can be overcome by placing the\nactive molecules in the vicinity of scatterers, typically rough surfaces or\nnanostructures with various shapes. This surface-enhanced Raman scattering\n(SERS) leads to substantial enhancement that depends on the scatterer that is\nused. In this work, we find fundamental upper bounds on the Raman enhancement\nfor arbitrary-shaped scatterers, depending only on its material constants and\nthe separation distance from the molecule. According to our metric, silver is\noptimal in visible wavelengths while aluminum is better in the near-UV region.\nOur general analytical bound scales as the volume of the scatterer and the\ninverse sixth power of the distance to the active molecule. Numerical\ncomputations show that simple geometries fall short of the bounds, suggesting\nfurther design opportunities for future improvement. For periodic scatterers,\nwe use two formulations to discover different bounds, and the tighter of the\ntwo always must apply. Comparing these bounds suggests an optimal period\ndepending on the volume of the scatterer.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  It is a widely accepted fact that state-sponsored Twitter accounts operated\nduring the 2016 US presidential election spreading millions of tweets with\nmisinformation and inflammatory political content. Whether these social media\ncampaigns of the so-called \"troll\" accounts were able to manipulate public\nopinion is still in question. Here we aim to quantify the influence of troll\naccounts and the impact they had on Twitter by analyzing 152.5 million tweets\nfrom 9.9 million users, including 822 troll accounts. The data collected during\nthe US election campaign, contain original troll tweets before they were\ndeleted by Twitter. From these data, we constructed a very large interaction\ngraph; a directed graph of 9.3 million nodes and 169.9 million edges. Recently,\nTwitter released datasets on the misinformation campaigns of 8,275\nstate-sponsored accounts linked to Russia, Iran and Venezuela as part of the\ninvestigation on the foreign interference in the 2016 US election. These data\nserve as ground-truth identifier of troll users in our dataset. Using graph\nanalysis techniques we qualify the diffusion cascades of web and media context\nthat have been shared by the troll accounts. We present strong evidence that\nauthentic users were the source of the viral cascades. Although the trolls were\nparticipating in the viral cascades, they did not have a leading role in them\nand only four troll accounts were truly influential.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  I describe a five-dimensional, polarised, Bethe-Heitler event generator of\n$\\gamma$-ray conversions to $\\mu^+\\mu^-$, based on a generator for conversion\nto $e^+e^-$ developed in the past. Verifications are performed from\nclose-to-threshold to high energies.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper proposes a novel adaptive guidance system developed using\nreinforcement meta-learning with a recurrent policy and value function\napproximator. The use of recurrent network layers allows the deployed policy to\nadapt real time to environmental forces acting on the agent. We compare the\nperformance of the DR/DV guidance law, an RL agent with a non-recurrent policy,\nand an RL agent with a recurrent policy in four difficult tasks with unknown\nbut highly variable dynamics. These tasks include a safe Mars landing with\nrandom engine failure and a landing on an asteroid with unknown environmental\ndynamics. We also demonstrate the ability of a recurrent policy to navigate\nusing only Doppler radar altimeter returns, thus integrating guidance and\nnavigation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The Green's function coupled cluster (GFCC) method is a powerful many-body\ntool for computing the electronic structure of molecular and periodic systems,\nespecially when electrons of the system are strongly correlated. However, for\nthe GFCC to be routinely used in the electronic structure calculations, robust\nnumerical techniques and approximations must be employed to reduce its high\ncomputational overhead. In our recent studies, we demonstrated that the GFCC\nequations can be solved directly in the frequency domain using iterative linear\nsolvers, which can be easily distributed in a massively parallel environment.\nIn the present work, we demonstrate a successful application of\nmodel-order-reduction (MOR) techniques in the GFCC framework. Briefly, for a\nfrequency regime which requires high resolution spectral function, instead of\nsolving GFCC linear equation of full dimension for every single frequency\npoint, an efficiently-solvable linear system model of a reduced dimension may\nbe built upon projecting the original GFCC linear system onto a subspace. From\nthis reduced order model is obtained a reasonable approximation to the full\ndimensional GFCC linear equations in both interpolative and extrapolative\nspectral regions. Here, we show that the subspace can be properly constructed\nin an iterative manner from the auxiliary vectors of the GFCC linear equations\nat some selected frequencies within the spectral region of interest. During the\niterations, the quality of the subspace and the linear system model can be\nsystematically improved. The method is tested in terms of the efficiency and\naccuracy of computing spectral functions for some typical molecular systems\nsuch as carbon monoxide, 1,3-butadiene, benzene, and adenine. As a byproduct,\nthe obtained reduced order model may provide a high quality initial guess which\nimproves the convergence rate for the existing iterative linear solver.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Robust clustering of high-dimensional data is an important topic because, in\nmany practical situations, real data sets are heavy-tailed and/or asymmetric.\nMoreover, traditional model-based clustering often fails for high dimensional\ndata due to the number of free covariance parameters. A parametrization of the\ncomponent scale matrices for the mixture of generalized hyperbolic\ndistributions is proposed by including a penalty term in the likelihood\nconstraining the parameters resulting in a flexible model for high dimensional\ndata and a meaningful interpretation. An analytically feasible EM algorithm is\ndeveloped by placing a gamma-Lasso penalty constraining the concentration\nmatrix. The proposed methodology is investigated through simulation studies and\ntwo real data sets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this article, we compute the motivic Igusa zeta function of a space\nmonomial curve that appears as the special fiber of an equisingular family\nwhose generic fiber is a complex plane branch. To this end, we determine the\nirreducible components of the jet schemes of such a space monomial curve. This\napproach does not only yield a closed formula for the motivic zeta function,\nbut also allows to determine its poles. We show that, while the family of the\njet schemes of the fibers is not flat, the number of poles of the motivic zeta\nfunction associated with the space monomial curve is equal to the number of\npoles of the motivic zeta function associated with a generic curve in the\nfamily.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study the physical properties of the ionized gas in local disks using the\nsample of 38 nearby $\\sim10^{8.5-11.2}$M$_\\odot$ Star-Forming Main Sequence\n(SFMS) galaxies observed so far as part of the MUSE Atlas of Disks (MAD).\nSpecifically, we use all strong emission lines in the MUSE wavelength range\n4650-9300 \\AA\\ to investigate the resolved ionized gas properties on $\\sim$100\npc scales. This spatial resolution enables us to disentangle HII regions from\nthe Diffuse Ionized Gas (DIG) in the computation of gas metallicities and Star\nFormation Rates (SFRs) of star forming regions.\n  The gas metallicities generally decrease with radius. The metallicity of the\nHII regions is on average $\\sim$0.1 dex higher than that of the DIG, but the\nmetallicity radial gradient in both components is similar. The mean\nmetallicities within the inner galaxy cores correlate with the total stellar\nmass of the galaxies. On our <100 pc scales, we find two correlations\npreviously reported at kpc scales: a spatially resolved Mass-Metallicity\nRelation (RMZR) and a spatially resolved SFMS (RSFMS). We find no secondary\ndependency of the RMZR with the SFR density. We find that both resolved\nrelations have a local origin, as they do not depend on the total stellar mass.\nThe observational results of this paper are consistent with the inside-out\nscenario for the growth of galactic disks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce an infinite-dimensional $p$-adic affine group and construct its\nirreducible unitary representation. Our approach follows the one used by\nVershik, Gelfand and Graev for the diffeomorphism group, but with modifications\nmade necessary by the fact that the group does not act on the phase space.\nHowever it is possible to define its action on some classes of functions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Classifying inflationary scenarios according to their scaling properties is a\npowerful way to connect theory with observations. A useful tool to make such a\nclassification is the beta-function formalism. By describing inflation in terms\nof renormalization group equations, within this framework, it is possible to\ndefine universality classes, which can be considered as sets of theories that\nshare a common scale invariant limit. In this paper we apply the formalism to\ndefine such classes of universality for models of inflation where the inflaton\nis coupled to gauge fields. We show that the formalism may consistently be\nextended to capture the peculiar features of these models such as statistical\nanisotropy. We also obtain some consistency conditions which serve as useful\nguidelines for model building.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A spectroscopic study was carried out on the surface chemical abundances of\nCNO and several heavier elements in the primary and secondary components of 5\neclipsing close binaries around A-type (AR Aur, beta Aur, YZ Cas, WW Aur, and\nRR Lyn), in order to investigate the nature of chemical differences between\nboth components (being comparatively slow rotators alike due to tidal\nsynchronization). Regarding the systems comprising similar components, beta Aur\nand WW Aur were confirmed to exhibit no compositional difference between the\nprimary and secondary both showing almost the same Am anomaly, though the\nchemical peculiarities seen in the component stars of AR Aur show distinct\ndifferences (HgMn star and Am star). In contrast, as to the systems (YZ Cas and\nRR Lyn) consisting of considerably different (A and early-F) components, the\nsurface abundances are markedly different between the primary (Am) and\nsecondary (normal). These observational results may indicate Teff-dependent\ncharacteristics regarding the chemical anomalies of non-magnetic stars on the\nupper main sequence: (1) In the effective temperature range of 10000K > Teff >\n7000K, rotational velocity is the most important factor for determining the\nextent of Am peculiarity. (2) However, the emergence of Am phenomenon seems to\nhave a lower Teff limit at ~7000K, below which no abundance anomaly is observed\nregardless of stellar rotation. (3) The transition from Am anomaly (mild\ndeficiency in CNO) to HgMn anomaly (unusually large N depletion) is likely to\ntake place as Teff increases from ~10000K to ~11000K.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present JHU's system submission to the ASVspoof 2019 Challenge:\nAnti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).\nAnti-spoofing has gathered more and more attention since the inauguration of\nthe ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from\nall three major types: text-to-speech, voice conversion, and replay. Built upon\nprevious research work on Deep Neural Network (DNN), ASSERT is a pipeline for\nDNN-based approach to anti-spoofing. ASSERT has four components: feature\nengineering, DNN models, network optimization and system combination, where the\nDNN models are variants of squeeze-excitation and residual networks. We\nconducted an ablation study of the effectiveness of each component on the\nASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more\nthan 93% and 17% relative improvements over the baseline systems in the two\nsub-challenges in ASVspooof 2019, ranking ASSERT one of the top performing\nsystems. Code and pretrained models will be made publicly available.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Layout is a fundamental component of any graphic design. Creating large\nvarieties of plausible document layouts can be a tedious task, requiring\nnumerous constraints to be satisfied, including local ones relating different\nsemantic elements and global constraints on the general appearance and spacing.\nIn this paper, we present a novel framework, coined READ, for REcursive\nAutoencoders for Document layout generation, to generate plausible 2D layouts\nof documents in large quantities and varieties. First, we devise an exploratory\nrecursive method to extract a structural decomposition of a single document.\nLeveraging a dataset of documents annotated with labeled bounding boxes, our\nrecursive neural network learns to map the structural representation, given in\nthe form of a simple hierarchy, to a compact code, the space of which is\napproximated by a Gaussian distribution. Novel hierarchies can be sampled from\nthis space, obtaining new document layouts. Moreover, we introduce a\ncombinatorial metric to measure structural similarity among document layouts.\nWe deploy it to show that our method is able to generate highly variable and\nrealistic layouts. We further demonstrate the utility of our generated layouts\nin the context of standard detection tasks on documents, showing that detection\nperformance improves when the training data is augmented with generated\ndocuments whose layouts are produced by READ.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Multi-access Edge Computing (MEC) is commonly recognized as a key supporting\ntechnology for the emerging 5G systems. When deployed in fully virtualized\nnetworks, i.e., following the Network Function Virtualization (NFV) paradigm,\nit will enable a multitude of new applications and use cases. However, the\ngrowing number of devices, combined with the vastly increasing traffic demand,\ncall for low End-to-End (E2E) latency packet transfer and processing in an NFV\nenvironment, both in user and control plane. In this paper, focusing on control\nplane packet traffic, we investigate the general case of a MEC application\nconsuming a MEC service running on a different MEC host. To enable flexible MEC\nplatform service consumption at different localities, based on a\nstate-of-the-art statistical model of the total processing time, we define\nlatency-aware proximity zones around MEC servers hosting MEC application\ninstances. Exemplary scenarios exhibit the E2E performance benefit of\nintroducing the awareness of proximity zones around MEC hosts and service\nproducing MEC application instances. This performance-aware service consumption\nwill be beneficial in view of the future evolution towards distributed\ncomputing systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study a relation between coupling introduced by Ebeling and the polytope\nduality among families of $K3$ surfaces.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce the notion of Reeb parallel structure Jacobi operator for real\nhypersurfaces in the complex hyperbolic quadric ${Q^*}^m=SO^0_{2,m}/SO_2 SO_m$,\n$m \\geq 3$, and give a classification theory for real hypersurfaces in\n${{Q^*}^m}$, $m \\geq 3$, with Reeb parallel structure Jacobi operator.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Interacting particle systems with many degrees of freedom may undergo phase\ntransitions to sustain atypical fluctuations of dynamical observables such as\nthe current or the activity. This leads in some cases to symmetry-broken\nspace-time trajectories which enhance the probability of such events due to the\nemergence of ordered structures. Despite their conceptual and practical\nimportance, these dynamical phase transitions (DPTs) at the trajectory level\nare difficult to characterize due to the low probability of their occurrence.\nHowever, during the last decade advanced computational techniques have been\ndeveloped to measure rare events in simulations of many-particle systems that\nallow for the first time the direct observation and characterization of these\nDPTs. Here we review the application of a particular rare-event simulation\ntechnique, based on cloning Monte Carlo methods, to characterize DPTs in\nparadigmatic stochastic lattice gases. In particular, we describe in detail\nsome tricks and tips of the trade, paying special attention to the measurement\nof order parameters capturing the physics of the different DPTs, as well as to\nthe finite-size effects (both in the system size and number of clones) that\naffect the measurements. Overall, we provide a consistent picture of the\nphenomenology associated with DPTs and their measurement.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We consider the decomposition of a signal over an overcomplete set of\nvectors. Minimization of the $\\ell^1$-norm of the coefficient vector can often\nretrieve the sparsest solution (so-called \"$\\ell^1/\\ell^0$-equivalence\"), a\ngenerally NP-hard task, and this fact has powered the field of compressed\nsensing. Wright et al.'s sparse representation-based classification (SRC)\napplies this relationship to machine learning, wherein the signal to be\ndecomposed represents the test sample and columns of the dictionary are\ntraining samples. We investigate the relationships between\n$\\ell^1$-minimization, sparsity, and classification accuracy in SRC. After\nproving that the tractable, deterministic approach to verifying\n$\\ell^1/\\ell^0$-equivalence fundamentally conflicts with the high coherence\nbetween same-class training samples, we demonstrate that $\\ell^1$-minimization\ncan still recover the sparsest solution when the classes are well-separated.\nFurther, using a nonlinear transform so that sparse recovery conditions may be\nsatisfied, we demonstrate that approximate (not strict) equivalence is key to\nthe success of SRC.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Echo State Networks (ESNs) are known for their fast and precise one-shot\nlearning of time series. But they often need good hyper-parameter tuning for\nbest performance. For this good validation is key, but usually, a single\nvalidation split is used. In this rather practical contribution we suggest\nseveral schemes for cross-validating ESNs and introduce an efficient algorithm\nfor implementing them. The component that dominates the time complexity of the\nalready quite fast ESN training remains constant (does not scale up with $k$)\nin our proposed method of doing $k$-fold cross-validation. The component that\ndoes scale linearly with $k$ starts dominating only in some not very common\nsituations. Thus in many situations $k$-fold cross-validation of ESNs can be\ndone for virtually the same time complexity as a simple single split\nvalidation. Space complexity can also remain the same. We also discuss when the\nproposed validation schemes for ESNs could be beneficial and empirically\ninvestigate them on several different real-world datasets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We have investigated the use of pure spin-3/2 propagator with consistent\ninteraction Lagrangians to describe the property of spin-3/2 resonance. For\nthis purpose we use the antisymmetric tensor spinor representation. By using\nthe primary and secondary constraints we obtain the interaction fields that\nhave the correct degrees of freedom. To visualize the result we calculate the\ncontribution of spin-3/2 $\\Delta$ resonance to the total cross section of pion\nscattering and pion photoproduction off the nucleon. The result confirms that\nthe scattering and photoproduction amplitudes obtained from the pure spin-3/2\nrepresentation with consistent interaction Lagrangians exhibit the required\nproperty of a resonance. Therefore, the formalism can be used for\nphenomenological investigations in the realm of nuclear and particle physics.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The use of analogs - similar weather patterns - for weather forecasting and\nanalysis is an established method in meteorology. The most challenging aspect\nof using this approach in the context of operational radar applications is to\nbe able to perform a fast and accurate search for similar spatiotemporal\nprecipitation patterns in a large archive of historical records. In this\ncontext, sequential pairwise search is too slow and computationally expensive.\nHere we propose an architecture to significantly speed-up spatiotemporal analog\nretrieval by combining nonlinear geometric dimensionality reduction (UMAP) with\nthe fastest known Euclidean search algorithm for time series (MASS) to find\nradar analogs in constant time, independently of the desired temporal length to\nmatch and the number of extracted analogs. We compare UMAP with Principal\ncomponent analysis (PCA) and show that UMAP outperforms PCA for spatial MSE\nanalog search with proper settings. Moreover, we show that MASS is 20 times\nfaster than brute force search on the UMAP embeddings space. We test the\narchitecture on a real dataset and show that it enables precise and fast\noperational analog ensemble search through more than 2 years of radar archive\nin less than 5 seconds on a single workstation.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We study, both theoretically and experimentally, tunable metasurfaces\nsupporting sharp Fano-resonances inspired by optical bound states in the\ncontinuum. We explore the use of arsenic trisulfide (a photosensitive\nchalcogenide glass) having optical properties which can be finely tuned by\nlight absorption at the post-fabrication stage. We select the resonant\nwavelength of the metasurface corresponding to the energy below the arsenic\ntrisulfide bandgap, and experimentally control the resonance spectral position\nvia exposure to the light of energies above the bandgap.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Four NASA Science and Technology Definition Teams have been convened in order\nto develop and study four mission concepts to be evaluated by the upcoming 2020\nDecadal Survey. The Lynx x-ray surveyor mission is one of these four large\nmissions. Lynx will couple fine angular resolution (<0.5 arcsec HPD) x-ray\noptics with large effective area (~2 m^2 at 1 keV), thus enabling exploration\nwithin a unique scientific parameter space. One of the primary soft x-ray\nimaging instruments being baselined for this mission concept is the\nhigh-definition x-ray imager, HDXI. This instrument would use a finely\npixelated silicon sensor array to achieve fine angular resolution imaging over\na wide field of view (~22 x 22 arcmin). Silicon sensors enable\nlarge-format/small-pixel devices, radiation tolerant designs, and high quantum\nefficiency across the entire soft x-ray bandpass. To fully exploit the large\ncollecting area of Lynx (~30x Chandra), with negligible or minimal x-ray event\npile-up, the HDXI will be capable of much faster frame rates than current x-ray\nimagers. We summarize the planned requirements, capabilities, and development\nstatus of the HDXI instrument, and associated papers in this special edition\nwill provide further details on some specific detector options.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Flavour physics, from now up to the operation of the next high energy\ncollider, will be an important tool for BSM searches at the TeV scale. Although\nfar from exhaustive, a particularly relevant case is represented by the\npossibility that the Higgs be a composite Pseudo-Nambu-Goldstone-Boson (PNGB)\nat a scale $l_H=1/m_*$. While a totally model-independent assessment of the\npotential of flavour physics in this case is impossible, here we illustrate\nwhat is likely to be a minimal sensitivity on $m_*$ by considering suitable\nexamples.\\\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The identical ideal individual hypothesis is proposed. According to this\nhypothesis, the identical ideal individuals should be classified into two\nclasses: the bosonic individuals and the fermionic individuals. The bosonic\nindividuals can occupy the same behavior state while the fermionic individuals\ncan not be in the same behavior state. We propose that human beings and many\nspecies of animals are fermionic, which can not occupy the same behavior state\naccording to the Pauli exclusion principle. An unified theoretical explanation\nis given for the natures of two important and seemingly irrelated phenomena in\npsychology: the existence of the personal space and the behavior\ndifferentiation under high population density condition.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We cross-correlate maps of the thermal Sunyaev-Zeldovich (tSZ) Compton-$y$\nparameter published by Planck with the projected distribution of galaxies in a\nset of low-redshift tomographic bins. We use the nearly full-sky 2MASS\nPhotometric Redshift and WISE $\\times$ SuperCOSMOS public catalogues, covering\nthe redshift range $z\\lesssim0.4$. Our measurements allow us to place\nconstraints on the redshift dependence of the mass-observable relation for tSZ\ncluster count analyses in terms of the so-called 'hydrostatic mass bias'\nparameter $1-b_{\\rm H}$. These results can also be interpreted as measurements\nof the bias-weighted average gas pressure $\\langle bP_e\\rangle$ as a function\nof redshift, a quantity that can be related to the thermodynamics of gas inside\nhaloes and used to constrain energy injection processes. We measure $1-b_{\\rm\nH}$ with $\\sim6\\%$ precision in 6 equispaced redshift bins, and find no\nevidence for a redshift-dependent mass bias parameter, in agreement with\nprevious analyses. Our mean value of $1-b_{\\rm H} = 0.75\\pm0.03$ is also in\ngood agreement with the one estimated by the joint analysis of Planck cluster\ncounts and CMB anisotropies calibrated with CMB lensing. Our measurements of\n$\\langle bP_e\\rangle$, at the level of $\\sim10\\%$ in each bin, are the most\nstringent constraints on the redshift dependence of this parameter to date, and\nagree well both with previous measurements and with theoretical expectations\nfrom shock-heating models.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Cross domain image matching between image collections from different source\nand target domains is challenging in times of deep learning due to i) limited\nvariation of image conditions in a training set, ii) lack of paired-image\nlabels during training, iii) the existing of outliers that makes image matching\ndomains not fully overlap. To this end, we propose an end-to-end architecture\nthat can match cross domain images without labels in the target domain and\nhandle non-overlapping domains by outlier detection. We leverage domain\nadaptation and triplet constraints for training a network capable of learning\ndomain invariant and identity distinguishable representations, and iteratively\ndetecting the outliers with an entropy loss and our proposed weighted MK-MMD.\nExtensive experimental evidence on Office [17] dataset and our proposed\ndatasets Shape, Pitts-CycleGAN shows that the proposed approach yields\nstate-of-the-art cross domain image matching and outlier detection performance\non different benchmarks. The code will be made publicly available.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We classify all geometric torsion points on the Fermat quotients $y^n = x^d +\n1$ where $n, d \\ge 2$ are coprime. In addition, we classify all geometric\ntorsion points on the generic superelliptic curve $y^n = (x - a_1) \\cdots (x -\na_d)$, extending a result of Poonen and Stoll, who considered the hyperelliptic\n$n = 2$ case.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The validity of a complex reaction pathway proposed to treat Inflammatory\nBowel Disease (IBD) was verified by a comprehensive time and frequency domain\nanalysis. The model was taken to the frequency domain to study the effect and\nthe significance of the negative feedback loop introduced by the reaction\npathways. It could be shown that such proposed probiotics have very interesting\npotentials that could be used extensively in the near future.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we confirm some congruences conjectured by V.J.W. Guo and M.J.\nSchlosser recently. For example, we show that for primes $p>3$, $$\n\\sum_{k=0}^{p-1}(2pk-2k-1)\\frac{\\left(\\frac{-1}{p-1}\\right)_k^{2p-2}}{(k!)^{2p-2}}\\equiv0\\pmod{p^5}.\n$$\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove the existence of Siegel disks with smooth boundaries in most\nfamilies of holomorphic maps fixing the origin. The method can also yield other\ntypes of regularity conditions for the boundary. The family is required to have\nan indifferent fixed point at $0$, to be parameterized by the rotation number\n$\\alpha$, to depend on $\\alpha$ in a Lipschitz-continuous way, and to be\nnon-degenerate. A degenerate family is one for which the set of\nnon-linearizable maps is not dense. We give a characterization of degenerate\nfamilies, which proves that they are quite exceptional.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we determine the homotopy type of the quotient space\n$\\mathcal{Z}_{\\Delta^k_m}/S^1_d$, given by the moment-angle complex\n$\\mathcal{Z}_{\\Delta_m^k}$ under the diagonal circle action.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The identification of syllables within phonetic sequences is known as\nsyllabification. This task is thought to play an important role in natural\nlanguage understanding, speech production, and the development of speech\nrecognition systems. The concept of the syllable is cross-linguistic, though\nformal definitions are rarely agreed upon, even within a language. In response,\ndata-driven syllabification methods have been developed to learn from\nsyllabified examples. These methods often employ classical machine learning\nsequence labeling models. In recent years, recurrence-based neural networks\nhave been shown to perform increasingly well for sequence labeling tasks such\nas named entity recognition (NER), part of speech (POS) tagging, and chunking.\nWe present a novel approach to the syllabification problem which leverages\nmodern neural network techniques. Our network is constructed with long\nshort-term memory (LSTM) cells, a convolutional component, and a conditional\nrandom field (CRF) output layer. Existing syllabification approaches are rarely\nevaluated across multiple language families. To demonstrate cross-linguistic\ngeneralizability, we show that the network is competitive with state of the art\nsystems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque\ndatasets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We analyze the electrostatic self-energy of a point like electrically charged\nparticle induced by a cosmic string in the context of gravity's rainbow, as\nwell the electrostatic self-force on this particle. The possibility of the\nsolution associated with a charged particle to be altered by modifications in\ndispersion relations of the space-time is discussed. We show that the\nself-energy depends on the rainbow functions and that this quantity can either\nincrease or decrease depending on the rainbow function chosen, as compared with\nanalogous result in the framework of general relativity. With respect to the\nself-force, its dependence with the rainbow functions is also pointed out\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A famous conjecture about group algebras of torsion-free groups states that\nthere is no zero divisor in such group algebras. A recent approach to settle\nthe conjecture is to show the non-existence of zero divisors with respect to\nthe length of possible ones, where by the length we mean the size of the\nsupport of an element of the group algebra. The case length $2$ cannot be\nhappen. The first unsettled case is the existence of zero divisors of length\n$3$. Here we study possible length $3$ zero divisors in rational group algebras\nand in the group algebras over the field with $p$ elements for some prime $p$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this work, we present a holographic renormalization scheme for\nasymptotically anti-de Sitter spacetimes in which the dual renormalization\nscheme of the boundary field theory is dimensional regularization. This\nconstitutes a new level of precision in the holographic dictionary and paves\nthe way for the exact matching of scheme dependent quantities, such as\nholographic beta functions, with field theory computations. Furthermore, the\nrenormalization procedure identifies a local source field which satisfies the\nequations of motion along renormalization group flows, resolving a\nlong-standing puzzle regarding the Wilsonian coupling in holography. This\nidentification of the source field also provides new insight into field\ntheories deformed by marginal operators, which have been traditionally\ndifficult to analyze due to altered bulk asymptotics. Finally, we demonstrate a\nnew relation equating the analyticity of the holographic beta function to the\nabsence of conformal anomalies, and conjecture that the conformal anomaly\nshould vanish in the UV for all holographic constructions.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we present a novel approach for incorporating external\nknowledge in Recurrent Neural Networks (RNNs). We propose the integration of\nlexicon features into the self-attention mechanism of RNN-based architectures.\nThis form of conditioning on the attention distribution, enforces the\ncontribution of the most salient words for the task at hand. We introduce three\nmethods, namely attentional concatenation, feature-based gating and affine\ntransformation. Experiments on six benchmark datasets show the effectiveness of\nour methods. Attentional feature-based gating yields consistent performance\nimprovement across tasks. Our approach is implemented as a simple add-on module\nfor RNN-based models with minimal computational overhead and can be adapted to\nany deep neural architecture.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper is the third which examines galaxy morphology from the point of\nview of comprehensive de Vaucouleurs revised Hubble-Sandage (CVRHS)\nclassification, a variation on the original de Vaucouleurs classification\nvolume that accounts for finer details of galactic structure, including lenses,\nnuclear structures, embedded disks, boxy and disky components, and other\nfeatures. The classification is applied to the EFIGI sample, a well-defined set\nof nearby galaxies which were previously examined by Baillard et al. and de\nLapparent et al. The survey is focussed on statistics of features, and brings\nattention to exceptional examples of some morphologies, such as skewed bars,\nblue bar ansae, bar-outer pseudoring misalignment, extremely elongated inner SB\nrings, outer rings and lenses, and other features that are likely relevant to\ngalactic secular evolution and internal dynamics. The possibility of using\nthese classifications as a training set for automated classification algorithms\nis also discussed.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Self-healing, as an exotic effect, has showed many potential applications. In\nthis paper, we focus on the self-healing effect of Laguerre-Gaussian beams\nafter an obstacle. By taking advantage of angular spectrum theory, we study\nself-healing limit of the beam against on-axis obstacle. The dependence of\nself-healing capability on the radius of obstacle is analyzed. Additionally, we\nbriefly discuss the self-healing limit of the beam in an off-axis scenario. Our\nresults indicate that field amplitude of the beam will be healed well when the\nobstacle is approximately on-axis without oversized radius, perhaps providing\nadvantages for optical communication, imaging, and remote sensing systems.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Massive MIMO system yields significant improvements in spectral and energy\nefficiency for future wireless communication systems. The regularized\nzero-forcing (RZF) beamforming is able to provide good performance with the\ncapability of achieving numerical stability and robustness to the channel\nuncertainty. However, in massive MIMO systems, the matrix inversion operation\nin RZF beamforming becomes computationally expensive. To address this\ncomputational issue, we shall propose a novel randomized sketching based RZF\nbeamforming approach with low computational complexity. This is achieved by\nsolving a linear system via randomized sketching based on the preconditioned\nRichard iteration, which guarantees high quality approximations to the optimal\nsolution. We theoretically prove that the sequence of approximations obtained\niteratively converges to the exact RZF beamforming matrix linearly fast as the\nnumber of iterations increases. Also, it turns out that the system sum-rate for\nsuch sequence of approximations converges to the exact one at a linear\nconvergence rate. Our simulation results verify our theoretical findings.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate online scheduling with commitment for parallel identical\nmachines. Our objective is to maximize the total processing time of accepted\njobs. As soon as a job has been submitted, the commitment constraint forces us\nto decide immediately whether we accept or reject the job. Upon acceptance of a\njob, we must complete it before its deadline $d$ that satisfies $d \\geq\n(1+\\epsilon)\\cdot p + r$, with $p$ and $r$ being the processing time and the\nsubmission time of the job, respectively while $\\epsilon>0$ is the slack of the\nsystem. Since the hard case typically arises for near-tight deadlines, we\nconsider $\\varepsilon\\leq 1$. We use competitive analysis to evaluate our\nalgorithms. Our first main contribution is a deterministic preemptive online\nalgorithm with an almost tight competitive ratio on any number of machines. For\na single machine, the competitive factor matches the optimal bound\n$\\frac{1+\\epsilon}{\\epsilon}$ of the greedy acceptance policy. Then the\ncompetitive ratio improves with an increasing number of machines and approaches\n$(1+\\epsilon)\\cdot\\ln \\frac{1+\\epsilon}{\\epsilon}$ as the number of machines\nconverges to infinity. This is an exponential improvement over the greedy\nacceptance policy for small $\\epsilon$. In the non-preemptive case, we present\na deterministic algorithm on $m$ machines with a competitive ratio of $1+m\\cdot\n\\left(\\frac{1+\\epsilon}{\\epsilon}\\right)^{\\frac{1}{m}}$. This matches the\noptimal bound of $2+\\frac{1}{\\epsilon}$ of the greedy acceptance policy for a\nsingle machine while it again guarantees an exponential improvement over the\ngreedy acceptance policy for small $\\epsilon$ and large $m$. In addition, we\ndetermine an almost tight lower bound that approaches $m\\cdot\n\\left(\\frac{1}{\\epsilon}\\right)^{\\frac{1}{m}}$ for large $m$ and small\n$\\epsilon$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Hitherto unknown elementary particles can be searched for with atomic\nspectroscopy. We conduct such a search using a potential that results from the\nlongitudinal polarization of a pseudovector particle. We show that such a\npotential, inversely proportional to the boson's mass squared, $V \\propto\n1/M^2$, can stay finite at $M \\to 0$ if the theory is renormalizable. We also\nlook for a pseudoscalar boson, which induces a contact spin-dependent potential\nthat does not contribute to new forces searched for in experiments with\nmacroscopic objects, but may be seen in atomic spectroscopy. We extract limits\non the interaction constants of these potentials from the experimental spectra\nof antiprotonic helium, muonium, positronium, helium, and hydrogen.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Neural network interpretability is a vital component for applications across\na wide variety of domains. In such cases it is often useful to analyze a\nnetwork which has already been trained for its specific purpose. In this work,\nwe develop a method to produce explanation masks for pre-trained networks. The\nmask localizes the most important aspects of each input for prediction of the\noriginal network. Masks are created by a secondary network whose goal is to\ncreate as small an explanation as possible while still preserving the\npredictive accuracy of the original network. We demonstrate the applicability\nof our method for image classification with CNNs, sentiment analysis with RNNs,\nand chemical property prediction with mixed CNN/RNN architectures.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In this paper, we consider the electroweak production cross-section of a\nsingle anti-top-quark, a neutrino and a photon via charged current through the\n$e^-p \\to e^-\\bar b \\to \\bar t \\nu_e \\gamma \\to \\bar t(\\to W^- \\to (qq', l^-\n\\bar\\nu_l)+b) \\nu_e\\gamma$ signal. Further, we derived the sensitivity expected\nto the magnetic dipole moment $(\\hat a_V)$ and the electric dipole moment\n$(\\hat a_A)$ of the top-quark at the Future Circular Collider Hadron Electron\n(FCC-he). We present our study for $\\sqrt{s}=7.07, 10\\hspace{0.8mm}TeV$, ${\\cal\nL}=50, 100, 300, 500, 1000\\hspace{0.8mm}fb^{-1}$, $\\delta_{sys}=0, 3,\n5\\hspace{0.8mm}\\%$ and $P_{e^-}=0\\%, 80\\%, -80\\%$, respectively. We find that\nthe sensitivity estimated on dipole moments of the top-quark is of the order of\nmagnitude ${\\cal O}(10^{-1})$ for both hadronic and leptonic decay modes of\n$W^-$: $\\hat a_V=[-0.2308, 0.2204]$, $|\\hat a_A|=0.2259$ at $95\\%$ C.L. in the\nhadronic channel with unpolarized electron beam $P_{e^-}=0\\%$. Our results with\npolarized electron beam for $P_{e^-}=80\\%$ and $P_{e^-}=-80\\%$ are $\\hat\na_V=[-0.3428, 0.3321]$, $|\\hat a_A|=0.3371$ and $\\hat a_V=[-0.2041, 0.1858]$,\n$|\\hat a_A|=0.1939$ at $95\\%$ C.L. in the hadronic channel. The corresponding\nresults for the leptonic channel with $P_{e^-}=0\\%, 80\\% -80\\%$ are $\\hat\na_V=[-0.3067, 0.2963]$, $|\\hat a_A|=0.3019$, $\\hat a_V=[-0.4563, 0.4456]$,\n$|\\hat a_A|=0.4505$ and $\\hat a_V=[-0.2695, 0.2512]$, $|\\hat a_A|=0.2592$,\nrespectively. The results for $\\hat a_V$ and $\\hat a_A$ in the leptonic channel\nare weaker by a factor of 0.75 than those corresponding to the hadronic\nchannel. Given these prospective sensitivities we highlight that the FCC-he is\npotential top-quark factory that is particularly well suited to sensitivity\nstudy on its dipole moments and with cleaner environments.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We introduce geometric structures on the space of stability conditions of a\nthree-dimensional Calabi-Yau category which encode the Donaldson-Thomas\ninvariants of the category. We explain in detail a close analogy between these\nstructures, which we call Joyce structures, and Frobenius structures. In the\nsecond half of the paper we give explicit calculations of Joyce structures in\nthree interesting classes of examples.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Angiogenesis, the formation of new vessels, is one of the key mechanisms in\ntumor development and an appealing target for therapy. Non-invasive,\nhigh-resolution, high sensitivity, quantitative 3D imaging techniques are\nrequired to correctly depict tumor heterogeneous vasculature over time.\nUltrafast Doppler was recently introduced and provides an unprecedented\ncombination of resolution, penetration depth and sensitivity without requiring\nany contrast agents. The technique was further extended to 3D with Ultrafast\nDoppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring\nof tumor angiogenesis in vivo providing structural and functional information\nat different stages of development. UFD-T volume renderings showed that our\nmurine model's vasculature stems from pre-existing vessels and sprouts to\nperfuse the whole volume as the tumor grows until a critical size is reached.\nThen, as the network becomes insufficient, the tumor core is no longer\nirrigated because the vasculature is mainly concentrated in the periphery. In\naddition to spatial distribution and growth patterns, UFD-T allowed a\nquantitative analysis of vessel size and length, revealing that the\ndiameter-distribution of vessels remained relatively constant throughout tumor\ngrowth. The network is dominated by small vessels at all stages of tumor\ndevelopment with more than 74% of the vessels less than 200 $\\mu$m in diameter.\nThis study also showed that cumulative vessel length is more closely related to\ntumor radius than volume, indicating that the vascularization becomes\ninsufficient when a critical mass is reached. UFD-T was also compared with\ndynamic contrast-enhanced ultrasound (DCE-US) and shown to provide\ncomplementary information regarding the link between structure and perfusion.\nIn conclusion, UFD-T is capable of an in vivo quantitative assessment of the\ndevelopment of tumor vasculature (vessels with blood speed >1mm/s (sensitivity\nlimit) assessed with a resolution limit of 80 $\\mu$m) in 3D. The technique has\nvery interesting potential as a tool for treatment monitoring, response\nassessment and treatment planning for optimal drug efficiency.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper presents the findings of a case study of spreadsheet use in a\nhigher education institution in the UK. The paper considers the use of\nspreadsheets in two units of the organisation, academic registry and finance.\nSpreadsheet use is explored in terms of importance, training, experience,\npurpose, techniques deployed, size of spreadsheets created and sharing of\nspreadsheets. The implications of the results are then considered in terms of\naccurate reporting to external funding bodies such the funding councils,\ninternal data integrity and internal data efficiencies. The results show a\nlarge volume of spreadsheets being created and used, that the profile of\nspreadsheet developers is typical of other studies of spreadsheet use and the\nneed for the organisation to have clear principles and guidelines for the\ndevelopment of spreadsheet models in the organisation to ensure data integrity,\nreduce duplication of effort and to optimise the use of spreadsheets to meet\nthe institutions goals.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We present FAKTA which is a unified framework that integrates various\ncomponents of a fact checking process: document retrieval from media sources\nwith various types of reliability, stance detection of documents with respect\nto given claims, evidence extraction, and linguistic analysis. FAKTA predicts\nthe factuality of given claims and provides evidence at the document and\nsentence level to explain its predictions\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Score based learning (SBL) is a promising approach for learning Bayesian\nnetworks. The initial step in the majority of the SBL algorithms consists of\ncomputing the scores of all possible child and parent-set combinations for the\nvariables. For Bayesian networks with continuous variables, a particular score\nis usually calculated as a function of the regression of the child over the\nvariables in the parent-set. The sheer number of regressions models to be\nsolved necessitates the design of efficient numerical algorithms. In this\npaper, we propose an algorithm for an efficient and exact calculation of\nregressions for all child and parent-set combinations. In the proposed\nalgorithm, we use QR decompositions (QRDs) to capture the dependencies between\nthe regressions for different families and Givens rotations to efficiently\ntraverse through the space of QRDs such that all the regression models are\naccounted for in the shortest path possible. We compare the complexity of the\nsuggested method with different algorithms, mainly those arising in all subset\nregression problems, and show that our algorithm has the smallest algorithmic\ncomplexity. We also explain how to parallelize the proposed method so as to\ndecrease the runtime by a factor proportional to the number of processors\nutilized.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This study aims to reconcile quantum theory with the universality of the\nspeed of light in vacuum and its implications on relativity, through an\ninformation-theoretic approach. We introduce the concepts of a holographic\nsphere and variational potential. Entropy variation expressed in terms of\ninformation capacity of this sphere results in the concept of binary potential\nin units of negative, squared speed of light in vacuum. Accordingly, the event\nhorizon is a fundamental holographic sphere in thermodynamic equilibrium with\nonly one exterior side: a noncompressible binary message that maximizes Shannon\nentropy. Therefore, the Jordan-Brouwer separation theorem and generalized\nStokes theorem do not hold for black holes. We introduce the concept of\ninertial potential and demonstrate its equivalence to the variational\npotential, which ensures that any inertial acceleration represents a\nnonequilibrium thermodynamic condition. We introduce the concept of the\ncomplementary time period and relate it with the classical time period through\nintegral powers of the imaginary unit to formulate the notions of unobservable\nvelocity and acceleration, which are perpendicular and tangential to the\nholographic sphere, respectively, and bounded with the observable velocity and\nacceleration based on Pythagorean relations. We further discuss certain\ndynamics scenarios between the two masses. The concept of black hole\ninformation-less emission is introduced as a complement to informationless\nBekenstein absorption and extended to arbitrary wavelengths. Black hole quantum\nstatistics with degeneracy interpret-ed as the number of Planck areas on the\nevent horizon are discussed. The study concludes that holographic screens and\nequipotential surfaces are spherical equivalents, and every observer is a\nsphere in nonequilibrium thermodynamic condition. Lastly, we propose a solution\nto the black hole information paradox.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We investigate Bartnik's static metric extension conjecture under the\nadditional assumption of axisymmetry of both the given Bartnik data and the\ndesired static extensions. To do so, we suggest a geometric flow approach,\ncoupled to the Weyl-Papapetrou formalism for axisymmetric static solutions to\nthe Einstein vacuum equations. The elliptic Weyl-Papapetrou system becomes a\nfree boundary value problem in our approach. We study this new flow and the\ncoupled flow--free boundary value problem numerically and find axisymmetric\nstatic extensions for axisymmetric Bartnik data in many situations, including\nnear round spheres in spatial Schwarzschild of positive mass.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  We prove Wasserstein contraction of simple slice sampling for approximate\nsampling w.r.t. distributions with log-concave and rotational invariant\nLebesgue densities. This yields, in particular, an explicit quantitative lower\nbound of the spectral gap of simple slice sampling. Moreover, this lower bound\ncarries over to more general target distributions depending only on the volume\nof the (super-)level sets of their unnormalized density.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Face super-resolution methods usually aim at producing visually appealing\nresults rather than preserving distinctive features for further face\nidentification. In this work, we propose a deep learning method for face\nverification on very low-resolution face images that involves\nidentity-preserving face super-resolution. Our framework includes a\nsuper-resolution network and a feature extraction network. We train a VGG-based\ndeep face recognition network (Parkhi et al. 2015) to be used as feature\nextractor. Our super-resolution network is trained to minimize the feature\ndistance between the high resolution ground truth image and the super-resolved\nimage, where features are extracted using our pre-trained feature extraction\nnetwork. We carry out experiments on FRGC, Multi-PIE, LFW-a, and MegaFace\ndatasets to evaluate our method in controlled and uncontrolled settings. The\nresults show that the presented method outperforms conventional\nsuper-resolution methods in low-resolution face verification.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  In the framework of bilinear control systems, we present reachable sets of\ncoherently controllable open quantum systems with switchable coupling to a\nthermal bath of arbitrary temperature $T geq 0$. The core problem boils down to\nstudying points in the standard simplex amenable to two types of controls that\ncan be used interleaved:\n  (i) permutations within the simplex,\n  (ii) contractions by a dissipative one-parameter semigroup.\n  Our work illustrates how the solutions of the core problem pertain to the\nreachable set of the original controlled Markovian quantum system. We\ncompletely characterize the case $T=0$ and present inclusions for $T>0$.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Recent progress in AutoML has lead to state-of-the-art methods (e.g.,\nAutoSKLearn) that can be readily used by non-experts to approach any supervised\nlearning problem. Whereas these methods are quite effective, they are still\nlimited in the sense that they work for tabular (matrix formatted) data only.\nThis paper describes one step forward in trying to automate the design of\nsupervised learning methods in the context of text mining. We introduce a meta\nlearning methodology for automatically obtaining a representation for text\nmining tasks starting from raw text. We report experiments considering 60\ndifferent textual representations and more than 80 text mining datasets\nassociated to a wide variety of tasks. Experimental results show the proposed\nmethodology is a promising solution to obtain highly effective off the shell\ntext classification pipelines.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  This paper develops several average-case reduction techniques to show new\nhardness results for three central high-dimensional statistics problems,\nimplying a statistical-computational gap induced by robustness, a\ndetection-recovery gap and a universality principle for these gaps. A main\nfeature of our approach is to map to these problems via a common intermediate\nproblem that we introduce, which we call Imbalanced Sparse Gaussian Mixtures.\nWe assume the planted clique conjecture for a version of the planted clique\nproblem where the position of the planted clique is mildly constrained, and\nfrom this obtain the following computational lower bounds: (1) a $k$-to-$k^2$\nstatistical-computational gap for robust sparse mean estimation, providing the\nfirst average-case evidence for a conjecture of Li (2017) and Balakrishnan et\nal. (2017); (2) a tight lower bound for semirandom planted dense subgraph,\nwhich shows that a semirandom adversary shifts the detection threshold in\nplanted dense subgraph to the conjectured recovery threshold; and (3) a\nuniversality principle for $k$-to-$k^2$ gaps in a broad class of sparse mixture\nproblems that includes many natural formulations such as the spiked covariance\nmodel.\n  Our main approach is to introduce several average-case techniques to produce\nstructured and Gaussianized versions of an input graph problem, and then to\nrotate these high-dimensional Gaussians by matrices carefully constructed from\nhyperplanes in $\\mathbb{F}_r^t$. For our universality result, we introduce a\nnew method to perform an algorithmic change of measure tailored to sparse\nmixtures. We also provide evidence that the mild promise in our variant of\nplanted clique does not change the complexity of the problem.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  GJ 1132b, which orbits an M dwarf, is one of the few known Earth-sized\nplanets, and at 12 pc away it is one of the closest known transiting planets.\nReceiving roughly 19x Earth's insolation, this planet is too hot to be\nhabitable but can inform us about the volatile content of rocky planet\natmospheres around cool stars. Using Hubble STIS spectra, we search for a\ntransit in the Lyman-alpha line of neutral hydrogen (Ly-alpha). If we were to\nobserve a deep Ly-alpha absorption signature, that would indicate the presence\nof a neutral hydrogen envelope flowing from GJ 1132b. On the other hand, ruling\nout deep absorption from neutral hydrogen may indicate that this planet does\nnot have a detectable amount of hydrogen loss, is not losing hydrogen, or lost\nhydrogen and other volatiles early in the star's life. We do not detect a\ntransit and determine a 2-sigma upper limit on the effective envelope radius of\n0.36 R* in the red wing of the Ly-alpha line, which is the only portion of the\nspectrum we detect after absorption by the ISM. We analyze the Ly-alpha\nspectrum and stellar variability of GJ1132, which is a slowly-rotating 0.18\nsolar mass M dwarf with previously uncharacterized UV activity. Our data show\nstellar variabilities of 5-22%, which is consistent with the M dwarf UV\nvariabilities of up to 41% found by \\citet{Loyd2014}. Understanding the role\nthat UV variability plays in planetary atmospheres is crucial to assess\natmospheric evolution and the habitability of cooler rocky exoplanets.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A large-area thermoelectric generator (TEG) utilizing a folded thin-film\nconcept is implemented and the performance evaluated for near room temperature\napplications having modest temperature gradients (< 50 K). The TEGs with the\narea of ~0.33 m^2 are shown capable of powering a wireless sensor node of\nmultiple sensors suitable e.g. for monitoring environmental variables in\nbuildings. The TEGs are based on a transparent, non-toxic and abundant\nthermoelectric material, i.e. aluminium-doped zinc oxide (AZO), deposited on\nflexible substrates. After folding, both the electrical current and heat flux\nare in the plane of the thermoelectric thin-film. Heat leakage in the folded\nTEG is shown to be minimal (close to that of air), enabling sufficient\ntemperature gradients without efficient heat sinks, contrary to the\nconventional TEGs having the thermal flux and electrical current perpendicular\nto the plane of the thermoelectric films. The long-term stability studies\nreveal that there are no significant changes in the electrical or\nthermoelectric properties of AZO over several months, while the contact\nresistance between AZO and silver ink is an issue exhibiting a continuous\nincrease over time. The performance of the TEGs and technological implications\nin relation to a state-of-the-art thermoelectric material are further assessed\nvia a computational study.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Feature generating networks face to the most important question, which is the\nfitting difference (inconsistence) of the distribution between the generated\nfeature and the real data. This inconsistence further influence the performance\nof the networks model, because training samples from seen classes is disjointed\nwith testing samples from unseen classes in zero-shot learning (ZSL). In\ngeneralization zero-shot learning (GZSL), testing samples come from not only\nseen classes but also unseen classes for closer to the practical situation.\nTherefore, most of feature generating networks difficultly obtain satisfactory\nperformance for the challenging GZSL by adversarial learning the distribution\nof semantic classes. To alleviate the negative influence of this inconsistence\nfor ZSL and GZSL, transfer feature generating networks with semantic classes\nstructure (TFGNSCS) is proposed to construct networks model for improving the\nperformance of ZSL and GZSL. TFGNSCS can not only consider the semantic\nstructure relationship between seen and unseen classes, but also learn the\ndifference of generating features by transferring classification model\ninformation from seen to unseen classes in networks. The proposed method can\nintegrate the transfer loss, the classification loss and the Wasserstein\ndistance loss to generate enough CNN features, on which softmax classifiers are\ntrained for ZSL and GZSL. Experiments demonstrate that the performance of\nTFGNSCS outperforms that of the state of the arts on four challenging datasets,\nwhich are CUB,FLO,SUN, AWA in GZSL.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A finite Coxeter group $W$ has a natural metric $d$ and if $\\mathcal{M}$ is a\nsubset of $W$, then for each $u\\in W$, there is $q\\in \\mathcal{M}$ such that\n$d(u,q)=d(u,\\mathcal{M})$. Such $q$ is not unique in general but if\n$\\mathcal{M}$ is a Coxeter matroid, then it is unique, and we define a\nretraction $\\mathcal{R}^m_{\\mathcal{M}}\\colon W\\to \\mathcal{M}\\subset W$ so\nthat $\\mathcal{R}^m_{\\mathcal{M}}(u)=q$.\n  The $T$-fixed point set $Y^T$ of a $T$-orbit closure $Y$ in a flag variety\n$G/B$ is a Coxeter matroid, where $G$ is a semisimple algebraic group, $B$ is a\nBorel subgroup, and $T$ is a maximal torus of $G$ contained in $B$. We define a\nretraction $\\mathcal{R}^g_{Y}\\colon W\\to Y^T\\subset W$ geometrically, where $W$\nis the Weyl group of $G$, and show that\n$\\mathcal{R}^g_{Y}=\\mathcal{R}^m_{Y^T}$. We introduce another retraction\n$\\mathcal{R}^a_{\\mathcal{M}}\\colon W\\to \\mathcal{M}\\subset W$ algebraically for\nan arbitrary subset $\\mathcal{M}$ of $W$ when $W$ is a Weyl group of classical\nLie type, and show that\n$\\mathcal{R}^a_{\\mathcal{M}}=\\mathcal{R}^m_{\\mathcal{M}}$ when $\\mathcal{M}$ is\na Coxeter matroid.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  The segmentation of the breast from the chest wall is an important first step\nin the analysis of breast magnetic resonance images. 3D U-nets have been shown\nto obtain high segmentation accuracy and appear to generalize well when trained\non one scanner type and tested on another scanner, provided that a very similar\nT1-weighted MR protocol is used. There has, however, been little work\naddressing the problem of domain adaptation when image intensities or patient\norientation differ markedly between the training set and an unseen test set. To\novercome the domain shift we propose to apply extensive intensity augmentation\nin addition to geometric augmentation during training. We explored both style\ntransfer and a novel intensity remapping approach as intensity augmentation\nstrategies. For our experiments, we trained a 3D U-net on T1-weighted scans and\ntested on T2-weighted scans. By applying intensity augmentation we increased\nsegmentation performance from a DSC of 0.71 to 0.90. This performance is very\nclose to the baseline performance of training and testing on T2-weighted scans\n(0.92). Furthermore, we applied our network to an independent test set made up\nof publicly available scans acquired using a T1-weighted TWIST sequence and a\ndifferent coil configuration. On this dataset we obtained a performance of\n0.89, close to the inter-observer variability of the ground truth segmentations\n(0.92). Our results show that using intensity augmentation in addition to\ngeometric augmentation is a suitable method to overcome the intensity domain\nshift and we expect it to be useful for a wide range of segmentation tasks.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  A grand challenge in designing polymeric materials is to tune their\nproperties by macromolecular engineering. In this context, one of the drawbacks\nthat often limits broader applications under high temperature conditions is\ntheir poor thermal conductivity $\\kappa$. Using molecular dynamics simulations,\nwe establish a structure-property relationship in hydrogen bonded polymer\nblends for possible improvement of $\\kappa$. For this purpose, we investigate\ntwo experimentally relevant hydrogen bonded systems -- one system consists of\nshort poly({N}-acryloyl piperidine) (PAP) blended with longer chains of\npoly(acrylic acid) (PAA) and the second system is a mixture of PAA and short\npoly(acrylamide) (PAM) chains. Simulation results show that PAA-PAP blends are\nat the onset of phase separation over the full range of PAP monomer mole\nfraction $\\phi_{PAP}$, which intensifies even more for $\\phi_{PAP} > 0.3$.\nWhile PAA and PAP interact with preferential hydrogen bonding, phase separation\nis triggered by the dominant van der Waals attraction between the hydrophobic\nside groups of PAP. However, if PAP is replaced with PAM, which has a similar\nchemical structure as PAP without the hydrophobic side group, PAA-PAM blends\nshow much improved solubility. Better solubility is due to the preferential\nhydrogen bonding between PAA and PAM. As a result, PAM oligomers act as\ncross-linking bridges between PAA chains resulting in a three dimensional\nhighly cross-linked network. While $\\kappa$ for PAA-PAP blends remain almost\ninvariant with $\\phi_{PAP}$, PAA-PAM systems show improved $\\kappa$ with\nincreasing PAM concentration and also with respect to PAA-PAP blends.\nConsistent with the theoretical prediction for the thermal transport of\namorphous polymers, we show that $\\kappa$ is proportional to the materials\nstiffness, i.e., the bulk modulus K and sound velocity v of PAA-PAM blends.\n\n\n###\n\n", "completion": " 19\n"}
{"prompt": "  Gas and dust properties in the Chamaeleon molecular cloud complex have been\ninvestigated with emission lines from atomic hydrogen (HI) and 12CO molecule,\ndust optical depth at 353 GHz ($\\tau_{353}$), and $J$-band infrared extinction\n($A_{J}$). We have found a scatter correlation between the HI integrated\nintensity ($W_{\\rm HI}$) and $\\tau_{353}$ in the Chamaeleon region. The\nscattering has been examined in terms of possible large optical depth in HI\nemission ($\\tau_{\\rm HI}$) using a total column density ($N_{\\rm H}$) model\nbased on $\\tau_{353}$. A nonlinear relation of $\\tau_{353}$ with the $\\sim$1.2\npower of $A_{J}$ has been found in opaque regions ($A_{J}$ $\\gtrsim$ 0.3 mag),\nwhich may indicate dust evolution effect. If we apply this nonlinear relation\nto the $N_{\\rm H}$ model (i.e., $N_{\\rm H} \\propto \\tau_{353}^{1/1.2}$)\nallowing arbitrary $\\tau_{\\rm HI}$, the model curve reproduces well the $W_{\\rm\nHI}$-$\\tau_{353}$ scatter correlation, suggesting optically thick HI\n($\\tau_{\\rm HI} \\sim$1.3) extended around the molecular clouds. Based on the\ncorrelations between the CO integrated intensity and the $N_{\\rm H}$ model, we\nhave then derived the CO-to-H$_{2}$ conversion factor ($X_{\\rm CO}$) on\n$\\sim$1.5$^{\\circ}$ scales (corresponding to $\\sim$4 persec) and found spatial\nvariations of $X_{\\rm CO}$ $\\sim$(0.5-3)$\\times$10$^{20}$ cm$^{-2}$ K$^{-1}$\nkm$^{-1}$ s across the cloud complex, possibly depending on the radiation field\ninside or surrounding the molecular clouds. These gas properties found in the\nChamaeleon region are discussed through a comparison with other local molecular\ncloud complexes.\n\n\n###\n\n", "completion": " 19\n"}
