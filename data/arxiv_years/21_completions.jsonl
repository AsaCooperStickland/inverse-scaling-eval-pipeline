{"prompt": "  We study the radiation from charged particles crossing a cholesteric plate in\nthe shortwave approximation when the wavelength of photons is much smaller than\nthe pitch of the cholesteric helix whereas the escaping angle of the photon and\nthe anisotropy of the permittivity tensor can be arbitrary. The radiation of\nphotons is treated in the framework of quantum electrodynamics with classical\ncurrents. The radiation of the plane-wave photons and the photons with definite\nprojection of the angular momentum (the twisted photons) produced by charged\nparticles crossing the cholesteric plate and moving rectilinearly and uniformly\nis considered. The explicit expressions for the average number of radiated\nphotons and their spectra with respect to the energy and the projection of the\nangular momentum are obtained in this case. It turns out that in the paraxial\napproximation the projection of the orbital angular momentum, $l$, of radiated\ntwisted photons is related to the harmonic number $n\\in \\mathbb{Z}$ as\n$l=2n+1$, i.e., the given system is a pure source of twisted photons as\nexpected. It is shown that in the paraxial shortwave regime the main part of\nradiated photons is linearly polarized with $l=\\pm1$ at the harmonics\n$n=\\{-1,0\\}$. The applicability conditions of the approach developed are\ndiscussed. As the examples, we consider the production of $6.3$ eV twisted\nphotons from uranium nuclei and the production of X-ray twisted photons from\n$120$ MeV electrons.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  We report a comprehensive study of the centrosymmetric Re$_3$B and\nnoncentrosymmetric Re$_7$B$_3$ superconductors. At a macroscopic level, their\nbulk superconductivity (SC), with $T_c$ = 5.1 K (Re$_3$B) and 3.3 K\n(Re$_7$B$_3$), was characterized via electrical-resistivity, magnetization, and\nheat-capacity measurements, while their microscopic superconducting properties\nwere investigated by means of muon-spin rotation/relaxation ($\\mu$SR). In both\nRe$_3$B and Re$_7$B$_3$ the low-$T$ zero-field electronic specific heat and the\nsuperfluid density (determined via tranverse-field $\\mu$SR) suggest a nodeless\nSC. Both compounds exhibit some features of multigap SC, as evidenced by\ntemperature-dependent upper critical fields $H_\\mathrm{c2}(T)$, as well as by\nelectronic band-structure calculations. The absence of spontaneous magnetic\nfields below the onset of SC, as determined from zero-field $\\mu$SR\nmeasurements, indicates a preserved time-reversal symmetry in the\nsuperconducting state of both Re$_3$B and Re$_7$B$_3$. Our results suggest that\na lack of inversion symmetry and the accompanying antisymmetric spin-orbit\ncoupling effects are not essential for the occurrence of multigap SC in these\nrhenium-boron compounds.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  Subsampling is a general statistical method developed in the 1990s aimed at\nestimating the sampling distribution of a statistic $\\hat \\theta _n$ in order\nto conduct nonparametric inference such as the construction of confidence\nintervals and hypothesis tests. Subsampling has seen a resurgence in the Big\nData era where the standard, full-resample size bootstrap can be infeasible to\ncompute. Nevertheless, even choosing a single random subsample of size $b$ can\nbe computationally challenging with both $b$ and the sample size $n$ being very\nlarge. In the paper at hand, we show how a set of appropriately chosen,\nnon-random subsamples can be used to conduct effective -- and computationally\nfeasible -- distribution estimation via subsampling. Further, we show how the\nsame set of subsamples can be used to yield a procedure for subsampling\naggregation -- also known as subagging -- that is scalable with big data.\nInterestingly, the scalable subagging estimator can be tuned to have the same\n(or better) rate of convergence as compared to $\\hat \\theta _n$. The paper is\nconcluded by showing how to conduct inference, e.g., confidence intervals,\nbased on the scalable subagging estimator instead of the original $\\hat \\theta\n_n$.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We investigate the switching dynamics in a $\\mathcal{PT}$-symmetric fiber\ncoupler composed of a saturable nonlinear material as the core. In such a\nsaturable nonlinear medium, bistable solitons may evolve due to the balance\nbetween dispersion and saturable nonlinearity, which we extend in the context\nof the $\\mathcal{PT}$-symmetric coupler. Our investigations of power-controlled\nand phase-sensitive switching show richer soliton switching dynamics than the\ncurrently existing conventional counterparts, which may lead to ultrafast and\nefficient all-optical switching dynamics at very low power owing to the\ncombined effects of $\\mathcal{PT}$ symmetry and saturable nonlinearity. In\naddition to the input power, the relative phase of the input solitons and\nsaturable coefficient are additional controlling parameters that efficiently\ntailor the switching dynamics. Also, we provide a suitable range of system and\npulse parameters that would be helpful for the practical realization of the\ncoupler to use in all-optical switching devices and photonic circuits. Finally,\nwe develop a variational approach to analytically investigate the switching\ndynamics in such $\\mathcal{PT}$-symmetric couplers that excellently predicts\nthe numerical findings.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  The Backprop algorithm for learning in neural networks utilizes two\nmechanisms: first, stochastic gradient descent and second, initialization with\nsmall random weights, where the latter is essential to the effectiveness of the\nformer. We show that in continual learning setups, Backprop performs well\ninitially, but over time its performance degrades. Stochastic gradient descent\nalone is insufficient to learn continually; the initial randomness enables only\ninitial learning but not continual learning. To the best of our knowledge, ours\nis the first result showing this degradation in Backprop's ability to learn. To\naddress this degradation in Backprop's plasticity, we propose an algorithm that\ncontinually injects random features alongside gradient descent using a new\ngenerate-and-test process. We call this the \\textit{Continual Backprop}\nalgorithm. We show that, unlike Backprop, Continual Backprop is able to\ncontinually adapt in both supervised and reinforcement learning (RL) problems.\nContinual Backprop has the same computational complexity as Backprop and can be\nseen as a natural extension of Backprop for continual learning.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  We study the problem of set discovery where given a few example tuples of a\ndesired set, we want to find the set in a collection of sets. A challenge is\nthat the example tuples may not uniquely identify a set, and a large number of\ncandidate sets may be returned. Our focus is on interactive exploration to set\ndiscovery where additional example tuples from the candidate sets are shown and\nthe user either accepts or rejects them as members of the target set. The goal\nis to find the target set with the least number of user interactions. The\nproblem can be cast as an optimization problem where we want to find a decision\ntree that can guide the search to the target set with the least number of\nquestions to be answered by the user. We propose a general algorithm that is\ncapable of reaching an optimal solution and two variations of it that strike a\nbalance between the quality of a solution and the running time. We also propose\na novel pruning strategy that safely reduces the search space without\nintroducing false negatives. We evaluate the efficiency and the effectiveness\nof our algorithms through an extensive experimental study using both real and\nsynthetic datasets and comparing them to previous approaches in the literature.\nWe show that our pruning strategy reduces the running time of the search\nalgorithms by 2-5 orders of magnitude.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Antibiotic resistance is a topical problem for both humans and animals and\nhas been the subject of special monitoring for two decades. Several recent\nstudies, including ours, have shown that this phenomenon is accentuated by the\ntransfer of certain genetic elements and cross resistance acquisition, the\nlatter or both resulting from the misuse of these antimicrobial drugs. A more\ncareful use of antimicrobials, the search for new antibacterial compounds\n(including probiotics and phages) are the most recommended alternatives to\novercome this situation. However, tests for modulations of antimicrobial\nactivity can also play a major role. The main goal of synergy studies is to\nassess whether substances with antibacterial properties can improve the\neffectiveness of existing antimicrobials or give them a second life against\nresistant germs. Moreover, recent studies have demonstrated the ability of\nsilver nanoparticles and extracts of certain plants to boost the effectiveness\nof certain antibiotics. such as ampicillin, benzylpenicillin, cefazolin,\nciprofloxacin, nitrofurantoin, and kanamycin. Yet, from these studies we found\nthat there was a serious problem with the interpretation of the results when\nusing the disk method with determination of the increase in fold area. Indeed,\nthe use of this method firstly requires the determination of the diameter of\ninhibition of the antibiotic alone; follow by the determination of the\ncombination of antibiotic + modulating substance (MS) or extract.....\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Setting an effective reserve price for strategic bidders in repeated auctions\nis a central question in online advertising. In this paper, we investigate how\nto set an anonymous reserve price in repeated auctions based on historical bids\nin a way that balances revenue and incentives to misreport. We propose two\nsimple and computationally efficient methods to set reserve prices based on the\nnotion of a clearing price and make them robust to bidder misreports. The first\napproach adds random noise to the reserve price, drawing on techniques from\ndifferential privacy. The second method applies a smoothing technique by adding\nnoise to the training bids used to compute the reserve price. We provide\ntheoretical guarantees on the trade-offs between the revenue performance and\nbid-shading incentives of these two mechanisms. Finally, we empirically\nevaluate our mechanisms on synthetic data to validate our theoretical findings.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  The non-trivial topology in the layered $\\text{FeTe}_{0.55}\\text{Se}_{0.45}$\n(FTS) superconductor has been suggested by both theory and experiment to be\nstrongly dependent on the Te concentration. Motivated by this together with the\nTe fluctuations expected from alloy disorder, we develop a simple layered model\nfor a strong topological insulator that allows us to describe a scenario where\ntopologically trivial domains permeate the sample. We refer to such a phase as\ntopological domain disordered and study the local density (LDOS) of the\ntopological surface states that can be measured using scanning tunneling\nspectroscopy (STS) in this phase. We find that topologically trivial domains on\nthe surface, where one would expect the topological surface state to be absent,\nappear as regions of suppressed LDOS surrounded by domain walls with enhanced\nLDOS. Furthermore, we show that studying the energy dependence of the STS\nshould allow us to distinguish the topologically trivial parts of the surface\nfrom other forms of disorder. Finally, we discuss implications of such local\ndisappearance of the topological surface states for the observation of Majorana\nmodes in vortices.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  The simulation of fermionic relativistic physics, e.g., Dirac and Weyl\nphysics, has led to the discovery of many unprecedented phenomena in photonics,\nof which the optical-frequency realization is, however, still challenging.\nHere, surprisingly, we discover that the woodpile photonic crystals commonly\nused for optical frequency applications host exotic fermion-like relativistic\ndegeneracies: a Dirac nodal line and a fourfold quadratic point, as protected\nby the nonsymmorphic crystalline symmetry. Deforming the woodpile photonic\ncrystal leads to the emergence of type-II Dirac points from the fourfold\nquadratic point. Such type-II Dirac points can be detected by its anomalous\nrefraction property which is manifested as a giant birefringence in a slab\nsetup. Our findings provide a promising route towards 3D optical Dirac physics\nin all-dielectric photonic crystals.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Multi-source-extractors are functions that extract uniform randomness from\nmultiple (weak) sources of randomness. Quantum multi-source-extractors were\nconsidered by Kasher and Kempe (for the quantum-independent-adversary and the\nquantum-bounded-storage-adversary), Chung, Li and Wu (for the\ngeneral-entangled-adversary) and Arnon-Friedman, Portmann and Scholz (for the\nquantum-Markov-adversary). One of the main objectives of this work is to unify\nall the existing quantum multi-source adversary models. We propose two new\nmodels of adversaries: 1) the quantum-measurement-adversary (qm-adv), which\ngenerates side-information using entanglement and on post-measurement and 2)\nthe quantum-communication-adversary (qc-adv), which generates side-information\nusing entanglement and communication between multiple sources. We show that, 1.\nqm-adv is the strongest adversary among all the known adversaries, in the sense\nthat the side-information of all other adversaries can be generated by qm-adv.\n2. The (generalized) inner-product function (in fact a general class of\ntwo-wise independent functions) continues to work as a good extractor against\nqm-adv with matching parameters as that of Chor and Goldreich. 3. A\nnon-malleable-extractor proposed by Li (against classical-adversaries)\ncontinues to be secure against quantum side-information. This result implies a\nnon-malleable-extractor result of Aggarwal, Chung, Lin and Vidick with uniform\nseed. We strengthen their result via a completely different proof to make the\nnon-malleable-extractor of Li secure against quantum side-information even when\nthe seed is not uniform. 4. A modification (working with weak sources instead\nof uniform sources) of the Dodis and Wichs protocol for privacy-amplification\nis secure against active quantum adversaries. This strengthens on a recent\nresult due to Aggarwal, Chung, Lin and Vidick which uses uniform sources.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  We introduce a novel thick-target concept tailored to the extraction of\nrefractory 4d and 5d transition metal radionuclides of molybdenum, technetium,\nruthenium and tungsten for radioactive ion beam production. Despite the more\nthan 60-year old history of thick-target ISOL mass-separation facilities like\nISOLDE, the extraction of the most refractory elements as radioactive ion beam\nhas so far not been successful. In ordinary thick ISOL targets, their\nradioisotopes produced in the target are stopped within the condensed target\nmaterial and have to diffuse through a solid material. Here, we present a\nconcept which overcomes limitations associated with this method. We exploit the\nrecoil momentum of nuclear reaction products for their release from the solid\ntarget material. They are thermalized in a carbon monoxide-containing\natmosphere, in which volatile carbonyl complexes form readily at ambient\ntemperature and pressure. This compound serves as volatile carrier for\ntransport to the ion source. Excess carbon monoxide is removed by cryogenic gas\nseparation to enable low pressures in the source region, in which the species\nare ionized and hence made available for radioactive ion beam formation. The\nsetup is operated in batch mode, with the aim to extract isotopes having\nhalf-lives of at least several seconds. We report parameter studies of the key\nprocesses of the method, which validate this concept and which define the\nparameters for the setup. This would allow for the first time the extraction of\nradioactive molybdenum, tungsten and several other transition metals at\nthick-target ISOL facilities.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Quantum simulation, the simulation of quantum processes on quantum computers,\nsuggests a path forward for the efficient simulation of problems in\ncondensed-matter physics, quantum chemistry, and materials science. While the\nmajority of quantum simulation algorithms are deterministic, a recent surge of\nideas has shown that randomization can greatly benefit algorithmic performance.\nIn this work, we introduce a scheme for quantum simulation that unites the\nadvantages of randomized compiling on the one hand and higher-order\nmulti-product formulas, as they are used for example in\nlinear-combination-of-unitaries (LCU) algorithms or quantum error mitigation,\non the other hand. In doing so, we propose a framework of randomized sampling\nthat is expected to be useful for programmable quantum simulators and present\ntwo new multi-product formula algorithms tailored to it. Our framework reduces\nthe circuit depth by circumventing the need for oblivious amplitude\namplification required by the implementation of multi-product formulas using\nstandard LCU methods, rendering it especially useful for early quantum\ncomputers used to estimate the dynamics of quantum systems instead of\nperforming full-fledged quantum phase estimation. Our algorithms achieve a\nsimulation error that shrinks exponentially with the circuit depth. To\ncorroborate their functioning, we prove rigorous performance bounds as well as\nthe concentration of the randomized sampling procedure. We demonstrate the\nfunctioning of the approach for several physically meaningful examples of\nHamiltonians, including fermionic systems and the Sachdev-Ye-Kitaev model, for\nwhich the method provides a favorable scaling in the effort.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  To obtain the highest confidence on the correction of numerical simulation\nprograms implementing the finite element method, one has to formalize the\nmathematical notions and results that allow to establish the soundness of the\nmethod. Sobolev spaces are the mathematical framework in which most weak\nformulations of partial derivative equations are stated, and where solutions\nare sought. These functional spaces are built on integration and measure\ntheory. Hence, this chapter in functional analysis is a mandatory theoretical\ncornerstone for the definition of the finite element method. The purpose of\nthis document is to provide the formal proof community with very detailed\npen-and-paper proofs of the main results from integration and measure theory.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Large-scale strongly nonlinear and nonconvex mixed-integer nonlinear\nprogramming (MINLP) models frequently appear in optimisation-based process\nsynthesis, integration, intensification, and process control. However, they are\nusually difficult to solve by existing algorithms within an acceptable time. In\nthis work, we propose two robust homotopy continuation enhanced branch and\nbound (HCBB) algorithms (denoted as HCBB-FP and HCBB-RB) where the homotopy\ncontinuation method is employed to gradually approach the optimum of the NLP\nsubproblem at a node from the solution at its parent node. A variable step\nlength is adapted to effectively balance feasibility and computational\nefficiency. The computational results from solving four existing process\nsynthesis problems demonstrate that the proposed HCBB algorithms can find the\nsame optimal solution from different initial points, while the existing MINLP\nalgorithms fail or find much worse solutions. In addition, HCBB-RB is superior\nto HCBB-FP due to the much lower computational effort required for the same\nlocally optimal solution.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  This is the first paper in a series aimed at modeling the black hole (BH)\nmass function, from the stellar to the intermediate to the (super)massive\nregime. In the present work we focus on stellar BHs and provide an ab-initio\ncomputation of their mass function across cosmic times. Specifically, we\nexploit the state-of-the-art stellar and binary evolutionary code\n\\texttt{SEVN}, and couple its outputs with redshift-dependent galaxy statistics\nand empirical scaling relations involving galaxy metallicity, star-formation\nrate and stellar mass. The resulting relic mass function ${\\rm d}N/{\\rm d}V{\\rm\nd}\\log m_\\bullet$ as a function of the BH mass $m_\\bullet$ features a rather\nflat shape up to $m_\\bullet\\approx 50\\, M_\\odot$ and then a log-normal decline\nfor larger masses, while its overall normalization at a given mass increases\nwith decreasing redshift. We highlight the contribution to the local mass\nfunction from isolated stars evolving into BHs and from binary stellar systems\nending up in single or binary BHs. We also include the distortion on the mass\nfunction induced by binary BH mergers, finding that it has a minor effect at\nthe high-mass end. We estimate a local stellar BH relic mass density of\n$\\rho_\\bullet\\approx 5\\times 10^7\\, M_\\odot$ Mpc$^{-3}$, which exceeds by more\nthan two orders of magnitude that in supermassive BHs; this translates into an\nenergy density parameter $\\Omega_\\bullet\\approx 4\\times 10^{-4}$, implying that\nthe total mass in stellar BHs amounts to $\\lesssim 1\\%$ of the local baryonic\nmatter. We show how our mass function for merging BH binaries compares with the\nrecent estimates from gravitational wave observations by LIGO/Virgo, and\ndiscuss the possible implications for dynamical formation of BH binaries in\ndense environments like star clusters. [abridged]\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  For every naturla numbers $m,n $ and every field $K$, let $M(m \\times n, K)$\nbe the vector space of the $(m \\times n)$-matrices over $K$ and let $S(n,K)$ be\nthe vector space of the symmetric $(n \\times n)$-matrices over $K$. We say that\nan affine subspace $S$ of $M(m \\times n, K)$ or of $S(n,K)$ has constant rank\n$r$ if every matrix of $S$ has rank $r$. Define $${\\cal A}^K(m \\times n; r)= \\{\nS \\;| \\; S \\; \\mbox{\\rm affine subsapce of $M(m \\times n, K)$ of constant rank\n} r\\}$$ $${\\cal A}_{sym}^K(n;r)= \\{ S \\;| \\; S \\; \\mbox{\\rm affine subsapce of\n$S(n,K)$ of constant rank } r\\}$$ $$a^K(m \\times n;r) = \\max \\{\\dim S \\mid S\n\\in {\\cal A}^K(m \\times n; r ) \\}.$$ $$a_{sym}^K(n;r) = \\max \\{\\dim S \\mid S\n\\in {\\cal A}_{sym}^K(n,r) \\}.$$\n  In this paper we prove the following two formulas for $r \\leq m \\leq n$:\n$a_{sym}^{\\R}(n,r) = r(n-r)+ \\left[\\frac{r^2}{4} \\right],$ $a^{\\R}(m \\times\nn,r) = r(n-r)+ \\frac{r(r-1)}{2} .$\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  We present results on the nature of extreme ejective feedback episodes and\nthe physical conditions of a population of massive ($\\rm M_* \\sim 10^{11}\nM_{\\odot}$), compact starburst galaxies at z = 0.4-0.7. We use data from\nKeck/NIRSPEC, SDSS, Gemini/GMOS, MMT, and Magellan/MagE to measure rest-frame\noptical and near-IR spectra of 14 starburst galaxies with extremely high star\nformation rate surface densities (mean $\\rm \\Sigma_{SFR} \\sim 3000 \\,M_{\\odot}\nyr^{-1} kpc^{-2}$) and powerful galactic outflows (maximum speeds v$_{98} \\sim$\n1000-3000 km s$^{-1}$). Our unique data set includes an ensemble of both\nemission [OII]$\\lambda\\lambda$3726,3729, H$\\beta$,\n[OIII]$\\lambda\\lambda$4959,5007, H$\\alpha$, [NII]$\\lambda\\lambda$6548,6583, and\n[SII]$\\lambda\\lambda$6716,6731) and absorption MgII$\\lambda\\lambda$2796,2803,\nand FeII$\\lambda$2586) lines that allow us to investigate the kinematics of the\ncool gas phase (T$\\sim$10$^4$ K) in the outflows. Employing a suite of line\nratio diagnostic diagrams, we find that the central starbursts are\ncharacterized by high electron densities (median n$_e \\sim$ 530 cm$^{-3}$), and\nhigh metallicity (solar or super-solar). We show that the outflows are most\nlikely driven by stellar feedback emerging from the extreme central starburst,\nrather than by an AGN. We also present multiple intriguing observational\nsignatures suggesting that these galaxies may have substantial Lyman continuum\n(LyC) photon leakage, including weak [SII] nebular emission lines. Our results\nimply that these galaxies may be captured in a short-lived phase of extreme\nstar formation and feedback where much of their gas is violently blown out by\npowerful outflows that open up channels for LyC photons to escape.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  A chain of connections in compressed baryonic matter, up-to-date glaringly\nmissing in nuclear effective field theory, between intrinsic or emergent\nsymmetries of QCD, mesons-gluons dualities, vector meson dominance and\nChern-Simons fields has recently been revealed, presaging a possible new\nparadigm in nuclear theory. It indicates a ubiquitous role, thus far\nunexplored, of hidden symmetries -- flavor-local and scale -- permeating from\ndilute baryonic systems to normal nuclear matter and then to compact-star\nmatter. Here I give a brief account of the possibly \"indispensable\" relevance\nof the $\\eta^\\prime$ singular ring, a.k.a. fractional quantum Hall (FQH)\ndroplet, to the properties of the lowest-lying vector mesons $\\omega$ and\n$\\rho$, relevant to dilepton production processes, argued to be Seiberg-dual to\nthe gluons near the chiral restoration.\n\n\n###\n\n", "completion": " 07"}
{"prompt": "  Quantum machine learning (QML) is a rapidly growing area of research at the\nintersection of classical machine learning and quantum information theory. One\narea of considerable interest is the use of QML to learn information contained\nwithin quantum states themselves. In this work, we propose a novel approach in\nwhich the extraction of information from quantum states is undertaken in a\nclassical representational-space, obtained through the training of a hybrid\nquantum autoencoder (HQA). Hence, given a set of pure states, this variational\nQML algorithm learns to identify, and classically represent, their essential\ndistinguishing characteristics, subsequently giving rise to a new paradigm for\nclustering and semi-supervised classification. The analysis and employment of\nthe HQA model are presented in the context of amplitude encoded states - which\nin principle can be extended to arbitrary states for the analysis of structure\nin non-trivial quantum data sets.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Never is the difference between thermal equilibrium and turbulence so\ndramatic, as when a quadratic invariant makes the equilibrium statistics\nexactly Gaussian with independently fluctuating modes. That happens in two very\ndifferent yet deeply connected classes of systems: incompressible hydrodynamics\nand resonantly interacting waves. This work presents the first case of a\ndetailed information-theoretic analysis of turbulence in such strongly\ninteracting systems. The analysis elucidates the fundamental roles of space and\ntime in setting the cascade direction and the changes of the statistics along\nit. We introduce a beautifully simple yet rich family of discrete models with\nneighboring triplet interactions and show that it has families of quadratic\nconservation laws defined by the Fibonacci numbers. Depending on the single\nmodel parameter, three types of turbulence were found: single direct cascade,\ndouble cascade, and the first ever case of a single inverse cascade. We\ndescribe quantitatively how deviation from thermal equilibrium all the way to\nturbulent cascades makes statistics increasingly non-Gaussian and find the\nself-similar form of the one-mode probability distribution. We reveal where the\ninformation (entropy deficit) is encoded and disentangle the communication\nchannels between modes, as quantified by the mutual information in pairs and\nthe interaction information inside triplets.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Many researchers are trying to replace the aggregation server in federated\nlearning with a blockchain system to achieve better privacy, robustness and\nscalability. In this case, clients will upload their updated models to the\nblockchain ledger, and use a smart contract on the blockchain system to perform\nmodel averaging. However, running machine learning applications on the\nblockchain is almost impossible because a blockchain system, which usually\ntakes over half minute to generate a block, is extremely slow and unable to\nsupport machine learning applications.\n  This paper proposes a completely new public blockchain architecture called\nDFL, which is specially optimized for distributed federated machine learning.\nThis architecture inherits most traditional blockchain merits and achieves\nextremely high performance with low resource consumption by waiving global\nconsensus. To characterize the performance and robustness of our architecture,\nwe implement the architecture as a prototype and test it on a physical\nfour-node network. To test more nodes and more complex situations, we build a\nsimulator to simulate the network. The LeNet results indicate our system can\nreach over 90% accuracy for non-I.I.D. datasets even while facing model\npoisoning attacks, with the blockchain consuming less than 5% of hardware\nresources.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Tropical geometry with the max-plus algebra has been applied to statistical\nlearning models over tree spaces because geometry with the tropical metric over\ntree spaces has some nice properties such as convexity in terms of the tropical\nmetric. One of the challenges in applications of tropical geometry to tree\nspaces is the difficulty interpreting outcomes of statistical models with the\ntropical metric. This paper focuses on combinatorics of tree topologies along a\ntropical line segment, an intrinsic geodesic with the tropical metric, between\ntwo phylogenetic trees over the tree space and we show some properties of a\ntropical line segment between two trees. Specifically we show that a\nprobability of a tropical line segment of two randomly chosen trees going\nthrough the origin (the star tree) is zero if the number of leave is greater\nthan four, and we also show that if two given trees differ only one nearest\nneighbor interchange (NNI) move, then the tree topology of a tree in the\ntropical line segment between them is the same tree topology of one of these\ngiven two trees with possible zero branch lengths.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  The high-precision measurement of spatial clustering of emission line\ngalaxies (ELGs) is a primary objective for upcoming cosmological spectroscopic\nsurveys. The source of strong emission of ELGs is nebular emission from\nsurrounding ionized gas irradiated by massive short-lived stars in star-forming\ngalaxies. As a result, ELGs are more likely to reside in newly-formed halos and\nthis leads to a nonlinear relation between ELG number density and matter\ndensity fields. In order to estimate the covariance matrix of cosmological\nobservables, it is essential to produce many independent realisations to\nsimulate ELG distributions for large survey volumes. To this end, we present a\nnovel and fast scheme to populate ELGs in dark-matter only $N$-body simulations\nbased on local density field. This method enables fast production of mock ELG\ncatalogues suitable for verifying analysis methods and quantifying\nobservational systematics in upcoming spectroscopic surveys and can populate\nELGs in moderately high-density regions even though the halo structure cannot\nbe resolved due to low resolution. The power spectrum of simulated ELGs is\nconsistent with results of hydrodynamical simulations up to fairly small scales\n($\\lesssim 1 h \\, \\mathrm{Mpc}^{-1}$), and the simulated ELGs are more likely\nto be found in filamentary structures, which is consistent with results of\nsemi-analytic and hydrodynamical simulations. Furthermore, we address the\nredshift-space power spectrum of simulated ELGs. The measured multipole moments\nof simulated ELGs clearly exhibit a weaker Finger-of-God effect than those of\nmatter due to infalling motions towards halo centre, rather than random virial\nmotions inside halos.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We prove that in the non-orientable setting, the minimal stretch factor of a\npseudo-Anosov homeomorphism of a surface of genus $g$ with a fixed number of\npunctures is asymptotically on the order of $\\frac{1}{g}$. Our result adapts\nthe work of Yazdi to non-orientable surfaces. We include the details of\nThurston's theory of fibered faces for non-orientable 3-manifolds.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Grant-free multiple-access (GFMA) is a valuable research topic, since it can\nsupport multiuser transmission with low latency. This paper constructs novel\nuniquely-decodable multi-amplitude sequence (UDAS) sets for GFMA systems, which\ncan provide high spectrum efficiency (SE) with low-complexity active user\ndetection (AUD) algorithm. First of all, we propose an UDAS-based\nmulti-dimensional bit interleaving coded modulation (MD-BICM) transmitter; then\nintroduce the definition of UDAS and construct two kinds of UDAS sets based on\ncyclic and quasi-cyclic matrix modes. Besides, we present a statistic of UDAS\nfeature based AUD algorithm (SoF-AUD), and a joint multiuser detection and\nimproved message passing algorithm for the proposed system. Finally, the active\nuser error rate (AUER) and Shannon limits of the proposed system are deduced in\ndetails. Simulation results show that our proposed system can simultaneously\nsupport four users without additional redundancy, and the AUER can reach an\nextremely low value $10^{-5}$ when $E_b/N_0$ is $0$ dB and the length of\ntransmit block is larger than a given value, i.e., 784, verifying the validity\nand flexibility of the proposed UDAS sets.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We continue our study of the correspondence between BPS structures and\ntopological recursion in the uncoupled case, this time from the viewpoint of\nquantum curves. For spectral curves of hypergeometric type, we show the\nBorel-resummed Voros symbols of the corresponding quantum curves solve\nBridgeland's \"BPS Riemann-Hilbert problem\". In particular, they satisfy the\nrequired jump property in agreement with the generalized definition of BPS\nindices $\\Omega$ in our previous work. Furthermore, we observe the Voros\ncoefficients define a closed one-form on the parameter space, and show that\n(log of) Bridgeland's $\\tau$-function encoding the solution is none other than\nthe corresponding potential, up to a constant. When the quantization parameter\nis set to a special value, this agrees with the Borel sum of the topological\nrecursion partition function $Z_{\\rm TR}$, up to a simple factor.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We study the set $\\mathcal{S}$ of odd positive integers $n$ with the property\n${2n}/{\\sigma(n)} - 1 = 1/x$, for positive integer $x$, i.e., the set that\nrelates to odd perfect and odd \"spoof perfect\" numbers. As a consequence, we\nfind that if $D=pq$ denotes a spoof odd perfect number other than Descartes'\nexample, with pseudo-prime factor $p$, then $q>10^{12}$. Furthermore, we find\nirregularities in the ending digits of integers $n\\in\\mathcal{S}$ and study\naspects of its density, leading us to conjecture that the amount of numbers in\n$\\mathcal{S}$ below $k$ is $\\sim10 \\log(k)$.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  We consider positively supported Borel measures for which all moments exist.\nOn the set of compactly supported measures in this class a partial order is\ndefined via eventual dominance of the moment sequences. Special classes are\nidentified on which the order is total, but it is shown that already for the\nset of distributions with compactly supported smooth densities the order is not\ntotal. In particular we construct a pair of measures with smooth density for\nwhich infinitely many moments agree and another one for which the moments\nalternate infinitely often. This disproves some recently published claims to\nthe contrary. Some consequences for games with distributional payoffs are\ndiscussed.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  An eigenfunction of the Laplacian on a metric (quantum) graph has an excess\nnumber of zeros due to the graph's non-trivial topology. This number, called\nthe nodal surplus, is an integer between 0 and the graph's first Betti number\n$\\beta$. We study the distribution of the nodal surplus values in the countably\ninfinite set of the graph's eigenfunctions. We conjecture that this\ndistribution converges to Gaussian for any sequence of graphs of growing\n$\\beta$. We prove this conjecture for several special graph sequences and test\nit numerically for a variety of well-known graph families. Accurate computation\nof the distribution is made possible by a formula expressing the nodal surplus\ndistribution as an integral over a high-dimensional torus.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  We study the two-dimensional multiphase Muskat problem describing the motion\nof three immiscible fluids with equal viscosities in a vertical homogeneous\nporous medium identified with $\\mathbb{R}^2$ under the effect of gravity. We\nfirst formulate the governing equations as a strongly coupled evolution problem\nfor the functions that parameterize the sharp interfaces between the fluids.\nAfterwards we prove that the problem is of parabolic type and establish its\nwell-posedness together with two parabolic smoothing properties. For solutions\nthat are not global we exclude, in a certain regime, that the interfaces come\ninto contact along a curve segment.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  The last decade has witnessed a rapid growth in understanding of the pivotal\nroles of mechanical stresses and physical forces in cell biology. As a result\nan integrated view of cell biology is evolving, where genetic and molecular\nfeatures are scrutinized hand in hand with physical and mechanical\ncharacteristics of cells. Physics of liquid crystals has emerged as a\nburgeoning new frontier in cell biology over the past few years, fueled by an\nincreasing identification of orientational order and topological defects in\ncell biology, spanning scales from subcellular filaments to individual cells\nand multicellular tissues. Here, we provide an account of most recent findings\nand developments together with future promises and challenges in this rapidly\nevolving interdisciplinary research direction.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Access control is a fundamental component of the design of distributed\nledgers, influencing many aspects of their design, such as fairness,\nefficiency, traditional notions of network security, and adversarial attacks\nsuch as Denial-of-Service (DoS) attacks. In this work, we consider the security\nof a recently proposed access control protocol for Directed Acyclic Graph-based\ndistributed ledgers. We present a number of attack scenarios and potential\nvulnerabilities of the protocol and introduce a number of additional features\nwhich enhance its resilience. Specifically, a blacklisting algorithm, which is\nbased on a reputation-weighted threshold, is introduced to handle both spamming\nand multi-rate malicious attackers. The introduction of a solidification\nrequest component is also introduced to ensure the fairness and consistency of\nnetwork in the presence of attacks. Finally, a timestamp component is also\nintroduced to maintain the consistency of the network in the presence of\nmulti-rate attackers. Simulations to illustrate the efficacy and robustness of\nthe revised protocol are also described.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  The pressure-induced insulator to metal transition (IMT) of layered magnetic\nnickel phosphorous tri-sulfide NiPS3 was studied in-situ under quasi-uniaxial\nconditions by means of electrical resistance (R) and X-ray diffraction (XRD)\nmeasurements. This sluggish transition is shown to occur at 35 GPa. Transport\nmeasurements show no evidence of superconductivity to the lowest measured\ntemperature (~ 2 K). The structure results presented here differ from earlier\nin-situ work that subjected the sample to a different pressure state,\nsuggesting that in NiPS3 the phase stability fields are highly dependent on\nstrain. It is suggested that careful control of the strain is essential when\nstudying the electronic and magnetic properties of layered van der Waals\nsolids.\n\n\n###\n\n", "completion": " 07"}
{"prompt": "  The classical Khintchine-Groshev theorem is a generalization of Khintchine's\ntheorem on simultaneous Diophantine approximation, from approximation of points\nin $\\mathbb R^m$ to approximation of systems of linear forms in $\\mathbb\nR^{nm}$. In this paper, we present an inhomogeneous version of the\nKhintchine-Groshev theorem which does not carry a monotonicity assumption when\n$nm>2$. Our results bring the inhomogeneous theory almost in line with the\nhomogeneous theory, where it is known by a result of Beresnevich and Velani\n(2010) that monotonicity is not required when $nm>1$. That result resolved a\nconjecture of Beresnevich, Bernik, Dodson, and Velani (2009), and our work\nresolves almost every case of the natural inhomogeneous generalization of that\nconjecture. Regarding the two cases where $nm=2$, we are able to remove\nmonotonicity by assuming extra divergence of a measure sum, akin to a linear\nforms version of the Duffin-Schaeffer conjecture. When $nm=1$ it is known by\nwork of Duffin and Schaeffer (1941) that the monotonicity assumption cannot be\ndropped.\n  The key new result is an independence inheritance phenomenon; the underlying\nidea is that the sets involved in the $((n+k)\\times m)$-dimensional\nKhintchine-Groshev theorem ($k\\geq 0$) are always $k$-levels more\nprobabilistically independent than the sets involved the $(n\\times\nm)$-dimensional theorem. Hence, it is shown that Khintchine's theorem itself\nunderpins the Khintchine-Groshev theory.\n\n\n###\n\n", "completion": " 11"}
{"prompt": "  Common sense has always been of interest in Artificial Intelligence, but has\nrarely taken center stage. Despite its mention in one of John McCarthy's\nearliest papers and years of work by dedicated researchers, arguably no AI\nsystem with a serious amount of general common sense has ever emerged. Why is\nthat? What's missing? Examples of AI systems' failures of common sense abound,\nand they point to AI's frequent focus on expertise as the cause. Those\nattempting to break the resulting brittleness barrier, even in the context of\nmodern deep learning, have tended to invest their energy in large numbers of\nsmall bits of commonsense knowledge. While important, all the commonsense\nknowledge fragments in the world don't add up to a system that actually\ndemonstrates common sense in a human-like way. We advocate examining common\nsense from a broader perspective than in the past. Common sense should be\nconsidered in the context of a full cognitive system with history, goals,\ndesires, and drives, not just in isolated circumscribed examples. A fresh look\nis needed: common sense is worthy of its own dedicated scientific exploration.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  This paper presents a new fast power series solution method to solve the\nHierarchal Method of Moment(MoM) matrix for a large complex,perfectly electric\nconducting (PEC) 3D structures. The proposed power series solution converges in\njust two iterations which is faster than the conventional fast solver-based\niterative solution. The method is purely algebraic in nature and, as such\napplicable to existing conventional methods. The method uses regular fast\nsolver Hierarchal Matrix (H-Matrix) and can also be applied to Multilevel Fast\nMultipole Method Algorithm(MLFMA). In the proposed method, we use the scaling\nof the symmetric near-field matrix to develop a diagonally dominant overall\nmatrix to enable a power series solution. Left and right block scaling\ncoefficients are required for scaling near-field blocks to diagonal blocks\nusing Schur's complement method. However,only the right-hand scaling\ncoefficients are computed for symmetric near-field matrix leading to saving of\ncomputation time and memory. Due to symmetric property, the left side-block\nscaling coefficients are just the transpose of the right-scaling blocks. Next,\nthe near-field blocks are replaced by scaled near-field diagonal blocks. Now\nthe scaled near-field blocks in combination with far-field and scaling\ncoefficients are subjected to power series solution terminating after only two\nterms. As all the operations are performed on the near-field blocks, the\ncomplexity of scaling coefficient computation is retained as O(N). The power\nseries solution only involves the matrix-vector product of the far-field,\nscaling coefficients blocks, and inverse of scaled near-field blocks. Hence,\nthe solution cost remains O(NlogN). Several numerical results are presented to\nvalidate the efficiency and robustness of the proposed numerical method.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Against common sense, auxetic materials expand or contract perpendicularly\nwhen stretched or compressed, respectively, by uniaxial strain, being\ncharacterized by a negative Poisson's ratio $\\nu$. The amount of deformation in\nresponse to the applied force can be at most equal to the imposed one, so that\n$\\nu=-1$ is the lowest bound for the mechanical stability of solids, a\ncondition here defined as \"hyper-auxeticity\". In this work, we numerically show\nthat ultra-low-crosslinked polymer networks under tension display hyper-auxetic\nbehavior at a finite crosslinker concentration. At this point, the nearby\nmechanical instability triggers the onset of a critical-like transition between\ntwo states of different densities. This phenomenon displays similar features as\nwell as important differences with respect to gas-liquid phase separation.\nSince our model is able to faithfully describe real-world hydrogels, the\npresent results can be readily tested in laboratory experiments, paving the way\nto explore this unconventional phase behavior.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  In deep reinforcement learning (RL), data augmentation is widely considered\nas a tool to induce a set of useful priors about semantic consistency and\nimprove sample efficiency and generalization performance. However, even when\nthe prior is useful for generalization, distilling it to RL agent often\ninterferes with RL training and degenerates sample efficiency. Meanwhile, the\nagent is forgetful of the prior due to the non-stationary nature of RL. These\nobservations suggest two extreme schedules of distillation: (i) over the entire\ntraining; or (ii) only at the end. Hence, we devise a stand-alone network\ndistillation method to inject the consistency prior at any time (even after\nRL), and a simple yet efficient framework to automatically schedule the\ndistillation. Specifically, the proposed framework first focuses on mastering\ntrain environments regardless of generalization by adaptively deciding which\n{\\it or no} augmentation to be used for the training. After this, we add the\ndistillation to extract the remaining benefits for generalization from all the\naugmentations, which requires no additional new samples. In our experiments, we\ndemonstrate the utility of the proposed framework, in particular, that\nconsiders postponing the augmentation to the end of RL training.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We present a thorough numerical study on the MRI using the smoothed particle\nmagnetohydrodynamics method (SPMHD) with the geometric density average force\nexpression (GDSPH). We perform shearing box simulations with different initial\nsetups and a wide range of resolution and dissipation parameters. We show, for\nthe first time, that MRI with sustained turbulence can be simulated\nsuccessfully with SPH, with results consistent with prior work with grid-based\ncodes. In particular, for the stratified boxes, our simulations reproduce the\ncharacteristic butterfly diagram of the MRI dynamo with saturated turbulence\nfor at least 100 orbits. On the contrary, traditional SPH simulations suffer\nfrom runaway growth and develop unphysically large azimuthal fields, similar to\nthe results from a recent study with mesh-less methods. We investigated the\ndependency of MRI turbulence on the numerical Prandtl number in SPH, focusing\non the unstratified, zero net-flux case. We found that turbulence can only be\nsustained with a Prandtl number larger than $\\sim$2.5, similar to the critical\nvalues of physical Prandtl number found in grid-code simulations. However,\nunlike grid-based codes, the numerical Prandtl number in SPH increases with\nresolution, and for a fixed Prandtl number, the resulting magnetic energy and\nstresses are independent of resolution. Mean-field analyses were performed on\nall simulations, and the resulting transport coefficients indicate no\n$\\alpha$-effect in the unstratified cases, but an active $\\alpha\\Omega$ dynamo\nand a diamagnetic pumping effect in the stratified medium, which are generally\nin agreement with previous studies. There is no clear indication of a\nshear-current dynamo in our simulation, which is likely to be responsible for a\nweaker mean-field growth in the tall, unstratified, zero net-flux simulation.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  (See the complete abstract within the thesis in both English and German\nversions)\n  In this thesis, the process conditions of the epitaxial graphene growth\nthrough a socalled polymer-assisted sublimation growth method are minutely\ninvestigated. Atomic force microscopy (AFM) is used to show that the previously\nneglected flow-rate of the argon process gas has a significant influence on the\nmorphology of the SiC substrate and atop carbon layers. The results can be well\nexplained using a simple model for the thermodynamic conditions at the layer\nadjacent to the surface. The resulting control option of step-bunching on the\nsub-nanometer scales is used to produce the ultra-flat, monolayer graphene\nlayers without the bilayer inclusions that exhibit the vanishing of the\nresistance anisotropy. The comparison of four-point and scanning tunneling\npotentiometry measurements shows that the remaining small anisotropy represents\nthe ultimate limit, which is given solely by the remaining resistances at the\nSiC terrace steps. ... The precise control of step-bunching using the Ar flow\nalso enables the preparation of periodic non-identical SiC surfaces under the\ngraphene layer. Based on the work function measurements by Kelvin-Probe force\nmicroscopy and X-ray photoemission electron microscopy, it is shown for the\nfirst time that there is a doping variation in graphene, induced by a proximity\neffect of the different near-surface SiC stacks. The comparison of the AFM and\nlow-energy electron microscopy measurements have enabled the exact assignment\nof the SiC stacks, and the examinations have led to an improved understanding\nof the surface restructuring in the framework of a step-flow mode. ...\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  In this work, we present a rigorous end-to-end control strategy for\nautonomous vehicles aimed at minimizing lap times in a time attack racing\nevent. We also introduce AutoRACE Simulator developed as a part of this\nresearch project, which was employed to simulate accurate vehicular and\nenvironmental dynamics along with realistic audio-visual effects. We adopted a\nhybrid imitation-reinforcement learning architecture and crafted a novel reward\nfunction to train a deep neural network policy to drive (using imitation\nlearning) and race (using reinforcement learning) a car autonomously in less\nthan 20 hours. Deployment results were reported as a direct comparison of 10\nautonomous laps against 100 manual laps by 10 different human players. The\nautonomous agent not only exhibited superior performance by gaining 0.96\nseconds over the best manual lap, but it also dominated the human players by\n1.46 seconds with regard to the mean lap time. This dominance could be\njustified in terms of better trajectory optimization and lower reaction time of\nthe autonomous agent.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  In contrast to offline working fashions, two research paradigms are devised\nfor online learning: (1) Online Meta Learning (OML) learns good priors over\nmodel parameters (or learning to learn) in a sequential setting where tasks are\nrevealed one after another. Although it provides a sub-linear regret bound,\nsuch techniques completely ignore the importance of learning with fairness\nwhich is a significant hallmark of human intelligence. (2) Online\nFairness-Aware Learning. This setting captures many classification problems for\nwhich fairness is a concern. But it aims to attain zero-shot generalization\nwithout any task-specific adaptation. This therefore limits the capability of a\nmodel to adapt onto newly arrived data. To overcome such issues and bridge the\ngap, in this paper for the first time we proposed a novel online meta-learning\nalgorithm, namely FFML, which is under the setting of unfairness prevention.\nThe key part of FFML is to learn good priors of an online fair classification\nmodel's primal and dual parameters that are associated with the model's\naccuracy and fairness, respectively. The problem is formulated in the form of a\nbi-level convex-concave optimization. Theoretic analysis provides sub-linear\nupper bounds for loss regret and for violation of cumulative fairness\nconstraints. Our experiments demonstrate the versatility of FFML by applying it\nto classification on three real-world datasets and show substantial\nimprovements over the best prior work on the tradeoff between fairness and\nclassification accuracy\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  In our paper, we give a necessary and sufficient conditions for the kernels\nof the linear maps of finite Abelian group algebras to be Mathieu-Zhao spaces\nof $K[G]$ if $G$ is a finite Abelian group and $K$ is a split field for $G$.\nHence we classify all Mathieu-Zhao spaces of the finite Abelian group algebras\nif $K$ is a split field for $G$.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  In this work we demonstrate spectral-temporal correlation measurements of the\nHong-Ou-Mandel (HOM) interference effect with the use of a spectrometer based\non a photon-counting camera. This setup allows us to take, within seconds,\nspectral temporal correlation measurements on entangled photon sources with\nsub-nanometer spectral resolution and nanosecond timing resolution. Through\npost processing, we can observe the HOM behaviour for any number of spectral\nfilters of any shape and width at any wavelength over the observable spectral\nrange. Our setup also offers great versatility in that it is capable of\noperating at a wide spectral range from the visible to the near infrared and\ndoes not require a pulsed pump laser for timing purposes. This work offers the\nability to gain large amounts of spectral and temporal information from a HOM\ninterferometer quickly and efficiently and will be a very useful tool for many\nquantum technology applications and fundamental quantum optics research.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Multi-access edge computing (MEC) and network virtualization technologies are\nimportant enablers for fifth-generation (5G) networks to deliver diverse\napplications and services. Services are often provided as fully connected\nvirtual network functions (VNF)s, through service function chaining (SFC).\nHowever, the problem of allocating SFC resources at the network edge still\nfaces many challenges related to the way VNFs are placed, chained and\nscheduled. In this paper, to solve these problems, we propose a game\ntheory-based approach with the objective to reduce service latency in the\ncontext of SFC at the network edge. The problem of allocating SFC resources can\nbe divided into two subproblems. 1) The VNF placement and routing subproblem,\nand 2) the VNF scheduling subproblem. For the former subproblem, we formulate\nit as a mean field game (MFG) in which VNFs are modeled as entities contending\nover edge resources with the goal of reducing the resource consumption of MEC\nnodes and reducing latency for users. We propose a on a reinforcement\nlearning-based technique, where the Ishikawa-Mann learning algorithm (IMLA) is\nused. For the later subproblem we formulate it as a matching game between the\nVFNs and an edge resources in order to find the execution order of the VNFs\nwhile reducing the latency. To efficiently solve it, we propose a modified\nversion of the many-to-one deferred acceptance algorithm (DAA), called the\nenhanced multi-step deferred acceptance algorithm (eMSDA). To illustrate the\nperformance of the proposed approaches, we perform extensive simulations. The\nobtained results show that the proposed approaches outperform the benchmarks\nother state-of-the-art methods.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  In this paper, the sharp maximal theorem is generalized to mixed-norm ball\nBanach function spaces, which is defined as Definition 2.7. As an application,\nwe give a characterization of BMO via the boundedness of commutators of\nfractional integral operators on mixed-norm Lebesgue spaces. Moreover, the\ncharacterization of homogeneous Lipschitz space is also given by the\nboundedness of commutators of fractional integral operators on mixed-norm\nLebesgue spaces. Finally, two applications of Corollary 6.4 are given.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  The set of synchronizing words of a given $n$-state automaton forms a regular\nlanguage recognizable by an automaton with $2^n - n$ states. The size of a\nrecognizing automaton for the set of synchronizing words is linked to\ncomputational problems related to synchronization and to the length of\nsynchronizing words. Hence, it is natural to investigate synchronizing automata\nextremal with this property, i.e., such that the minimal deterministic\nautomaton for the set of synchronizing words has $2^n - n$ states. The\nsync-maximal permutation groups have been introduced in [{\\sc S. Hoffmann},\nCompletely Reachable Automata, Primitive Groups and the State Complexity of the\nSet of Synchronizing Words, LATA 2021] by stipulating that an associated\nautomaton to the group and a non-permutation has this extremal property. The\ndefinition is in analogy with the synchronizing groups and analog to a\ncharacterization of primitivity obtained in the mentioned work. The precise\nrelation to other classes of groups was mentioned as an open problem. Here, we\nsolve this open problem by showing that the sync-maximal groups are precisely\nthe primitive groups. Our result gives a new characterization of the primitive\ngroups. Lastly, we explore an alternative and stronger definition than\nsync-maximality.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We report on the evolution of the largest physics course at the Technical\nUniversity Berlin during the COVID-19 pandemic, which boosted the development\nof online learning content, electronic assessments, and, not at least, the\nreconceptualization of our teaching methods. By shifting to distance education,\nwe created features that can be easily implemented and combined with any\nphysics-type course at a university level. Thereby, we incidentally made\nintroductory physics sustainably accessible to students having difficulties\nvisiting the university. Besides our experiences gained on blended learning\nmethods, we provide a guide to online exercises, microlearning videos, and\ne-assessments, including exams realized in a virtual setting via the\nuniversity's Moodle. Since the course is attended annually by more than 800\nstudents enrolled in at least twelve bachelor degree engineering and all STEM\n(science, technology, engineering, and mathematics) fields, our reformations\ngreatly impact the student's learning style. The teaching concepts presented\nhere are supported by results from the Force Concept Inventory, student\nfeedback, and exam results. The Force Concept Inventory shows that our online\nteaching did not negatively affect the student's learning process compared with\na traditional face-to-face lecture. Student's feedback shows that the new\nformat and material being available online are received very well. Finally, the\nexam results show that virtual exams conducted in a remote setting can be\ndesigned to minimize cheating possibilities. In addition, the test was able to\nyield a similar distribution of points as in the previous traditional ones.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Multi-Label Image Classification (MLIC) aims to predict a set of labels that\npresent in an image. The key to deal with such problem is to mine the\nassociations between image contents and labels, and further obtain the correct\nassignments between images and their labels. In this paper, we treat each image\nas a bag of instances, and reformulate the task of MLIC as an instance-label\nmatching selection problem. To model such problem, we propose a novel deep\nlearning framework named Graph Matching based Multi-Label Image Classification\n(GM-MLIC), where Graph Matching (GM) scheme is introduced owing to its\nexcellent capability of excavating the instance and label relationship.\nSpecifically, we first construct an instance spatial graph and a label semantic\ngraph respectively, and then incorporate them into a constructed assignment\ngraph by connecting each instance to all labels. Subsequently, the graph\nnetwork block is adopted to aggregate and update all nodes and edges state on\nthe assignment graph to form structured representations for each instance and\nlabel. Our network finally derives a prediction score for each instance-label\ncorrespondence and optimizes such correspondence with a weighted cross-entropy\nloss. Extensive experiments conducted on various image datasets demonstrate the\nsuperiority of our proposed method.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  A model capturing the dynamics between virus and tumour cells in the context\nof oncolytic virotherapy is presented and analysed. The ability of the virus to\nbe internalised by uninfected cells is described by an infectivity parameter,\nwhich is inferred from available experimental data. The parameter is also able\nto describe the effects of changes in the tumour environment that affect viral\nuptake from tumour cells. Results show that when a virus is inoculated inside a\ngrowing tumour, strategies for enhancing infectivity do not lead to a complete\neradication of the tumour. Within typical times of experiments and treatments,\nwe observe the onset of oscillations, which always prevent a full destruction\nof the tumour mass. These findings are in good agreement with available\nlaboratory results. Further analysis shows why a fully successful therapy\ncannot exist for the proposed model and that care must be taken when designing\nand engineering viral vectors with enhanced features. In particular,\nbifurcation analysis reveals that creating longer lasting virus particles or\nusing strategies for reducing infected cell lifespan can cause unexpected and\nunwanted surges in the overall tumour load over time. Our findings suggest that\nvirotherapy alone seems unlikely to be effective in clinical settings unless\nadjuvant strategies are included.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We propose a lightweight CPU network based on the MKLDNN acceleration\nstrategy, named PP-LCNet, which improves the performance of lightweight models\non multiple tasks. This paper lists technologies which can improve network\naccuracy while the latency is almost constant. With these improvements, the\naccuracy of PP-LCNet can greatly surpass the previous network structure with\nthe same inference time for classification. As shown in Figure 1, it\noutperforms the most state-of-the-art models. And for downstream tasks of\ncomputer vision, it also performs very well, such as object detection, semantic\nsegmentation, etc. All our experiments are implemented based on PaddlePaddle.\nCode and pretrained models are available at PaddleClas.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Automatic meter reading technology is not yet widespread. Gas, electricity,\nor water accumulation meters reading is mostly done manually on-site either by\nan operator or by the homeowner. In some countries, the operator takes a\npicture as reading proof to confirm the reading by checking offline with\nanother operator and/or using it as evidence in case of conflicts or\ncomplaints. The whole process is time-consuming, expensive, and prone to\nerrors. Automation can optimize and facilitate such labor-intensive and human\nerror-prone processes. With the recent advances in the fields of artificial\nintelligence and computer vision, automatic meter reading systems are becoming\nmore viable than ever. Motivated by the recent advances in the field of\nartificial intelligence and inspired by open-source open-access initiatives in\nthe research community, we introduce a novel large benchmark dataset of\nreal-life gas meter images, named the NRC-GAMMA dataset. The data were\ncollected from an Itron 400A diaphragm gas meter on January 20, 2020, between\n00:05 am and 11:59 pm. We employed a systematic approach to label the images,\nvalidate the labellings, and assure the quality of the annotations. The dataset\ncontains 28,883 images of the entire gas meter along with 57,766 cropped images\nof the left and the right dial displays. We hope the NRC-GAMMA dataset helps\nthe research community to design and implement accurate, innovative,\nintelligent, and reproducible automatic gas meter reading solutions.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Background: The evolution of symptoms over time is at the heart of\nunderstanding and treating mental disorders. However, a principled,\nquantitative framework explaining symptom dynamics remains elusive. Here, we\npropose a Network Control Theory of Psychopathology allowing us to formally\nderive a theoretical control energy which we hypothesize quantifies resistance\nto future symptom improvement in Major Depressive Disorder (MDD). We test this\nhypothesis and investigate the relation to genetic and environmental risk as\nwell as resilience.\n  Methods: We modelled longitudinal symptom-network dynamics derived from\nN=2,059 Beck Depression Inventory measurements acquired over a median of 134\ndays in a sample of N=109 patients suffering from MDD. We quantified the\ntheoretical energy required for each patient and time-point to reach a\nsymptom-free state given individual symptom-network topology (E 0 ) and 1)\ntested if E 0 predicts future symptom improvement and 2) whether this\nrelationship is moderated by Polygenic Risk Scores (PRS) of mental disorders,\nchildhood maltreatment experience, and self-reported resilience.\n  Outcomes: We show that E 0 indeed predicts symptom reduction at the next\nmeasurement and reveal that this coupling between E 0 and future symptom change\nincreases with higher genetic risk and childhood maltreatment while it\ndecreases with resilience.\n  Interpretation: Our study provides a mechanistic framework capable of\npredicting future symptom improvement based on individual symptom-network\ntopology and clarifies the role of genetic and environmental risk as well as\nresilience. Our control-theoretic framework makes testable, quantitative\npredictions for individual therapeutic response and provides a starting-point\nfor the theory-driven design of personalized interventions.\n  Funding: German Research Foundation and Interdisciplinary Centre for Clinical\nResearch, M\\\"unster\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Relatively dominated representations give a common generalization of\ngeometrically finiteness in rank one on the one hand, and the Anosov condition\nwhich serves as a higher-rank analogue of convex cocompactness on the other.\nThis note proves three results about these representations.\n  Firstly, we remove the technical assumption of quadratic gaps involved in the\noriginal definition. Secondly, we give a characterization using eigenvalue\ngaps, providing a relative analogue of a result of Kassel-Potrie for Anosov\nrepresentations. Thirdly, we formulate characterizations in terms of singular\nvalue or eigenvalue gaps combined with limit maps, in the spirit of\nGu\\'eritaud-Guichard-Kassel-Wienhard for Anosov representations, and use them\nto show that inclusion representations of certain groups playing weak ping-pong\nand positive representations in the sense of Fock-Goncharov are relatively\ndominated.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  We highlight the fact that in undergraduate calculus, the number pi is\ndefined via the length of the circle, the length of the circle is defined as a\ncertain value of an inverse trigonometric function, and this value is defined\nvia pi, thus forming a circular definition. We present a way in which this\nerror can be rectified. We explain that this error is instructive and can be\nused as an enlightening topic for discussing different approaches to\nmathematics with undergraduate students.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  Recommendation is the task of ranking items (e.g. movies or products)\naccording to individual user needs. Current systems rely on collaborative\nfiltering and content-based techniques, which both require structured training\ndata. We propose a framework for recommendation with off-the-shelf pretrained\nlanguage models (LM) that only used unstructured text corpora as training data.\nIf a user $u$ liked \\textit{Matrix} and \\textit{Inception}, we construct a\ntextual prompt, e.g. \\textit{\"Movies like Matrix, Inception, ${<}m{>}$\"} to\nestimate the affinity between $u$ and $m$ with LM likelihood. We motivate our\nidea with a corpus analysis, evaluate several prompt structures, and we compare\nLM-based recommendation with standard matrix factorization trained on different\ndata regimes. The code for our experiments is publicly available\n(https://colab.research.google.com/drive/1f1mlZ-FGaLGdo5rPzxf3vemKllbh2esT?usp=sharing).\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Rate-induced tipping (R-tipping) occurs when time-variation of input\nparameters of a dynamical system interacts with system timescales to give\ngenuine nonautonomous instabilities. Such instabilities appear as the input\nvaries at some critical rates and cannot, in general, be understood in terms of\nautonomous bifurcations in the frozen system with a fixed-in-time input.\n  This paper develops an accessible mathematical framework for R-tipping in\nmultidimensional nonautonomous dynamical systems with an autonomous future\nlimit. We focus on R-tipping via loss of tracking of base attractors that are\nequilibria in the frozen system, due to crossing what we call regular\nthresholds. These thresholds are associated with regular edge states: compact\nhyperbolic invariant sets with one unstable direction and orientable stable\nmanifold, that lie on a basin boundary in the frozen system. We define\nR-tipping and critical rates for the nonautonomous system in terms of special\nsolutions that limit to a compact invariant set of the future limit system that\nis not an attractor. We focus on the case when the limit set is a regular edge\nstate, which we call the regular R-tipping edge state that anchors the\nassociated regular R-tipping threshold at infinity. We introduce the concept of\nedge tails to rigorously classify R-tipping into reversible, irreversible, and\ndegenerate cases.\n  The main idea is to compactify the problem and use regular edge states of the\nfuture limit system to analyse R-tipping in the nonautonomous system. This\nallows us to give sufficient conditions for the occurrence of R-tipping in\nterms of easily testable properties of the frozen system and input variation,\nand necessary and sufficient conditions for the occurrence of reversible and\nirreversible R-tipping in terms of computationally verifiable (heteroclinic)\nconnections to regular R-tipping edge states in the compactified system.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Advanced persistent threat (APT) is widely acknowledged to be the most\nsophisticated and potent class of security threat. APT refers to knowledgeable\nhuman attackers that are organized, highly sophisticated and motivated to\nachieve their objectives against a targeted organization(s) over a prolonged\nperiod. Strategically-motivated APTs or S-APTs are distinct in that they draw\ntheir objectives from the broader strategic agenda of third parties such as\ncriminal syndicates, nation-states, and rival corporations. In this paper we\nreview the use of the term - Advanced Persistent Threat - and present a formal\ndefinition. We then draw on military science, the science of organized\nconflict, for a theoretical basis to develop a rigorous and holistic model of\nthe stages of an APT operation which we subsequently use to explain how S-APTs\nexecute their strategically motivated operations using tactics, techniques and\nprocedures. Finally, we present a general disinformation model, derived from\nsituation awareness theory, and explain how disinformation can be used to\nattack the situation awareness and decision making of not only S-APT operators,\nbut also the entities that back them.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Quantum computing can become scalable through error correction, but logical\nerror rates only decrease with system size when physical errors are\nsufficiently uncorrelated. During computation, unused high energy levels of the\nqubits can become excited, creating leakage states that are long-lived and\nmobile. Particularly for superconducting transmon qubits, this leakage opens a\npath to errors that are correlated in space and time. Here, we report a reset\nprotocol that returns a qubit to the ground state from all relevant higher\nlevel states. We test its performance with the bit-flip stabilizer code, a\nsimplified version of the surface code for quantum error correction. We\ninvestigate the accumulation and dynamics of leakage during error correction.\nUsing this protocol, we find lower rates of logical errors and an improved\nscaling and stability of error suppression with increasing qubit number. This\ndemonstration provides a key step on the path towards scalable quantum\ncomputing.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Generative adversarial networks (GAN) are a widely used class of deep\ngenerative models, but their minimax training dynamics are not understood very\nwell. In this work, we show that GANs with a 2-layer infinite-width generator\nand a 2-layer finite-width discriminator trained with stochastic gradient\nascent-descent have no spurious stationary points. We then show that when the\nwidth of the generator is finite but wide, there are no spurious stationary\npoints within a ball whose radius becomes arbitrarily large (to cover the\nentire parameter space) as the width goes to infinity.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Let $F_{a_1,\\dots,a_k}$ be a graph consisting of $k$ cycles of odd length\n$2a_1+1,\\dots, 2a_k+1$, respectively which intersect in exactly a common\nvertex, where $k\\geq1$ and $a_1\\ge a_2\\ge \\cdots\\ge a_k\\ge 1$. In this paper,\nwe present a sharp upper bound for the signless Laplacian spectral radius of\nall $F_{a_1,\\dots,a_k}$-free graphs and characterize all extremal graphs which\nattain the bound. The stability methods and structure of graphs associated with\nthe eigenvalue are adapted for the proof.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  The rapid development in visual crowd analysis shows a trend to count people\nby positioning or even detecting, rather than simply summing a density map. It\nalso enlightens us back to the essence of the field, detection to count, which\ncan give more abundant crowd information and has more practical applications.\nHowever, some recent work on crowd localization and detection has two\nlimitations: 1) The typical detection methods can not handle the dense crowds\nand a large variation in scale; 2) The density map heuristic methods suffer\nfrom performance deficiency in position and box prediction, especially in high\ndensity or large-size crowds. In this paper, we devise a tailored baseline for\ndense crowds location, detection, and counting from a new perspective, named as\nLDC-Net for convenience, which has the following features: 1) A strong but\nminimalist paradigm to detect objects by only predicting a location map and a\nsize map, which endows an ability to detect in a scene with any capacity ($0\n\\sim 10,000+$ persons); 2) Excellent cross-scale ability in facing a large\nvariation, such as the head ranging in $0 \\sim 100,000+$ pixels; 3) Achieve\nsuperior performance in location and box prediction tasks, as well as a\ncompetitive counting performance compared with the density-based methods.\nFinally, the source code and pre-trained models will be released.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We consider quantum stochastic processes and discuss a level 2.5 large\ndeviation formalism providing an explicit and complete characterisation of\nfluctuations of time-averaged quantities, in the large-time limit. We analyse\ntwo classes of quantum stochastic dynamics, within this framework. The first\nclass consists of the quantum jump trajectories related to photon detection;\nthe second is quantum state diffusion related to homodyne detection. For both\nprocesses, we present the level 2.5 functional starting from the corresponding\nquantum stochastic Schr\\\"odinger equation and we discuss connections of these\nfunctionals to optimal control theory.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Medical image acquisition is often intervented by unwanted noise that\ncorrupts the information content. This paper introduces an unsupervised medical\nimage denoising technique that learns noise characteristics from the available\nimages and constructs denoised images. It comprises of two blocks of data\nprocessing, viz., patch-based dictionaries that indirectly learn the noise and\nresidual learning (RL) that directly learns the noise. The model is generalized\nto account for both 2D and 3D images considering different medical imaging\ninstruments. The images are considered one-by-one from the stack of MRI/CT\nimages as well as the entire stack is considered, and decomposed into\noverlapping image/volume patches. These patches are given to the patch-based\ndictionary learning to learn noise characteristics via sparse representation\nwhile given to the RL part to directly learn the noise properties. K-singular\nvalue decomposition (K-SVD) algorithm for sparse representation is used for\ntraining patch-based dictionaries. On the other hand, residue in the patches is\ntrained using the proposed deep residue network. Iterating on these two parts,\nan optimum noise characterization for each image/volume patch is captured and\nin turn it is subtracted from the available respective image/volume patch. The\nobtained denoised image/volume patches are finally assembled to a denoised\nimage or 3D stack. We provide an analysis of the proposed approach with other\napproaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer\nand the comparative results show that the proposed algorithm preserves the\ncritical information in the images as well as improves the visual quality of\nthe images.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We strengthen some results in \\'etale (and real \\'etale) motivic stable\nhomotopy theory, by eliminating finiteness hypotheses, additional localizations\nand/or extending to spectra from HZ-modules.\n\n\n###\n\n", "completion": " 09"}
{"prompt": "  The optical properties of monolayer transition metal dichalcogenides are\ndominated by tightly-bound excitons. They form at distinct valleys in\nreciprocal space, and can interact via the valley-exchange coupling, modifying\ntheir dispersion considerably. Here, we predict that angle-resolved\nphotoluminescence can be used to probe the changes of the excitonic dispersion.\nThe exchange-coupling leads to a unique angle dependence of the emission\nintensity for both circularly and linearly-polarised light. We show that these\nemission characteristics can be strongly tuned by an external magnetic field\ndue to the valley-specific Zeeman-shift. We propose that angle-dependent\nphotoluminescence measurements involving both circular and linear optical\npolarisation as well as magnetic fields should act as strong verification of\nthe role of valley-exchange coupling on excitonic dispersionand its signatures\nin optical spectra\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Information management has enter a completely new era, quantum era. However,\nthere exists a lack of sufficient theory to extract truly useful quantum\ninformation and transfer it to a form which is intuitive and straightforward\nfor decision making. Therefore, based on the quantum model of mass function, a\nfortified dual check system is proposed to ensure the judgment generated\nretains enough high accuracy. Moreover, considering the situations in real\nlife, everything takes place in an observable time interval, then the concept\nof time interval is introduced into the frame of the check system. The proposed\nmodel is very helpful in disposing uncertain quantum information in this paper.\nAnd some applications are provided to verify the rationality and correctness of\nthe proposed method.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Long-lived particles have become a new frontier in the exploration of physics\nbeyond the Standard Model. In this paper, we present the implementation of four\ntypes of long-lived particle searches, viz. displaced leptons, disappearing\ntrack, displaced vertex (together with muons or with missing energy), and heavy\ncharged tracks. These four categories cover the signatures of a large range of\nphysics models. We illustrate their potential for exclusion and discuss their\nmutual overlaps in mass-lifetime space for two simple phenomenological models\ninvolving either a U(1)-charged or a coloured scalar.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  Vision-based semantic segmentation of waterbodies and nearby related objects\nprovides important information for managing water resources and handling\nflooding emergency. However, the lack of large-scale labeled training and\ntesting datasets for water-related categories prevents researchers from\nstudying water-related issues in the computer vision field. To tackle this\nproblem, we present ATLANTIS, a new benchmark for semantic segmentation of\nwaterbodies and related objects. ATLANTIS consists of 5,195 images of\nwaterbodies, as well as high quality pixel-level manual annotations of 56\nclasses of objects, including 17 classes of man-made objects, 18 classes of\nnatural objects and 21 general classes. We analyze ATLANTIS in detail and\nevaluate several state-of-the-art semantic segmentation networks on our\nbenchmark. In addition, a novel deep neural network, AQUANet, is developed for\nwaterbody semantic segmentation by processing the aquatic and non-aquatic\nregions in two different paths. AQUANet also incorporates low-level feature\nmodulation and cross-path modulation for enhancing feature representation.\nExperimental results show that the proposed AQUANet outperforms other\nstate-of-the-art semantic segmentation networks on ATLANTIS. We claim that\nATLANTIS is the largest waterbody image dataset for semantic segmentation\nproviding a wide range of water and water-related classes and it will benefit\nresearchers of both computer vision and water resources engineering.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Making predictions and quantifying their uncertainty when the input data is\nsequential is a fundamental learning challenge, recently attracting increasing\nattention. We develop SigGPDE, a new scalable sparse variational inference\nframework for Gaussian Processes (GPs) on sequential data. Our contribution is\ntwofold. First, we construct inducing variables underpinning the sparse\napproximation so that the resulting evidence lower bound (ELBO) does not\nrequire any matrix inversion. Second, we show that the gradients of the GP\nsignature kernel are solutions of a hyperbolic partial differential equation\n(PDE). This theoretical insight allows us to build an efficient\nback-propagation algorithm to optimize the ELBO. We showcase the significant\ncomputational gains of SigGPDE compared to existing methods, while achieving\nstate-of-the-art performance for classification tasks on large datasets of up\nto 1 million multivariate time series.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  We present CrossSum, a large-scale cross-lingual abstractive summarization\ndataset comprising 1.7 million article-summary samples in 1500+ language pairs.\nWe create CrossSum by aligning identical articles written in different\nlanguages via cross-lingual retrieval from a multilingual summarization\ndataset. We propose a multi-stage data sampling algorithm to effectively train\na cross-lingual summarization model capable of summarizing an article in any\ntarget language. We also propose LaSE, a new metric for automatically\nevaluating model-generated summaries and showing a strong correlation with\nROUGE. Performance on ROUGE and LaSE indicate that pretrained models fine-tuned\non CrossSum consistently outperform baseline models, even when the source and\ntarget language pairs are linguistically distant. To the best of our knowledge,\nCrossSum is the largest cross-lingual summarization dataset and the first-ever\nthat does not rely solely on English as the pivot language. We are releasing\nthe dataset, alignment and training scripts, and the models to spur future\nresearch on cross-lingual abstractive summarization. The resources can be found\nat https://github.com/csebuetnlp/CrossSum.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We construct a weak categorification of the quantum toroidal algebra action\non the Grothendieck group of moduli space of stable (or framed) sheaves over an\nalgebraic surface, which is constructed by Schiffmann-Vasserot and Negu\\c{t}.\nThe new ingredient is two intersection-theoretic descriptions of the quadruple\nmoduli space of stable sheaves.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Accurate and efficient valuation of property is of utmost importance in a\nvariety of settings, such as when securing mortgage finance to purchase a\nproperty, or where residential property taxes are set as a percentage of a\nproperty's resale value. Internationally, resale based property taxes are most\ncommon due to ease of implementation and the difficulty of establishing site\nvalues. In an Irish context, property valuations are currently based on\ncomparison to recently sold neighbouring properties, however, this approach is\nlimited by low property turnover. National property taxes based on property\nvalue, as opposed to site value, also act as a disincentive to improvement\nworks due to the ensuing increased tax burden. In this article we develop a\nspatial hedonic regression model to separate the spatial and non-spatial\ncontributions of property features to resale value. We mitigate the issue of\nlow property turnover through geographic correlation, borrowing information\nacross multiple property types and finishes. We investigate the impact of\naddress mislabelling on predictive performance, where vendors erroneously\nsupply a more affluent postcode, and evaluate the contribution of improvement\nworks to increased values. Our flexible geo-spatial model outperforms all\ncompetitors across a number of different evaluation metrics, including the\naccuracy of both price prediction and associated uncertainty intervals. While\nour models are applied in an Irish context, the ability to accurately value\nproperties in markets with low property turnover and to quantify the value\ncontributions of specific property features has widespread application. The\nability to separate spatial and non-spatial contributions to a property's value\nalso provides an avenue to site-value based property taxes.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  In this article, we derive first-order necessary optimality conditions for a\nconstrained optimal control problem formulated in the Wasserstein space of\nprobability measures. To this end, we introduce a new notion of localised\nmetric subdifferential for compactly supported probability measures, and\ninvestigate the intrinsic linearised Cauchy problems associated to non-local\ncontinuity equations. In particular, we show that when the velocity\nperturbations belong to the tangent cone to the convexification of the set of\nadmissible velocities, the solutions of these linearised problems are tangent\nto the solution set of the corresponding continuity inclusion. We then make use\nof these novel concepts to provide a synthetic and geometric proof of the\ncelebrated Pontryagin Maximum Principle for an optimal control problem with\ninequality final-point constraints. In addition, we propose sufficient\nconditions ensuring the normality of the maximum principle.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Automatically translating images to texts involves image scene understanding\nand language modeling. In this paper, we propose a novel model, termed\nRefineCap, that refines the output vocabulary of the language decoder using\ndecoder-guided visual semantics, and implicitly learns the mapping between\nvisual tag words and images. The proposed Visual-Concept Refinement method can\nallow the generator to attend to semantic details in the image, thereby\ngenerating more semantically descriptive captions. Our model achieves superior\nperformance on the MS-COCO dataset in comparison with previous visual-concept\nbased models.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  In bidisperse particle mixtures varying in size or density alone, large\nparticles rise (driven by percolation) and heavy particles sink (driven by\nbuoyancy). When the two particle species differ from each other in both size\nand density, the two segregation mechanisms either enhance (large/light and\nsmall/heavy) or oppose (large/heavy and small/light) each other. In the latter\ncase, an equilibrium condition exists in which the two segregation mechanisms\nbalance and the particles no longer segregate. This leads to a methodology to\ndesign non-segregating particle mixtures by specifying particle size ratio,\ndensity ratio, and mixture concentration to achieve the equilibrium condition.\nUsing DEM simulations of quasi-2D bounded heap flow, we show that segregation\nis significantly reduced for particle mixtures near the equilibrium condition.\nIn addition, the rise-sink transition for a range of particle size and density\nratios matches the combined size and density segregation model predictions.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We study the pair production of the long-lived mediator particles from the\ndecay of the SM Higgs boson and their subsequent decay into standard model\nparticles. We compute the projected sensitivity, both model-independently and\nwith a minimal model, of using the muon spectrometer of the CMS detector at the\nHL-LHC experiment for ggF, VBF, and Vh production modes of the Higgs boson and\nvarious decay modes of the mediator particle, along with dedicated detectors\nfor LLP searches like CODEX-b and MATHUSLA. Subsequently, we study the\nimprovement with the FCC-hh detector at the 100\\,TeV collider experiment for\nsuch long-lived mediators, again focusing on the muon spectrometer. We propose\ndedicated LLP detector designs for the 100\\,TeV collider experiment, DELIGHT\n(\\textbf{De}tector for \\textbf{l}ong-l\\textbf{i}ved particles at hi\\textbf{gh}\nenergy of 100\\,\\textbf{T}eV), and study their sensitivities.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Recently a breakthrough has been achieved in laser-spectroscopic studies of\nshort-lived radioactive compounds with the first measurements of the radium\nmonofluoride molecule (RaF) UV/vis spectra. We report results from high\naccuracy \\emph{ab initio} calculations of the RaF electronic structure for\nground and low-lying excited electronic states. Two different methods agree\nexcellently with experimental excitation energies from the electronic ground\nstate to the $^2\\Pi_{1/2}$ and $^2\\Pi_{3/2}$ states, but lead consistently and\nunambiguously to deviations from experimental-based adiabatic transition energy\nestimates for the $^2\\Sigma_{1/2}$ excited electronic state and show that more\nmeasurements are needed to clarify spectroscopic assignment of the $^2\\Delta$\nstates.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  We consider the logarithmic Schr{\\\"o}dinger equations with damping, also\ncalled Schr{\\\"o}dinger-Langevin equation. On a periodic domain, this equation\npossesses plane wave solutions that are explicit. We prove that these solutions\nare asymptotically stable in Sobolev regularity. In the case without damping,\nwe prove that for almost all value of the nonlinear parameter, these solutions\nare stable in high Sobolev regularity for arbitrary long times when the\nsolution is close to a plane wave. We also show and discuss numerical\nexperiments illustrating our results.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Terahertz (THz) Time domain spectroscopy (THz-TDS) is a broadband\nspectroscopic technique spreading its uses in multiple fields: in science from\nmaterial science to biology, in industry where it measures the thickness of a\npaint layer during the painting operation. Using such practical commercial\napparatus with broad spectrum for gas spectroscopy could be a major asset for\nair quality monitoring and tracking of atmospheric composition. However, gas\nspectroscopy needs high resolution and the usual approach in THz-TDS, where the\nrecorded time trace is Fourier transform, suffers from resolution limitation\ndue to the size of the delay line in the system. In this letter, we introduce\nthe concept of constraint reconstruction for super-resolution spectroscopy\nbased on the modeling of the spectroscopic lines in a sparse spectrum. Light\nmolecule gas typically shows sparse and narrow lines on a broad spectrum and we\npropose an algorithm reconstructing these lines with a resolution improvement\nof 10 the ultimate resolution reachable by the apparatus. We envision the\nproposed technique to lead to broadband, selective, rapid and cheap gas\nmonitoring applications.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  In this work, we introduce statistical testing under distributional shifts.\nWe are interested in the hypothesis $P^* \\in H_0$ for a target distribution\n$P^*$, but observe data from a different distribution $Q^*$. We assume that\n$P^*$ is related to $Q^*$ through a known shift $\\tau$ and formally introduce\nhypothesis testing in this setting. We propose a general testing procedure that\nfirst resamples from the observed data to construct an auxiliary data set and\nthen applies an existing test in the target domain. We prove that if the size\nof the resample is at most $o(\\sqrt{n})$ and the resampling weights are\nwell-behaved, this procedure inherits the pointwise asymptotic level and power\nfrom the target test. If the map $\\tau$ is estimated from data, we can maintain\nthe above guarantees under mild conditions if the estimation works sufficiently\nwell. We further extend our results to finite sample level, uniform asymptotic\nlevel and a different resampling scheme. Testing under distributional shifts\nallows us to tackle a diverse set of problems. We argue that it may prove\nuseful in reinforcement learning and covariate shift, we show how it reduces\nconditional to unconditional independence testing and we provide example\napplications in causal inference.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We provide a new interpretation of the Mazur-Tate Conjecture and then use it\nto obtain the first (unconditional) theoretical evidence in support of the\nconjecture for elliptic curves of strictly positive rank.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  We show that spectral walls are common phenomena in the dynamics of kinks in\n(1+1) dimensions. They occur in models based on two or more scalar fields with\na nonempty Bogomol'nyi-Prasam-Sommerfield (BPS) sector, hosting two zero modes,\nwhere they are one of the main factors governing the soliton dynamics. We also\nshow that spectral walls appear as singularities of the dynamical vibrational\nmoduli space.\n\n\n###\n\n", "completion": " 06"}
{"prompt": "  Recent large-scale spectropolarimetric surveys have established that a small\nbut significant percentage of massive stars host stable, surface dipolar\nmagnetic fields with strengths on the order of kG. These fields channel the\ndense, radiatively driven stellar wind into circumstellar magnetospheres, whose\ndensity and velocity structure can be probed using ultraviolet (UV)\nspectroscopy of wind-sensitive resonance lines. Coupled with appropriate\nmagnetosphere models, UV spectroscopy provides a valuable way to investigate\nthe wind-field interaction, and can yield quantitative estimates of the wind\nparameters of magnetic massive stars. We report a systematic investigation of\nthe formation of UV resonance lines in slowly rotating magnetic massive stars\nwith dynamical magnetospheres. We pair the Analytic Dynamical Magnetosphere\n(ADM) formalism with a simplified radiative transfer technique to produce\nsynthetic UV line profiles. Using a grid of models, we examine the effect of\nmagnetosphere size, the line strength parameter, and the cooling parameter on\nthe structure and modulation of the line profile. We find that magnetic massive\nstars uniquely exhibit redshifted absorption at most viewing angles and\nmagnetosphere sizes, and that significant changes to the shape and variation of\nthe line profile with varying line strengths can be explained by examining the\nindividual wind components described in the ADM formalism. Finally, we show\nthat the cooling parameter has a negligible effect on the line profiles.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  The ground state of the energy super-critical Gross--Pitaevskii equation with\na harmonic potential converges in the energy space to the singular solution in\nthe limit of large amplitudes. The ground state can be represented by a\nsolution curve which has either oscillatory or monotone behavior, depending on\nthe dimension of the system and the power of the focusing nonlinearity. We\naddress here the monotone case for the cubic nonlinearity in the spatial\ndimensions $d \\geq 13$. By using the shooting method for the radial\nSchr\\\"{o}dinger operators, we prove that the Morse index of the ground state is\nfinite and is independent of the (large) amplitude. It is equal to the Morse\nindex of the limiting singular solution, which can be computed from numerical\napproximations. The numerical results suggest that the Morse index of the\nground state is one and that it is stable in the time evolution of the cubic\nGross--Pitaevskii equation in dimensions $d \\geq 13$.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  We establish a new lower bound for Mathieu's series and present a new\nderivation of its expansions in terms of Riemann Zeta functions.\n\n\n###\n\n", "completion": " 11"}
{"prompt": "  We study the active flow around isolated defects and the self-propulsion\nvelocity of $+1/2$ defects in an active nematic film with both viscous\ndissipation (with viscosity $\\eta$) and frictional damping $\\Gamma$ with a\nsubstrate. The interplay between these two dissipation mechanisms is controlled\nby the hydrodynamic dissipation length $\\ell_d=\\sqrt{\\eta/\\Gamma}$ that screens\nthe flows. For an isolated defect, in the absence of screening from other\ndefects, the size of the vortical flows around the defect is controlled by the\nsystem size $R$. In the presence of friction that leads to a finite value of\n$\\ell_d$, the vorticity field decays to zero on the lengthscales larger than\n$\\ell_d$. We show that the self-propulsion velocity of $+1/2$ defects grows\nwith $R$ in small systems where $R<\\ell_d$, while in the infinite system limit\nor when $R\\gg \\ell_d$, it approaches a constant value determined by $\\ell_d$.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We present timing and spectral results of 2018 outburst of Cepheus X-4,\nobserved twice by AstroSat at luminosity of $2.04 \\times 10^{37}$ erg s$^{-1}$\nand $1.02 \\times 10^{37}$ erg s$^{-1}$ respectively. The light curves showed\nstrong pulsation and co-related X-ray intensity variation in SXT (0.5--8.0 keV)\nand LAXPC (3--60 keV) energy bands. Spin-period and spin-down rate of the\npulsar were determined from two observations as $65.35080\\pm0.00014$ s ,\n$(-2.10\\pm0.8)\\times10^{-12}$ Hz s$^{-1}$ at an epoch MJD 58301.61850 and\n$65.35290\\pm0.00017$ s, $(-1.6\\pm0.8)\\times10^{-12}$ Hz s$^{-1}$ for an epoch\nMJD 58307.40211. Pulse-shape studies with AstroSat showed energy and intensity\ndependent variations. The pulsar showed an overall continuous spin-down, over\n30 years at an average-rate of $(-2.455\\pm0.004)\\times10^{-14}$ Hz s$^{-1}$,\nattributed to propeller-effect in the subsonic-regime of the pulsar, in\naddition to variations during its outburst activities. Spectra between 0.7--55\nkeV energy band were well fitted by two continuum models, an absorbed\ncompTT-model and an absorbed power-law with a Fermi-Dirac cutoff (FD-cutoff)\nmodel with a black-body. These were combined with an iron-emission line and a\ncyclotron absorption line. The prominent cyclotron resonance scattering\nfeatures with a peak absorption energy of $30.48^{+0.33}_{-0.34}$ keV and\n$30.68^{+0.45}_{-0.44}$ keV for FD-cutoff-model and $30.46^{+0.32}_{-0.28}$ keV\nand $30.30^{+0.36}_{-0.34}$ keV for compTT-model were detected during two\nAstroSat observations. These when compared with earlier results, showed long\nterm stability of its average value of $30.23 \\pm 0.22$ keV. The pulsar showed\npulse-phase as well as luminosity dependent variations in cyclotron-line energy\nand width and in plasma optical-depth of its spectral continuum.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  Iridates have been providing a fertile ground for studying emergent phases of\nmatter that arise from delicate interplay of various fundamental interactions\nwith approximate energy scale. Among these highly focused quantum materials,\nperovskite Sr2IrO4 belonging to the Ruddlesden-Popper series stands out and has\nbeen intensively addressed in the last decade, since it hosts a novel Jeff =\n1/2 state which is a profound manifestation of strong spin-orbit coupling.\nMoreover, the Jeff = 1/2 state represents a rare example of iridates that has\nbeen better understood both theoretically and experimentally. In this progress\nreport, we take Sr2IrO4 as an example to overview the recent advances of the\nJeff = 1/2 state in two aspects: materials fundamentals and functionality\npotentials. In the fundamentals part, we first illustrate basic issues for the\nlayered canted antiferromagnetic order of the Jeff = 1/2 magnetic moments in\nSr2IrO4, and then review the progress of the antiferromagnetic order modulation\nthrough diverse routes. Subsequently, for the functionality potentials,\nfascinating properties such as atomic-scale giant magnetoresistance,\nanisotropic magnetoresistance, and nonvolatile memory, will be addressed. This\nreport will be concluded with our prospected remarks and outlooks.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Chinese word segmentation (CWS) is the basic of Chinese natural language\nprocessing (NLP). The quality of word segmentation will directly affect the\nrest of NLP tasks. Recently, with the artificial intelligence tide rising\nagain, Long Short-Term Memory (LSTM) neural network, as one of easily modeling\nin sequence, has been widely utilized in various kinds of NLP tasks, and\nfunctions well. Attention mechanism is an ingenious method to solve the memory\ncompression problem on LSTM. Furthermore, inspired by the powerful abilities of\nbidirectional LSTM models for modeling sequence and CRF model for decoding, we\npropose a Bidirectional LSTM-CRF Attention-based Model in this paper.\nExperiments on PKU and MSRA benchmark datasets show that our model performs\nbetter than the baseline methods modeling by other neural networks.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Let $R$ be a commutative ring. An $R$-module $M$ is called a semi-regular\n$w$-flat module if Tor$_1^R(R/I,M)$ is GV-torsion for any finitely generated\nsemi-regular ideal $I$. In this article, we showed that the class of\nsemi-regular $w$-flat modules is a covering class. Moreover, we introduce the\nsemi-regular $w$-flat dimensions of $R$-modules and the $sr$-$w$-weak global\ndimensions of the commutative ring $R$. Utilizing these notions, we give some\nhomological characterizations of WQ-rings and $Q_0$-PvMRs.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  Many clustering algorithms are guided by certain cost functions such as the\nwidely-used $k$-means cost. These algorithms divide data points into clusters\nwith often complicated boundaries, creating difficulties in explaining the\nclustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and\nRashtchian (ICML 2020) introduced explainable clustering, where the cluster\nboundaries are axis-parallel hyperplanes and the clustering is obtained by\napplying a decision tree to the data. The central question here is: how much\ndoes the explainability constraint increase the value of the cost function?\n  Given $d$-dimensional data points, we show an efficient algorithm that finds\nan explainable clustering whose $k$-means cost is at most $k^{1 -\n2/d}\\,\\mathrm{poly}(d\\log k)$ times the minimum cost achievable by a clustering\nwithout the explainability constraint, assuming $k,d\\ge 2$. Taking the minimum\nof this bound and the $k\\,\\mathrm{polylog} (k)$ bound in independent work by\nMakarychev-Shan (ICML 2021), Gamlath-Jia-Polak-Svensson (2021), or\nEsfandiari-Mirrokni-Narayanan (2021), we get an improved bound of $k^{1 -\n2/d}\\,\\mathrm{polylog}(k)$, which we show is optimal for every choice of\n$k,d\\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in particular,\nwe show an $O(\\log k\\log\\log k)$ bound, improving near-exponentially over the\nprevious best bound of $O(k\\log k)$ by Laber and Murtinho (ICML 2021).\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  MOLLER is a future experiment designed to measure parity violation in Moller\nscattering to extremely high precision. MOLLER will measure the right-left\nscattering differential cross-section parity-violating asymmetry APV , in the\nelastic scattering of polarized electrons off an unpolarized LH2 target to\nextreme ppb precision. To make this measurement, the polarized electron source,\ngenerated with a circularly polarized laser beam, must have the ability to\nswitch quickly between right and left helicity polarization states. The\npolarized source must also maintain minimal right-left helicity correlated beam\nasymmetries, including energy changes, position changes, intensity changes, or\nspot-size changes. These requirements can be met with appropriate choice and\ndesign of the Pockels cell used to generate the circularly polarized light.\nRubidium Titanyl Phosphate (RTP) has been used in recent years for ultra-fast\nPockels cell switches due to its lack of piezo-electric resonances at\nfrequencies up to several hundred MHz. However, crystal non-uniformity in this\nmaterial leads to poorer extinction ratios than in commonly used KD*P Pockels\ncells when used in hald-wave configuration. It leads to voltage dependent beam\nsteering when used in quarter-wave configuration. Here we present an innovative\nRTP Pockels cell design which uses electric field gradients to counteract\ncrystal non-uniformities and control beam steering down to the nm-level. We\ndemonstrate this RTP Pockels cell design is capable of producing precisely\ncontrolled polarized electron beam at Jefferson Laboratory, a national\naccelerator facility, for current experiments, including the recent PREX II\nmeasurement, as well as the future MOLLER experiment.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  Browder (1960) proved that for every continuous function $F : X \\times Y \\to\nY$, where $X$ is the unit interval and $Y$ is a nonempty, convex, and compact\nsubset of $\\dR^n$, the set of fixed points of $F$, defined by $C_F := \\{ (x,y)\n\\in X \\times Y \\colon F(x,y)=y\\}$ has a connected component whose projection to\nthe first coordinate is $X$. We extend this result to the case where $X$ is a\nconnected and compact Hausdorff space.\n\n\n###\n\n", "completion": " 06"}
{"prompt": "  Images captured under extremely low light conditions are noise-limited, which\ncan cause existing robotic vision algorithms to fail. In this paper we develop\nan image processing technique for aiding 3D reconstruction from images acquired\nin low light conditions. Our technique, based on burst photography, uses direct\nmethods for image registration within bursts of short exposure time images to\nimprove the robustness and accuracy of feature-based structure-from-motion\n(SfM). We demonstrate improved SfM performance in challenging light-constrained\nscenes, including quantitative evaluations that show improved feature\nperformance and camera pose estimates. Additionally, we show that our method\nconverges more frequently to correct reconstructions than the state-of-the-art.\nOur method is a significant step towards allowing robots to operate in low\nlight conditions, with potential applications to robots operating in\nenvironments such as underground mines and night time operation.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We study the strong magnetic field limit for a nonlinear Iwatsuka-type model,\ni.e. a nonlinear Schr\\\"odinger equation in two spatial dimensions with a\nmagnetic vector potential that only depends on the $x$-coordinate. Using a\nhigh-frequency averaging technique, we show that this equation can be\neffectively described by a nonlocal nonlinear model, which is no longer\ndispersive. We also prove that, in this asymptotic regime, inhomogeneous\nnonlinearities are confined along the $y$-axis.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Python type inference is challenging in practice. Due to its dynamic\nproperties and extensive dependencies on third-party libraries without type\nannotations, the performance of traditional static analysis techniques is\nlimited. Although semantics in source code can help manifest intended usage for\nvariables (thus help infer types), they are usually ignored by existing tools.\nIn this paper, we propose PYInfer, an end-to-end learning-based type inference\ntool that automatically generates type annotations for Python variables. The\nkey insight is that contextual code semantics is critical in inferring the type\nfor a variable. For each use of a variable, we collect a few tokens within its\ncontextual scope, and design a neural network to predict its type. One\nchallenge is that it is difficult to collect a high-quality human-labeled\ntraining dataset for this purpose. To address this issue, we apply an existing\nstatic analyzer to generate the ground truth for variables in source code.\n  Our main contribution is a novel approach to statically infer variable types\neffectively and efficiently. Formulating the type inference as a classification\nproblem, we can handle user-defined types and predict type probabilities for\neach variable. Our model achieves 91.2% accuracy on classifying 11 basic types\nin Python and 81.2% accuracy on classifying 500 most common types. Our results\nsubstantially outperform the state-of-the-art type annotators. Moreover,\nPYInfer achieves 5.2X more code coverage and is 187X faster than a\nstate-of-the-art learning-based tool. With similar time consumption, our model\nannotates 5X more variables than a state-of-the-art static analysis tool. Our\nmodel also outperforms a learning-based function-level annotator on annotating\ntypes for variables and function arguments. All our tools and datasets are\npublicly available to facilitate future research in this direction.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Determining whether a dynamical system is integrable is generally a difficult\ntask which is currently done on a case by case basis requiring large human\ninput. Here we propose and test an automated method to search for the existence\nof relevant structures, the Lax pair and Lax connection respectively. By\nformulating this search as an optimization problem, we are able to identify\nappropriate structures via machine learning techniques. We test our method on\nstandard systems of classical integrability and find that we can single out\nsome integrable deformations of a system. Due to the ambiguity in defining a\nLax pair our algorithm identifies novel Lax pairs which can be easily verified\nanalytically.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  A group with a geometric action on some hyperbolic space is necessarily word\nhyperbolic, but on the other hand every countable group acts (metrically)\nproperly by isometries on a locally finite hyperbolic graph. In this paper we\nconsider what happens when a group acts isometrically on a restricted class of\nhyperbolic spaces, for instance quasitrees. We obtain strong conclusions on the\ngroup structure if the action has a locally finite orbit, especially if the\ngroup is finitely generated.\n  We also look at group actions on finite products of quasitrees, where our\nactions may be by automorphisms or by isometries, including the Leary -\nMinasyan group.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  In the event of a black hole (BH) forming stellar collapse, the neutrino\nsignal should terminate abruptly at the moment of BH formation, after a phase\nof steady accretion. Since neutrinos are expected to reach Earth hours before\nthe electromagnetic signal, the combined detection of the neutrino burst\nthrough multiple neutrino telescopes could allow to promptly determine the\nangular location of a nearby stellar collapse in the sky with high precision.\nIn this paper, we contrast the triangulation pointing procedure that relies on\nthe rise time of the neutrino curve, often considered in the literature, to the\none that takes advantage of the cutoff of the neutrino curve at the moment of\nBH formation. By forecasting the neutrino signal expected in the IceCube\nNeutrino Observatory, Hyper-Kamiokande and DUNE, we devise a strategy to\noptimize the identification of the rise and cutoff time of the neutrino curve.\nWe show that the triangulation method developed by employing the end tail of\nthe neutrino curve allows to achieve at least one order of magnitude\nimprovement in the pointing precision for a galactic burst, while being\ninsensitive to the neutrino mixing scenario. The triangulation pointing method\nbased on the cutoff of the neutrino curve will also guarantee a better\nperformance for BH forming collapses occurring beyond our own Galaxy.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  In this manuscript we show that the metric mean dimension of a free semigroup\naction satisfies three variational principles: (a) the first one is based on a\ndefinition of Shapira's entropy, introduced in \\cite{SH} for a singles dynamics\nand extended for a semigroup action in this note; (b) the second one treats\nabout a definition of Katok's entropy for a free semigroup action introduced in\n\\cite{CRV-IV}; (c) lastly we consider the local entropy function for a free\nsemigroup action and show that the metric mean dimension satisfies a\nvariational principle in terms of such function. Our results are inspired in\nthe ones obtained by \\cite{LT2019}, \\cite{VV}, \\cite{GS1} and \\cite{RX}.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We consider universal quantization with side information for Gaussian\nobservations, where the side information is a noisy version of the sender's\nobservation with noise variance unknown to the sender. In this paper, we\npropose a universally rate optimal and practical quantization scheme for all\nvalues of unknown noise variance. Our scheme uses Polar lattices from prior\nwork, and proceeds based on a structural decomposition of the underlying\nauxiliaries so that even when recovery fails in a round, the parties agree on a\ncommon \"reference point\" that is closer than the previous one. We also present\nthe finite blocklength analysis showing an sub-exponential convergence for\ndistortion and exponential convergence for rate. The overall complexity of our\nscheme is $O(N^2\\log^2 N)$ for any target distortion and fixed rate larger than\nthe rate-distortion bound.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Fields exhibit a variety of topological properties, like different\ntopological charges, when field space in the continuum is composed by more than\none topological sector. Lattice treatments usually encounter difficulties\ndescribing those properties. In this work, we show that by augmenting the usual\nlattice fields to include extra variables describing local topological\ninformation (more precisely, regarding homotopy), the topology of the space of\nfields in the continuum is faithfully reproduced in the lattice. We apply this\nextended lattice formulation to some simple models with non-trivial topological\ncharges, and we study their properties both analytically and via Monte Carlo\nsimulations.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  In this work we show that, given a linear map from a general operator space\ninto the dual of a C$^*$-algebra, its completely bounded norm is upper bounded\nby a universal constant times its $(1,cb)$-summing norm. This problem is\nmotivated by the study of quantum XOR games in the field of quantum information\ntheory. In particular, our results imply that for such games entangled\nstrategies cannot be arbitrarily better than those strategies using one-way\nclassical communication.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Automated commonsense reasoning is essential for building human-like AI\nsystems featuring, for example, explainable AI. Event Calculus (EC) is a family\nof formalisms that model commonsense reasoning with a sound, logical basis.\nPrevious attempts to mechanize reasoning using EC faced difficulties in the\ntreatment of the continuous change in dense domains (e.g., time and other\nphysical quantities), constraints among variables, default negation, and the\nuniform application of different inference methods, among others. We propose\nthe use of s(CASP), a query-driven, top-down execution model for Predicate\nAnswer Set Programming with Constraints, to model and reason using EC. We show\nhow EC scenarios can be naturally and directly encoded in s(CASP) and how it\nenables deductive and abductive reasoning tasks in domains featuring\nconstraints involving both dense time and dense fluents.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We report the discovery of an extended very-high-energy (VHE) gamma-ray\nsource around the location of the middle-aged (207.8 kyr) pulsar PSR J0622+3749\nwith the Large High Altitude Air Shower Observatory (LHAASO). The source is\ndetected with a significance of $8.2\\sigma$ for $E>25$~TeV assuming a Gaussian\ntemplate. The best-fit location is (R.A.,\nDec.)$=(95^{\\circ}\\!.47\\pm0^{\\circ}\\!.11,\\,37^{\\circ}\\!.92 \\pm0^{\\circ}\\!.09)$,\nand the extension is $0^{\\circ}\\!.40\\pm0^{\\circ}\\!.07$. The energy spectrum can\nbe described by a power-law spectrum with an index of ${-2.92 \\pm 0.17_{\\rm\nstat} \\pm 0.02_{\\rm sys} }$. No clear extended multi-wavelength counterpart of\nthe LHAASO source has been found from the radio to sub-TeV bands. The LHAASO\nobservations are consistent with the scenario that VHE electrons escaped from\nthe pulsar, diffused in the interstellar medium, and scattered the interstellar\nradiation field. If interpreted as the pulsar halo scenario, the diffusion\ncoefficient, inferred for electrons with median energies of $\\sim160$~TeV, is\nconsistent with those obtained from the extended halos around Geminga and\nMonogem and much smaller than that derived from cosmic ray secondaries. The\nLHAASO discovery of this source thus likely enriches the class of so-called\npulsar halos and confirms that high-energy particles generally diffuse very\nslowly in the disturbed medium around pulsars.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  Peer-review is a necessary and essential quality control step for scientific\npublications but lacks proper incentives. Indeed, the process, which is very\ncostly in terms of time and intellectual investment, not only is not\nremunerated by the journals but is also not openly recognized by the academic\ncommunity as a relevant scientific output for a researcher. Therefore,\nscientific dissemination is affected in timeliness, quality, and fairness.\nHere, to solve this issue, we propose a blockchain-based incentive system that\nrewards scientists for peer-reviewing other scientists' work and that builds up\ntrust and reputation. We designed a privacy-oriented protocol of smart\ncontracts called Ants-Review that allows authors to issue a bounty for open\nanonymous peer-reviews on Ethereum. If requirements are met, peer-reviews will\nbe accepted and paid by the approver proportionally to their assessed quality.\nTo promote ethical behavior and inclusiveness the system implements a gamified\nmechanism that allows the whole community to evaluate the peer-reviews and vote\nfor the best ones.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Data augmentation refers to a wide range of techniques for improving model\ngeneralization by augmenting training examples. Oftentimes such methods require\ndomain knowledge about the dataset at hand, spawning a plethora of recent\nliterature surrounding automated techniques for data augmentation. In this work\nwe apply one such method, bilevel optimization, to tackle the problem of graph\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\nmakes it the most effective augmenter for this classifier on the leaderboard.\nThis framework combines a GIN layer augmentation generator with a bias\ntransformation and outperforms the same classifier augmented using the\nstate-of-the-art FLAG augmentation.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  We consider the sequential composite binary hypothesis testing problem in\nwhich one of the hypotheses is governed by a single distribution while the\nother is governed by a family of distributions whose parameters belong to a\nknown set $\\Gamma$. We would like to design a test to decide which hypothesis\nis in effect. Under the constraints that the probabilities that the length of\nthe test, a stopping time, exceeds $n$ are bounded by a certain threshold\n$\\epsilon$, we obtain certain fundamental limits on the asymptotic behavior of\nthe sequential test as $n$ tends to infinity. Assuming that $\\Gamma$ is a\nconvex and compact set, we obtain the set of all first-order error exponents\nfor the problem. We also prove a strong converse. Additionally, we obtain the\nset of second-order error exponents under the assumption that $\\mathcal{X}$ is\na finite alphabet. In the proof of second-order asymptotics, a main technical\ncontribution is the derivation of a central limit-type result for a maximum of\nan uncountable set of log-likelihood ratios under suitable conditions. This\nresult may be of independent interest. We also show that some important\nstatistical models satisfy the conditions.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  The verification of multimedia content over social media is one of the\nchallenging and crucial issues in the current scenario and gaining prominence\nin an age where user-generated content and online social web platforms are the\nleading sources in shaping and propagating news stories. As these sources allow\nusers to share their opinions without restriction, opportunistic users often\npost misleading/ unreliable content on social media such as Twitter, Facebook,\netc. At present, to lure users towards the news story, the text is often\nattached with some multimedia content (images/videos/audios). Verifying these\ncontents to maintain the credibility and reliability of social media\ninformation is of paramount importance. Motivated by this, we proposed a\ngeneralized system that supports the automatic classification of images into\ncredible or misleading. In this paper, we investigated machine learning-based\nas well as deep learning-based approaches utilized to verify misleading\nmultimedia content, where the available image traces are used to identify the\ncredibility of the content. The experiment is performed on the real-world\ndataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates\nthe efficiency of our proposed approach and features using both Machine and\nDeep Learning Model (Bi-directional LSTM). The experiment result reveals that\nthe Microsoft bings image search engine is quite effective in retrieving titles\nand performs better than our study's Google image search engine. It also shows\nthat gathering clues from attached multimedia content (image) is more effective\nthan detecting only posted content-based features.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Hairy black holes in the gravitational decoupling setup are studied from the\nperspective of conformal anomalies. Fluctuations of decoupled sources can be\ncomputed by measuring the way the trace anomaly-to-holographic Weyl anomaly\nratio differs from unit. Therefore the gravitational decoupling parameter\ngoverning three hairy black hole metrics is then bounded to a range wherein one\ncan reliably emulate AdS/CFT with gravitational decoupled solutions, in the\ntensor vacuum regime.\n\n\n###\n\n", "completion": " 10"}
{"prompt": "  Employing very simple electro-mechanical principles known from classical\nphysics, the Kibble balance establishes a very precise and absolute link\nbetween quantum electrical standards and macroscopic mass or force\nmeasurements. The success of the Kibble balance, in both determining\nfundamental constants ($h$, $N_A$, $e$) and realizing a quasi-quantum mass in\nthe 2019 newly revised International System of Units, relies on the perfection\nof Maxwell's equations and the symmetry they describe between Lorentz's force\nand Faraday's induction, a principle and a symmetry stunningly demonstrated in\nthe weighing and velocity modes of Kibble balances to within $1\\times10^{-8}$,\nwith nothing but imperfect wires and magnets. However, recent advances in the\nunderstanding of the current effect in Kibble balances reveal a troubling\nparadox. A diamagnetic effect, a force that does not cancel between mass-on and\nmass-off measurement, is challenging balance maker's assumptions of symmetry at\nlevels that are almost two orders of magnitude larger than the reported\nuncertainties. The diamagnetic effect, if it exists, shows up in weighing mode\nwithout a readily apparent reciprocal effect in the velocity mode, begging\nquestions about systematic errors at the very foundation of the new measurement\nsystem. The hypothetical force is caused by the coil current changing the\nmagnetic field, producing an unaccounted force that is systematically modulated\nwith the weighing current. Here we show that this diamagnetic force exists, but\nthe additional force does not change the equivalence between weighing and\nvelocity measurements. We reveal the unexpected way that symmetry is preserved\nand show that for typical materials and geometries the total relative effect on\nthe measurement is $\\approx 1\\times10^{-9}$.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Millions of patients suffer from rare diseases around the world. However, the\nsamples of rare diseases are much smaller than those of common diseases. In\naddition, due to the sensitivity of medical data, hospitals are usually\nreluctant to share patient information for data fusion citing privacy concerns.\nThese challenges make it difficult for traditional AI models to extract rare\ndisease features for the purpose of disease prediction. In this paper, we\novercome this limitation by proposing a novel approach for rare disease\nprediction based on federated meta-learning. To improve the prediction accuracy\nof rare diseases, we design an attention-based meta-learning (ATML) approach\nwhich dynamically adjusts the attention to different tasks according to the\nmeasured training effect of base learners. Additionally, a dynamic-weight based\nfusion strategy is proposed to further improve the accuracy of federated\nlearning, which dynamically selects clients based on the accuracy of each local\nmodel. Experiments show that with as few as five shots, our approach\nout-performs the original federated meta-learning algorithm in accuracy and\nspeed. Compared with each hospital's local model, the proposed model's average\nprediction accuracy increased by 13.28%.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We investigate the amplification of turbulence through gravitational\ncontraction of the primordial gas in minihalos. We perform numerical\nsimulations to follow the cloud collapse, assuming polytropic equations of\nstate for different initial turbulent Mach numbers and resolutions. We find\nthat the turbulent velocity is amplified solely by gravitational contraction,\nand eventually becomes comparable to the sound speed, even for small initial\nturbulent Mach numbers (${\\cal M}_0 \\gtrsim 0.05$). We derive an analytic\nformula for the amplification of turbulent velocity in a collapsing cloud, and\nfind that our numerical results are consistent with the formula. These results\nsuggest that the turbulence can play an important role in collapsing clouds for\ngeneral cases.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  Modern vehicles become increasingly digitalized with advanced information\ntechnology-based solutions like advanced driving assistance systems and\nvehicle-to-x communications. These systems are complex and interconnected.\nRising complexity and increasing outside exposure has created a steadily rising\ndemand for more cyber-secure systems. Thus, also standardization bodies and\nregulators issued standards and regulations to prescribe more secure\ndevelopment processes. This security, however, also has to be validated and\nverified. In order to keep pace with the need for more thorough, quicker and\ncomparable testing, today's generally manual testing processes have to be\nstructured and optimized. Based on existing and emerging standards for\ncybersecurity engineering, this paper therefore outlines a structured testing\nprocess for verifying and validating automotive cybersecurity, for which there\nis no standardized method so far. Despite presenting a commonly structured\nframework, the process is flexible in order to allow implementers to utilize\ntheir own, accustomed toolsets.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  Liquid Crystal Elastomers (LCEs) are an exciting category of material that\nhas tremendous application potential across a variety of fields, owing to their\nunique properties that enable both sensing and actuation. To some, LCEs are\nsimply another type of Shape Memory Polymer, while to others they are an\ninteresting on-going scientific experiment. In this visionary article, we bring\nan interdisciplinary discussion around creative and impactful ways that LCEs\ncan be applied in the Built Environment to support kinematic and kinetic\nbuildings and situational awareness. We focus particularly on the autonomy made\npossible by using LCEs, potentially removing needs for motors, wiring and\ntubing, and even enabling fully independent operation in response to natural\nenvironment variations, requiring no power sources. To illustrate the\npotential, we propose a number of concrete application scenarios where LCEs\ncould offer innovative solutions to problems of great societal importance, such\nas autonomous active ventilation, heliotropic solar panel systems which can\nalso remove snow or sand autonomously, and invisible coatings with strain\nmapping functionality, alerting residents in case of dangerous (static or\ndynamic) loads on roofs or windows, as well as assisting building safety\ninspection teams after earthquakes.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  A biomedical relation statement is commonly expressed in multiple sentences\nand consists of many concepts, including gene, disease, chemical, and mutation.\nTo automatically extract information from biomedical literature, existing\nbiomedical text-mining approaches typically formulate the problem as a\ncross-sentence n-ary relation-extraction task that detects relations among n\nentities across multiple sentences, and use either a graph neural network (GNN)\nwith long short-term memory (LSTM) or an attention mechanism. Recently,\nTransformer has been shown to outperform LSTM on many natural language\nprocessing (NLP) tasks. In this work, we propose a novel architecture that\ncombines Bidirectional Encoder Representations from Transformers with Graph\nTransformer (BERT-GT), through integrating a neighbor-attention mechanism into\nthe BERT architecture. Unlike the original Transformer architecture, which\nutilizes the whole sentence(s) to calculate the attention of the current token,\nthe neighbor-attention mechanism in our method calculates its attention\nutilizing only its neighbor tokens. Thus, each token can pay attention to its\nneighbor information with little noise. We show that this is critically\nimportant when the text is very long, as in cross-sentence or abstract-level\nrelation-extraction tasks. Our benchmarking results show improvements of 5.44%\nand 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and\nchemical-protein relation datasets, suggesting BERT-GT is a robust approach\nthat is applicable to other biomedical relation extraction tasks or datasets.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We study the confinement/deconfinement phase transition of the radion field\nin a warped model with a polynomial bulk potential. The backreaction of the\nradion on the metric is taken into account by using the superpotential\nformalism, while the radion effective potential is obtained from a novel\nformulation which can incorporate the backreaction. The phase transition leads\nto a stochastic gravitational wave background that depends on the energy scale\nof the first Kaluza-Klein resonance, $m_{\\textrm{KK}}$. This work completes\nprevious studies in the following aspects: i) we detail the evaluation of the\nradion spectrum; ii) we report on the mismatches between the thick wall\napproximation and the numerical bounce solution; iii) we include a suppression\nfactor in the spectrum of sound waves accounting for their finite lifetime;\nand, iv) we update the bound on $m_{\\textrm{KK}}$ in view of the O3 LIGO and\nVirgo data. We find that the forthcoming gravitational wave interferometers can\nprobe scenarios where $m_{\\textrm{KK}} \\lesssim 10^9$ TeV, while the O3-run\nbounds rule out warped models with $10^4 \\textrm{TeV} \\lesssim m_{\\textrm{KK}}\n\\lesssim 10^7$ TeV exhibiting an extremely strong confinement/deconfinement\nphase transition.\n\n\n###\n\n", "completion": " 10"}
{"prompt": "  When a chaotic, ergodic Hamiltonian system with $N$ degrees of freedom is\nsubject to sufficiently rapid periodic driving, its energy evolves diffusively.\nWe derive a Fokker-Planck equation that governs the evolution of the system's\nprobability distribution in energy space, and we provide explicit expressions\nfor the energy drift and diffusion rates. Our analysis suggests that the system\ngenerically relaxes to a long-lived \"prethermal\" state characterized by minimal\nenergy absorption, eventually followed by more rapid heating. When $N\\gg 1$,\nthe system ultimately absorbs energy indefinitely from the drive, or at least\nuntil an infinite temperature state is reached.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Janus monolayers have long been captivated as a popular notion for breaking\nin-plane and out-of-plane structural symmetry. Originated from chemistry and\nmaterials science, the concept of Janus functions have been recently extended\nto ultrathin metasurfaces by arranging meta-atoms asymmetrically with respect\nto the propagation or polarization direction of the incident light. However,\nsuch metasurfaces are intrinsically static and the information they carry can\nbe straightforwardly decrypted by scanning the incident light directions and\npolarization states once the devices are fabricated. In this Letter, we present\na dynamic Janus metasurface scheme in the visible spectral region. In each\nsuper unit cell, three plasmonic pixels are categorized into two sets. One set\ncontains a magnesium nanorod and a gold nanorod that are orthogonally oriented\nwith respect to each other, working as counter pixels. The other set only\ncontains a magnesium nanorod. The effective pixels on the Janus metasurface can\nbe reversibly regulated by hydrogenation/dehydrogenation of the magnesium\nnanorods. Such dynamic controllability at visible frequencies allows for flat\noptical elements with novel functionalities including beam steering, bifocal\nlensing, holographic encryption, and dual optical function switching.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Large language models are increasingly capable of generating fluent-appearing\ntext with relatively little task-specific supervision. But can these models\naccurately explain classification decisions? We consider the task of generating\nfree-text explanations using human-written examples in a few-shot manner. We\nfind that (1) authoring higher quality prompts results in higher quality\ngenerations; and (2) surprisingly, in a head-to-head comparison, crowdworkers\noften prefer explanations generated by GPT-3 to crowdsourced explanations in\nexisting datasets. Our human studies also show, however, that while models\noften produce factual, grammatical, and sufficient explanations, they have room\nto improve along axes such as providing novel information and supporting the\nlabel. We create a pipeline that combines GPT-3 with a supervised filter that\nincorporates binary acceptability judgments from humans in the loop. Despite\nthe intrinsic subjectivity of acceptability judgments, we demonstrate that\nacceptability is partially correlated with various fine-grained attributes of\nexplanations. Our approach is able to consistently filter GPT-3-generated\nexplanations deemed acceptable by humans.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  Interpretability has attracted increasing attention in earth observation\nproblems. We apply interactive visualization and representation analysis to\nguide interpretation of glacier segmentation models. We visualize the\nactivations from a U-Net to understand and evaluate the model performance. We\nbuild an online interface using the Shiny R package to provide comprehensive\nerror analysis of the predictions. Users can interact with the panels and\ndiscover model failure modes. Further, we discuss how visualization can provide\nsanity checks during data preprocessing and model training.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  Stochastic modelling of complex systems plays an essential, yet often\ncomputationally intensive role across the quantitative sciences. Recent\nadvances in quantum information processing have elucidated the potential for\nquantum simulators to exhibit memory advantages for such tasks. Heretofore, the\nfocus has been on lossless memory compression, wherein the advantage is\ntypically in terms of lessening the amount of information tracked by the model,\nwhile -- arguably more practical -- reductions in memory dimension are not\nalways possible. Here we address the case of lossy compression for quantum\nstochastic modelling of continuous-time processes, introducing a method for\ncoarse-graining in quantum state space that drastically reduces the requisite\nmemory dimension for modelling temporal dynamics whilst retaining near-exact\nstatistics. In contrast to classical coarse-graining, this compression is not\nbased on sacrificing temporal resolution, and brings memory-efficient,\nhigh-fidelity stochastic modelling within reach of present quantum\ntechnologies.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  We demonstrate that the conformal loop ensemble (CLE) has a rich integrable\nstructure by establishing exact formulas for two CLE observables. The first\ndescribes the joint moments of the conformal radii of loops surrounding three\npoints for CLE on the sphere. Up to normalization, our formula agrees with the\nimaginary DOZZ formula due to Zamolodchikov (2005) and Kostov-Petkova (2007),\nwhich is the three-point structure constant of certain conformal field theories\nthat generalize the minimal models. This verifies the CLE interpretation of the\nimaginary DOZZ formula by Ikhlef, Jacobsen and Saleur (2015). Our second result\nis for the moments of the electrical thickness of CLE loops first considered by\nKenyon and Wilson (2004). Our proofs rely on the conformal welding of random\nsurfaces and two sources of integrability concerning CLE and Liouville quantum\ngravity (LQG). First, LQG surfaces decorated with CLE inherit a rich integrable\nstructure from random planar maps decorated with the O(n) loop model. Second,\nas the field theory describing LQG, Liouville conformal field theory is\nintegrable. In particular, the DOZZ formula and the FZZ formula for its\nstructure constants are crucial inputs to our results.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  I partially identify the marginal treatment effect (MTE) function when the\ntreatment variable is misclassified. To do so, I explore three sets of\nrestrictions on the relationship between the instrument, the misclassified\ntreatment and the correctly measured treatment, allowing for dependence between\nthe instrument and the misclassification decision. If the signs of the\nderivatives of the correctly measured propensity score and the mismeasured one\nare the same, I identify the sign of the MTE function at every point in the\ninstrument's support. If those derivatives are close to each other, I bound the\nMTE function. Finally, by imposing a functional restriction between those two\npropensity scores, I derive sharp bounds around the MTE function and any\nweighted average of the MTE function. To illustrate the usefulness of my\npartial identification method, I analyze the impact of alternative sentences --\ne.g., fines or community services -- on recidivism using random assignment of\njudges within Brazilian court districts. In this context, misclassification is\nan issue when the researcher measures the treatment based solely on trial\njudge's rulings, ignoring that the Appeals Court may reverse sentences. I show\nthat, when I use the trial judge's rulings as my misclassified treatment\nvariable, the misclassification bias may be as large as 10% of the MTE\nfunction, which can be estimated using the final ruling in each case as my\ncorrectly measured treatment variable. Moreover, I show that the proposed\nbounds contain the MTE function in this empirical example.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Future wireless networks are expected to connect large-scale low-powered\ncommunication devices using the available spectrum resources. Backscatter\ncommunications (BC) is an emerging technology towards battery-free transmission\nin future wireless networks by leveraging ambient radio frequency (RF) waves\nthat enable communications among wireless devices. Non-orthogonal multiple\naccess (NOMA) has recently drawn significant attention due to its high spectral\nefficiency. The combination of these two technologies can play an important\nrole in the development of future networks. This paper proposes a new\noptimization approach to enhance the spectral efficiency of nonorthogonal\nmultiple access (NOMA)-BC network. Our framework simultaneously optimizes the\npower allocation of base station and reflection coefficient (RC) of the\nbackscatter device in each cell under the assumption of imperfect signal\ndecoding. The problem of spectral efficiency maximization is coupled on power\nand RC which is challenging to solve. To make this problem tractable, we first\ndecouple it into two subproblems and then apply the decomposition method and\nKarush-Kuhn-Tucker conditions to obtain the efficient solution. Numerical\nresults show the performance of the proposed NOMA-BC network over the pure NOMA\nnetwork without BC.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  This paper examines long-term temporal and spatial fluctuations in the solar\nrotation (more than four solar cycles) by investigating radio emission escapes\nfrom various layers of the solar atmosphere during the years 1967-2010. The\nflux modulation approach can also be used to investigate variations in solar\nrotation, which is a contentious topic in solar physics. The current study\nmakes use of a time series of radio flux data at different frequencies\n(245-15400 MHz) obtained at Sagamore Hill Solar Radio Observatory in\nMassachusetts, USA, and other observatories from 1967 to 2010. The periodicity\npresent in the temporal variation of time series is estimated through Lomb\nScargle Periodogram (LSP). The rotation period estimated for five radio\nemissions (606, 1415, \\& 2695 MHz; from corona, and 4995 \\& 8800 MHz; from\ntransition region) through statistical approach shows continuous temporal and\nspatial variation throughout the years. The smoothed rotation period shows the\npresence of $\\sim$ 22-yrs periodic and $\\sim$ 11-yrs components in it. The\n22-year component could be linked to the reversal of the solar magnetic field\n(Hale's) cycle, while the 11-yrs component is most likely related to the\nsunspot (Schwabe's) cycle. Besides these two components, random components are\nalso prominently present in the analyzed data. The cross-correlation between\nthe sunspot number and the rotation period obtained shows a strong correlation\nwith 11-yrs Schwabe's and 22-yr Hale cycle. The corona rotates faster or slower\nthan transition region in different epoch. The swap of faster rotation speed\nbetween corona and transition region also follows the 22-yrs cycle.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  This paper formulates and solves the optimal stopping problem for a loan made\nto one's self from a tax-advantaged retirement account such as a 401(k),\n403(b), or 457(b) plan. If the plan participant has access to an external asset\nwith a higher expected rate of return than the investment funds and indices\nthat are available within the retirement account, then he must decide how long\nto wait before exercising the loan option. On the one hand, taking the loan\nquickly will result in many years of exponential capital growth at the higher\n(external) rate; on the other hand, if we wait to accumulate more funds in the\n457(b), then we can make a larger deposit into the external asset (albeit for a\nshorter period of time). I derive a variety of cutoff rules for optimal loan\ncontrol; in general, the investor must wait until he accumulates a certain\namount of money (measured in contribution-years) that depends on the disparate\nyields, the loan parameters, and the date certain at which he will liquidate\nthe retirement account. Letting the horizon tend to infinity, the optimal\n(horizon-free) policy gains in elegance, simplicity, and practical robustness\nto different life outcomes. When asset prices and returns are stochastic, the\n(continuous time) cutoff rule turns into a \"wait region,\" whereby the mean of\nterminal wealth is rising and the variance of terminal wealth is falling. After\nhis sojourn through the wait region is over, the participant finds himself on\nthe mean-variance frontier, at which point his subsequent behavior is a matter\nof personal risk preference.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  Discrete algebraic Riccati equations and their fixed points are well\nunderstood and arise in a variety of applications, however, the time-varying\nequations have not yet been fully explored in the literature. In this article\nwe provide a self-contained study of discrete time Riccati matrix difference\nequations. In particular, we provide a novel Riccati semigroup duality formula\nand a new Floquet-type representation for these equations. Due to the\naperiodicity of the underlying flow of the solution matrix, conventional\nFloquet theory does not apply in this setting and thus further analysis is\nrequired. We illustrate the impact of these formulae with an explicit\ndescription of the solution of time-varying Riccati difference equations and\nits fundamental-type solution in terms of the fixed point of the equation and\nan invertible linear matrix map, as well as uniform upper and lower bounds on\nthe Riccati maps. These are the first results of this type for time varying\nRiccati matrix difference equations.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Signals can be interpreted as composed of a rapidly varying component\nmodulated by a slower varying envelope. Identifying this envelope is an\nessential operation in signal processing, with applications in areas ranging\nfrom seismology to medicine. Conventional envelope detection approaches based\non classic methods tend to lack generality, however, and need to be tailored to\neach specific application in order to yield reasonable results. Taking\ninspiration from geometric concepts, most notably the theory of alpha-shapes,\nwe introduce a general-purpose library to efficiently extract the envelope of\narbitrary signals.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  The optimization problems with simple bounds are an important class of\nproblems. To facilitate the computation of such problems, an unconstrained-like\ndynamic method, motivated by the Lyapunov control principle, is proposed. This\nmethod employs the anti-windup limited integrator to address the bounds of\nparameters upon the dynamics for unconstrained problem, and then solves the\ntransformed Initial-value Problems (IVPs) with mature Ordinary Differential\nEquation (ODE) integration methods. It is proved that when the gain matrix is\ndiagonal, the result is equivalent to that of the general dynamic method which\ninvolves an intermediate Quadratic Programming (QP) sub-problem. Thus, the\nglobal convergence to the optimal solution is guaranteed without the\nrequirement of the strict complementarity condition. Since the estimation of\nthe right active constraints is avoided and no programming sub-problem is\ninvolved in the computation process, it shows higher efficiency than the\ngeneral dynamic method and other common iterative methods through the numerical\nexamples. In particular, the implementation is simple, and the proposed method\nis easy-to-use.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We show that counts of squarefree integers up to $X$ in short intervals of\nsize $H$ tend to a Gaussian distribution as long as $H\\rightarrow\\infty$ and $H\n= X^{o(1)}$. This answers a question posed by R.R. Hall in 1989. More generally\nwe prove a variant of Donsker's theorem, showing that these counts scale to a\nfractional Brownian motion with Hurst parameter $1/4$. In fact we are able to\nprove these results hold in general for collections of $B$-free integers as\nlong as the sieving set $B$ satisfies a very mild regularity property, for\nHurst parameter varying with the set $B$.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  EUSO-TA is a ground-based experiment, placed at Black Rock Mesa of the\nTelescope Array site as a part of the JEM-EUSO (Joint Experiment Missions for\nthe Extreme Universe Space Observatory) program. The UV fluorescence imaging\ntelescope with a field of view of about 10.6 deg x 10.6 deg consisting of 2304\npixels (36 Multi-Anode Photomultipliers, 64 pixels each) works with\n2.5-microsecond time resolution. An experimental setup with two Fresnel lenses\nallows for measurements of Ultra High Energy Cosmic Rays in parallel with the\nTA experiment as well as the other sources like flashes of lightning,\nartificial signals from UV calibration lasers, meteors and stars. Stars\nincrease counts on pixels while crossing the field of view as the point-like\nsources. In this work, we discuss the method for calibration of EUSO\nfluorescence detectors based on signals from stars registered by the EUSO-TA\nexperiment during several campaigns. As the star position is known, the\nanalysis of signals gives an opportunity to determine the pointing accuracy of\nthe detector. This can be applied to space-borne or balloon-borne EUSO\nmissions. We describe in details the method of the analysis which provides\ninformation about detector parameters like the shape of the point spread\nfunction and is the way to perform absolute calibration of EUSO cameras.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Kerala, the south-west coastal state of India, was ravaged by a series of\nfloods during the South-West Monsoon of 2018. The season was marked by severely\nanomalous rainfall trends, with upto 150 mm of departures from the mean daily\nprecipitation in the northern districts of the State. Although, there were many\nstudies about the hydrogeological factors which aggravated the floods in\nKerala, no attempt was made to delve into the physics which actually resulted\nin anomalous precipitation during the year. This study intends to document the\ndynamical phenomenon which caused the Kerala Floods of 2018. The westward\npropagating convectively-coupled Mixed Rossby-Gravity (MRG) waves were excited\nby the synoptic disturbances of the tropical Pacific at the pressure level of\n700 hPa, during the Indian Monsoon of 2018. They travelled across the Indian\nOcean in two significant modes -- a predominant slow moving wave of 20-40 days\nperiod (as Madden-Julian Oscillation) and a secondary faster wave of 5-8 days\nperiod. They are characterised by vertical phase propagation to the upper\ntroposphere, a precursor to deep convection and intense precipitation. Further,\nthe propagation of these waves through a medium enhances its relative vorticity\nand the gyres or circulations thus formed are symmetrical about the equator.\nConsequently, the meanders in the wind field and widening of the Intertropical\nConvergence Zone were observed. The MRG waves, especially the slow mode induced\ndivergence in the wind field, which fueled convection in tropics and brought\nvery heavy rainfall to the State of Kerala in 2018.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  The quantification of modern slavery has received increased attention\nrecently as organizations have come together to produce global estimates, where\nmultiple systems estimation (MSE) is often used to this end. Echoing a\nlong-standing controversy, disagreements have re-surfaced regarding the\nunderlying MSE assumptions, the robustness of MSE methodology, and the accuracy\nof MSE estimates in this application. Our goal is to help address and move past\nthese controversies. To do so, we review MSE, its assumptions, and commonly\nused models for modern slavery applications. We introduce all of the publicly\navailable modern slavery datasets in the literature, providing a reproducible\nanalysis and highlighting current issues. Specifically, we utilize an internal\nconsistency approach that constructs subsets of data for which ground truth is\navailable, allowing us to evaluate the accuracy of MSE estimators. Next, we\npropose a characterization of the large sample bias of estimators as a function\nof misspecified assumptions. Then, we propose an alternative to traditional\n(e.g., bootstrap-based) assessments of reliability, which allows us to\nvisualize trajectories of MSE estimates to illustrate the robustness of\nestimates. Finally, our complementary analyses are used to provide guidance\nregarding the application and reliability of MSE methodology.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  In nonequilibrium systems, the relative fluctuation of a current has a\nuniversal trade-off relation with the entropy production, called the\nthermodynamic uncertainty relation (TUR). For systems with broken time reversal\nsymmetry, its violation has been reported in specific models or in the linear\nresponse regime. Here, we derive a modified version of the TUR analytically in\nthe overdamped limit for general Langevin dynamics with a magnetic Lorentz\nforce causing time reversal broken. Remarkably, this modified version is simply\ngiven by the conventional TUR scaled by the ratio of the reduced effective\ntemperature of the overdamped motion to the reservoir temperature, permitting a\nviolation of the conventional TUR. Without the Lorentz force, this ratio\nbecomes unity and the conventional TUR is restored. We verify our results both\nanalytically and numerically in a specific solvable system.\n\n\n###\n\n", "completion": " 11"}
{"prompt": "  Selectivity is an important phenomenon in chemical reaction dynamics. This\ncan be quantified by the branching ratio of the trajectories that visit one or\nthe other wells to the total number of trajectories in a system with a\npotential with two sequential index-1 saddles and two wells (top well and\nbottom well). In our case, the branching ratio is 1:1 because of the symmetry\nof our potential energy surface. The mechanisms of transport and the behavior\nof the trajectories in this kind of systems have been studied recently. In this\npaper we study the time evolution after the selectivity as energy varies using\nperiodic orbit dividing surfaces. We investigate what happens after the first\nvisit of a trajectory to the region of the top or the bottom well for different\nvalues of energy. We answer the natural question, what is the destiny of these\ntrajectories?\n\n\n###\n\n", "completion": " 09"}
{"prompt": "  We give a new description to obtain the shear viscosity in QCD at finite\ntemperature. Firstly, we obtain the correlation function of the renormalized\nenergy-momentum tensor using the gradient flow method. Secondly, we estimate\nthe spectral function from the smeared correlation functions using the sparse\nmodeling method. The combination of these two methods looks promising to\ndetermine the shear viscosity precisely.\n\n\n###\n\n", "completion": " 99"}
{"prompt": "  $\\beta$-decay half-lives of neutron-rich nuclei around $N=82$ are key data to\nunderstand the $r$-process nucleosynthesis. We performed large-scale\nshell-model calculations in this region using a newly constructed shell-model\nHamiltonian, and successfully described the low-lying spectra and half-lives of\nneutron-rich $N=82$ and $N=81$ isotones with $Z=42-49$ in a unified way. We\nfound that their Gamow-Teller strength distributions have a peak in the\nlow-excitation energies, which significantly contributes to the half-lives.\nThis peak, dominated by $\\nu 0g_{7/2} \\to \\pi 0g_{9/2}$ transitions, is\nenhanced on the proton deficient side because the Pauli-blocking effect caused\nby occupying the valence proton $0g_{9/2}$ orbit is weakened.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  The paper focuses on improving the recent plug-and-play patch rescaling\nmodule (PRM) based approaches for crowd counting. In order to make full use of\nthe PRM potential and obtain more reliable and accurate results for challenging\nimages with crowd-variation, large perspective, extreme occlusions, and\ncluttered background regions, we propose a new PRM based multi-resolution and\nmulti-task crowd counting network by exploiting the PRM module with more\neffectiveness and potency. The proposed model consists of three deep-layered\nbranches with each branch generating feature maps of different resolutions.\nThese branches perform a feature-level fusion across each other to build the\nvital collective knowledge to be used for the final crowd estimate.\nAdditionally, early-stage feature maps undergo visual attention to strengthen\nthe later-stage channels understanding of the foreground regions. The\nintegration of these deep branches with the PRM module and the early-attended\nblocks proves to be more effective than the original PRM based schemes through\nextensive numerical and visual evaluations on four benchmark datasets. The\nproposed approach yields a significant improvement by a margin of 12.6% in\nterms of the RMSE evaluation criterion. It also outperforms state-of-the-art\nmethods in cross-dataset evaluations.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  The spin Hall effect of light, a spin-dependent transverse splitting of light\nat an optical interface, is intrinsically an incident-polarization-sensitive\nphenomenon. Recently, an approach to eliminate the polarization dependence by\nequalizing the reflection coefficients of two linear polarizations has been\nproposed, but is only valid when the beam waist is sufficiently larger than the\nwavelength. Here, we demonstrate that an interface, at which the reflection\ncoefficients of the two linear polarizations are the same and so are their\nderivatives with respect to the incident angle, supports the\npolarization-independent spin Hall shift, even when the beam waist is\ncomparable to the wavelength. In addition, an isotropic-anisotropic interface\nthat exhibits the polarization-independent spin Hall shift over the entire\nrange of incident angles is presented. Monte-Carlo simulations prove that spin\nHall shifts are degenerate under any polarization and reaches a half of beam\nwaist under unpolarized incidence. We suggest an application of the\nbeam-waist-scale spin Hall effect of light as a tunable beam-splitting device\nthat is responsive to the incident polarization. The spin Hall shift that is\nindependent of the incident polarization at any incident angle will facilitate\na wide range of applications including practical spin-dependent devices and\nactive beam splitters.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  In this article we propose a $\\alpha$-hypergeometric model with uncertain\nvolatility (UV) where we derive a worst-case scenario for option pricing. The\napproach is based on the connexion between a certain class of nonlinear partial\ndifferential equations of HJB-type (G-HJB equations), that govern the nonlinear\nexpectation of the UV model and that provide an alternative to the difficult\nmodel calibration problem of UV models, and second-order backward stochastic\ndifferential equations (2BSDEs). Using asymptotic analysis for the G-HJB\nequation and the equivalent 2BSDE representation, we derive a limit model that\nprovides an accurate description of the worst-case price scenario in cases when\nthe bounds of the UV model are slowly varying. The analytical results are\ntested by numerical simulations using a deep learning based approximation of\nthe underlying 2BSDE.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  This paper proposes a novel integrated dynamic method based on Behavior Trees\nfor planning and allocating tasks in mixed human robot teams, suitable for\nmanufacturing environments. The Behavior Tree formulation allows encoding a\nsingle job as a compound of different tasks with temporal and logic\nconstraints. In this way, instead of the well-studied offline centralized\noptimization problem, the role allocation problem is solved with multiple\nsimplified online optimization sub-problem, without complex and cross-schedule\ntask dependencies. These sub-problems are defined as Mixed-Integer Linear\nPrograms, that, according to the worker-actions related costs and the workers'\navailability, allocate the yet-to-execute tasks among the available workers. To\ncharacterize the behavior of the developed method, we opted to perform\ndifferent simulation experiments in which the results of the action-worker\nallocation and computational complexity are evaluated. The obtained results,\ndue to the nature of the algorithm and to the possibility of simulating the\nagents' behavior, should describe well also how the algorithm performs in real\nexperiments.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  In this paper, we propose a new continuously learning generative model,\ncalled the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs\nlearns a sequence of tasks from several databases and its architecture consists\nof three components: two identical generators, namely the Teacher and\nAssistant, and one Discriminator. In order to allow for the LT-GANs to learn\nnew concepts without forgetting, we introduce a new lifelong training approach,\nnamely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the\nTeacher and Assistant to alternately teach each other, while learning a new\ndatabase. This training approach favours transferring knowledge from a more\nknowledgeable player to another player which knows less information about a\npreviously given task.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  We consider the task of grasping a target object based on a natural language\ncommand query. Previous work primarily focused on localizing the object given\nthe query, which requires a separate grasp detection module to grasp it. The\ncascaded application of two pipelines incurs errors in overlapping multi-object\ncases due to ambiguity in the individual outputs. This work proposes a model\nnamed Command Grasping Network(CGNet) to directly output command satisficing\ngrasps from RGB image and textual command inputs. A dataset with ground truth\n(image, command, grasps) tuple is generated based on the VMRD dataset to train\nthe proposed network. Experimental results on the generated test set show that\nCGNet outperforms a cascaded object-retrieval and grasp detection baseline by a\nlarge margin. Three physical experiments demonstrate the functionality and\nperformance of CGNet.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  JavaScript implementations are tested for conformance to the ECMAScript\nstandard using a large hand-written test suite. Not only in this a tedious\napproach, it also relies solely on the natural language specification for\ndifferentiating behaviors, while hidden implementation details can also affect\nbehavior and introduce divergences. We propose to generate conformance tests\nthrough dynamic symbolic execution of polyfills, drop-in replacements for newer\nJavaScript language features that are not yet widely supported. We then run\nthese generated tests against multiple implementations of JavaScript, using a\nmajority vote to identify the correct behavior. To facilitate test generation\nfor polyfill code, we introduce a model for structured symbolic inputs that is\nsuited to the dynamic nature of JavaScript. In our evaluation, we found 17\ndivergences in the widely used core-js polyfill and were able to increase\nbranch coverage in interpreter code by up to 15%. Because polyfills are\ntypically written even before standardization, our approach will allow to\nmaintain and extend standardization test suites with reduced effort.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  There is a long-time quest for understanding physical mechanisms of weak\nmagnetic field interaction with biological matter. Two factors impeded the\ndevelopment of such mechanisms: first, a high (room) temperature of a cellular\nenvironment, where a weak, static magnetic field induces a (classically) zero\nequilibrium response. Second, the friction in the cellular environment is\nlarge, preventing a weak field to alter non-equilibrium processes such as a\nfree diffusion of charges. Here we study a class of non-equilibrium steady\nstates of a cellular ion in a confining potential, where the response to a\n(weak, homogeneous, static) magnetic field survives strong friction and thermal\nfluctuations. The magnetic field induces a rotational motion of the ion that\nproceeds with the cyclotron frequency. Such non-equilibrium states are\ngenerated by a white noise acting on the ion additionally to the non-local\n(memory-containing) friction and noise generated by an equilibrium thermal\nbath. The intensity of this white noise can be weak, i.e. much smaller than the\nthermal noise intensity.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Symmetry transformations have proved useful in determining the algebraic\nstructure and internal dynamical properties of physical systems. In the quantum\nRabi model, invariance under parity symmetry transformation has been used to\nobtain exact solutions of the eigenvalue equation and very good approximations\nof the internal dynamics of the interacting atom-light system. In this article,\ntwo symmetry operators, characterized as \"duality\" symmetry operators, have\nbeen introduced which transform the quantum Rabi Hamiltonian into duality\nconjugates. The parity and duality symmetry operators constitute an\nalgebraically closed set of symmetry transformation operators of the quantum\nRabi model. The closed $SU(2)$ Lie algebra provides the standard eigenvalues\nand eigenstates of the parity symmetry operator. It is established that\nJaynes-Cummings and anti-Jaynes-Cummings operators are duality symmetry\nconjugates. Symmetric or antisymmetric linear combinations of the Rabi\nHamiltonian and a corresponding duality conjugate yield the familiar\nspin-dependent force driven bosonic , coupling-only or quantized light mode\nquadrature-driven fermionic Hamiltonian. It is established that the effective\nbosonic, fermionic and coupling-only Hamiltonians are exact, not approximate\nforms of the quantum Rabi Hamiltonian as they have generally been interpreted.\nThe effective bosonic form generates the dynamics of the light mode driven by\nthe atomic spin-dependent force, while the fermionic form generates the\ndynamics of the atomic spin driven by the quantized light mode\nquadrature-dependent force, thus providing a complete picture of the quantum\nRabi dynamics.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  We study the typical behavior of the size of the ratio set $A/A$ for a random\nsubset $A\\subset \\{1,\\dots , n\\}$. For example, we prove that $|A/A|\\sim\n\\frac{2\\text{Li}_2(3/4)}{\\pi^2}n^2 $ for almost all subsets $A \\subset\\{1,\\dots\n,n\\}$. We also prove that the proportion of visible lattice points in the\nlattice $A_1\\times\\cdots \\times A_d$, where $A_i$ is taken at random in $[1,n]$\nwith $\\mathbb P(m\\in A_i)=\\alpha_i$ for any $m\\in [1,n]$, is asymptotic to a\nconstant $\\mu(\\alpha_1,\\dots,\\alpha_d)$ that involves the polylogarithm of\norder $d$.\n\n\n###\n\n", "completion": " 08"}
{"prompt": "  A new algorithm to perform coherent mode decomposition of the undulator\nradiation is proposed. It is based in separating the horizontal and vertical\ndirections, reducing the problem by working with one-dimension wavefronts. The\nvalidity conditions of this approximation are discussed. Simulations require\nlow computer resources, and run interactively in a laptop. We study the\nfocusing with lenses of the radiation emitted by an undulator in a\nfourth-generation storage ring (EBS-ESRF). Results are compared against\nmultiple optics packages implementing a variety of methods for dealing with\npartial coherence: full 2D coherent mode decomposition, Monte-Carlo combination\nof wavefronts from electrons entering the undulator with different initial\nconditions, and hybrid ray-tracing correcting geometrical optics with wave\noptics.\n\n\n###\n\n", "completion": " 09"}
{"prompt": "  There is a general agreement that it is important to consider the practical\nrelevance of an effect in addition to its statistical significance, yet a\nformal definition of practical relevance is still pending and shall be provided\nwithin this paper. It appears that an underlying decision problem,\ncharacterized by actions and a loss function, is required to define the notion\nof practical relevance, rendering it a decision theoretic concept. In the\ncontext of hypothesis-based analyses, the notion of practical relevance relates\nto specifying the hypotheses reasonably, such that the null hypothesis does not\ncontain only a single parameter null value, but also all parameter values that\nare equivalent to the null value on a practical level. In that regard, the\ndefinition of practical relevance is also extended into the context of\nhypotheses. The formal elaborations on the notion of practical relevance within\nthis paper indicate that, typically, a specific decision problem is implicitly\nassumed when dealing with the practical relevance of an effect or some results.\nAs a consequence, involving decision theoretic considerations into a\nstatistical analysis suggests itself by the mere nature of the notion of\npractical relevance.\n\n\n###\n\n", "completion": " 17"}
{"prompt": "  We propose a theoretical model to elucidate intermolecular electrostatic\ninteractions between a virus and a substrate. Our model treats the virus as a\nhomogeneous particle having surface charge and the polymer fiber of the\nrespirator as a charged plane. Electric potentials surrounding the virus and\nfiber are influenced by the surface charge distribution of the virus. We use\nPoisson-Boltzmann equations to calculate electric potentials. Then, Derjaguin's\napproximation and a linear superposition of the potential function are extended\nto determine the electrostatic force. In this work, we apply this model for\ncoronavirus or SARS-CoV-2 case and numerical results quantitatively agree with\nprior simulation. We find that the influence of fiber's potential on the\nsurface charge of the virus is important and is considered in interaction\ncalculations to obtain better accuracy. The electrostatic interaction\nsignificantly decays with increasing separation distance, and this curve\nbecomes steeper when adding more salt. Although the interaction force increases\nwith heating, one can observe the repulsive-attractive transition when the\nenvironment is acidic.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  We consider a diffusion in $\\mathbb{R}^n$ whose coordinates each behave as\none-dimensional Brownian motions, that behave independently when apart, but\nhave a sticky interaction when they meet. The diffusion in $\\mathbb{R}^n$ can\nbe viewed as the $n$-point motion of a stochastic flow of kernels. We derive\nthe Kolmogorov backwards equation and show that for a specific choice of\ninteraction it can be solved exactly with the Bethe ansatz. We then use our\nformulae to study the behaviour of the flow of kernels for the exactly solvable\nchoice of interaction.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  Digital tools play an important role in fighting the current global COVID-19\npandemic. We conducted a representative online study in Germany on a sample of\n599 participants to evaluate the user perception of vaccination certificates.\nWe investigated five different variants of vaccination certificates, based on\ndeployed and planned designs in a between-group design, including paper-based\nand app-based variants. Our main results show that the willingness to use and\nadopt vaccination certificates is generally high. Overall, paper-based\nvaccination certificates were favored over app-based solutions. The willingness\nto use digital apps decreased significantly by a higher disposition to privacy,\nand increased by higher worries about the pandemic and acceptance of the\ncoronavirus vaccination. Vaccination certificates resemble an interesting use\ncase for studying privacy perceptions for health related data. We hope that our\nwork will be able to educate the currently ongoing design of vaccination\ncertificates, will give us deeper insights into privacy of health-related data\nand apps, and prepare us for future potential applications of vaccination\ncertificates and health apps in general.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  Persistent current is a small but perpetual electric current that flows in\nmetallic rings in the absence of any applied source. We compute the persistent\ncurrents of one-dimensional disordered metallic rings of interacting electrons\nin the presence of impurity on lattices up to 8-sites at half-filling and also\naway from half-filling using the Lanczos algorithm. For the case of\nhalf-filling, we observe that both interaction and disorder suppress the\namplitude of the persistent currents by localizing the electrons. However, in\nthe presence of disorder and away from half-filling, the Coulomb interaction is\nobserved to enhance the persistent current. Furthermore, in the half-filled\ncase, there is a transition from metal to insulator as U is increased\nsignificantly. In addition, shifting away from half-filling, the system is\nobserved to remain in the metallic state irrespective of the value of the\nCoulomb repulsion (U). The observations are quite in agreement with the results\nfrom other techniques.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  We classify the maximal algebraic subgroups of Bir(CxPP^1), when C is a\nsmooth projective curve of positive genus.\n\n\n###\n\n", "completion": " 09"}
{"prompt": "  The brain is in a state of perpetual reverberant neural activity, even in the\nabsence of specific tasks or stimuli. Shedding light on the origin and\nfunctional significance of such a dynamical state is essential to understanding\nhow the brain transmits, processes, and stores information. An inspiring,\nalbeit controversial, conjecture proposes that some statistical characteristics\nof empirically observed neuronal activity can be understood by assuming that\nbrain networks operate in a dynamical regime near the edge of a phase\ntransition. Moreover, the resulting critical behavior, with its concomitant\nscale invariance, is assumed to carry crucial functional advantages. Here, we\npresent a data-driven analysis based on simultaneous high-throughput recordings\nof the activity of thousands of individual neurons in various regions of the\nmouse brain. To analyze these data, we synergistically combine cutting-edge\nmethods for the study of brain activity (such as a phenomenological\nrenormalization group approach and techniques that infer the general dynamical\nstate of a neural population), while designing complementary tools. This\nstrategy allows us to uncover strong signatures of scale invariance that is\n\"quasi-universal\" across brain regions and reveal that all these areas operate,\nto a greater or lesser extent, near the edge of instability. Furthermore, this\nframework allows us to distinguish between quasi-universal background activity\nand non-universal input-related activity. Taken together, this study provides\nstrong evidence that brain networks actually operate in a critical regime\nwhich, among other functional advantages, provides them with a scale-invariant\nsubstrate of activity covariances that can sustain optimal input\nrepresentations.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We show the existence of self-dual (topological) solitons in a gauged version\nof the baby Skyrme model in which the Born-Infeld term governs the gauge field\ndynamics. The successful implementation of the Bogomol'nyi-Prasad-Sommerfield\nformalism provides a lower bound for the energy and the respective self-dual\nequations whose solutions are the solitons saturating such a limit. The energy\nlower bound (Bogomol'nyi bound) is proportional to the topological charge of\nthe Skyrme field and therefore quantized. In contrast, the total magnetic flux\nis a nonquantized quantity. Furthermore, the model supports three types of\nself-dual solitons profiles: the first describes compacton solitons, the second\nfollows a Gaussian decay law, and the third portrays a power-law decay.\nFinally, we perform numerical solutions of the self-dual equations and depicted\nthe soliton profiles for different values of the parameters controlling the\nnonlinearity of the model.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  While learning-based control techniques often outperform classical controller\ndesigns, safety requirements limit the acceptance of such methods in many\napplications. Recent developments address this issue through so-called\npredictive safety filters, which assess if a proposed learning-based control\ninput can lead to constraint violations and modifies it if necessary to ensure\nsafety for all future time steps. The theoretical guarantees of such predictive\nsafety filters rely on the model assumptions and minor deviations can lead to\nfailure of the filter putting the system at risk. This paper introduces an\nauxiliary soft-constrained predictive control problem that is always feasible\nat each time step and asymptotically stabilizes the feasible set of the\noriginal safety filter, thereby providing a recovery mechanism in\nsafety-critical situations. This is achieved by a simple constraint tightening\nin combination with a terminal control barrier function. By extending\ndiscrete-time control barrier function theory, we establish that the proposed\nauxiliary problem provides a `predictive' control barrier function. The\nresulting algorithm is demonstrated using numerical examples.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Ultra-fast transmission electron microscopy (UTEM) combines sub-picosecond\ntime-resolution with the versatility of TEM spectroscopies. It allows one to\nstudy the dynamics of materials properties combining complementary techniques.\nHowever, until now, time-resolved cathodoluminescence, which is expected to\ngive access to the optical properties dynamics, was still unavailable in a\nUTEM. In this paper, we report time-resolved cathodoluminescence measurements\nin an ultrafast transmission electron microscope. We measured lifetime maps,\nwith a 12 nm spatial resolution and sub-nanoseconds resolution, of\nnano-diamonds with a high density of NV center. This study paves the way to new\napplications of UTEM and to correlative studies of optically active\nnanostructures.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  Sudoku is a famous logic puzzle where the player has to fill a number between\n1 and 9 into each empty cell of a $9 \\times 9$ grid such that every number\nappears exactly once in each row, each column, and each $3 \\times 3$ block. In\n2020, Sasaki et al. developed a physical card-based protocol of zero-knowledge\nproof (ZKP) for Sudoku, which enables a prover to convince a verifier that\nhe/she knows a solution of the puzzle without revealing it. Their protocol uses\n90 cards, but requires nine identical copies of some cards, which cannot be\nfound in a standard deck of playing cards (consisting of 52 different cards and\ntwo jokers). Hence, nine identical standard decks are required to perform that\nprotocol, making the protocol not very practical. In this paper, we propose a\nnew ZKP protocol for Sudoku that can be performed using only two standard decks\nof playing cards, regardless of whether the two decks are identical or\ndifferent. In general, we also develop the first ZKP protocol for a generalized\n$n \\times n$ Sudoku that can be performed using a deck of all different cards.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Let $S$ be a Shimura variety and let $h$ be a Weil height function on $S$. We\nconjecture that the heights of special points in $S$ are discriminant\nnegligible. Assuming this conjecture to be true, we prove that the sizes of the\nGalois orbits of special points grow as a fixed power of their discriminant (an\ninvariant we will define in the text). In the case of Shimura varieties of\nabelian type, the height bound holds by the recently proved averaged Colmez\nformula, and our theorem gives a new proof of Tsimerman's Galois lower bound in\nthis case. The main novelty is that our approach avoids the use of\nMasser-W\\\"ustholz isogeny estimates, replacing them by a point-counting\nargument, and establishes lower bounds for Galois orbits conditional on height\nbounds for \\emph{arbitrary} Shimura varieties. In particular, following the\nPila-Zannier strategy (and Gao's work in the mixed case) this implies that the\nAndre-Oort conjecture for an arbitrary (mixed) Shimura variety follows from the\ncorresponding conjecture on heights of special points.\n\n\n###\n\n", "completion": " 11"}
{"prompt": "  One of the most fascinating aspects of quantum fields in curved spacetime is\nthe Unruh effect. The direct experimental detection of Unruh temperature has\nremained an elusive challenge up to now. Gradient optical waveguides\nmanipulating the dispersion of photons are assumed to realize the great\nacceleration of effective particles, leading to a high effective Unruh\ntemperature. However, experimentally achieving this optical waveguide has not\nyet been reported. In this work, we exploit a tapered fiber to simulate the\naccelerated motion of effective particles and obtain an effective Unruh\ntemperature. When light propagating in a tapered fiber is affected by the\nexternal high refractive index medium, a leaky phenomenon akin to\nbremsstrahlung will be observed, and the pattern of leaky radiation is\ndependent on the acceleration of photons. During the experiments, different\naccelerations corresponding to different Unruh temperatures are achieved by\ncontrolling the shape of the tapered waveguide.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  We study the status of fair sampling on Noisy Intermediate Scale Quantum\n(NISQ) devices, in particular the IBM Q family of backends. Using the recently\nintroduced Grover Mixer-QAOA algorithm for discrete optimization, we generate\nfair sampling circuits to solve six problems of varying difficulty, each with\nseveral optimal solutions, which we then run on twenty backends across the IBM\nQ system. For a given circuit evaluated on a specific set of qubits, we\nevaluate: how frequently the qubits return an optimal solution to the problem,\nthe fairness with which the qubits sample from all optimal solutions, and the\nreported hardware error rate of the qubits. To quantify fairness, we define a\nnovel metric based on Pearson's $\\chi^2$ test. We find that fairness is\nrelatively high for circuits with small and large error rates, but drops for\ncircuits with medium error rates. This indicates that structured errors\ndominate in this regime, while unstructured errors, which are random and thus\ninherently fair, dominate in noisier qubits and longer circuits. Our results\nshow that fairness can be a powerful tool for understanding the intricate web\nof errors affecting current NISQ hardware.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  The charmoniumlike state $Y(4260)$ is described as predominantly a $D_1\n\\bar{D}$ molecule in a coupled-channel quark model\n[Phys.\\,Rev.\\,D\\,\\textbf{96},\\,114022\\,(2017)]. The heavy quark spin symmetry\n(HQSS) thus implies the possible emergence of its heavy quark spin partners\nwith molecular configuration as $D_1 \\bar{D}^*$ and $D_2^* \\bar{D}^*$ below\nthese charmed mesons' thresholds. We analyze the probabilities of various\nintermediate charmed meson loops for $J^{PC}=1^{--}$ exotic state $Y(4360)$ and\nfind that the channel $D_1 \\bar{D}^*$ couples more strongly around its mass\nregime, and the coupling behavior remains the same even if the mass of\n$Y(4360)$ is pushed closer to $D_1 \\bar{D}^*$ threshold. This enlightens that\nthe most favorable molecular scenario for the $Y(4360)$ could be $D_1\n\\bar{D}^*$, and hence it can be interpreted as HQSS partner of the $Y(4260)$.\nWe also find the strong coupling behavior of $D_2^* \\bar{D}^*$ channel with the\n$\\psi(4415)$, which makes it a good candidate for a dominant $D_2^* \\bar{D}^*$\nmolecule. We discuss the important decay patterns of these resonances to\ndisentangle their long- and short-distance structures.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Plasticity in hexagonal close-packed zirconium is controlled by screw\ndislocations which easily glide in the prismatic planes where they are\ndissociated. At high enough temperatures, these dislocations can deviate out of\nthe prism planes to also glide in the first order pyramidal and basal planes.\nTo get a better understanding of these secondary slip systems, we have\nperformed molecular dynamics (MD) simulations of a screw dislocation gliding in\na basal plane. The gliding dislocation remains dissociated in the prism plane\nwhere it performs a random motion and occasionally cross-slips out of its habit\nplane by the nucleation and propagation of a kink-pair. Deviation planes are\nalways pyramidal, with an equal probability to cross-slip in the two pyramidal\nplanes on both sides of the basal plane, thus leading to basal slip on average.\nBasal slip appears therefore as a combination of prismatic and pyramidal slip\nin the high stress regime explored in MD simulations. This is confirmed by\nnudged elastic band (NEB) calculations. But NEB calculations also reveal a\nchange of glide mechanism for a decreasing applied stress. At low stress, kinks\ndo not lie anymore in the pyramidal planes. They are now spread in the basal\nplanes, thus fully compatible with a motion of the screw dislocation confined\nto the basal plane as seen in experiments. Basal slip, which is in competition\nwith pyramidal slip, appears therefore favoured at low stress in pure\nzirconium.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  Spectroscopic signatures associated with symmetric Lorentzian and asymmetric\nFano line shapes are ubiquitous. Distinct features of Fano resonances in\ncontrast with conventional symmetric resonances have found several applications\nin photonics such as optical switching, sensing, lasing, and nonlinear and\nslow-light devices. Therefore, it is important to have control over the\ngeneration of these resonances. In this work, we show through ab initio\nsimulations of coupled light-matter systems that Fano interference phenomena\ncan be realized in a multimode photonic environment by strong coupling to the\nelectromagnetic continuum. Specifically, we show that by effectively enhancing\nthe light-matter coupling strength to the photon continuum in an experimentally\nfeasible way, we can achieve a transition from Lorentzian to Fano lines shapes\nfor both electronic and polaritonic excitations. An important outcome of\nswitching between these spectral signatures is the possibility to control the\nPurcell enhancement of spontaneous emission alongside electromagnetically\ninduced transparency which is a special case of Fano resonances. Switching from\nFano back to a Lorentzian profile can be achieved by physically reducing the\ncoupling strength to the continuum of modes. Our results hold potential for\nrealizing tunable Fano resonances of molecules and materials interacting with\nthe electromagnetic continuum within multimode photonic environments.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  General reduction of the elliptic hypergeometric equation to the level of\ncomplex hypergeometric functions is described. The derived equation is\ngeneralized to the Hamiltonian eigenvalue problem for new rational integrable\n$N$-body systems emerging from particular degenerations of the elliptic\nRuijsenaars and van Diejen models.\n\n\n###\n\n", "completion": " 01"}
{"prompt": "  Probabilistic circuits (PCs) are a family of generative models which allows\nfor the computation of exact likelihoods and marginals of its probability\ndistributions. PCs are both expressive and tractable, and serve as popular\nchoices for discrete density estimation tasks. However, large PCs are\nsusceptible to overfitting, and only a few regularization strategies (e.g.,\ndropout, weight-decay) have been explored. We propose HyperSPNs: a new paradigm\nof generating the mixture weights of large PCs using a small-scale neural\nnetwork. Our framework can be viewed as a soft weight-sharing strategy, which\ncombines the greater expressiveness of large models with the better\ngeneralization and memory-footprint properties of small models. We show the\nmerits of our regularization strategy on two state-of-the-art PC families\nintroduced in recent literature -- RAT-SPNs and EiNETs -- and demonstrate\ngeneralization improvements in both models on a suite of density estimation\nbenchmarks in both discrete and continuous domains.\n\n\n###\n\n", "completion": " 19"}
{"prompt": "  The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  For a linear code $C$ of length $n$ with dimension $k$ and minimum distance\n$d$, it is desirable that the quantity $kd/n$ is large. Given an arbitrary\nfield $\\mathbb{F}$, we introduce a novel, but elementary, construction that\nproduces a recursively defined sequence of $\\mathbb{F}$-linear codes $C_1,C_2,\nC_3, \\dots$ with parameters $[n_i, k_i, d_i]$ such that $k_id_i/n_i$ grows\nquickly in the sense that $k_id_i/n_i>\\sqrt{k_i}-1>2i-1$. Another example of\nquick growth comes from a certain subsequence of Reed-Muller codes. Here the\nfield is $\\mathbb{F}=\\mathbb{F}_2$ and $k_i d_i/n_i$ is asymptotic to\n$3n_i^{c}/\\sqrt{\\pi\\log_2(n_i)}$ where $c=\\log_2(3/2)\\approx 0.585$.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  A tapered floating point encoding is proposed which uses the redundant signed\nradix 2 system and is based on the canonical recoding. By making use of ternary\ntechnology, the encoding has a dynamic range exceeding that of the\nrecently-proposed Posit number system and the IEEE 754-1985 Standard for\nFloating Point Arithmetic (IEEE-754-1985), and precision equal to or better\nthan that of the IEEE-754-1985 system and the recently proposed Posit system\nwhen equal input sizes are compared. In addition, the encoding is capable of\nsupporting several proposed extensions, including extensions to integers,\nboolean values, complex numbers, higher number systems, low-dimensional\nvectors, and system artifacts such as machine instructions. A detailed analytic\ncomparison is provided between the proposed encoding, the IEEE-754-1985 system,\nand the recently proposed Posit number system.\n\n\n###\n\n", "completion": " 12"}
{"prompt": "  In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry\nwith the popular Affine-Invariant (AI) geometry for Riemannian optimization on\nthe symmetric positive definite (SPD) matrix manifold. Our study begins with an\nobservation that the BW metric has a linear dependence on SPD matrices in\ncontrast to the quadratic dependence of the AI metric. We build on this to show\nthat the BW metric is a more suitable and robust choice for several Riemannian\noptimization problems over ill-conditioned SPD matrices. We show that the BW\ngeometry has a non-negative curvature, which further improves convergence rates\nof algorithms over the non-positively curved AI geometry. Finally, we verify\nthat several popular cost functions, which are known to be geodesic convex\nunder the AI geometry, are also geodesic convex under the BW geometry.\nExtensive experiments on various applications support our findings.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Leading methods in the domain of action recognition try to distill\ninformation from both the spatial and temporal dimensions of an input video.\nMethods that reach State of the Art (SotA) accuracy, usually make use of 3D\nconvolution layers as a way to abstract the temporal information from video\nframes. The use of such convolutions requires sampling short clips from the\ninput video, where each clip is a collection of closely sampled frames. Since\neach short clip covers a small fraction of an input video, multiple clips are\nsampled at inference in order to cover the whole temporal length of the video.\nThis leads to increased computational load and is impractical for real-world\napplications. We address the computational bottleneck by significantly reducing\nthe number of frames required for inference. Our approach relies on a temporal\ntransformer that applies global attention over video frames, and thus better\nexploits the salient information in each frame. Therefore our approach is very\ninput efficient, and can achieve SotA results (on Kinetics dataset) with a\nfraction of the data (frames per video), computation and latency. Specifically\non Kinetics-400, we reach $80.5$ top-1 accuracy with $\\times 30$ less frames\nper video, and $\\times 40$ faster inference than the current leading method.\nCode is available at: https://github.com/Alibaba-MIIL/STAM\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Additive manufacturing, called 3D printing, starts to play an unprecedented\nrole in developing many applications in industrial or personalized products.\nThe conductive composite structures require additional treatment to achieve an\nelectroactive surface useful for electrochemical devices. In this paper, the\nsurfaces of carbon black/poly(lactic acid) CB-PLA printouts were activated by\nelectrolysis or enzymatic digestion with proteinase K, or a simultaneous\ncombination of both. Proposed modification protocols allowed for tailoring\nelectrochemically active surface area and electron transfer kinetics determined\nby electrochemical techniques (CV, EIS) with [Fe(CN)6]4-/3- redox probe. The\nX-ray photon spectroscopy and SEM imaging were applied to determine the\ndelivered surface chemistry. The CB-PLA hydrolysis in alkaline conditions and\nunder anodic polarization greatly impacts the charge transfer kinetics. The\nenzymatic hydrolysis of PLA with proteinase K has led to highly efficient\nresults yet requiring an unsatisfactory prolonged activation duration of 72 h,\nefficiently reduced by the electrolysis carried out in the presence of the\nenzyme. Our studies hint that the activation protocol originates from surface\nelectropolymerization rather than synergistic interaction between electrolysis\nand enzymatic hydrolysis. The detailed mechanism of CB-PLA hydrolysis supported\nby electrolysis has been elaborated since it pawed a new route towards a\ntime-efficient and environmentally-friendly activation procedure.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Assist-as-needed (AAN) control aims at promoting therapeutic outcomes in\nrobot-assisted rehabilitation by encouraging patients' active participation.\nImpedance control is used by most AAN controllers to create a compliant force\nfield around a target motion to ensure tracking accuracy while allowing\nmoderate kinematic errors. However, since the parameters governing the shape of\nthe force field are often tuned manually or adapted online based on simplistic\nassumptions about subjects' learning abilities, the effectiveness of\nconventional AAN controllers may be limited. In this work, we propose a novel\nadaptive AAN controller that is capable of autonomously reshaping the force\nfield in a phase-dependent manner according to each individual's motor\nabilities and task requirements. The proposed controller consists of a modified\nPolicy Improvement with Path Integral algorithm, a model-free, sampling-based\nreinforcement learning method that learns a subject-specific impedance\nlandscape in real-time, and a hierarchical policy parameter evaluation\nstructure that embeds the AAN paradigm by specifying performance-driven\nlearning goals. The adaptability of the proposed control strategy to subjects'\nmotor responses and its ability to promote short-term motor adaptations are\nexperimentally validated through treadmill training sessions with able-bodied\nsubjects who learned altered gait patterns with the assistance of a powered\nankle-foot orthosis.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  While counterfactual examples are useful for analysis and training of NLP\nmodels, current generation methods either rely on manual labor to create very\nfew counterfactuals, or only instantiate limited types of perturbations such as\nparaphrases or word substitutions. We present Polyjuice, a general-purpose\ncounterfactual generator that allows for control over perturbation types and\nlocations, trained by finetuning GPT-2 on multiple datasets of paired\nsentences. We show that Polyjuice produces diverse sets of realistic\ncounterfactuals, which in turn are useful in various distinct applications:\nimproving training and evaluation on three different tasks (with around 70%\nless annotation effort than manual generation), augmenting state-of-the-art\nexplanation techniques, and supporting systematic counterfactual error analysis\nby revealing behaviors easily missed by human experts.\n\n\n###\n\n", "completion": " 20"}
{"prompt": "  In many life science experiments or medical studies, subjects are repeatedly\nobserved and measurements are collected in factorial designs with multivariate\ndata. The analysis of such multivariate data is typically based on multivariate\nanalysis of variance (MANOVA) or mixed models, requiring complete data, and\ncertain assumption on the underlying parametric distribution such as continuity\nor a specific covariance structure, e.g., compound symmetry. However, these\nmethods are usually not applicable when discrete data or even ordered\ncategorical data are present. In such cases, nonparametric rank-based methods\nthat do not require stringent distributional assumptions are the preferred\nchoice. However, in the multivariate case, most rank-based approaches have only\nbeen developed for complete observations. It is the aim of this work is to\ndevelop asymptotic correct procedures that are capable of handling missing\nvalues, allowing for singular covariance matrices and are applicable for\nordinal or ordered categorical data. This is achieved by applying a wild\nbootstrap procedure in combination with quadratic form-type test statistics.\nBeyond proving their asymptotic correctness, extensive simulation studies\nvalidate their applicability for small samples. Finally, two real data examples\nare analyzed.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  We present analysis of the spatial density structure for the outer disk from\n8$-$14 \\,kpc with the LAMOST DR5 13534 OB-type stars and observe similar\nflaring on north and south sides of the disk implying that the flaring\nstructure is symmetrical about the Galactic plane, for which the scale height\nat different Galactocentric distance is from 0.14 to 0.5 \\,kpc. By using the\naverage slope to characterize the flaring strength we find that the thickness\nof the OB stellar disk is similar but flaring is slightly stronger compared to\nthe thin disk as traced by red giant branch stars, possibly implying that\nsecular evolution is not the main contributor to the flaring but perturbation\nscenarios such as interactions with passing dwarf galaxies should be more\npossible. When comparing the scale height of OB stellar disk of the north and\nsouth sides with the gas disk, the former one is slightly thicker than the\nlater one by $\\approx$ 33 and 9 \\,pc, meaning that one could tentatively use\nyoung OB-type stars to trace the gas properties. Meanwhile, we unravel that the\nradial scale length of the young OB stellar disk is 1.17 $\\pm$ 0.05 \\,kpc,\nwhich is shorter than that of the gas disk, confirming that the gas disk is\nmore extended than stellar disk. What is more, by considering the mid-plane\ndisplacements ($Z_{0}$) in our density model we find that almost all of $Z_{0}$\nare within 100 \\,pc with the increasing trend as Galactocentric distance\nincreases.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  This article studies point-vortex models for the Euler and surface\nquasi-geostrophic equations. In the case of an inviscid fluid with planar\nmotion, the point-vortex model gives account of dynamics where the vorticity\nprofile is sharply concentrated around some points and approximated by Dirac\nmasses. This article contains three main results with several links between\neach other. In the first part, we provide two uniform bounds on the\ntrajectories for Euler and quasi-geostrophic vortices related to the\nnon-neutral cluster hypothesis. In a second part we focus on the Euler\npoint-vortex model and under the non-neutral cluster hypothesis we prove a\nconvergence result. The third part is devoted to the generalization of a\nclassical result by Marchioro and Pulvirenti concerning the improbability of\ncollapses and the extension of this result to the quasi-geostrophic case.\n\n\n###\n\n", "completion": " 16"}
{"prompt": "  Longevity and safety of lithium-ion batteries are facilitated by efficient\nmonitoring and adjustment of the battery operating conditions. Hence, it is\ncrucial to implement fast and accurate algorithms for State of Health (SoH)\nmonitoring on the Battery Management System. The task is challenging due to the\ncomplexity and multitude of the factors contributing to the battery\ndegradation, especially because the different degradation processes occur at\nvarious timescales and their interactions play an important role. Data-driven\nmethods bypass this issue by approximating the complex processes with\nstatistical or machine learning models. This paper proposes a data-driven\napproach which is understudied in the context of battery degradation, despite\nits simplicity and ease of computation: the Multivariable Fractional Polynomial\n(MFP) regression. Models are trained from historical data of one exhausted cell\nand used to predict the SoH of other cells. The data are characterised by\nvarying loads simulating dynamic operating conditions. Two hypothetical\nscenarios are considered: one assumes that a recent capacity measurement is\nknown, the other is based only on the nominal capacity. It was shown that the\ndegradation behaviour of the batteries under examination is influenced by their\nhistorical data, as supported by the low prediction errors achieved (root mean\nsquared errors from 1.2% to 7.22% when considering data up to the battery End\nof Life). Moreover, we offer a multi-factor perspective where the degree of\nimpact of each different factor is analysed. Finally, we compare with a Long\nShort-Term Memory Neural Network and other works from the literature on the\nsame dataset. We conclude that the MFP regression is effective and competitive\nwith contemporary works, and provides several additional advantages e.g. in\nterms of interpretability, generalisability, and implementability.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Aspect sentiment triplet extraction (ASTE), which aims to identify aspects\nfrom review sentences along with their corresponding opinion expressions and\nsentiments, is an emerging task in fine-grained opinion mining. Since ASTE\nconsists of multiple subtasks, including opinion entity extraction, relation\ndetection, and sentiment classification, it is critical and challenging to\nappropriately capture and utilize the associations among them. In this paper,\nwe transform ASTE task into a multi-turn machine reading comprehension (MTMRC)\ntask and propose a bidirectional MRC (BMRC) framework to address this\nchallenge. Specifically, we devise three types of queries, including\nnon-restrictive extraction queries, restrictive extraction queries and\nsentiment classification queries, to build the associations among different\nsubtasks. Furthermore, considering that an aspect sentiment triplet can derive\nfrom either an aspect or an opinion expression, we design a bidirectional MRC\nstructure. One direction sequentially recognizes aspects, opinion expressions,\nand sentiments to obtain triplets, while the other direction identifies opinion\nexpressions first, then aspects, and at last sentiments. By making the two\ndirections complement each other, our framework can identify triplets more\ncomprehensively. To verify the effectiveness of our approach, we conduct\nextensive experiments on four benchmark datasets. The experimental results\ndemonstrate that BMRC achieves state-of-the-art performances.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Advances in biosignal signal processing and machine learning, in particular\nDeep Neural Networks (DNNs), have paved the way for the development of\ninnovative Human-Machine Interfaces for decoding the human intent and\ncontrolling artificial limbs. DNN models have shown promising results with\nrespect to other algorithms for decoding muscle electrical activity, especially\nfor recognition of hand gestures. Such data-driven models, however, have been\nchallenged by their need for a large number of trainable parameters and their\nstructural complexity. Here we propose the novel Temporal Convolutions-based\nHand Gesture Recognition architecture (TC-HGR) to reduce this computational\nburden. With this approach, we classified 17 hand gestures via surface\nElectromyogram (sEMG) signals by the adoption of attention mechanisms and\ntemporal convolutions. The proposed method led to 81.65% and 80.72%\nclassification accuracy for window sizes of 300ms and 200ms, respectively. The\nnumber of parameters to train the proposed TC-HGR architecture is 11.9 times\nless than that of its state-of-the-art counterpart.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Perovskite photovoltaics (PV) have achieved rapid development in the past\ndecade in terms of power conversion efficiency of small-area lab-scale devices;\nhowever, successful commercialization still requires further development of\nlow-cost, scalable, and high-throughput manufacturing techniques. One of the\ncritical challenges of developing a new fabrication technique is the\nhigh-dimensional parameter space for optimization, but machine learning (ML)\ncan readily be used to accelerate perovskite PV scaling. Herein, we present an\nML-guided framework of sequential learning for manufacturing process\noptimization. We apply our methodology to the Rapid Spray Plasma Processing\n(RSPP) technique for perovskite thin films in ambient conditions. With a\nlimited experimental budget of screening 100 process conditions, we\ndemonstrated an efficiency improvement to 18.5% as the best-in-our-lab device\nfabricated by RSPP, and we also experimentally found 10 unique process\nconditions to produce the top-performing devices of more than 17% efficiency,\nwhich is 5 times higher rate of success than the control experiments with\npseudo-random Latin hypercube sampling. Our model is enabled by three\ninnovations: (a) flexible knowledge transfer between experimental processes by\nincorporating data from prior experimental data as a probabilistic constraint;\n(b) incorporation of both subjective human observations and ML insights when\nselecting next experiments; (c) adaptive strategy of locating the region of\ninterest using Bayesian optimization first, and then conducting local\nexploration for high-efficiency devices. Furthermore, in virtual benchmarking,\nour framework achieves faster improvements with limited experimental budgets\nthan traditional design-of-experiments methods (e.g., one-variable-at-a-time\nsampling).\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Object Skeletonization is the process of extracting skeletal, line-like\nrepresentations of shapes. It provides a very useful tool for geometric shape\nunderstanding and minimal shape representation. It also has a wide variety of\napplications, most notably in anatomical research and activity detection.\nSeveral mathematical algorithmic approaches have been developed to solve this\nproblem, and some of them have been proven quite robust. However, a lesser\namount of attention has been invested into deep learning solutions for it. In\nthis paper, we use a 2-stage variant of the famous U-Net architecture to split\nthe problem space into two sub-problems: shape minimization and corrective\nskeleton thinning. Our model produces results that are visually much better\nthan the baseline SkelNetOn model. We propose a new metric, M-CCORR, based on\nnormalized correlation coefficients as an alternative to F1 for this challenge\nas it solves the problem of class imbalance, managing to recognize skeleton\nsimilarity without suffering from F1's over-sensitivity to pixel-shifts.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  Detection and recognition of scene texts of arbitrary shapes remain a grand\nchallenge due to the super-rich text shape variation in text line orientations,\nlengths, curvatures, etc. This paper presents a mask-guided multi-task network\nthat detects and rectifies scene texts of arbitrary shapes reliably. Three\ntypes of keypoints are detected which specify the centre line and so the shape\nof text instances accurately. In addition, four types of keypoint links are\ndetected of which the horizontal links associate the detected keypoints of each\ntext instance and the vertical links predict a pair of landmark points (for\neach keypoint) along the upper and lower text boundary, respectively. Scene\ntexts can be located and rectified by linking up the associated landmark points\n(giving localization polygon boxes) and transforming the polygon boxes via thin\nplate spline, respectively. Extensive experiments over several public datasets\nshow that the use of text keypoints is tolerant to the variation in text\norientations, lengths, and curvatures, and it achieves superior scene text\ndetection and rectification performance as compared with state-of-the-art\nmethods.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  In this paper three heuristic algorithms using the Divide-and-Conquer\nparadigm are developed and assessed for three integer optimizations problems:\nMultidimensional Knapsack Problem (d-KP), Bin Packing Problem (BPP) and\nTravelling Salesman Problem (TSP). For each case, the algorithm is introduced,\ntogether with the design of numerical experiments, in order to empirically\nestablish its performance from both points of view: its computational time and\nits numerical accuracy.\n\n\n###\n\n", "completion": " 13"}
{"prompt": "  We consider two kinds of superpositions of squeezed states of light. In the\ncase of superpositions of first kind, the squeezing and all higher order\nsqueezing vanishes. However, in the case of the second kind, it is possible to\nachieve a maximum amount of squeezing by adjusting the parameters in the\nsuperposition. The emergence and vanishing of squeezing for the superposition\nstates are explained on the basis of expectation values of the energy density.\nWe show that expectation values of energy density of quantum states which show\nno squeezing will be always positive and that of squeezed states will be\nnegative for some values of spacetime-dependent phase.\n\n\n###\n\n", "completion": " 07"}
{"prompt": "  Given any $d$-dimensional Lipschitz Riemannian manifold $(M,g)$ with heat\nkernel $\\mathsf{p}$, we establish uniform upper bounds on $\\mathsf{p}$ which\ncan always be decoupled in space and time. More precisely, we prove the\nexistence of a constant $C>0$ and a bounded Lipschitz function $R\\colon M \\to\n(0,\\infty)$ such that for every $x\\in M$ and every $t>0$, \\begin{align*}\n\\sup_{y\\in M} \\mathsf{p}(t,x,y) \\leq C\\min\\{t, R^2(x)\\}^{-d/2}. \\end{align*}\nThis allows us to identify suitable weighted Lebesgue spaces w.r.t. the given\nvolume measure as subsets of the Kato class induced by $(M,g)$. In the case\n$\\partial M \\neq \\emptyset$, we also provide an analogous inclusion for\nLebesgue spaces w.r.t. the surface measure on $\\partial M$.\n  We use these insights to give sufficient conditions for a possibly\nnoncomplete Lipschitz Riemannian manifold to be tamed, i.e. to admit a\nmeasure-valued lower bound on the Ricci curvature, formulated in a synthetic\nsense.\n\n\n###\n\n", "completion": " 18"}
{"prompt": "  Gamma-ray bursts (GRBs), as a possible probe to extend the Hubble diagram to\nhigh redshifts, have attracted much attention recently. In this paper, we\nselect two samples of GRBs that have a plateau phase in X-ray afterglow. One is\nshort GRBs with plateau phases dominated by magnetic dipole (MD) radiations.\nThe other is long GRBs with gravitational-wave (GW) dominated plateau phases.\nThese GRBs can be well standardized using the correlation between the plateau\nluminosity $L_0$ and the end time of plateau $t_b$. The so-called circularity\nproblem is mitigated by using the observational Hubble parameter data and\nGaussian process method. The calibrated \\ltb ~correlations are also used to\nconstrain $\\Lambda$CDM and $w(z)$ = $w_{0}$ models. Combining the MD-LGRBs\nsample from Wang et al. (2021) and the MD-SGRBs sample, we find $\\Omega_{m} =\n0.33_{-0.09}^{+0.06}$ and $\\Omega_{\\Lambda}$ = $1.06_{-0.34}^{+0.15}$ excluding\nsystematic uncertainties in the nonflat $\\Lambda$CDM model. Adding type Ia\nsupernovae from Pantheon sample, the best-fitting results are $w_{0}$ =\n$-1.11_{-0.15}^{+0.11}$ and $\\Omega_{m}$ = $0.34_{-0.04}^{+0.05}$ in the\n$w=w_0$ model. These results are in agreement with the $\\Lambda$CDM model. Our\nresult supports that selection of GRBs from the same physical mechanism is\ncrucial for cosmological purposes.\n\n\n###\n\n", "completion": " 21"}
{"prompt": "  The amount of scholarly data has been increasing dramatically over the last\nyears. For newcomers to a particular science domain (e.g., IR, physics, NLP) it\nis often difficult to spot larger trends and to position the latest research in\nthe context of prior scientific achievements and breakthroughs. Similarly,\nresearchers in the history of science are interested in tools that allow them\nto analyze and visualize changes in particular scientific domains. Temporal\nsummarization and related methods should be then useful for making sense of\nlarge volumes of scientific discourse data aggregated over time. We demonstrate\na novel approach to analyze the collections of research papers published over\nlonger time periods to provide a high-level overview of important semantic\nchanges that occurred over the progress of time. Our approach is based on\ncomparing word semantic representations over time and aims to support users in\na better understanding of large domain-focused archives of scholarly\npublications. As an example dataset we use the ACL Anthology Reference Corpus\nthat spans from 1979 to 2015 and contains 22,878 scholarly articles.\n\n\n###\n\n", "completion": " 15"}
{"prompt": "  Stellar activity due to different processes (magnetic activity, photospheric\nflows) affects the measurement of radial velocities (RV). Radial velocities\nhave been widely used to detect exoplanets, although the stellar signal\nsignificantly impacts the detection and characterisation performance,\nespecially for low mass planets. On the other hand, RV time series are also\nvery rich in information on stellar processes. In this lecture, I review the\ncontext of RV observations, describe how radial velocities are measured, and\nthe properties of typical observations. I present the challenges represented by\nstellar activity for exoplanet studies, and describe the processes at play.\nFinally, I review the approaches which have been developed, including\nobservations and simulations, as well as solar and stellar comparisons.\n\n\n###\n\n", "completion": " 14"}
{"prompt": "  We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.\n\n\n###\n\n", "completion": " 22"}
{"prompt": "  A result due to Williams, Stampfli and Fillmore shows that an essential\nisometry $T$ on a Hilbert space $\\mathcal{H}$ is a compact perturbation of an\nisometry if and only if ind$(T)\\le 0$. A recent result of S. Chavan yields an\nanalogous characterization of essential spherical isometries\n$T=(T_1,\\dots,T_n)\\in\\mathcal{B}(\\mathcal{H})^n$ with\ndim($\\bigcap_{i=1}^n\\ker(T_i))\\le$ dim$(\\bigcap_{i=1}^n\\ker(T_i^*))$. In the\npresent note we show that in dimension $n>1$ the result of Chavan holds without\nany condition on the dimensions of the joint kernels of $T$ and $T^*$.\n\n\n###\n\n", "completion": " 13"}
